Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2866c33f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f28b18ec170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f3380462490>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f33cb0b6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fea7adf2f90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7feac5aac170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2d7798bbd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2dc2646170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f016b16f310>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f01adb33170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f48e897f750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4932d68170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f41ca9f31d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f420bd6b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f10b1b70890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f10fc278170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.684684693813324
                                                              len(train_dataloader) 30037
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 0.0
valid accuracy: 0.5669291019439697
len(train_dataloader) 30037
Training:   0%|          | 0/4381 [00:00<00:00, 11554.56it/s]Epoch 1:   0%|          | 0/4381 [00:00<00:00, 4424.37it/s]  validation_epoch_end
graph acc: 0.0
valid accuracy: 0.5229357481002808
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.6779661178588867
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.5978261232376099
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.6439394354820251
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.5284091234207153
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.5899280905723572
len(train_dataloader) 30037
Epoch 1:   0%|          | 10/4381 [00:22<2:30:50,  2.07s/it]Epoch 1:   0%|          | 10/4381 [00:22<2:30:50,  2.07s/it, loss=9.38, v_num=641]Epoch 1:   0%|          | 20/4381 [00:39<2:17:02,  1.89s/it, loss=9.38, v_num=641]Epoch 1:   0%|          | 20/4381 [00:39<2:17:02,  1.89s/it, loss=9.34, v_num=641]Epoch 1:   1%|          | 30/4381 [00:56<2:12:51,  1.83s/it, loss=9.34, v_num=641]Epoch 1:   1%|          | 30/4381 [00:56<2:12:53,  1.83s/it, loss=9.29, v_num=641]Epoch 1:   1%|          | 40/4381 [01:13<2:09:15,  1.79s/it, loss=9.29, v_num=641]Epoch 1:   1%|          | 40/4381 [01:13<2:09:15,  1.79s/it, loss=9.21, v_num=641]Epoch 1:   1%|          | 50/4381 [01:28<2:05:50,  1.74s/it, loss=9.21, v_num=641]Epoch 1:   1%|          | 50/4381 [01:28<2:05:50,  1.74s/it, loss=9.12, v_num=641]Epoch 1:   1%|▏         | 60/4381 [01:46<2:05:23,  1.74s/it, loss=9.12, v_num=641]Epoch 1:   1%|▏         | 60/4381 [01:46<2:05:23,  1.74s/it, loss=9.1, v_num=641] Epoch 1:   2%|▏         | 70/4381 [02:03<2:05:07,  1.74s/it, loss=9.1, v_num=641]Epoch 1:   2%|▏         | 70/4381 [02:03<2:05:08,  1.74s/it, loss=9.07, v_num=641]Epoch 1:   2%|▏         | 80/4381 [02:19<2:03:34,  1.72s/it, loss=9.07, v_num=641]Epoch 1:   2%|▏         | 80/4381 [02:19<2:03:34,  1.72s/it, loss=9.04, v_num=641]Epoch 1:   2%|▏         | 90/4381 [02:38<2:04:22,  1.74s/it, loss=9.04, v_num=641]Epoch 1:   2%|▏         | 90/4381 [02:38<2:04:22,  1.74s/it, loss=9, v_num=641]   Epoch 1:   2%|▏         | 100/4381 [02:53<2:02:17,  1.71s/it, loss=9, v_num=641]Epoch 1:   2%|▏         | 100/4381 [02:53<2:02:17,  1.71s/it, loss=8.95, v_num=641]Epoch 1:   3%|▎         | 110/4381 [03:09<2:01:23,  1.71s/it, loss=8.95, v_num=641]Epoch 1:   3%|▎         | 110/4381 [03:09<2:01:23,  1.71s/it, loss=8.92, v_num=641]Epoch 1:   3%|▎         | 120/4381 [03:28<2:02:19,  1.72s/it, loss=8.92, v_num=641]Epoch 1:   3%|▎         | 120/4381 [03:28<2:02:19,  1.72s/it, loss=8.87, v_num=641]Epoch 1:   3%|▎         | 130/4381 [03:44<2:01:22,  1.71s/it, loss=8.87, v_num=641]Epoch 1:   3%|▎         | 130/4381 [03:44<2:01:22,  1.71s/it, loss=8.83, v_num=641]Epoch 1:   3%|▎         | 140/4381 [04:01<2:01:13,  1.72s/it, loss=8.83, v_num=641]Epoch 1:   3%|▎         | 140/4381 [04:01<2:01:13,  1.72s/it, loss=8.8, v_num=641] Epoch 1:   3%|▎         | 150/4381 [04:17<2:00:28,  1.71s/it, loss=8.8, v_num=641]Epoch 1:   3%|▎         | 150/4381 [04:17<2:00:28,  1.71s/it, loss=8.78, v_num=641]Epoch 1:   4%|▎         | 160/4381 [04:34<1:59:57,  1.71s/it, loss=8.78, v_num=641]Epoch 1:   4%|▎         | 160/4381 [04:34<1:59:57,  1.71s/it, loss=8.76, v_num=641]Epoch 1:   4%|▍         | 170/4381 [04:51<1:59:35,  1.70s/it, loss=8.76, v_num=641]Epoch 1:   4%|▍         | 170/4381 [04:51<1:59:35,  1.70s/it, loss=8.75, v_num=641]Epoch 1:   4%|▍         | 180/4381 [05:06<1:58:41,  1.70s/it, loss=8.75, v_num=641]Epoch 1:   4%|▍         | 180/4381 [05:06<1:58:41,  1.70s/it, loss=8.71, v_num=641]Epoch 1:   4%|▍         | 190/4381 [05:26<1:59:17,  1.71s/it, loss=8.71, v_num=641]Epoch 1:   4%|▍         | 190/4381 [05:26<1:59:17,  1.71s/it, loss=8.65, v_num=641]Epoch 1:   5%|▍         | 200/4381 [05:42<1:58:43,  1.70s/it, loss=8.65, v_num=641]Epoch 1:   5%|▍         | 200/4381 [05:42<1:58:43,  1.70s/it, loss=8.6, v_num=641] Epoch 1:   5%|▍         | 210/4381 [05:57<1:57:50,  1.70s/it, loss=8.6, v_num=641]Epoch 1:   5%|▍         | 210/4381 [05:57<1:57:50,  1.70s/it, loss=8.6, v_num=641]Epoch 1:   5%|▌         | 220/4381 [06:16<1:58:05,  1.70s/it, loss=8.6, v_num=641]Epoch 1:   5%|▌         | 220/4381 [06:16<1:58:05,  1.70s/it, loss=8.52, v_num=641]Epoch 1:   5%|▌         | 230/4381 [06:30<1:57:01,  1.69s/it, loss=8.52, v_num=641]Epoch 1:   5%|▌         | 230/4381 [06:30<1:57:01,  1.69s/it, loss=8.44, v_num=641]Epoch 1:   5%|▌         | 240/4381 [06:44<1:55:52,  1.68s/it, loss=8.44, v_num=641]Epoch 1:   5%|▌         | 240/4381 [06:44<1:55:52,  1.68s/it, loss=8.41, v_num=641]Epoch 1:   6%|▌         | 250/4381 [07:04<1:56:33,  1.69s/it, loss=8.41, v_num=641]Epoch 1:   6%|▌         | 250/4381 [07:04<1:56:33,  1.69s/it, loss=8.33, v_num=641]Epoch 1:   6%|▌         | 260/4381 [07:19<1:55:31,  1.68s/it, loss=8.33, v_num=641]Epoch 1:   6%|▌         | 260/4381 [07:19<1:55:31,  1.68s/it, loss=8.32, v_num=641]Epoch 1:   6%|▌         | 270/4381 [07:36<1:55:22,  1.68s/it, loss=8.32, v_num=641]Epoch 1:   6%|▌         | 270/4381 [07:36<1:55:22,  1.68s/it, loss=8.3, v_num=641] Epoch 1:   6%|▋         | 280/4381 [07:50<1:54:25,  1.67s/it, loss=8.3, v_num=641]Epoch 1:   6%|▋         | 280/4381 [07:50<1:54:25,  1.67s/it, loss=8.29, v_num=641]Epoch 1:   7%|▋         | 290/4381 [08:07<1:54:15,  1.68s/it, loss=8.29, v_num=641]Epoch 1:   7%|▋         | 290/4381 [08:07<1:54:15,  1.68s/it, loss=8.24, v_num=641]Epoch 1:   7%|▋         | 300/4381 [08:24<1:53:59,  1.68s/it, loss=8.24, v_num=641]Epoch 1:   7%|▋         | 300/4381 [08:24<1:53:59,  1.68s/it, loss=8.2, v_num=641] Epoch 1:   7%|▋         | 310/4381 [08:39<1:53:24,  1.67s/it, loss=8.2, v_num=641]Epoch 1:   7%|▋         | 310/4381 [08:39<1:53:24,  1.67s/it, loss=8.15, v_num=641]Epoch 1:   7%|▋         | 320/4381 [08:57<1:53:15,  1.67s/it, loss=8.15, v_num=641]Epoch 1:   7%|▋         | 320/4381 [08:57<1:53:15,  1.67s/it, loss=8.13, v_num=641]Epoch 1:   8%|▊         | 330/4381 [09:14<1:53:02,  1.67s/it, loss=8.13, v_num=641]Epoch 1:   8%|▊         | 330/4381 [09:14<1:53:02,  1.67s/it, loss=8.12, v_num=641]Epoch 1:   8%|▊         | 340/4381 [09:31<1:52:47,  1.67s/it, loss=8.12, v_num=641]Epoch 1:   8%|▊         | 340/4381 [09:31<1:52:47,  1.67s/it, loss=8.03, v_num=641]Epoch 1:   8%|▊         | 350/4381 [09:48<1:52:43,  1.68s/it, loss=8.03, v_num=641]Epoch 1:   8%|▊         | 350/4381 [09:48<1:52:43,  1.68s/it, loss=7.99, v_num=641]Epoch 1:   8%|▊         | 360/4381 [10:02<1:51:56,  1.67s/it, loss=7.99, v_num=641]Epoch 1:   8%|▊         | 360/4381 [10:02<1:51:56,  1.67s/it, loss=8.02, v_num=641]Epoch 1:   8%|▊         | 370/4381 [10:17<1:51:20,  1.67s/it, loss=8.02, v_num=641]Epoch 1:   8%|▊         | 370/4381 [10:17<1:51:20,  1.67s/it, loss=8, v_num=641]   Epoch 1:   9%|▊         | 380/4381 [10:36<1:51:19,  1.67s/it, loss=8, v_num=641]Epoch 1:   9%|▊         | 380/4381 [10:36<1:51:19,  1.67s/it, loss=7.93, v_num=641]Epoch 1:   9%|▉         | 390/4381 [10:51<1:50:52,  1.67s/it, loss=7.93, v_num=641]Epoch 1:   9%|▉         | 390/4381 [10:51<1:50:52,  1.67s/it, loss=7.91, v_num=641]Epoch 1:   9%|▉         | 400/4381 [11:09<1:50:42,  1.67s/it, loss=7.91, v_num=641]Epoch 1:   9%|▉         | 400/4381 [11:09<1:50:42,  1.67s/it, loss=7.85, v_num=641]Epoch 1:   9%|▉         | 410/4381 [11:28<1:50:50,  1.67s/it, loss=7.85, v_num=641]Epoch 1:   9%|▉         | 410/4381 [11:28<1:50:50,  1.67s/it, loss=7.8, v_num=641] Epoch 1:  10%|▉         | 420/4381 [11:41<1:49:59,  1.67s/it, loss=7.8, v_num=641]Epoch 1:  10%|▉         | 420/4381 [11:41<1:49:59,  1.67s/it, loss=7.72, v_num=641]Epoch 1:  10%|▉         | 430/4381 [11:58<1:49:44,  1.67s/it, loss=7.72, v_num=641]Epoch 1:  10%|▉         | 430/4381 [11:58<1:49:44,  1.67s/it, loss=7.58, v_num=641]Epoch 1:  10%|█         | 440/4381 [12:17<1:49:53,  1.67s/it, loss=7.58, v_num=641]Epoch 1:  10%|█         | 440/4381 [12:17<1:49:53,  1.67s/it, loss=7.59, v_num=641]Epoch 1:  10%|█         | 450/4381 [12:32<1:49:22,  1.67s/it, loss=7.59, v_num=641]Epoch 1:  10%|█         | 450/4381 [12:32<1:49:22,  1.67s/it, loss=7.61, v_num=641]Epoch 1:  10%|█         | 460/4381 [12:46<1:48:40,  1.66s/it, loss=7.61, v_num=641]Epoch 1:  10%|█         | 460/4381 [12:46<1:48:40,  1.66s/it, loss=7.59, v_num=641]Epoch 1:  11%|█         | 470/4381 [13:04<1:48:32,  1.67s/it, loss=7.59, v_num=641]Epoch 1:  11%|█         | 470/4381 [13:04<1:48:32,  1.67s/it, loss=7.55, v_num=641]Epoch 1:  11%|█         | 480/4381 [13:20<1:48:12,  1.66s/it, loss=7.55, v_num=641]Epoch 1:  11%|█         | 480/4381 [13:20<1:48:12,  1.66s/it, loss=7.48, v_num=641]Epoch 1:  11%|█         | 490/4381 [13:35<1:47:40,  1.66s/it, loss=7.48, v_num=641]Epoch 1:  11%|█         | 490/4381 [13:35<1:47:40,  1.66s/it, loss=7.43, v_num=641]Epoch 1:  11%|█▏        | 500/4381 [13:55<1:47:52,  1.67s/it, loss=7.43, v_num=641]Epoch 1:  11%|█▏        | 500/4381 [13:55<1:47:52,  1.67s/it, loss=7.41, v_num=641]Epoch 1:  12%|█▏        | 510/4381 [14:09<1:47:12,  1.66s/it, loss=7.41, v_num=641]Epoch 1:  12%|█▏        | 510/4381 [14:09<1:47:12,  1.66s/it, loss=7.34, v_num=641]Epoch 1:  12%|█▏        | 520/4381 [14:24<1:46:45,  1.66s/it, loss=7.34, v_num=641]Epoch 1:  12%|█▏        | 520/4381 [14:24<1:46:45,  1.66s/it, loss=7.29, v_num=641]Epoch 1:  12%|█▏        | 530/4381 [14:42<1:46:42,  1.66s/it, loss=7.29, v_num=641]Epoch 1:  12%|█▏        | 530/4381 [14:42<1:46:42,  1.66s/it, loss=7.28, v_num=641]Epoch 1:  12%|█▏        | 540/4381 [14:58<1:46:21,  1.66s/it, loss=7.28, v_num=641]Epoch 1:  12%|█▏        | 540/4381 [14:58<1:46:21,  1.66s/it, loss=7.22, v_num=641]Epoch 1:  13%|█▎        | 550/4381 [15:12<1:45:44,  1.66s/it, loss=7.22, v_num=641]Epoch 1:  13%|█▎        | 550/4381 [15:12<1:45:44,  1.66s/it, loss=7.22, v_num=641]Epoch 1:  13%|█▎        | 560/4381 [15:31<1:45:45,  1.66s/it, loss=7.22, v_num=641]Epoch 1:  13%|█▎        | 560/4381 [15:31<1:45:45,  1.66s/it, loss=7.19, v_num=641]Epoch 1:  13%|█▎        | 570/4381 [15:46<1:45:16,  1.66s/it, loss=7.19, v_num=641]Epoch 1:  13%|█▎        | 570/4381 [15:46<1:45:16,  1.66s/it, loss=7.11, v_num=641]Epoch 1:  13%|█▎        | 580/4381 [16:01<1:44:48,  1.65s/it, loss=7.11, v_num=641]Epoch 1:  13%|█▎        | 580/4381 [16:01<1:44:48,  1.65s/it, loss=7.1, v_num=641] Epoch 1:  13%|█▎        | 590/4381 [16:20<1:44:51,  1.66s/it, loss=7.1, v_num=641]Epoch 1:  13%|█▎        | 590/4381 [16:20<1:44:51,  1.66s/it, loss=7.11, v_num=641]Epoch 1:  14%|█▎        | 600/4381 [16:36<1:44:26,  1.66s/it, loss=7.11, v_num=641]Epoch 1:  14%|█▎        | 600/4381 [16:36<1:44:26,  1.66s/it, loss=7.07, v_num=641]Epoch 1:  14%|█▍        | 610/4381 [16:50<1:43:59,  1.65s/it, loss=7.07, v_num=641]Epoch 1:  14%|█▍        | 610/4381 [16:50<1:43:59,  1.65s/it, loss=6.97, v_num=641]Epoch 1:  14%|█▍        | 620/4381 [17:08<1:43:47,  1.66s/it, loss=6.97, v_num=641]Epoch 1:  14%|█▍        | 620/4381 [17:08<1:43:47,  1.66s/it, loss=6.93, v_num=641]Epoch 1:  14%|█▍        | 630/4381 [17:23<1:43:22,  1.65s/it, loss=6.93, v_num=641]Epoch 1:  14%|█▍        | 630/4381 [17:23<1:43:22,  1.65s/it, loss=6.93, v_num=641]Epoch 1:  15%|█▍        | 640/4381 [17:36<1:42:43,  1.65s/it, loss=6.93, v_num=641]Epoch 1:  15%|█▍        | 640/4381 [17:36<1:42:43,  1.65s/it, loss=6.89, v_num=641]Epoch 1:  15%|█▍        | 650/4381 [17:52<1:42:28,  1.65s/it, loss=6.89, v_num=641]Epoch 1:  15%|█▍        | 650/4381 [17:52<1:42:28,  1.65s/it, loss=6.86, v_num=641]Epoch 1:  15%|█▌        | 660/4381 [18:08<1:42:09,  1.65s/it, loss=6.86, v_num=641]Epoch 1:  15%|█▌        | 660/4381 [18:08<1:42:09,  1.65s/it, loss=6.77, v_num=641]Epoch 1:  15%|█▌        | 670/4381 [18:23<1:41:45,  1.65s/it, loss=6.77, v_num=641]Epoch 1:  15%|█▌        | 670/4381 [18:23<1:41:45,  1.65s/it, loss=6.72, v_num=641]Epoch 1:  16%|█▌        | 680/4381 [18:42<1:41:38,  1.65s/it, loss=6.72, v_num=641]Epoch 1:  16%|█▌        | 680/4381 [18:42<1:41:38,  1.65s/it, loss=6.68, v_num=641]Epoch 1:  16%|█▌        | 690/4381 [18:56<1:41:12,  1.65s/it, loss=6.68, v_num=641]Epoch 1:  16%|█▌        | 690/4381 [18:56<1:41:12,  1.65s/it, loss=6.64, v_num=641]Epoch 1:  16%|█▌        | 700/4381 [19:12<1:40:52,  1.64s/it, loss=6.64, v_num=641]Epoch 1:  16%|█▌        | 700/4381 [19:12<1:40:52,  1.64s/it, loss=6.66, v_num=641]Epoch 1:  16%|█▌        | 710/4381 [19:28<1:40:33,  1.64s/it, loss=6.66, v_num=641]Epoch 1:  16%|█▌        | 710/4381 [19:28<1:40:33,  1.64s/it, loss=6.66, v_num=641]Epoch 1:  16%|█▋        | 720/4381 [19:43<1:40:08,  1.64s/it, loss=6.66, v_num=641]Epoch 1:  16%|█▋        | 720/4381 [19:43<1:40:08,  1.64s/it, loss=6.61, v_num=641]Epoch 1:  17%|█▋        | 730/4381 [19:57<1:39:42,  1.64s/it, loss=6.61, v_num=641]Epoch 1:  17%|█▋        | 730/4381 [19:57<1:39:42,  1.64s/it, loss=6.6, v_num=641] Epoch 1:  17%|█▋        | 740/4381 [20:17<1:39:42,  1.64s/it, loss=6.6, v_num=641]Epoch 1:  17%|█▋        | 740/4381 [20:17<1:39:42,  1.64s/it, loss=6.57, v_num=641]Epoch 1:  17%|█▋        | 750/4381 [20:32<1:39:17,  1.64s/it, loss=6.57, v_num=641]Epoch 1:  17%|█▋        | 750/4381 [20:32<1:39:17,  1.64s/it, loss=6.51, v_num=641]Epoch 1:  17%|█▋        | 760/4381 [20:46<1:38:51,  1.64s/it, loss=6.51, v_num=641]Epoch 1:  17%|█▋        | 760/4381 [20:46<1:38:51,  1.64s/it, loss=6.55, v_num=641]Epoch 1:  18%|█▊        | 770/4381 [21:08<1:38:58,  1.64s/it, loss=6.55, v_num=641]Epoch 1:  18%|█▊        | 770/4381 [21:08<1:38:58,  1.64s/it, loss=6.57, v_num=641]Epoch 1:  18%|█▊        | 780/4381 [21:20<1:38:24,  1.64s/it, loss=6.57, v_num=641]Epoch 1:  18%|█▊        | 780/4381 [21:20<1:38:24,  1.64s/it, loss=6.51, v_num=641]Epoch 1:  18%|█▊        | 790/4381 [21:33<1:37:50,  1.63s/it, loss=6.51, v_num=641]Epoch 1:  18%|█▊        | 790/4381 [21:33<1:37:50,  1.63s/it, loss=6.49, v_num=641]Epoch 1:  18%|█▊        | 800/4381 [21:49<1:37:35,  1.64s/it, loss=6.49, v_num=641]Epoch 1:  18%|█▊        | 800/4381 [21:49<1:37:35,  1.64s/it, loss=6.49, v_num=641]Epoch 1:  18%|█▊        | 810/4381 [22:05<1:37:16,  1.63s/it, loss=6.49, v_num=641]Epoch 1:  18%|█▊        | 810/4381 [22:05<1:37:16,  1.63s/it, loss=6.39, v_num=641]Epoch 1:  19%|█▊        | 820/4381 [22:21<1:36:56,  1.63s/it, loss=6.39, v_num=641]Epoch 1:  19%|█▊        | 820/4381 [22:21<1:36:56,  1.63s/it, loss=6.31, v_num=641]Epoch 1:  19%|█▉        | 830/4381 [22:37<1:36:39,  1.63s/it, loss=6.31, v_num=641]Epoch 1:  19%|█▉        | 830/4381 [22:37<1:36:39,  1.63s/it, loss=6.26, v_num=641]Epoch 1:  19%|█▉        | 840/4381 [22:51<1:36:14,  1.63s/it, loss=6.26, v_num=641]Epoch 1:  19%|█▉        | 840/4381 [22:51<1:36:14,  1.63s/it, loss=6.26, v_num=641]Epoch 1:  19%|█▉        | 850/4381 [23:05<1:35:48,  1.63s/it, loss=6.26, v_num=641]Epoch 1:  19%|█▉        | 850/4381 [23:05<1:35:48,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  20%|█▉        | 860/4381 [23:26<1:35:51,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  20%|█▉        | 860/4381 [23:26<1:35:51,  1.63s/it, loss=6.3, v_num=641] Epoch 1:  20%|█▉        | 870/4381 [23:42<1:35:35,  1.63s/it, loss=6.3, v_num=641]Epoch 1:  20%|█▉        | 870/4381 [23:42<1:35:35,  1.63s/it, loss=6.31, v_num=641]Epoch 1:  20%|██        | 880/4381 [23:57<1:35:12,  1.63s/it, loss=6.31, v_num=641]Epoch 1:  20%|██        | 880/4381 [23:57<1:35:12,  1.63s/it, loss=6.24, v_num=641]Epoch 1:  20%|██        | 890/4381 [24:13<1:34:53,  1.63s/it, loss=6.24, v_num=641]Epoch 1:  20%|██        | 890/4381 [24:13<1:34:53,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  21%|██        | 900/4381 [24:28<1:34:34,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  21%|██        | 900/4381 [24:28<1:34:34,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  21%|██        | 910/4381 [24:45<1:34:21,  1.63s/it, loss=6.25, v_num=641]Epoch 1:  21%|██        | 910/4381 [24:45<1:34:21,  1.63s/it, loss=6.18, v_num=641]Epoch 1:  21%|██        | 920/4381 [25:04<1:34:12,  1.63s/it, loss=6.18, v_num=641]Epoch 1:  21%|██        | 920/4381 [25:04<1:34:12,  1.63s/it, loss=6.11, v_num=641]Epoch 1:  21%|██        | 930/4381 [25:19<1:33:51,  1.63s/it, loss=6.11, v_num=641]Epoch 1:  21%|██        | 930/4381 [25:19<1:33:51,  1.63s/it, loss=6.09, v_num=641]Epoch 1:  21%|██▏       | 940/4381 [25:35<1:33:35,  1.63s/it, loss=6.09, v_num=641]Epoch 1:  21%|██▏       | 940/4381 [25:35<1:33:35,  1.63s/it, loss=6.15, v_num=641]Epoch 1:  22%|██▏       | 950/4381 [25:54<1:33:27,  1.63s/it, loss=6.15, v_num=641]Epoch 1:  22%|██▏       | 950/4381 [25:54<1:33:27,  1.63s/it, loss=6.14, v_num=641]Epoch 1:  22%|██▏       | 960/4381 [26:09<1:33:06,  1.63s/it, loss=6.14, v_num=641]Epoch 1:  22%|██▏       | 960/4381 [26:09<1:33:06,  1.63s/it, loss=6.07, v_num=641]Epoch 1:  22%|██▏       | 970/4381 [26:24<1:32:46,  1.63s/it, loss=6.07, v_num=641]Epoch 1:  22%|██▏       | 970/4381 [26:24<1:32:46,  1.63s/it, loss=6.07, v_num=641]Epoch 1:  22%|██▏       | 980/4381 [26:42<1:32:36,  1.63s/it, loss=6.07, v_num=641]Epoch 1:  22%|██▏       | 980/4381 [26:42<1:32:36,  1.63s/it, loss=6.04, v_num=641]Epoch 1:  23%|██▎       | 990/4381 [26:58<1:32:17,  1.63s/it, loss=6.04, v_num=641]Epoch 1:  23%|██▎       | 990/4381 [26:58<1:32:17,  1.63s/it, loss=5.99, v_num=641]Epoch 1:  23%|██▎       | 1000/4381 [27:13<1:31:55,  1.63s/it, loss=5.99, v_num=641]Epoch 1:  23%|██▎       | 1000/4381 [27:13<1:31:55,  1.63s/it, loss=6.01, v_num=641]Epoch 1:  23%|██▎       | 1010/4381 [27:31<1:31:45,  1.63s/it, loss=6.01, v_num=641]Epoch 1:  23%|██▎       | 1010/4381 [27:31<1:31:45,  1.63s/it, loss=5.97, v_num=641]Epoch 1:  23%|██▎       | 1020/4381 [27:43<1:31:17,  1.63s/it, loss=5.97, v_num=641]Epoch 1:  23%|██▎       | 1020/4381 [27:43<1:31:17,  1.63s/it, loss=6, v_num=641]   Epoch 1:  24%|██▎       | 1030/4381 [27:58<1:30:54,  1.63s/it, loss=6, v_num=641]Epoch 1:  24%|██▎       | 1030/4381 [27:58<1:30:54,  1.63s/it, loss=6, v_num=641]Epoch 1:  24%|██▎       | 1040/4381 [28:16<1:30:43,  1.63s/it, loss=6, v_num=641]Epoch 1:  24%|██▎       | 1040/4381 [28:16<1:30:43,  1.63s/it, loss=5.92, v_num=641]Epoch 1:  24%|██▍       | 1050/4381 [28:31<1:30:24,  1.63s/it, loss=5.92, v_num=641]Epoch 1:  24%|██▍       | 1050/4381 [28:31<1:30:24,  1.63s/it, loss=5.92, v_num=641]Epoch 1:  24%|██▍       | 1060/4381 [28:46<1:30:04,  1.63s/it, loss=5.92, v_num=641]Epoch 1:  24%|██▍       | 1060/4381 [28:46<1:30:04,  1.63s/it, loss=5.95, v_num=641]Epoch 1:  24%|██▍       | 1070/4381 [29:04<1:29:54,  1.63s/it, loss=5.95, v_num=641]Epoch 1:  24%|██▍       | 1070/4381 [29:04<1:29:54,  1.63s/it, loss=5.93, v_num=641]Epoch 1:  25%|██▍       | 1080/4381 [29:17<1:29:25,  1.63s/it, loss=5.93, v_num=641]Epoch 1:  25%|██▍       | 1080/4381 [29:17<1:29:25,  1.63s/it, loss=5.87, v_num=641]Epoch 1:  25%|██▍       | 1090/4381 [29:32<1:29:05,  1.62s/it, loss=5.87, v_num=641]Epoch 1:  25%|██▍       | 1090/4381 [29:32<1:29:05,  1.62s/it, loss=5.85, v_num=641]Epoch 1:  25%|██▌       | 1100/4381 [29:50<1:28:55,  1.63s/it, loss=5.85, v_num=641]Epoch 1:  25%|██▌       | 1100/4381 [29:50<1:28:55,  1.63s/it, loss=5.87, v_num=641]Epoch 1:  25%|██▌       | 1110/4381 [30:04<1:28:32,  1.62s/it, loss=5.87, v_num=641]Epoch 1:  25%|██▌       | 1110/4381 [30:04<1:28:32,  1.62s/it, loss=5.82, v_num=641]Epoch 1:  26%|██▌       | 1120/4381 [30:21<1:28:17,  1.62s/it, loss=5.82, v_num=641]Epoch 1:  26%|██▌       | 1120/4381 [30:21<1:28:17,  1.62s/it, loss=5.78, v_num=641]Epoch 1:  26%|██▌       | 1130/4381 [30:39<1:28:06,  1.63s/it, loss=5.78, v_num=641]Epoch 1:  26%|██▌       | 1130/4381 [30:39<1:28:06,  1.63s/it, loss=5.81, v_num=641]Epoch 1:  26%|██▌       | 1140/4381 [30:54<1:27:47,  1.63s/it, loss=5.81, v_num=641]Epoch 1:  26%|██▌       | 1140/4381 [30:54<1:27:47,  1.63s/it, loss=5.8, v_num=641] Epoch 1:  26%|██▌       | 1150/4381 [31:08<1:27:25,  1.62s/it, loss=5.8, v_num=641]Epoch 1:  26%|██▌       | 1150/4381 [31:08<1:27:25,  1.62s/it, loss=5.77, v_num=641]Epoch 1:  26%|██▋       | 1160/4381 [31:27<1:27:15,  1.63s/it, loss=5.77, v_num=641]Epoch 1:  26%|██▋       | 1160/4381 [31:27<1:27:15,  1.63s/it, loss=5.77, v_num=641]Epoch 1:  27%|██▋       | 1170/4381 [31:41<1:26:52,  1.62s/it, loss=5.77, v_num=641]Epoch 1:  27%|██▋       | 1170/4381 [31:41<1:26:52,  1.62s/it, loss=5.77, v_num=641]Epoch 1:  27%|██▋       | 1180/4381 [31:55<1:26:30,  1.62s/it, loss=5.77, v_num=641]Epoch 1:  27%|██▋       | 1180/4381 [31:55<1:26:30,  1.62s/it, loss=5.74, v_num=641]Epoch 1:  27%|██▋       | 1190/4381 [32:11<1:26:15,  1.62s/it, loss=5.74, v_num=641]Epoch 1:  27%|██▋       | 1190/4381 [32:11<1:26:15,  1.62s/it, loss=5.71, v_num=641]Epoch 1:  27%|██▋       | 1200/4381 [32:28<1:26:00,  1.62s/it, loss=5.71, v_num=641]Epoch 1:  27%|██▋       | 1200/4381 [32:28<1:26:00,  1.62s/it, loss=5.67, v_num=641]Epoch 1:  28%|██▊       | 1210/4381 [32:44<1:25:43,  1.62s/it, loss=5.67, v_num=641]Epoch 1:  28%|██▊       | 1210/4381 [32:44<1:25:43,  1.62s/it, loss=5.68, v_num=641]Epoch 1:  28%|██▊       | 1220/4381 [32:59<1:25:24,  1.62s/it, loss=5.68, v_num=641]Epoch 1:  28%|██▊       | 1220/4381 [32:59<1:25:24,  1.62s/it, loss=5.73, v_num=641]Epoch 1:  28%|██▊       | 1230/4381 [33:16<1:25:09,  1.62s/it, loss=5.73, v_num=641]Epoch 1:  28%|██▊       | 1230/4381 [33:16<1:25:09,  1.62s/it, loss=5.68, v_num=641]Epoch 1:  28%|██▊       | 1240/4381 [33:32<1:24:53,  1.62s/it, loss=5.68, v_num=641]Epoch 1:  28%|██▊       | 1240/4381 [33:32<1:24:53,  1.62s/it, loss=5.65, v_num=641]Epoch 1:  29%|██▊       | 1250/4381 [33:49<1:24:39,  1.62s/it, loss=5.65, v_num=641]Epoch 1:  29%|██▊       | 1250/4381 [33:49<1:24:39,  1.62s/it, loss=5.63, v_num=641]Epoch 1:  29%|██▉       | 1260/4381 [34:04<1:24:19,  1.62s/it, loss=5.63, v_num=641]Epoch 1:  29%|██▉       | 1260/4381 [34:04<1:24:19,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  29%|██▉       | 1270/4381 [34:18<1:23:57,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  29%|██▉       | 1270/4381 [34:18<1:23:57,  1.62s/it, loss=5.59, v_num=641]Epoch 1:  29%|██▉       | 1280/4381 [34:38<1:23:50,  1.62s/it, loss=5.59, v_num=641]Epoch 1:  29%|██▉       | 1280/4381 [34:38<1:23:50,  1.62s/it, loss=5.64, v_num=641]Epoch 1:  29%|██▉       | 1290/4381 [34:53<1:23:33,  1.62s/it, loss=5.64, v_num=641]Epoch 1:  29%|██▉       | 1290/4381 [34:53<1:23:33,  1.62s/it, loss=5.62, v_num=641]Epoch 1:  30%|██▉       | 1300/4381 [35:10<1:23:18,  1.62s/it, loss=5.62, v_num=641]Epoch 1:  30%|██▉       | 1300/4381 [35:10<1:23:18,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  30%|██▉       | 1310/4381 [35:29<1:23:08,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  30%|██▉       | 1310/4381 [35:29<1:23:08,  1.62s/it, loss=5.59, v_num=641]Epoch 1:  30%|███       | 1320/4381 [35:44<1:22:48,  1.62s/it, loss=5.59, v_num=641]Epoch 1:  30%|███       | 1320/4381 [35:44<1:22:48,  1.62s/it, loss=5.6, v_num=641] Epoch 1:  30%|███       | 1330/4381 [36:01<1:22:34,  1.62s/it, loss=5.6, v_num=641]Epoch 1:  30%|███       | 1330/4381 [36:01<1:22:34,  1.62s/it, loss=5.55, v_num=641]Epoch 1:  31%|███       | 1340/4381 [36:16<1:22:15,  1.62s/it, loss=5.55, v_num=641]Epoch 1:  31%|███       | 1340/4381 [36:16<1:22:15,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  31%|███       | 1350/4381 [36:32<1:21:58,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  31%|███       | 1350/4381 [36:32<1:21:58,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  31%|███       | 1360/4381 [36:46<1:21:38,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  31%|███       | 1360/4381 [36:46<1:21:38,  1.62s/it, loss=5.5, v_num=641] Epoch 1:  31%|███▏      | 1370/4381 [37:06<1:21:29,  1.62s/it, loss=5.5, v_num=641]Epoch 1:  31%|███▏      | 1370/4381 [37:06<1:21:29,  1.62s/it, loss=5.55, v_num=641]Epoch 1:  31%|███▏      | 1380/4381 [37:21<1:21:10,  1.62s/it, loss=5.55, v_num=641]Epoch 1:  31%|███▏      | 1380/4381 [37:21<1:21:10,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  32%|███▏      | 1390/4381 [37:37<1:20:54,  1.62s/it, loss=5.57, v_num=641]Epoch 1:  32%|███▏      | 1390/4381 [37:37<1:20:54,  1.62s/it, loss=5.46, v_num=641]Epoch 1:  32%|███▏      | 1400/4381 [37:54<1:20:40,  1.62s/it, loss=5.46, v_num=641]Epoch 1:  32%|███▏      | 1400/4381 [37:54<1:20:40,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  32%|███▏      | 1410/4381 [38:09<1:20:20,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  32%|███▏      | 1410/4381 [38:09<1:20:20,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  32%|███▏      | 1420/4381 [38:22<1:19:57,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  32%|███▏      | 1420/4381 [38:22<1:19:57,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  33%|███▎      | 1430/4381 [38:42<1:19:49,  1.62s/it, loss=5.42, v_num=641]Epoch 1:  33%|███▎      | 1430/4381 [38:42<1:19:49,  1.62s/it, loss=5.45, v_num=641]Epoch 1:  33%|███▎      | 1440/4381 [38:58<1:19:33,  1.62s/it, loss=5.45, v_num=641]Epoch 1:  33%|███▎      | 1440/4381 [38:58<1:19:33,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  33%|███▎      | 1450/4381 [39:13<1:19:14,  1.62s/it, loss=5.47, v_num=641]Epoch 1:  33%|███▎      | 1450/4381 [39:13<1:19:14,  1.62s/it, loss=5.46, v_num=641]Epoch 1:  33%|███▎      | 1460/4381 [39:31<1:19:01,  1.62s/it, loss=5.46, v_num=641]Epoch 1:  33%|███▎      | 1460/4381 [39:31<1:19:01,  1.62s/it, loss=5.37, v_num=641]Epoch 1:  34%|███▎      | 1470/4381 [39:46<1:18:42,  1.62s/it, loss=5.37, v_num=641]Epoch 1:  34%|███▎      | 1470/4381 [39:46<1:18:42,  1.62s/it, loss=5.39, v_num=641]Epoch 1:  34%|███▍      | 1480/4381 [39:58<1:18:18,  1.62s/it, loss=5.39, v_num=641]Epoch 1:  34%|███▍      | 1480/4381 [39:58<1:18:18,  1.62s/it, loss=5.4, v_num=641] Epoch 1:  34%|███▍      | 1490/4381 [40:16<1:18:05,  1.62s/it, loss=5.4, v_num=641]Epoch 1:  34%|███▍      | 1490/4381 [40:16<1:18:05,  1.62s/it, loss=5.39, v_num=641]Epoch 1:  34%|███▍      | 1500/4381 [40:31<1:17:46,  1.62s/it, loss=5.39, v_num=641]Epoch 1:  34%|███▍      | 1500/4381 [40:31<1:17:46,  1.62s/it, loss=5.34, v_num=641]Epoch 1:  34%|███▍      | 1510/4381 [40:45<1:17:27,  1.62s/it, loss=5.34, v_num=641]Epoch 1:  34%|███▍      | 1510/4381 [40:45<1:17:27,  1.62s/it, loss=5.32, v_num=641]Epoch 1:  35%|███▍      | 1520/4381 [41:03<1:17:13,  1.62s/it, loss=5.32, v_num=641]Epoch 1:  35%|███▍      | 1520/4381 [41:03<1:17:13,  1.62s/it, loss=5.36, v_num=641]Epoch 1:  35%|███▍      | 1530/4381 [41:15<1:16:50,  1.62s/it, loss=5.36, v_num=641]Epoch 1:  35%|███▍      | 1530/4381 [41:15<1:16:50,  1.62s/it, loss=5.32, v_num=641]Epoch 1:  35%|███▌      | 1540/4381 [41:30<1:16:31,  1.62s/it, loss=5.32, v_num=641]Epoch 1:  35%|███▌      | 1540/4381 [41:30<1:16:31,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  35%|███▌      | 1550/4381 [41:49<1:16:20,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  35%|███▌      | 1550/4381 [41:49<1:16:20,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  36%|███▌      | 1560/4381 [42:03<1:15:59,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  36%|███▌      | 1560/4381 [42:03<1:15:59,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  36%|███▌      | 1570/4381 [42:15<1:15:37,  1.61s/it, loss=5.31, v_num=641]Epoch 1:  36%|███▌      | 1570/4381 [42:15<1:15:37,  1.61s/it, loss=5.31, v_num=641]Epoch 1:  36%|███▌      | 1580/4381 [42:36<1:15:28,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  36%|███▌      | 1580/4381 [42:36<1:15:28,  1.62s/it, loss=5.3, v_num=641] Epoch 1:  36%|███▋      | 1590/4381 [42:52<1:15:12,  1.62s/it, loss=5.3, v_num=641]Epoch 1:  36%|███▋      | 1590/4381 [42:52<1:15:12,  1.62s/it, loss=5.3, v_num=641]Epoch 1:  37%|███▋      | 1600/4381 [43:09<1:14:57,  1.62s/it, loss=5.3, v_num=641]Epoch 1:  37%|███▋      | 1600/4381 [43:09<1:14:57,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  37%|███▋      | 1610/4381 [43:27<1:14:44,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  37%|███▋      | 1610/4381 [43:27<1:14:44,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  37%|███▋      | 1620/4381 [43:42<1:14:26,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  37%|███▋      | 1620/4381 [43:42<1:14:26,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  37%|███▋      | 1630/4381 [43:59<1:14:11,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  37%|███▋      | 1630/4381 [43:59<1:14:11,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  37%|███▋      | 1640/4381 [44:15<1:13:55,  1.62s/it, loss=5.31, v_num=641]Epoch 1:  37%|███▋      | 1640/4381 [44:15<1:13:55,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  38%|███▊      | 1650/4381 [44:28<1:13:33,  1.62s/it, loss=5.29, v_num=641]Epoch 1:  38%|███▊      | 1650/4381 [44:28<1:13:33,  1.62s/it, loss=5.28, v_num=641]Epoch 1:  38%|███▊      | 1660/4381 [44:44<1:13:16,  1.62s/it, loss=5.28, v_num=641]Epoch 1:  38%|███▊      | 1660/4381 [44:44<1:13:16,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  38%|███▊      | 1670/4381 [45:00<1:13:01,  1.62s/it, loss=5.25, v_num=641]Epoch 1:  38%|███▊      | 1670/4381 [45:00<1:13:01,  1.62s/it, loss=5.2, v_num=641] Epoch 1:  38%|███▊      | 1680/4381 [45:13<1:12:39,  1.61s/it, loss=5.2, v_num=641]Epoch 1:  38%|███▊      | 1680/4381 [45:13<1:12:39,  1.61s/it, loss=5.16, v_num=641]Epoch 1:  39%|███▊      | 1690/4381 [45:29<1:12:23,  1.61s/it, loss=5.16, v_num=641]Epoch 1:  39%|███▊      | 1690/4381 [45:29<1:12:23,  1.61s/it, loss=5.13, v_num=641]Epoch 1:  39%|███▉      | 1700/4381 [45:48<1:12:11,  1.62s/it, loss=5.13, v_num=641]Epoch 1:  39%|███▉      | 1700/4381 [45:48<1:12:11,  1.62s/it, loss=5.14, v_num=641]Epoch 1:  39%|███▉      | 1710/4381 [46:03<1:11:54,  1.62s/it, loss=5.14, v_num=641]Epoch 1:  39%|███▉      | 1710/4381 [46:03<1:11:54,  1.62s/it, loss=5.14, v_num=641]Epoch 1:  39%|███▉      | 1720/4381 [46:18<1:11:35,  1.61s/it, loss=5.14, v_num=641]Epoch 1:  39%|███▉      | 1720/4381 [46:18<1:11:35,  1.61s/it, loss=5.1, v_num=641] Epoch 1:  39%|███▉      | 1730/4381 [46:37<1:11:24,  1.62s/it, loss=5.1, v_num=641]Epoch 1:  39%|███▉      | 1730/4381 [46:37<1:11:24,  1.62s/it, loss=5.09, v_num=641]Epoch 1:  40%|███▉      | 1740/4381 [46:54<1:11:09,  1.62s/it, loss=5.09, v_num=641]Epoch 1:  40%|███▉      | 1740/4381 [46:54<1:11:09,  1.62s/it, loss=5.12, v_num=641]Epoch 1:  40%|███▉      | 1750/4381 [47:08<1:10:49,  1.62s/it, loss=5.12, v_num=641]Epoch 1:  40%|███▉      | 1750/4381 [47:08<1:10:49,  1.62s/it, loss=5.13, v_num=641]Epoch 1:  40%|████      | 1760/4381 [47:26<1:10:36,  1.62s/it, loss=5.13, v_num=641]Epoch 1:  40%|████      | 1760/4381 [47:26<1:10:36,  1.62s/it, loss=5.08, v_num=641]Epoch 1:  40%|████      | 1770/4381 [47:42<1:10:20,  1.62s/it, loss=5.08, v_num=641]Epoch 1:  40%|████      | 1770/4381 [47:42<1:10:20,  1.62s/it, loss=5.07, v_num=641]Epoch 1:  41%|████      | 1780/4381 [47:58<1:10:04,  1.62s/it, loss=5.07, v_num=641]Epoch 1:  41%|████      | 1780/4381 [47:58<1:10:04,  1.62s/it, loss=5.11, v_num=641]Epoch 1:  41%|████      | 1790/4381 [48:18<1:09:53,  1.62s/it, loss=5.11, v_num=641]Epoch 1:  41%|████      | 1790/4381 [48:18<1:09:53,  1.62s/it, loss=5.1, v_num=641] Epoch 1:  41%|████      | 1800/4381 [48:32<1:09:34,  1.62s/it, loss=5.1, v_num=641]Epoch 1:  41%|████      | 1800/4381 [48:32<1:09:34,  1.62s/it, loss=5.07, v_num=641]Epoch 1:  41%|████▏     | 1810/4381 [48:48<1:09:17,  1.62s/it, loss=5.07, v_num=641]Epoch 1:  41%|████▏     | 1810/4381 [48:48<1:09:17,  1.62s/it, loss=5.06, v_num=641]Epoch 1:  42%|████▏     | 1820/4381 [49:07<1:09:05,  1.62s/it, loss=5.06, v_num=641]Epoch 1:  42%|████▏     | 1820/4381 [49:07<1:09:05,  1.62s/it, loss=5.08, v_num=641]Epoch 1:  42%|████▏     | 1830/4381 [49:23<1:08:48,  1.62s/it, loss=5.08, v_num=641]Epoch 1:  42%|████▏     | 1830/4381 [49:23<1:08:48,  1.62s/it, loss=5.06, v_num=641]Epoch 1:  42%|████▏     | 1840/4381 [49:38<1:08:31,  1.62s/it, loss=5.06, v_num=641]Epoch 1:  42%|████▏     | 1840/4381 [49:38<1:08:31,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  42%|████▏     | 1850/4381 [49:57<1:08:19,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  42%|████▏     | 1850/4381 [49:57<1:08:19,  1.62s/it, loss=5, v_num=641]   Epoch 1:  42%|████▏     | 1860/4381 [50:12<1:08:01,  1.62s/it, loss=5, v_num=641]Epoch 1:  42%|████▏     | 1860/4381 [50:12<1:08:01,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  43%|████▎     | 1870/4381 [50:25<1:07:41,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  43%|████▎     | 1870/4381 [50:26<1:07:41,  1.62s/it, loss=5.03, v_num=641]Epoch 1:  43%|████▎     | 1880/4381 [50:46<1:07:30,  1.62s/it, loss=5.03, v_num=641]Epoch 1:  43%|████▎     | 1880/4381 [50:46<1:07:30,  1.62s/it, loss=5, v_num=641]   Epoch 1:  43%|████▎     | 1890/4381 [50:59<1:07:09,  1.62s/it, loss=5, v_num=641]Epoch 1:  43%|████▎     | 1890/4381 [50:59<1:07:09,  1.62s/it, loss=5.04, v_num=641]Epoch 1:  43%|████▎     | 1900/4381 [51:15<1:06:54,  1.62s/it, loss=5.04, v_num=641]Epoch 1:  43%|████▎     | 1900/4381 [51:15<1:06:54,  1.62s/it, loss=5.04, v_num=641]Epoch 1:  44%|████▎     | 1910/4381 [51:33<1:06:40,  1.62s/it, loss=5.04, v_num=641]Epoch 1:  44%|████▎     | 1910/4381 [51:33<1:06:40,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  44%|████▍     | 1920/4381 [51:48<1:06:22,  1.62s/it, loss=5.02, v_num=641]Epoch 1:  44%|████▍     | 1920/4381 [51:48<1:06:22,  1.62s/it, loss=4.95, v_num=641]Epoch 1:  44%|████▍     | 1930/4381 [52:01<1:06:02,  1.62s/it, loss=4.95, v_num=641]Epoch 1:  44%|████▍     | 1930/4381 [52:01<1:06:02,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  44%|████▍     | 1940/4381 [52:20<1:05:48,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  44%|████▍     | 1940/4381 [52:20<1:05:48,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▍     | 1950/4381 [52:36<1:05:32,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▍     | 1950/4381 [52:36<1:05:32,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▍     | 1960/4381 [52:51<1:05:15,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▍     | 1960/4381 [52:51<1:05:15,  1.62s/it, loss=4.96, v_num=641]Epoch 1:  45%|████▍     | 1970/4381 [53:08<1:05:00,  1.62s/it, loss=4.96, v_num=641]Epoch 1:  45%|████▍     | 1970/4381 [53:08<1:05:00,  1.62s/it, loss=4.95, v_num=641]Epoch 1:  45%|████▌     | 1980/4381 [53:25<1:04:45,  1.62s/it, loss=4.95, v_num=641]Epoch 1:  45%|████▌     | 1980/4381 [53:25<1:04:45,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▌     | 1990/4381 [53:40<1:04:27,  1.62s/it, loss=4.97, v_num=641]Epoch 1:  45%|████▌     | 1990/4381 [53:40<1:04:27,  1.62s/it, loss=4.91, v_num=641]Epoch 1:  46%|████▌     | 2000/4381 [53:58<1:04:13,  1.62s/it, loss=4.91, v_num=641]Epoch 1:  46%|████▌     | 2000/4381 [53:58<1:04:13,  1.62s/it, loss=4.87, v_num=641]Epoch 1:  46%|████▌     | 2010/4381 [54:10<1:03:52,  1.62s/it, loss=4.87, v_num=641]Epoch 1:  46%|████▌     | 2010/4381 [54:10<1:03:52,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  46%|████▌     | 2020/4381 [54:25<1:03:34,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  46%|████▌     | 2020/4381 [54:25<1:03:34,  1.62s/it, loss=4.99, v_num=641]Epoch 1:  46%|████▋     | 2030/4381 [54:42<1:03:19,  1.62s/it, loss=4.99, v_num=641]Epoch 1:  46%|████▋     | 2030/4381 [54:42<1:03:19,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  47%|████▋     | 2040/4381 [55:00<1:03:05,  1.62s/it, loss=4.94, v_num=641]Epoch 1:  47%|████▋     | 2040/4381 [55:00<1:03:05,  1.62s/it, loss=4.91, v_num=641]Epoch 1:  47%|████▋     | 2050/4381 [55:14<1:02:46,  1.62s/it, loss=4.91, v_num=641]Epoch 1:  47%|████▋     | 2050/4381 [55:14<1:02:46,  1.62s/it, loss=4.93, v_num=641]Epoch 1:  47%|████▋     | 2060/4381 [55:32<1:02:32,  1.62s/it, loss=4.93, v_num=641]Epoch 1:  47%|████▋     | 2060/4381 [55:32<1:02:32,  1.62s/it, loss=4.89, v_num=641]Epoch 1:  47%|████▋     | 2070/4381 [55:48<1:02:16,  1.62s/it, loss=4.89, v_num=641]Epoch 1:  47%|████▋     | 2070/4381 [55:48<1:02:16,  1.62s/it, loss=4.85, v_num=641]Epoch 1:  47%|████▋     | 2080/4381 [56:04<1:02:00,  1.62s/it, loss=4.85, v_num=641]Epoch 1:  47%|████▋     | 2080/4381 [56:04<1:02:00,  1.62s/it, loss=4.86, v_num=641]Epoch 1:  48%|████▊     | 2090/4381 [56:20<1:01:44,  1.62s/it, loss=4.86, v_num=641]Epoch 1:  48%|████▊     | 2090/4381 [56:20<1:01:44,  1.62s/it, loss=4.87, v_num=641]Epoch 1:  48%|████▊     | 2100/4381 [56:33<1:01:23,  1.61s/it, loss=4.87, v_num=641]Epoch 1:  48%|████▊     | 2100/4381 [56:33<1:01:23,  1.61s/it, loss=4.85, v_num=641]Epoch 1:  48%|████▊     | 2110/4381 [56:48<1:01:06,  1.61s/it, loss=4.85, v_num=641]Epoch 1:  48%|████▊     | 2110/4381 [56:48<1:01:06,  1.61s/it, loss=4.82, v_num=641]Epoch 1:  48%|████▊     | 2120/4381 [57:08<1:00:55,  1.62s/it, loss=4.82, v_num=641]Epoch 1:  48%|████▊     | 2120/4381 [57:08<1:00:55,  1.62s/it, loss=4.81, v_num=641]Epoch 1:  49%|████▊     | 2130/4381 [57:22<1:00:35,  1.62s/it, loss=4.81, v_num=641]Epoch 1:  49%|████▊     | 2130/4381 [57:22<1:00:35,  1.62s/it, loss=4.8, v_num=641] Epoch 1:  49%|████▉     | 2140/4381 [57:38<1:00:19,  1.62s/it, loss=4.8, v_num=641]Epoch 1:  49%|████▉     | 2140/4381 [57:38<1:00:19,  1.62s/it, loss=4.84, v_num=641]Epoch 1:  49%|████▉     | 2150/4381 [57:57<1:00:06,  1.62s/it, loss=4.84, v_num=641]Epoch 1:  49%|████▉     | 2150/4381 [57:57<1:00:06,  1.62s/it, loss=4.84, v_num=641]Epoch 1:  49%|████▉     | 2160/4381 [58:10<59:47,  1.62s/it, loss=4.84, v_num=641]  Epoch 1:  49%|████▉     | 2160/4381 [58:10<59:47,  1.62s/it, loss=4.8, v_num=641] Epoch 1:  50%|████▉     | 2170/4381 [58:26<59:30,  1.61s/it, loss=4.8, v_num=641]Epoch 1:  50%|████▉     | 2170/4381 [58:26<59:30,  1.61s/it, loss=4.79, v_num=641]Epoch 1:  50%|████▉     | 2180/4381 [58:44<59:17,  1.62s/it, loss=4.79, v_num=641]Epoch 1:  50%|████▉     | 2180/4381 [58:44<59:17,  1.62s/it, loss=4.76, v_num=641]Epoch 1:  50%|████▉     | 2190/4381 [58:58<58:58,  1.62s/it, loss=4.76, v_num=641]Epoch 1:  50%|████▉     | 2190/4381 [58:58<58:58,  1.62s/it, loss=4.81, v_num=641]Epoch 1:  50%|█████     | 2200/4381 [59:11<58:39,  1.61s/it, loss=4.81, v_num=641]Epoch 1:  50%|█████     | 2200/4381 [59:11<58:39,  1.61s/it, loss=4.84, v_num=641]Epoch 1:  50%|█████     | 2210/4381 [59:31<58:26,  1.62s/it, loss=4.84, v_num=641]Epoch 1:  50%|█████     | 2210/4381 [59:31<58:26,  1.62s/it, loss=4.83, v_num=641]Epoch 1:  51%|█████     | 2220/4381 [59:43<58:06,  1.61s/it, loss=4.83, v_num=641]Epoch 1:  51%|█████     | 2220/4381 [59:43<58:06,  1.61s/it, loss=4.84, v_num=641]Epoch 1:  51%|█████     | 2230/4381 [1:00:05<57:56,  1.62s/it, loss=4.84, v_num=641]Epoch 1:  51%|█████     | 2230/4381 [1:00:05<57:56,  1.62s/it, loss=4.82, v_num=641]Epoch 1:  51%|█████     | 2240/4381 [1:00:20<57:38,  1.62s/it, loss=4.82, v_num=641]Epoch 1:  51%|█████     | 2240/4381 [1:00:20<57:38,  1.62s/it, loss=4.81, v_num=641]Epoch 1:  51%|█████▏    | 2250/4381 [1:00:37<57:23,  1.62s/it, loss=4.81, v_num=641]Epoch 1:  51%|█████▏    | 2250/4381 [1:00:37<57:23,  1.62s/it, loss=4.8, v_num=641] Epoch 1:  52%|█████▏    | 2260/4381 [1:00:53<57:07,  1.62s/it, loss=4.8, v_num=641]Epoch 1:  52%|█████▏    | 2260/4381 [1:00:53<57:07,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  52%|█████▏    | 2270/4381 [1:01:09<56:50,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  52%|█████▏    | 2270/4381 [1:01:09<56:50,  1.62s/it, loss=4.79, v_num=641]Epoch 1:  52%|█████▏    | 2280/4381 [1:01:24<56:33,  1.62s/it, loss=4.79, v_num=641]Epoch 1:  52%|█████▏    | 2280/4381 [1:01:24<56:33,  1.62s/it, loss=4.82, v_num=641]Epoch 1:  52%|█████▏    | 2290/4381 [1:01:42<56:19,  1.62s/it, loss=4.82, v_num=641]Epoch 1:  52%|█████▏    | 2290/4381 [1:01:42<56:19,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  52%|█████▏    | 2300/4381 [1:01:59<56:03,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  52%|█████▏    | 2300/4381 [1:01:59<56:03,  1.62s/it, loss=4.73, v_num=641]Epoch 1:  53%|█████▎    | 2310/4381 [1:02:15<55:47,  1.62s/it, loss=4.73, v_num=641]Epoch 1:  53%|█████▎    | 2310/4381 [1:02:15<55:47,  1.62s/it, loss=4.76, v_num=641]Epoch 1:  53%|█████▎    | 2320/4381 [1:02:29<55:29,  1.62s/it, loss=4.76, v_num=641]Epoch 1:  53%|█████▎    | 2320/4381 [1:02:29<55:29,  1.62s/it, loss=4.79, v_num=641]Epoch 1:  53%|█████▎    | 2330/4381 [1:02:47<55:15,  1.62s/it, loss=4.79, v_num=641]Epoch 1:  53%|█████▎    | 2330/4381 [1:02:47<55:15,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  53%|█████▎    | 2340/4381 [1:03:03<54:58,  1.62s/it, loss=4.78, v_num=641]Epoch 1:  53%|█████▎    | 2340/4381 [1:03:03<54:58,  1.62s/it, loss=4.73, v_num=641]Epoch 1:  54%|█████▎    | 2350/4381 [1:03:18<54:41,  1.62s/it, loss=4.73, v_num=641]Epoch 1:  54%|█████▎    | 2350/4381 [1:03:18<54:41,  1.62s/it, loss=4.74, v_num=641]Epoch 1:  54%|█████▍    | 2360/4381 [1:03:37<54:28,  1.62s/it, loss=4.74, v_num=641]Epoch 1:  54%|█████▍    | 2360/4381 [1:03:37<54:28,  1.62s/it, loss=4.72, v_num=641]Epoch 1:  54%|█████▍    | 2370/4381 [1:03:51<54:09,  1.62s/it, loss=4.72, v_num=641]Epoch 1:  54%|█████▍    | 2370/4381 [1:03:51<54:09,  1.62s/it, loss=4.7, v_num=641] Epoch 1:  54%|█████▍    | 2380/4381 [1:04:07<53:53,  1.62s/it, loss=4.7, v_num=641]Epoch 1:  54%|█████▍    | 2380/4381 [1:04:07<53:53,  1.62s/it, loss=4.71, v_num=641]Epoch 1:  55%|█████▍    | 2390/4381 [1:04:27<53:40,  1.62s/it, loss=4.71, v_num=641]Epoch 1:  55%|█████▍    | 2390/4381 [1:04:27<53:40,  1.62s/it, loss=4.69, v_num=641]Epoch 1:  55%|█████▍    | 2400/4381 [1:04:41<53:22,  1.62s/it, loss=4.69, v_num=641]Epoch 1:  55%|█████▍    | 2400/4381 [1:04:41<53:22,  1.62s/it, loss=4.63, v_num=641]Epoch 1:  55%|█████▌    | 2410/4381 [1:04:56<53:05,  1.62s/it, loss=4.63, v_num=641]Epoch 1:  55%|█████▌    | 2410/4381 [1:04:56<53:05,  1.62s/it, loss=4.67, v_num=641]Epoch 1:  55%|█████▌    | 2420/4381 [1:05:12<52:49,  1.62s/it, loss=4.67, v_num=641]Epoch 1:  55%|█████▌    | 2420/4381 [1:05:12<52:49,  1.62s/it, loss=4.71, v_num=641]Epoch 1:  55%|█████▌    | 2430/4381 [1:05:25<52:30,  1.61s/it, loss=4.71, v_num=641]Epoch 1:  55%|█████▌    | 2430/4381 [1:05:25<52:30,  1.61s/it, loss=4.68, v_num=641]Epoch 1:  56%|█████▌    | 2440/4381 [1:05:39<52:12,  1.61s/it, loss=4.68, v_num=641]Epoch 1:  56%|█████▌    | 2440/4381 [1:05:39<52:12,  1.61s/it, loss=4.65, v_num=641]Epoch 1:  56%|█████▌    | 2450/4381 [1:05:58<51:58,  1.62s/it, loss=4.65, v_num=641]Epoch 1:  56%|█████▌    | 2450/4381 [1:05:58<51:58,  1.62s/it, loss=4.64, v_num=641]Epoch 1:  56%|█████▌    | 2460/4381 [1:06:14<51:42,  1.61s/it, loss=4.64, v_num=641]Epoch 1:  56%|█████▌    | 2460/4381 [1:06:14<51:42,  1.61s/it, loss=4.68, v_num=641]Epoch 1:  56%|█████▋    | 2470/4381 [1:06:29<51:25,  1.61s/it, loss=4.68, v_num=641]Epoch 1:  56%|█████▋    | 2470/4381 [1:06:29<51:25,  1.61s/it, loss=4.71, v_num=641]Epoch 1:  57%|█████▋    | 2480/4381 [1:06:43<51:07,  1.61s/it, loss=4.71, v_num=641]Epoch 1:  57%|█████▋    | 2480/4381 [1:06:43<51:07,  1.61s/it, loss=4.66, v_num=641]Epoch 1:  57%|█████▋    | 2490/4381 [1:06:59<50:51,  1.61s/it, loss=4.66, v_num=641]Epoch 1:  57%|█████▋    | 2490/4381 [1:06:59<50:51,  1.61s/it, loss=4.61, v_num=641]Epoch 1:  57%|█████▋    | 2500/4381 [1:07:16<50:35,  1.61s/it, loss=4.61, v_num=641]Epoch 1:  57%|█████▋    | 2500/4381 [1:07:16<50:35,  1.61s/it, loss=4.64, v_num=641]Epoch 1:  57%|█████▋    | 2510/4381 [1:07:35<50:21,  1.61s/it, loss=4.64, v_num=641]Epoch 1:  57%|█████▋    | 2510/4381 [1:07:35<50:21,  1.61s/it, loss=4.66, v_num=641]Epoch 1:  58%|█████▊    | 2520/4381 [1:07:49<50:04,  1.61s/it, loss=4.66, v_num=641]Epoch 1:  58%|█████▊    | 2520/4381 [1:07:49<50:04,  1.61s/it, loss=4.64, v_num=641]Epoch 1:  58%|█████▊    | 2530/4381 [1:08:07<49:49,  1.61s/it, loss=4.64, v_num=641]Epoch 1:  58%|█████▊    | 2530/4381 [1:08:07<49:49,  1.61s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2540/4381 [1:08:24<49:34,  1.62s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2540/4381 [1:08:24<49:34,  1.62s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2550/4381 [1:08:37<49:15,  1.61s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2550/4381 [1:08:37<49:15,  1.61s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2560/4381 [1:08:53<48:58,  1.61s/it, loss=4.63, v_num=641]Epoch 1:  58%|█████▊    | 2560/4381 [1:08:53<48:58,  1.61s/it, loss=4.62, v_num=641]Epoch 1:  59%|█████▊    | 2570/4381 [1:09:07<48:41,  1.61s/it, loss=4.62, v_num=641]Epoch 1:  59%|█████▊    | 2570/4381 [1:09:07<48:41,  1.61s/it, loss=4.6, v_num=641] Epoch 1:  59%|█████▉    | 2580/4381 [1:09:22<48:24,  1.61s/it, loss=4.6, v_num=641]Epoch 1:  59%|█████▉    | 2580/4381 [1:09:22<48:24,  1.61s/it, loss=4.57, v_num=641]Epoch 1:  59%|█████▉    | 2590/4381 [1:09:39<48:09,  1.61s/it, loss=4.57, v_num=641]Epoch 1:  59%|█████▉    | 2590/4381 [1:09:39<48:09,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  59%|█████▉    | 2600/4381 [1:09:56<47:53,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  59%|█████▉    | 2600/4381 [1:09:56<47:53,  1.61s/it, loss=4.57, v_num=641]Epoch 1:  60%|█████▉    | 2610/4381 [1:10:14<47:38,  1.61s/it, loss=4.57, v_num=641]Epoch 1:  60%|█████▉    | 2610/4381 [1:10:14<47:38,  1.61s/it, loss=4.58, v_num=641]Epoch 1:  60%|█████▉    | 2620/4381 [1:10:28<47:21,  1.61s/it, loss=4.58, v_num=641]Epoch 1:  60%|█████▉    | 2620/4381 [1:10:28<47:21,  1.61s/it, loss=4.59, v_num=641]Epoch 1:  60%|██████    | 2630/4381 [1:10:44<47:05,  1.61s/it, loss=4.59, v_num=641]Epoch 1:  60%|██████    | 2630/4381 [1:10:44<47:05,  1.61s/it, loss=4.6, v_num=641] Epoch 1:  60%|██████    | 2640/4381 [1:11:01<46:49,  1.61s/it, loss=4.6, v_num=641]Epoch 1:  60%|██████    | 2640/4381 [1:11:01<46:49,  1.61s/it, loss=4.58, v_num=641]Epoch 1:  60%|██████    | 2650/4381 [1:11:18<46:33,  1.61s/it, loss=4.58, v_num=641]Epoch 1:  60%|██████    | 2650/4381 [1:11:18<46:33,  1.61s/it, loss=4.53, v_num=641]Epoch 1:  61%|██████    | 2660/4381 [1:11:35<46:17,  1.61s/it, loss=4.53, v_num=641]Epoch 1:  61%|██████    | 2660/4381 [1:11:35<46:17,  1.61s/it, loss=4.51, v_num=641]Epoch 1:  61%|██████    | 2670/4381 [1:11:51<46:01,  1.61s/it, loss=4.51, v_num=641]Epoch 1:  61%|██████    | 2670/4381 [1:11:51<46:01,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  61%|██████    | 2680/4381 [1:12:07<45:45,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  61%|██████    | 2680/4381 [1:12:07<45:45,  1.61s/it, loss=4.6, v_num=641] Epoch 1:  61%|██████▏   | 2690/4381 [1:12:26<45:31,  1.62s/it, loss=4.6, v_num=641]Epoch 1:  61%|██████▏   | 2690/4381 [1:12:26<45:31,  1.62s/it, loss=4.6, v_num=641]Epoch 1:  62%|██████▏   | 2700/4381 [1:12:41<45:14,  1.61s/it, loss=4.6, v_num=641]Epoch 1:  62%|██████▏   | 2700/4381 [1:12:41<45:14,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  62%|██████▏   | 2710/4381 [1:12:56<44:57,  1.61s/it, loss=4.56, v_num=641]Epoch 1:  62%|██████▏   | 2710/4381 [1:12:56<44:57,  1.61s/it, loss=4.55, v_num=641]Epoch 1:  62%|██████▏   | 2720/4381 [1:13:14<44:42,  1.62s/it, loss=4.55, v_num=641]Epoch 1:  62%|██████▏   | 2720/4381 [1:13:14<44:42,  1.62s/it, loss=4.52, v_num=641]Epoch 1:  62%|██████▏   | 2730/4381 [1:13:30<44:26,  1.62s/it, loss=4.52, v_num=641]Epoch 1:  62%|██████▏   | 2730/4381 [1:13:30<44:26,  1.62s/it, loss=4.5, v_num=641] Epoch 1:  63%|██████▎   | 2740/4381 [1:13:42<44:07,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  63%|██████▎   | 2740/4381 [1:13:42<44:07,  1.61s/it, loss=4.54, v_num=641]Epoch 1:  63%|██████▎   | 2750/4381 [1:14:03<43:54,  1.62s/it, loss=4.54, v_num=641]Epoch 1:  63%|██████▎   | 2750/4381 [1:14:03<43:54,  1.62s/it, loss=4.5, v_num=641] Epoch 1:  63%|██████▎   | 2760/4381 [1:14:16<43:36,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  63%|██████▎   | 2760/4381 [1:14:16<43:36,  1.61s/it, loss=4.48, v_num=641]Epoch 1:  63%|██████▎   | 2770/4381 [1:14:30<43:19,  1.61s/it, loss=4.48, v_num=641]Epoch 1:  63%|██████▎   | 2770/4381 [1:14:30<43:19,  1.61s/it, loss=4.54, v_num=641]Epoch 1:  63%|██████▎   | 2780/4381 [1:14:50<43:05,  1.61s/it, loss=4.54, v_num=641]Epoch 1:  63%|██████▎   | 2780/4381 [1:14:50<43:05,  1.61s/it, loss=4.54, v_num=641]Epoch 1:  64%|██████▎   | 2790/4381 [1:15:04<42:48,  1.61s/it, loss=4.54, v_num=641]Epoch 1:  64%|██████▎   | 2790/4381 [1:15:04<42:48,  1.61s/it, loss=4.5, v_num=641] Epoch 1:  64%|██████▍   | 2800/4381 [1:15:21<42:31,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  64%|██████▍   | 2800/4381 [1:15:21<42:31,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  64%|██████▍   | 2810/4381 [1:15:38<42:16,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  64%|██████▍   | 2810/4381 [1:15:38<42:16,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  64%|██████▍   | 2820/4381 [1:15:52<41:59,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  64%|██████▍   | 2820/4381 [1:15:52<41:59,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  65%|██████▍   | 2830/4381 [1:16:05<41:41,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  65%|██████▍   | 2830/4381 [1:16:05<41:41,  1.61s/it, loss=4.5, v_num=641] Epoch 1:  65%|██████▍   | 2840/4381 [1:16:21<41:25,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  65%|██████▍   | 2840/4381 [1:16:21<41:25,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  65%|██████▌   | 2850/4381 [1:16:36<41:08,  1.61s/it, loss=4.5, v_num=641]Epoch 1:  65%|██████▌   | 2850/4381 [1:16:36<41:08,  1.61s/it, loss=4.47, v_num=641]Epoch 1:  65%|██████▌   | 2860/4381 [1:16:50<40:50,  1.61s/it, loss=4.47, v_num=641]Epoch 1:  65%|██████▌   | 2860/4381 [1:16:50<40:50,  1.61s/it, loss=4.43, v_num=641]Epoch 1:  66%|██████▌   | 2870/4381 [1:17:09<40:36,  1.61s/it, loss=4.43, v_num=641]Epoch 1:  66%|██████▌   | 2870/4381 [1:17:09<40:36,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  66%|██████▌   | 2880/4381 [1:17:22<40:18,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  66%|██████▌   | 2880/4381 [1:17:22<40:18,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▌   | 2890/4381 [1:17:38<40:02,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▌   | 2890/4381 [1:17:38<40:02,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▌   | 2900/4381 [1:17:57<39:47,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▌   | 2900/4381 [1:17:57<39:47,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▋   | 2910/4381 [1:18:13<39:31,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  66%|██████▋   | 2910/4381 [1:18:13<39:31,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  67%|██████▋   | 2920/4381 [1:18:28<39:15,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  67%|██████▋   | 2920/4381 [1:18:28<39:15,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  67%|██████▋   | 2930/4381 [1:18:45<38:59,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  67%|██████▋   | 2930/4381 [1:18:45<38:59,  1.61s/it, loss=4.39, v_num=641]Epoch 1:  67%|██████▋   | 2940/4381 [1:19:02<38:43,  1.61s/it, loss=4.39, v_num=641]Epoch 1:  67%|██████▋   | 2940/4381 [1:19:02<38:43,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  67%|██████▋   | 2950/4381 [1:19:15<38:26,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  67%|██████▋   | 2950/4381 [1:19:15<38:26,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  68%|██████▊   | 2960/4381 [1:19:35<38:11,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  68%|██████▊   | 2960/4381 [1:19:35<38:11,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 2970/4381 [1:19:50<37:54,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 2970/4381 [1:19:50<37:54,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 2980/4381 [1:20:05<37:38,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 2980/4381 [1:20:05<37:38,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  68%|██████▊   | 2990/4381 [1:20:23<37:23,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  68%|██████▊   | 2990/4381 [1:20:23<37:23,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 3000/4381 [1:20:40<37:07,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  68%|██████▊   | 3000/4381 [1:20:40<37:07,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  69%|██████▊   | 3010/4381 [1:20:55<36:50,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  69%|██████▊   | 3010/4381 [1:20:55<36:50,  1.61s/it, loss=4.43, v_num=641]Epoch 1:  69%|██████▉   | 3020/4381 [1:21:13<36:35,  1.61s/it, loss=4.43, v_num=641]Epoch 1:  69%|██████▉   | 3020/4381 [1:21:13<36:35,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  69%|██████▉   | 3030/4381 [1:21:26<36:18,  1.61s/it, loss=4.44, v_num=641]Epoch 1:  69%|██████▉   | 3030/4381 [1:21:26<36:18,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  69%|██████▉   | 3040/4381 [1:21:40<36:01,  1.61s/it, loss=4.45, v_num=641]Epoch 1:  69%|██████▉   | 3040/4381 [1:21:40<36:01,  1.61s/it, loss=4.39, v_num=641]Epoch 1:  70%|██████▉   | 3050/4381 [1:21:55<35:44,  1.61s/it, loss=4.39, v_num=641]Epoch 1:  70%|██████▉   | 3050/4381 [1:21:55<35:44,  1.61s/it, loss=4.38, v_num=641]Epoch 1:  70%|██████▉   | 3060/4381 [1:22:11<35:28,  1.61s/it, loss=4.38, v_num=641]Epoch 1:  70%|██████▉   | 3060/4381 [1:22:11<35:28,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  70%|███████   | 3070/4381 [1:22:27<35:11,  1.61s/it, loss=4.41, v_num=641]Epoch 1:  70%|███████   | 3070/4381 [1:22:27<35:11,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  70%|███████   | 3080/4381 [1:22:44<34:56,  1.61s/it, loss=4.42, v_num=641]Epoch 1:  70%|███████   | 3080/4381 [1:22:44<34:56,  1.61s/it, loss=4.4, v_num=641] Epoch 1:  71%|███████   | 3090/4381 [1:23:02<34:41,  1.61s/it, loss=4.4, v_num=641]Epoch 1:  71%|███████   | 3090/4381 [1:23:02<34:41,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  71%|███████   | 3100/4381 [1:23:16<34:23,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  71%|███████   | 3100/4381 [1:23:16<34:23,  1.61s/it, loss=4.33, v_num=641]Epoch 1:  71%|███████   | 3110/4381 [1:23:32<34:07,  1.61s/it, loss=4.33, v_num=641]Epoch 1:  71%|███████   | 3110/4381 [1:23:32<34:07,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  71%|███████   | 3120/4381 [1:23:46<33:50,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  71%|███████   | 3120/4381 [1:23:46<33:50,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  71%|███████▏  | 3130/4381 [1:24:01<33:34,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  71%|███████▏  | 3130/4381 [1:24:01<33:34,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  72%|███████▏  | 3140/4381 [1:24:18<33:18,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  72%|███████▏  | 3140/4381 [1:24:18<33:18,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  72%|███████▏  | 3150/4381 [1:24:36<33:03,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  72%|███████▏  | 3150/4381 [1:24:36<33:03,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  72%|███████▏  | 3160/4381 [1:24:53<32:47,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  72%|███████▏  | 3160/4381 [1:24:53<32:47,  1.61s/it, loss=4.38, v_num=641]Epoch 1:  72%|███████▏  | 3170/4381 [1:25:06<32:30,  1.61s/it, loss=4.38, v_num=641]Epoch 1:  72%|███████▏  | 3170/4381 [1:25:06<32:30,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  73%|███████▎  | 3180/4381 [1:25:22<32:13,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  73%|███████▎  | 3180/4381 [1:25:22<32:13,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  73%|███████▎  | 3190/4381 [1:25:40<31:58,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  73%|███████▎  | 3190/4381 [1:25:40<31:58,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  73%|███████▎  | 3200/4381 [1:25:56<31:42,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  73%|███████▎  | 3200/4381 [1:25:56<31:42,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  73%|███████▎  | 3210/4381 [1:26:10<31:25,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  73%|███████▎  | 3210/4381 [1:26:10<31:25,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  73%|███████▎  | 3220/4381 [1:26:25<31:09,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  73%|███████▎  | 3220/4381 [1:26:25<31:09,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  74%|███████▎  | 3230/4381 [1:26:43<30:53,  1.61s/it, loss=4.31, v_num=641]Epoch 1:  74%|███████▎  | 3230/4381 [1:26:43<30:53,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  74%|███████▍  | 3240/4381 [1:26:57<30:36,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  74%|███████▍  | 3240/4381 [1:26:57<30:36,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  74%|███████▍  | 3250/4381 [1:27:12<30:20,  1.61s/it, loss=4.36, v_num=641]Epoch 1:  74%|███████▍  | 3250/4381 [1:27:12<30:20,  1.61s/it, loss=4.37, v_num=641]Epoch 1:  74%|███████▍  | 3260/4381 [1:27:28<30:04,  1.61s/it, loss=4.37, v_num=641]Epoch 1:  74%|███████▍  | 3260/4381 [1:27:28<30:04,  1.61s/it, loss=4.37, v_num=641]Epoch 1:  75%|███████▍  | 3270/4381 [1:27:47<29:48,  1.61s/it, loss=4.37, v_num=641]Epoch 1:  75%|███████▍  | 3270/4381 [1:27:47<29:48,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  75%|███████▍  | 3280/4381 [1:28:02<29:32,  1.61s/it, loss=4.35, v_num=641]Epoch 1:  75%|███████▍  | 3280/4381 [1:28:02<29:32,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  75%|███████▌  | 3290/4381 [1:28:16<29:15,  1.61s/it, loss=4.32, v_num=641]Epoch 1:  75%|███████▌  | 3290/4381 [1:28:16<29:15,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  75%|███████▌  | 3300/4381 [1:28:33<28:59,  1.61s/it, loss=4.34, v_num=641]Epoch 1:  75%|███████▌  | 3300/4381 [1:28:33<28:59,  1.61s/it, loss=4.3, v_num=641] Epoch 1:  76%|███████▌  | 3310/4381 [1:28:47<28:43,  1.61s/it, loss=4.3, v_num=641]Epoch 1:  76%|███████▌  | 3310/4381 [1:28:47<28:43,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  76%|███████▌  | 3320/4381 [1:29:05<28:27,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  76%|███████▌  | 3320/4381 [1:29:05<28:27,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  76%|███████▌  | 3330/4381 [1:29:22<28:11,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  76%|███████▌  | 3330/4381 [1:29:22<28:11,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  76%|███████▌  | 3340/4381 [1:29:38<27:55,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  76%|███████▌  | 3340/4381 [1:29:38<27:55,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  76%|███████▋  | 3350/4381 [1:29:59<27:41,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  76%|███████▋  | 3350/4381 [1:29:59<27:41,  1.61s/it, loss=4.3, v_num=641] Epoch 1:  77%|███████▋  | 3360/4381 [1:30:13<27:24,  1.61s/it, loss=4.3, v_num=641]Epoch 1:  77%|███████▋  | 3360/4381 [1:30:13<27:24,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  77%|███████▋  | 3370/4381 [1:30:28<27:08,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  77%|███████▋  | 3370/4381 [1:30:28<27:08,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  77%|███████▋  | 3380/4381 [1:30:46<26:52,  1.61s/it, loss=4.29, v_num=641]Epoch 1:  77%|███████▋  | 3380/4381 [1:30:46<26:52,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  77%|███████▋  | 3390/4381 [1:31:02<26:36,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  77%|███████▋  | 3390/4381 [1:31:02<26:36,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  78%|███████▊  | 3400/4381 [1:31:16<26:19,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  78%|███████▊  | 3400/4381 [1:31:16<26:19,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  78%|███████▊  | 3410/4381 [1:31:32<26:03,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  78%|███████▊  | 3410/4381 [1:31:32<26:03,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  78%|███████▊  | 3420/4381 [1:31:45<25:46,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  78%|███████▊  | 3420/4381 [1:31:45<25:46,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  78%|███████▊  | 3430/4381 [1:32:00<25:30,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  78%|███████▊  | 3430/4381 [1:32:00<25:30,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  79%|███████▊  | 3440/4381 [1:32:15<25:13,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  79%|███████▊  | 3440/4381 [1:32:15<25:13,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  79%|███████▊  | 3450/4381 [1:32:34<24:58,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  79%|███████▊  | 3450/4381 [1:32:34<24:58,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  79%|███████▉  | 3460/4381 [1:32:48<24:41,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  79%|███████▉  | 3460/4381 [1:32:48<24:41,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  79%|███████▉  | 3470/4381 [1:33:06<24:26,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  79%|███████▉  | 3470/4381 [1:33:06<24:26,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  79%|███████▉  | 3480/4381 [1:33:22<24:10,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  79%|███████▉  | 3480/4381 [1:33:22<24:10,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  80%|███████▉  | 3490/4381 [1:33:37<23:53,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  80%|███████▉  | 3490/4381 [1:33:37<23:53,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  80%|███████▉  | 3500/4381 [1:33:54<23:37,  1.61s/it, loss=4.25, v_num=641]Epoch 1:  80%|███████▉  | 3500/4381 [1:33:54<23:37,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  80%|████████  | 3510/4381 [1:34:11<23:21,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  80%|████████  | 3510/4381 [1:34:11<23:21,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  80%|████████  | 3520/4381 [1:34:26<23:05,  1.61s/it, loss=4.26, v_num=641]Epoch 1:  80%|████████  | 3520/4381 [1:34:26<23:05,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  81%|████████  | 3530/4381 [1:34:42<22:49,  1.61s/it, loss=4.28, v_num=641]Epoch 1:  81%|████████  | 3530/4381 [1:34:42<22:49,  1.61s/it, loss=4.3, v_num=641] Epoch 1:  81%|████████  | 3540/4381 [1:34:56<22:32,  1.61s/it, loss=4.3, v_num=641]Epoch 1:  81%|████████  | 3540/4381 [1:34:56<22:32,  1.61s/it, loss=4.23, v_num=641]Epoch 1:  81%|████████  | 3550/4381 [1:35:13<22:17,  1.61s/it, loss=4.23, v_num=641]Epoch 1:  81%|████████  | 3550/4381 [1:35:13<22:17,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  81%|████████▏ | 3560/4381 [1:35:34<22:02,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  81%|████████▏ | 3560/4381 [1:35:34<22:02,  1.61s/it, loss=4.17, v_num=641]Epoch 1:  81%|████████▏ | 3570/4381 [1:35:48<21:45,  1.61s/it, loss=4.17, v_num=641]Epoch 1:  81%|████████▏ | 3570/4381 [1:35:48<21:45,  1.61s/it, loss=4.16, v_num=641]Epoch 1:  82%|████████▏ | 3580/4381 [1:36:03<21:29,  1.61s/it, loss=4.16, v_num=641]Epoch 1:  82%|████████▏ | 3580/4381 [1:36:03<21:29,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  82%|████████▏ | 3590/4381 [1:36:20<21:13,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  82%|████████▏ | 3590/4381 [1:36:20<21:13,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  82%|████████▏ | 3600/4381 [1:36:36<20:57,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  82%|████████▏ | 3600/4381 [1:36:36<20:57,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  82%|████████▏ | 3610/4381 [1:36:51<20:40,  1.61s/it, loss=4.24, v_num=641]Epoch 1:  82%|████████▏ | 3610/4381 [1:36:51<20:40,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  83%|████████▎ | 3620/4381 [1:37:08<20:25,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  83%|████████▎ | 3620/4381 [1:37:08<20:25,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  83%|████████▎ | 3630/4381 [1:37:24<20:08,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  83%|████████▎ | 3630/4381 [1:37:24<20:08,  1.61s/it, loss=4.2, v_num=641] Epoch 1:  83%|████████▎ | 3640/4381 [1:37:41<19:52,  1.61s/it, loss=4.2, v_num=641]Epoch 1:  83%|████████▎ | 3640/4381 [1:37:41<19:52,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  83%|████████▎ | 3650/4381 [1:37:59<19:37,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  83%|████████▎ | 3650/4381 [1:37:59<19:37,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  84%|████████▎ | 3660/4381 [1:38:13<19:20,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  84%|████████▎ | 3660/4381 [1:38:13<19:20,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  84%|████████▍ | 3670/4381 [1:38:31<19:04,  1.61s/it, loss=4.21, v_num=641]Epoch 1:  84%|████████▍ | 3670/4381 [1:38:31<19:04,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  84%|████████▍ | 3680/4381 [1:38:46<18:48,  1.61s/it, loss=4.22, v_num=641]Epoch 1:  84%|████████▍ | 3680/4381 [1:38:46<18:48,  1.61s/it, loss=4.2, v_num=641] Epoch 1:  84%|████████▍ | 3690/4381 [1:39:04<18:32,  1.61s/it, loss=4.2, v_num=641]Epoch 1:  84%|████████▍ | 3690/4381 [1:39:04<18:32,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  84%|████████▍ | 3700/4381 [1:39:18<18:16,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  84%|████████▍ | 3700/4381 [1:39:18<18:16,  1.61s/it, loss=4.15, v_num=641]Epoch 1:  85%|████████▍ | 3710/4381 [1:39:34<18:00,  1.61s/it, loss=4.15, v_num=641]Epoch 1:  85%|████████▍ | 3710/4381 [1:39:34<18:00,  1.61s/it, loss=4.16, v_num=641]Epoch 1:  85%|████████▍ | 3720/4381 [1:39:52<17:44,  1.61s/it, loss=4.16, v_num=641]Epoch 1:  85%|████████▍ | 3720/4381 [1:39:52<17:44,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  85%|████████▌ | 3730/4381 [1:40:03<17:27,  1.61s/it, loss=4.19, v_num=641]Epoch 1:  85%|████████▌ | 3730/4381 [1:40:03<17:27,  1.61s/it, loss=4.17, v_num=641]Epoch 1:  85%|████████▌ | 3740/4381 [1:40:06<17:09,  1.61s/it, loss=4.17, v_num=641]Epoch 1:  85%|████████▌ | 3740/4381 [1:40:06<17:09,  1.61s/it, loss=4.18, v_num=641]Epoch 1:  86%|████████▌ | 3750/4381 [1:40:10<16:51,  1.60s/it, loss=4.18, v_num=641]Epoch 1:  86%|████████▌ | 3750/4381 [1:40:10<16:51,  1.60s/it, loss=4.2, v_num=641] Epoch 1:  86%|████████▌ | 3760/4381 [1:40:11<16:32,  1.60s/it, loss=4.2, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][A
Validating:   2%|▏         | 10/626 [00:04<04:17,  2.40it/s][AEpoch 1:  86%|████████▌ | 3770/4381 [1:40:15<16:14,  1.60s/it, loss=4.2, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:31,  3.99it/s][AEpoch 1:  86%|████████▋ | 3780/4381 [1:40:16<15:56,  1.59s/it, loss=4.2, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:32,  3.92it/s][AEpoch 1:  87%|████████▋ | 3790/4381 [1:40:19<15:38,  1.59s/it, loss=4.2, v_num=641]
Validating:   6%|▋         | 40/626 [00:09<01:58,  4.93it/s][AEpoch 1:  87%|████████▋ | 3800/4381 [1:40:20<15:20,  1.58s/it, loss=4.2, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:44,  5.54it/s][AEpoch 1:  87%|████████▋ | 3810/4381 [1:40:22<15:02,  1.58s/it, loss=4.2, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:43,  5.48it/s][AEpoch 1:  87%|████████▋ | 3820/4381 [1:40:23<14:44,  1.58s/it, loss=4.2, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:47,  5.17it/s][AEpoch 1:  87%|████████▋ | 3830/4381 [1:40:26<14:26,  1.57s/it, loss=4.2, v_num=641]
Validating:  13%|█▎        | 80/626 [00:16<01:34,  5.77it/s][AEpoch 1:  88%|████████▊ | 3840/4381 [1:40:27<14:08,  1.57s/it, loss=4.2, v_num=641]
Validating:  14%|█▍        | 90/626 [00:18<01:37,  5.52it/s][AEpoch 1:  88%|████████▊ | 3850/4381 [1:40:29<13:51,  1.57s/it, loss=4.2, v_num=641]
Validating:  16%|█▌        | 100/626 [00:19<01:37,  5.42it/s][AEpoch 1:  88%|████████▊ | 3860/4381 [1:40:31<13:33,  1.56s/it, loss=4.2, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:34,  5.44it/s][AEpoch 1:  88%|████████▊ | 3870/4381 [1:40:33<13:16,  1.56s/it, loss=4.2, v_num=641]
Validating:  19%|█▉        | 120/626 [00:23<01:29,  5.66it/s][AEpoch 1:  89%|████████▊ | 3880/4381 [1:40:34<12:59,  1.55s/it, loss=4.2, v_num=641]
Validating:  21%|██        | 130/626 [00:26<01:41,  4.90it/s][AEpoch 1:  89%|████████▉ | 3890/4381 [1:40:37<12:41,  1.55s/it, loss=4.2, v_num=641]
Validating:  22%|██▏       | 140/626 [00:28<01:46,  4.58it/s][AEpoch 1:  89%|████████▉ | 3900/4381 [1:40:39<12:24,  1.55s/it, loss=4.2, v_num=641]
Validating:  24%|██▍       | 150/626 [00:30<01:41,  4.70it/s][AEpoch 1:  89%|████████▉ | 3910/4381 [1:40:41<12:07,  1.54s/it, loss=4.2, v_num=641]
Validating:  26%|██▌       | 160/626 [00:31<01:27,  5.34it/s][AEpoch 1:  89%|████████▉ | 3920/4381 [1:40:43<11:50,  1.54s/it, loss=4.2, v_num=641]
Validating:  27%|██▋       | 170/626 [00:34<01:29,  5.08it/s][AEpoch 1:  90%|████████▉ | 3930/4381 [1:40:45<11:33,  1.54s/it, loss=4.2, v_num=641]
Validating:  29%|██▉       | 180/626 [00:36<01:29,  4.97it/s][AEpoch 1:  90%|████████▉ | 3940/4381 [1:40:47<11:16,  1.53s/it, loss=4.2, v_num=641]
Validating:  30%|███       | 190/626 [00:38<01:31,  4.77it/s][AEpoch 1:  90%|█████████ | 3950/4381 [1:40:49<10:59,  1.53s/it, loss=4.2, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:27,  4.85it/s][AEpoch 1:  90%|█████████ | 3960/4381 [1:40:51<10:43,  1.53s/it, loss=4.2, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:21,  5.13it/s][AEpoch 1:  91%|█████████ | 3970/4381 [1:40:53<10:26,  1.52s/it, loss=4.2, v_num=641]
Validating:  35%|███▌      | 220/626 [00:44<01:30,  4.51it/s][AEpoch 1:  91%|█████████ | 3980/4381 [1:40:56<10:10,  1.52s/it, loss=4.2, v_num=641]
Validating:  37%|███▋      | 230/626 [00:47<01:29,  4.41it/s][AEpoch 1:  91%|█████████ | 3990/4381 [1:40:58<09:53,  1.52s/it, loss=4.2, v_num=641]
Validating:  38%|███▊      | 240/626 [00:48<01:16,  5.07it/s][AEpoch 1:  91%|█████████▏| 4000/4381 [1:40:59<09:37,  1.51s/it, loss=4.2, v_num=641]
Validating:  40%|███▉      | 250/626 [00:50<01:12,  5.15it/s][AEpoch 1:  92%|█████████▏| 4010/4381 [1:41:01<09:20,  1.51s/it, loss=4.2, v_num=641]
Validating:  42%|████▏     | 260/626 [00:53<01:24,  4.35it/s][AEpoch 1:  92%|█████████▏| 4020/4381 [1:41:04<09:04,  1.51s/it, loss=4.2, v_num=641]
Validating:  43%|████▎     | 270/626 [00:55<01:14,  4.79it/s][AEpoch 1:  92%|█████████▏| 4030/4381 [1:41:06<08:48,  1.50s/it, loss=4.2, v_num=641]
Validating:  45%|████▍     | 280/626 [00:59<01:32,  3.73it/s][AEpoch 1:  92%|█████████▏| 4040/4381 [1:41:10<08:32,  1.50s/it, loss=4.2, v_num=641]
Validating:  46%|████▋     | 290/626 [01:01<01:22,  4.07it/s][AEpoch 1:  92%|█████████▏| 4050/4381 [1:41:12<08:16,  1.50s/it, loss=4.2, v_num=641]
Validating:  48%|████▊     | 300/626 [01:02<01:11,  4.57it/s][AEpoch 1:  93%|█████████▎| 4060/4381 [1:41:14<08:00,  1.50s/it, loss=4.2, v_num=641]
Validating:  50%|████▉     | 310/626 [01:04<01:02,  5.04it/s][AEpoch 1:  93%|█████████▎| 4070/4381 [1:41:15<07:44,  1.49s/it, loss=4.2, v_num=641]
Validating:  51%|█████     | 320/626 [01:05<00:53,  5.76it/s][AEpoch 1:  93%|█████████▎| 4080/4381 [1:41:16<07:28,  1.49s/it, loss=4.2, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:07<00:51,  5.70it/s][AEpoch 1:  93%|█████████▎| 4090/4381 [1:41:18<07:12,  1.49s/it, loss=4.2, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:08<00:48,  5.85it/s][AEpoch 1:  94%|█████████▎| 4100/4381 [1:41:20<06:56,  1.48s/it, loss=4.2, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:11<00:56,  4.90it/s][AEpoch 1:  94%|█████████▍| 4110/4381 [1:41:23<06:40,  1.48s/it, loss=4.2, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:13<00:51,  5.19it/s][AEpoch 1:  94%|█████████▍| 4120/4381 [1:41:24<06:25,  1.48s/it, loss=4.2, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:14<00:41,  6.18it/s][AEpoch 1:  94%|█████████▍| 4130/4381 [1:41:25<06:09,  1.47s/it, loss=4.2, v_num=641]
Validating:  61%|██████    | 380/626 [01:16<00:41,  5.93it/s][AEpoch 1:  94%|█████████▍| 4140/4381 [1:41:27<05:54,  1.47s/it, loss=4.2, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:17<00:38,  6.06it/s][AEpoch 1:  95%|█████████▍| 4150/4381 [1:41:28<05:38,  1.47s/it, loss=4.2, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:18<00:34,  6.64it/s][AEpoch 1:  95%|█████████▍| 4160/4381 [1:41:30<05:23,  1.46s/it, loss=4.2, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:20<00:32,  6.70it/s][AEpoch 1:  95%|█████████▌| 4170/4381 [1:41:31<05:08,  1.46s/it, loss=4.2, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:22<00:37,  5.55it/s][AEpoch 1:  95%|█████████▌| 4180/4381 [1:41:34<04:52,  1.46s/it, loss=4.2, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:24<00:33,  5.80it/s][AEpoch 1:  96%|█████████▌| 4190/4381 [1:41:35<04:37,  1.45s/it, loss=4.2, v_num=641]
Validating:  70%|███████   | 440/626 [01:26<00:34,  5.35it/s][AEpoch 1:  96%|█████████▌| 4200/4381 [1:41:37<04:22,  1.45s/it, loss=4.2, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:28<00:32,  5.46it/s][AEpoch 1:  96%|█████████▌| 4210/4381 [1:41:39<04:07,  1.45s/it, loss=4.2, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:29<00:29,  5.63it/s][AEpoch 1:  96%|█████████▋| 4220/4381 [1:41:41<03:52,  1.45s/it, loss=4.2, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:32<00:30,  5.16it/s][AEpoch 1:  97%|█████████▋| 4230/4381 [1:41:43<03:37,  1.44s/it, loss=4.2, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:34<00:30,  4.72it/s][AEpoch 1:  97%|█████████▋| 4240/4381 [1:41:46<03:23,  1.44s/it, loss=4.2, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:37<00:29,  4.62it/s][AEpoch 1:  97%|█████████▋| 4250/4381 [1:41:48<03:08,  1.44s/it, loss=4.2, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:38<00:25,  4.96it/s][AEpoch 1:  97%|█████████▋| 4260/4381 [1:41:50<02:53,  1.43s/it, loss=4.2, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:41<00:25,  4.55it/s][AEpoch 1:  97%|█████████▋| 4270/4381 [1:41:52<02:38,  1.43s/it, loss=4.2, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:42<00:19,  5.42it/s][AEpoch 1:  98%|█████████▊| 4280/4381 [1:41:53<02:24,  1.43s/it, loss=4.2, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:44<00:18,  5.11it/s][AEpoch 1:  98%|█████████▊| 4290/4381 [1:41:55<02:09,  1.43s/it, loss=4.2, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:46<00:15,  5.59it/s][AEpoch 1:  98%|█████████▊| 4300/4381 [1:41:57<01:55,  1.42s/it, loss=4.2, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:47<00:12,  5.92it/s][AEpoch 1:  98%|█████████▊| 4310/4381 [1:41:58<01:40,  1.42s/it, loss=4.2, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:49<00:11,  5.90it/s][AEpoch 1:  99%|█████████▊| 4320/4381 [1:42:00<01:26,  1.42s/it, loss=4.2, v_num=641]
Validating:  91%|█████████ | 570/626 [01:50<00:09,  6.08it/s][AEpoch 1:  99%|█████████▉| 4330/4381 [1:42:02<01:12,  1.41s/it, loss=4.2, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:53<00:08,  5.35it/s][AEpoch 1:  99%|█████████▉| 4340/4381 [1:42:04<00:57,  1.41s/it, loss=4.2, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:54<00:06,  5.41it/s][AEpoch 1:  99%|█████████▉| 4350/4381 [1:42:06<00:43,  1.41s/it, loss=4.2, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:56<00:04,  5.75it/s][AEpoch 1: 100%|█████████▉| 4360/4381 [1:42:07<00:29,  1.41s/it, loss=4.2, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:57<00:02,  6.66it/s][AEpoch 1: 100%|█████████▉| 4370/4381 [1:42:08<00:15,  1.40s/it, loss=4.2, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:58<00:00,  6.85it/s][AEpoch 1: 100%|█████████▉| 4380/4381 [1:42:10<00:01,  1.40s/it, loss=4.2, v_num=641]
Validating: 100%|██████████| 626/626 [01:58<00:00,  8.03it/s][Avalidation_epoch_end
graph acc: 0.0
valid accuracy: 0.813194751739502
Epoch 1: 100%|██████████| 4381/4381 [1:42:11<00:00,  1.40s/it, loss=4.19, v_num=641]
                                                             [AEpoch 1:   0%|          | 0/4381 [00:00<00:00, 11459.85it/s, loss=4.19, v_num=641]  Epoch 2:   0%|          | 0/4381 [00:00<00:01, 2953.74it/s, loss=4.19, v_num=641] Epoch 2:   0%|          | 0/4381 [00:15<18:18:19, 15.04s/it, loss=4.19, v_num=641]Epoch 2:   0%|          | 10/4381 [00:20<2:18:38,  1.90s/it, loss=4.19, v_num=641]Epoch 2:   0%|          | 10/4381 [00:20<2:18:38,  1.90s/it, loss=4.17, v_num=641]Epoch 2:   0%|          | 20/4381 [00:37<2:08:43,  1.77s/it, loss=4.17, v_num=641]Epoch 2:   0%|          | 20/4381 [00:37<2:08:43,  1.77s/it, loss=4.19, v_num=641]Epoch 2:   1%|          | 30/4381 [00:56<2:13:14,  1.84s/it, loss=4.19, v_num=641]Epoch 2:   1%|          | 30/4381 [00:56<2:13:14,  1.84s/it, loss=4.19, v_num=641]Epoch 2:   1%|          | 40/4381 [01:12<2:07:37,  1.76s/it, loss=4.19, v_num=641]Epoch 2:   1%|          | 40/4381 [01:12<2:07:37,  1.76s/it, loss=4.18, v_num=641]Epoch 2:   1%|          | 50/4381 [01:25<2:01:19,  1.68s/it, loss=4.18, v_num=641]Epoch 2:   1%|          | 50/4381 [01:25<2:01:19,  1.68s/it, loss=4.19, v_num=641]Epoch 2:   1%|▏         | 60/4381 [01:44<2:03:01,  1.71s/it, loss=4.19, v_num=641]Epoch 2:   1%|▏         | 60/4381 [01:44<2:03:01,  1.71s/it, loss=4.2, v_num=641] Epoch 2:   2%|▏         | 70/4381 [02:00<2:01:34,  1.69s/it, loss=4.2, v_num=641]Epoch 2:   2%|▏         | 70/4381 [02:00<2:01:34,  1.69s/it, loss=4.15, v_num=641]Epoch 2:   2%|▏         | 80/4381 [02:15<2:00:17,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   2%|▏         | 80/4381 [02:15<2:00:17,  1.68s/it, loss=4.11, v_num=641]Epoch 2:   2%|▏         | 90/4381 [02:35<2:01:53,  1.70s/it, loss=4.11, v_num=641]Epoch 2:   2%|▏         | 90/4381 [02:35<2:01:53,  1.70s/it, loss=4.12, v_num=641]Epoch 2:   2%|▏         | 100/4381 [02:49<1:59:44,  1.68s/it, loss=4.12, v_num=641]Epoch 2:   2%|▏         | 100/4381 [02:49<1:59:44,  1.68s/it, loss=4.12, v_num=641]Epoch 2:   3%|▎         | 110/4381 [03:09<2:01:16,  1.70s/it, loss=4.12, v_num=641]Epoch 2:   3%|▎         | 110/4381 [03:09<2:01:16,  1.70s/it, loss=4.11, v_num=641]Epoch 2:   3%|▎         | 120/4381 [03:25<2:00:37,  1.70s/it, loss=4.11, v_num=641]Epoch 2:   3%|▎         | 120/4381 [03:25<2:00:37,  1.70s/it, loss=4.13, v_num=641]Epoch 2:   3%|▎         | 130/4381 [03:41<1:59:43,  1.69s/it, loss=4.13, v_num=641]Epoch 2:   3%|▎         | 130/4381 [03:41<1:59:43,  1.69s/it, loss=4.15, v_num=641]Epoch 2:   3%|▎         | 140/4381 [03:56<1:58:22,  1.67s/it, loss=4.15, v_num=641]Epoch 2:   3%|▎         | 140/4381 [03:56<1:58:22,  1.67s/it, loss=4.14, v_num=641]Epoch 2:   3%|▎         | 150/4381 [04:14<1:59:01,  1.69s/it, loss=4.14, v_num=641]Epoch 2:   3%|▎         | 150/4381 [04:14<1:59:01,  1.69s/it, loss=4.15, v_num=641]Epoch 2:   4%|▎         | 160/4381 [04:29<1:57:51,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   4%|▎         | 160/4381 [04:29<1:57:51,  1.68s/it, loss=4.16, v_num=641]Epoch 2:   4%|▍         | 170/4381 [04:45<1:57:21,  1.67s/it, loss=4.16, v_num=641]Epoch 2:   4%|▍         | 170/4381 [04:45<1:57:21,  1.67s/it, loss=4.14, v_num=641]Epoch 2:   4%|▍         | 180/4381 [05:02<1:56:52,  1.67s/it, loss=4.14, v_num=641]Epoch 2:   4%|▍         | 180/4381 [05:02<1:56:52,  1.67s/it, loss=4.12, v_num=641]Epoch 2:   4%|▍         | 190/4381 [05:20<1:57:06,  1.68s/it, loss=4.12, v_num=641]Epoch 2:   4%|▍         | 190/4381 [05:20<1:57:06,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   5%|▍         | 200/4381 [05:37<1:57:03,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   5%|▍         | 200/4381 [05:37<1:57:03,  1.68s/it, loss=4.14, v_num=641]Epoch 2:   5%|▍         | 210/4381 [05:56<1:57:30,  1.69s/it, loss=4.14, v_num=641]Epoch 2:   5%|▍         | 210/4381 [05:56<1:57:30,  1.69s/it, loss=4.12, v_num=641]Epoch 2:   5%|▌         | 220/4381 [06:10<1:56:15,  1.68s/it, loss=4.12, v_num=641]Epoch 2:   5%|▌         | 220/4381 [06:10<1:56:15,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   5%|▌         | 230/4381 [06:26<1:55:52,  1.67s/it, loss=4.15, v_num=641]Epoch 2:   5%|▌         | 230/4381 [06:26<1:55:52,  1.67s/it, loss=4.15, v_num=641]Epoch 2:   5%|▌         | 240/4381 [06:48<1:56:58,  1.70s/it, loss=4.15, v_num=641]Epoch 2:   5%|▌         | 240/4381 [06:48<1:56:59,  1.70s/it, loss=4.16, v_num=641]Epoch 2:   6%|▌         | 250/4381 [07:03<1:56:12,  1.69s/it, loss=4.16, v_num=641]Epoch 2:   6%|▌         | 250/4381 [07:03<1:56:12,  1.69s/it, loss=4.15, v_num=641]Epoch 2:   6%|▌         | 260/4381 [07:17<1:55:14,  1.68s/it, loss=4.15, v_num=641]Epoch 2:   6%|▌         | 260/4381 [07:17<1:55:14,  1.68s/it, loss=4.1, v_num=641] Epoch 2:   6%|▌         | 270/4381 [07:37<1:55:39,  1.69s/it, loss=4.1, v_num=641]Epoch 2:   6%|▌         | 270/4381 [07:37<1:55:39,  1.69s/it, loss=4.08, v_num=641]Epoch 2:   6%|▋         | 280/4381 [07:51<1:54:47,  1.68s/it, loss=4.08, v_num=641]Epoch 2:   6%|▋         | 280/4381 [07:51<1:54:47,  1.68s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 290/4381 [08:04<1:53:33,  1.67s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 290/4381 [08:04<1:53:33,  1.67s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 300/4381 [08:20<1:53:09,  1.66s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 300/4381 [08:20<1:53:09,  1.66s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 310/4381 [08:38<1:53:03,  1.67s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 310/4381 [08:38<1:53:03,  1.67s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 320/4381 [08:51<1:51:58,  1.65s/it, loss=4.09, v_num=641]Epoch 2:   7%|▋         | 320/4381 [08:51<1:51:58,  1.65s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 330/4381 [09:08<1:51:56,  1.66s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 330/4381 [09:08<1:51:56,  1.66s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 340/4381 [09:20<1:50:45,  1.64s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 340/4381 [09:20<1:50:46,  1.64s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 350/4381 [09:39<1:50:50,  1.65s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 350/4381 [09:39<1:50:50,  1.65s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 360/4381 [09:57<1:50:59,  1.66s/it, loss=4.09, v_num=641]Epoch 2:   8%|▊         | 360/4381 [09:57<1:50:59,  1.66s/it, loss=4.06, v_num=641]Epoch 2:   8%|▊         | 370/4381 [10:15<1:50:50,  1.66s/it, loss=4.06, v_num=641]Epoch 2:   8%|▊         | 370/4381 [10:15<1:50:50,  1.66s/it, loss=4.08, v_num=641]Epoch 2:   9%|▊         | 380/4381 [10:30<1:50:16,  1.65s/it, loss=4.08, v_num=641]Epoch 2:   9%|▊         | 380/4381 [10:30<1:50:16,  1.65s/it, loss=4.1, v_num=641] Epoch 2:   9%|▉         | 390/4381 [10:47<1:50:08,  1.66s/it, loss=4.1, v_num=641]Epoch 2:   9%|▉         | 390/4381 [10:47<1:50:08,  1.66s/it, loss=4.08, v_num=641]Epoch 2:   9%|▉         | 400/4381 [11:03<1:49:48,  1.65s/it, loss=4.08, v_num=641]Epoch 2:   9%|▉         | 400/4381 [11:03<1:49:48,  1.65s/it, loss=4.06, v_num=641]Epoch 2:   9%|▉         | 410/4381 [11:16<1:48:54,  1.65s/it, loss=4.06, v_num=641]Epoch 2:   9%|▉         | 410/4381 [11:16<1:48:54,  1.65s/it, loss=4.07, v_num=641]Epoch 2:  10%|▉         | 420/4381 [11:32<1:48:39,  1.65s/it, loss=4.07, v_num=641]Epoch 2:  10%|▉         | 420/4381 [11:32<1:48:39,  1.65s/it, loss=4.09, v_num=641]Epoch 2:  10%|▉         | 430/4381 [11:48<1:48:11,  1.64s/it, loss=4.09, v_num=641]Epoch 2:  10%|▉         | 430/4381 [11:48<1:48:11,  1.64s/it, loss=4.09, v_num=641]Epoch 2:  10%|█         | 440/4381 [12:01<1:47:29,  1.64s/it, loss=4.09, v_num=641]Epoch 2:  10%|█         | 440/4381 [12:01<1:47:29,  1.64s/it, loss=4.07, v_num=641]Epoch 2:  10%|█         | 450/4381 [12:19<1:47:25,  1.64s/it, loss=4.07, v_num=641]Epoch 2:  10%|█         | 450/4381 [12:19<1:47:25,  1.64s/it, loss=4.06, v_num=641]Epoch 2:  10%|█         | 460/4381 [12:33<1:46:50,  1.63s/it, loss=4.06, v_num=641]Epoch 2:  10%|█         | 460/4381 [12:33<1:46:50,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  11%|█         | 470/4381 [12:47<1:46:16,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  11%|█         | 470/4381 [12:47<1:46:16,  1.63s/it, loss=4.06, v_num=641]Epoch 2:  11%|█         | 480/4381 [13:04<1:46:02,  1.63s/it, loss=4.06, v_num=641]Epoch 2:  11%|█         | 480/4381 [13:04<1:46:02,  1.63s/it, loss=4.07, v_num=641]Epoch 2:  11%|█         | 490/4381 [13:17<1:45:23,  1.63s/it, loss=4.07, v_num=641]Epoch 2:  11%|█         | 490/4381 [13:17<1:45:23,  1.63s/it, loss=4.05, v_num=641]Epoch 2:  11%|█▏        | 500/4381 [13:33<1:45:04,  1.62s/it, loss=4.05, v_num=641]Epoch 2:  11%|█▏        | 500/4381 [13:33<1:45:04,  1.62s/it, loss=4.05, v_num=641]Epoch 2:  12%|█▏        | 510/4381 [13:52<1:45:02,  1.63s/it, loss=4.05, v_num=641]Epoch 2:  12%|█▏        | 510/4381 [13:52<1:45:02,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  12%|█▏        | 520/4381 [14:07<1:44:44,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  12%|█▏        | 520/4381 [14:07<1:44:44,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  12%|█▏        | 530/4381 [14:25<1:44:40,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  12%|█▏        | 530/4381 [14:25<1:44:40,  1.63s/it, loss=4.03, v_num=641]Epoch 2:  12%|█▏        | 540/4381 [14:45<1:44:44,  1.64s/it, loss=4.03, v_num=641]Epoch 2:  12%|█▏        | 540/4381 [14:45<1:44:44,  1.64s/it, loss=4.05, v_num=641]Epoch 2:  13%|█▎        | 550/4381 [15:01<1:44:25,  1.64s/it, loss=4.05, v_num=641]Epoch 2:  13%|█▎        | 550/4381 [15:01<1:44:25,  1.64s/it, loss=4.1, v_num=641] Epoch 2:  13%|█▎        | 560/4381 [15:16<1:44:02,  1.63s/it, loss=4.1, v_num=641]Epoch 2:  13%|█▎        | 560/4381 [15:16<1:44:02,  1.63s/it, loss=4.1, v_num=641]Epoch 2:  13%|█▎        | 570/4381 [15:31<1:43:35,  1.63s/it, loss=4.1, v_num=641]Epoch 2:  13%|█▎        | 570/4381 [15:31<1:43:35,  1.63s/it, loss=4.06, v_num=641]Epoch 2:  13%|█▎        | 580/4381 [15:48<1:43:26,  1.63s/it, loss=4.06, v_num=641]Epoch 2:  13%|█▎        | 580/4381 [15:48<1:43:26,  1.63s/it, loss=3.99, v_num=641]Epoch 2:  13%|█▎        | 590/4381 [16:02<1:42:53,  1.63s/it, loss=3.99, v_num=641]Epoch 2:  13%|█▎        | 590/4381 [16:02<1:42:53,  1.63s/it, loss=3.97, v_num=641]Epoch 2:  14%|█▎        | 600/4381 [16:20<1:42:46,  1.63s/it, loss=3.97, v_num=641]Epoch 2:  14%|█▎        | 600/4381 [16:20<1:42:46,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  14%|█▍        | 610/4381 [16:34<1:42:19,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  14%|█▍        | 610/4381 [16:34<1:42:19,  1.63s/it, loss=4.05, v_num=641]Epoch 2:  14%|█▍        | 620/4381 [16:49<1:41:55,  1.63s/it, loss=4.05, v_num=641]Epoch 2:  14%|█▍        | 620/4381 [16:49<1:41:55,  1.63s/it, loss=4.02, v_num=641]Epoch 2:  14%|█▍        | 630/4381 [17:06<1:41:42,  1.63s/it, loss=4.02, v_num=641]Epoch 2:  14%|█▍        | 630/4381 [17:06<1:41:42,  1.63s/it, loss=4, v_num=641]   Epoch 2:  15%|█▍        | 640/4381 [17:20<1:41:15,  1.62s/it, loss=4, v_num=641]Epoch 2:  15%|█▍        | 640/4381 [17:20<1:41:15,  1.62s/it, loss=4.01, v_num=641]Epoch 2:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=4.01, v_num=641]Epoch 2:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=4.04, v_num=641]Epoch 2:  15%|█▌        | 660/4381 [17:54<1:40:51,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  15%|█▌        | 660/4381 [17:54<1:40:51,  1.63s/it, loss=4.04, v_num=641]Epoch 2:  15%|█▌        | 670/4381 [18:08<1:40:20,  1.62s/it, loss=4.04, v_num=641]Epoch 2:  15%|█▌        | 670/4381 [18:08<1:40:20,  1.62s/it, loss=4.01, v_num=641]Epoch 2:  16%|█▌        | 680/4381 [18:26<1:40:10,  1.62s/it, loss=4.01, v_num=641]Epoch 2:  16%|█▌        | 680/4381 [18:26<1:40:10,  1.62s/it, loss=4.01, v_num=641]Epoch 2:  16%|█▌        | 690/4381 [18:43<1:40:01,  1.63s/it, loss=4.01, v_num=641]Epoch 2:  16%|█▌        | 690/4381 [18:43<1:40:02,  1.63s/it, loss=4.05, v_num=641]Epoch 2:  16%|█▌        | 700/4381 [18:58<1:39:38,  1.62s/it, loss=4.05, v_num=641]Epoch 2:  16%|█▌        | 700/4381 [18:58<1:39:38,  1.62s/it, loss=4.04, v_num=641]Epoch 2:  16%|█▌        | 710/4381 [19:11<1:39:06,  1.62s/it, loss=4.04, v_num=641]Epoch 2:  16%|█▌        | 710/4381 [19:11<1:39:06,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  16%|█▋        | 720/4381 [19:29<1:39:00,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  16%|█▋        | 720/4381 [19:29<1:39:00,  1.62s/it, loss=4.02, v_num=641]Epoch 2:  17%|█▋        | 730/4381 [19:43<1:38:31,  1.62s/it, loss=4.02, v_num=641]Epoch 2:  17%|█▋        | 730/4381 [19:43<1:38:31,  1.62s/it, loss=4.06, v_num=641]Epoch 2:  17%|█▋        | 740/4381 [19:56<1:38:01,  1.62s/it, loss=4.06, v_num=641]Epoch 2:  17%|█▋        | 740/4381 [19:56<1:38:01,  1.62s/it, loss=4.05, v_num=641]Epoch 2:  17%|█▋        | 750/4381 [20:17<1:38:06,  1.62s/it, loss=4.05, v_num=641]Epoch 2:  17%|█▋        | 750/4381 [20:17<1:38:06,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  17%|█▋        | 760/4381 [20:32<1:37:42,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  17%|█▋        | 760/4381 [20:32<1:37:42,  1.62s/it, loss=4.07, v_num=641]Epoch 2:  18%|█▊        | 770/4381 [20:47<1:37:20,  1.62s/it, loss=4.07, v_num=641]Epoch 2:  18%|█▊        | 770/4381 [20:47<1:37:20,  1.62s/it, loss=4.02, v_num=641]Epoch 2:  18%|█▊        | 780/4381 [21:05<1:37:14,  1.62s/it, loss=4.02, v_num=641]Epoch 2:  18%|█▊        | 780/4381 [21:05<1:37:14,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  18%|█▊        | 790/4381 [21:19<1:36:50,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  18%|█▊        | 790/4381 [21:19<1:36:50,  1.62s/it, loss=4.03, v_num=641]Epoch 2:  18%|█▊        | 800/4381 [21:32<1:36:19,  1.61s/it, loss=4.03, v_num=641]Epoch 2:  18%|█▊        | 800/4381 [21:32<1:36:19,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  18%|█▊        | 810/4381 [21:49<1:36:06,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  18%|█▊        | 810/4381 [21:49<1:36:06,  1.61s/it, loss=4.06, v_num=641]Epoch 2:  19%|█▊        | 820/4381 [22:05<1:35:50,  1.61s/it, loss=4.06, v_num=641]Epoch 2:  19%|█▊        | 820/4381 [22:05<1:35:50,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  19%|█▉        | 830/4381 [22:21<1:35:33,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  19%|█▉        | 830/4381 [22:21<1:35:33,  1.61s/it, loss=3.96, v_num=641]Epoch 2:  19%|█▉        | 840/4381 [22:35<1:35:09,  1.61s/it, loss=3.96, v_num=641]Epoch 2:  19%|█▉        | 840/4381 [22:35<1:35:09,  1.61s/it, loss=3.98, v_num=641]Epoch 2:  19%|█▉        | 850/4381 [22:47<1:34:36,  1.61s/it, loss=3.98, v_num=641]Epoch 2:  19%|█▉        | 850/4381 [22:47<1:34:36,  1.61s/it, loss=3.99, v_num=641]Epoch 2:  20%|█▉        | 860/4381 [23:04<1:34:22,  1.61s/it, loss=3.99, v_num=641]Epoch 2:  20%|█▉        | 860/4381 [23:04<1:34:22,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  20%|█▉        | 870/4381 [23:22<1:34:13,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  20%|█▉        | 870/4381 [23:22<1:34:13,  1.61s/it, loss=3.96, v_num=641]Epoch 2:  20%|██        | 880/4381 [23:36<1:33:50,  1.61s/it, loss=3.96, v_num=641]Epoch 2:  20%|██        | 880/4381 [23:36<1:33:50,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  20%|██        | 890/4381 [23:51<1:33:29,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  20%|██        | 890/4381 [23:51<1:33:29,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 900/4381 [24:07<1:33:13,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 900/4381 [24:07<1:33:13,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 910/4381 [24:23<1:32:55,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 910/4381 [24:23<1:32:55,  1.61s/it, loss=3.98, v_num=641]Epoch 2:  21%|██        | 920/4381 [24:38<1:32:37,  1.61s/it, loss=3.98, v_num=641]Epoch 2:  21%|██        | 920/4381 [24:38<1:32:37,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 930/4381 [24:58<1:32:33,  1.61s/it, loss=3.97, v_num=641]Epoch 2:  21%|██        | 930/4381 [24:58<1:32:33,  1.61s/it, loss=3.94, v_num=641]Epoch 2:  21%|██▏       | 940/4381 [25:11<1:32:06,  1.61s/it, loss=3.94, v_num=641]Epoch 2:  21%|██▏       | 940/4381 [25:11<1:32:06,  1.61s/it, loss=3.95, v_num=641]Epoch 2:  22%|██▏       | 950/4381 [25:25<1:31:42,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  22%|██▏       | 950/4381 [25:25<1:31:42,  1.60s/it, loss=4.01, v_num=641]Epoch 2:  22%|██▏       | 960/4381 [25:43<1:31:32,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  22%|██▏       | 960/4381 [25:43<1:31:32,  1.61s/it, loss=4.01, v_num=641]Epoch 2:  22%|██▏       | 970/4381 [25:57<1:31:12,  1.60s/it, loss=4.01, v_num=641]Epoch 2:  22%|██▏       | 970/4381 [25:57<1:31:12,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  22%|██▏       | 980/4381 [26:13<1:30:55,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  22%|██▏       | 980/4381 [26:13<1:30:55,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  23%|██▎       | 990/4381 [26:29<1:30:40,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  23%|██▎       | 990/4381 [26:29<1:30:40,  1.60s/it, loss=4, v_num=641]   Epoch 2:  23%|██▎       | 1000/4381 [26:45<1:30:21,  1.60s/it, loss=4, v_num=641]Epoch 2:  23%|██▎       | 1000/4381 [26:45<1:30:21,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  23%|██▎       | 1010/4381 [27:00<1:30:01,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  23%|██▎       | 1010/4381 [27:00<1:30:01,  1.60s/it, loss=3.93, v_num=641]Epoch 2:  23%|██▎       | 1020/4381 [27:16<1:29:48,  1.60s/it, loss=3.93, v_num=641]Epoch 2:  23%|██▎       | 1020/4381 [27:16<1:29:48,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  24%|██▎       | 1030/4381 [27:33<1:29:35,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  24%|██▎       | 1030/4381 [27:33<1:29:35,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  24%|██▎       | 1040/4381 [27:48<1:29:14,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  24%|██▎       | 1040/4381 [27:48<1:29:14,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  24%|██▍       | 1050/4381 [28:05<1:29:01,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  24%|██▍       | 1050/4381 [28:05<1:29:01,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  24%|██▍       | 1060/4381 [28:21<1:28:46,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  24%|██▍       | 1060/4381 [28:21<1:28:46,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  24%|██▍       | 1070/4381 [28:34<1:28:21,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  24%|██▍       | 1070/4381 [28:34<1:28:21,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  25%|██▍       | 1080/4381 [28:53<1:28:12,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  25%|██▍       | 1080/4381 [28:53<1:28:12,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  25%|██▍       | 1090/4381 [29:09<1:27:57,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  25%|██▍       | 1090/4381 [29:09<1:27:57,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  25%|██▌       | 1100/4381 [29:25<1:27:41,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  25%|██▌       | 1100/4381 [29:25<1:27:41,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  25%|██▌       | 1110/4381 [29:44<1:27:33,  1.61s/it, loss=3.92, v_num=641]Epoch 2:  25%|██▌       | 1110/4381 [29:44<1:27:33,  1.61s/it, loss=3.92, v_num=641]Epoch 2:  26%|██▌       | 1120/4381 [29:59<1:27:13,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  26%|██▌       | 1120/4381 [29:59<1:27:13,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  26%|██▌       | 1130/4381 [30:12<1:26:50,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  26%|██▌       | 1130/4381 [30:12<1:26:50,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▌       | 1140/4381 [30:31<1:26:42,  1.61s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▌       | 1140/4381 [30:31<1:26:42,  1.61s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▌       | 1150/4381 [30:45<1:26:19,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▌       | 1150/4381 [30:45<1:26:19,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▋       | 1160/4381 [30:56<1:25:49,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  26%|██▋       | 1160/4381 [30:56<1:25:49,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  27%|██▋       | 1170/4381 [31:14<1:25:39,  1.60s/it, loss=3.95, v_num=641]Epoch 2:  27%|██▋       | 1170/4381 [31:14<1:25:39,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  27%|██▋       | 1180/4381 [31:30<1:25:24,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  27%|██▋       | 1180/4381 [31:30<1:25:24,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  27%|██▋       | 1190/4381 [31:45<1:25:06,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  27%|██▋       | 1190/4381 [31:45<1:25:06,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  27%|██▋       | 1200/4381 [32:01<1:24:49,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  27%|██▋       | 1200/4381 [32:01<1:24:49,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  28%|██▊       | 1210/4381 [32:20<1:24:40,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  28%|██▊       | 1210/4381 [32:20<1:24:40,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  28%|██▊       | 1220/4381 [32:36<1:24:25,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  28%|██▊       | 1220/4381 [32:36<1:24:25,  1.60s/it, loss=3.9, v_num=641] Epoch 2:  28%|██▊       | 1230/4381 [32:53<1:24:10,  1.60s/it, loss=3.9, v_num=641]Epoch 2:  28%|██▊       | 1230/4381 [32:53<1:24:10,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  28%|██▊       | 1240/4381 [33:07<1:23:49,  1.60s/it, loss=3.98, v_num=641]Epoch 2:  28%|██▊       | 1240/4381 [33:07<1:23:49,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  29%|██▊       | 1250/4381 [33:23<1:23:35,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  29%|██▊       | 1250/4381 [33:23<1:23:35,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  29%|██▉       | 1260/4381 [33:38<1:23:14,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  29%|██▉       | 1260/4381 [33:38<1:23:14,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  29%|██▉       | 1270/4381 [33:54<1:22:59,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  29%|██▉       | 1270/4381 [33:54<1:22:59,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  29%|██▉       | 1280/4381 [34:14<1:22:52,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  29%|██▉       | 1280/4381 [34:14<1:22:52,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  29%|██▉       | 1290/4381 [34:28<1:22:32,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  29%|██▉       | 1290/4381 [34:28<1:22:32,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  30%|██▉       | 1300/4381 [34:45<1:22:19,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  30%|██▉       | 1300/4381 [34:45<1:22:19,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  30%|██▉       | 1310/4381 [35:03<1:22:08,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  30%|██▉       | 1310/4381 [35:03<1:22:08,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  30%|███       | 1320/4381 [35:17<1:21:46,  1.60s/it, loss=3.94, v_num=641]Epoch 2:  30%|███       | 1320/4381 [35:17<1:21:46,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  30%|███       | 1330/4381 [35:30<1:21:24,  1.60s/it, loss=3.97, v_num=641]Epoch 2:  30%|███       | 1330/4381 [35:30<1:21:24,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  31%|███       | 1340/4381 [35:49<1:21:13,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  31%|███       | 1340/4381 [35:49<1:21:13,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  31%|███       | 1350/4381 [36:05<1:20:58,  1.60s/it, loss=3.96, v_num=641]Epoch 2:  31%|███       | 1350/4381 [36:05<1:20:58,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  31%|███       | 1360/4381 [36:17<1:20:33,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  31%|███       | 1360/4381 [36:17<1:20:33,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  31%|███▏      | 1370/4381 [36:36<1:20:22,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  31%|███▏      | 1370/4381 [36:36<1:20:22,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  31%|███▏      | 1380/4381 [36:51<1:20:06,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  31%|███▏      | 1380/4381 [36:51<1:20:06,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  32%|███▏      | 1390/4381 [37:05<1:19:45,  1.60s/it, loss=3.92, v_num=641]Epoch 2:  32%|███▏      | 1390/4381 [37:05<1:19:45,  1.60s/it, loss=3.9, v_num=641] Epoch 2:  32%|███▏      | 1400/4381 [37:23<1:19:33,  1.60s/it, loss=3.9, v_num=641]Epoch 2:  32%|███▏      | 1400/4381 [37:23<1:19:33,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  32%|███▏      | 1410/4381 [37:40<1:19:20,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  32%|███▏      | 1410/4381 [37:40<1:19:20,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  32%|███▏      | 1420/4381 [37:56<1:19:03,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  32%|███▏      | 1420/4381 [37:56<1:19:03,  1.60s/it, loss=3.9, v_num=641] Epoch 2:  33%|███▎      | 1430/4381 [38:11<1:18:45,  1.60s/it, loss=3.9, v_num=641]Epoch 2:  33%|███▎      | 1430/4381 [38:11<1:18:45,  1.60s/it, loss=3.9, v_num=641]Epoch 2:  33%|███▎      | 1440/4381 [38:27<1:18:30,  1.60s/it, loss=3.9, v_num=641]Epoch 2:  33%|███▎      | 1440/4381 [38:27<1:18:30,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  33%|███▎      | 1450/4381 [38:41<1:18:09,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  33%|███▎      | 1450/4381 [38:41<1:18:09,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  33%|███▎      | 1460/4381 [38:59<1:17:58,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  33%|███▎      | 1460/4381 [38:59<1:17:58,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  34%|███▎      | 1470/4381 [39:13<1:17:37,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  34%|███▎      | 1470/4381 [39:13<1:17:37,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1480/4381 [39:29<1:17:21,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1480/4381 [39:29<1:17:21,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1490/4381 [39:47<1:17:08,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1490/4381 [39:47<1:17:08,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1500/4381 [40:02<1:16:51,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  34%|███▍      | 1500/4381 [40:02<1:16:51,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  34%|███▍      | 1510/4381 [40:19<1:16:38,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  34%|███▍      | 1510/4381 [40:19<1:16:38,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  35%|███▍      | 1520/4381 [40:38<1:16:26,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  35%|███▍      | 1520/4381 [40:38<1:16:26,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  35%|███▍      | 1530/4381 [40:54<1:16:10,  1.60s/it, loss=3.91, v_num=641]Epoch 2:  35%|███▍      | 1530/4381 [40:54<1:16:10,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  35%|███▌      | 1540/4381 [41:10<1:15:53,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  35%|███▌      | 1540/4381 [41:10<1:15:53,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  36%|███▌      | 1560/4381 [41:40<1:15:19,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  36%|███▌      | 1560/4381 [41:40<1:15:19,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  36%|███▌      | 1570/4381 [41:57<1:15:03,  1.60s/it, loss=3.89, v_num=641]Epoch 2:  36%|███▌      | 1570/4381 [41:57<1:15:03,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  36%|███▌      | 1580/4381 [42:15<1:14:51,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  36%|███▌      | 1580/4381 [42:15<1:14:51,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  36%|███▋      | 1590/4381 [42:30<1:14:34,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  36%|███▋      | 1590/4381 [42:30<1:14:34,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  37%|███▋      | 1600/4381 [42:44<1:14:14,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  37%|███▋      | 1600/4381 [42:44<1:14:14,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  37%|███▋      | 1610/4381 [43:03<1:14:04,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  37%|███▋      | 1610/4381 [43:03<1:14:04,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  37%|███▋      | 1620/4381 [43:19<1:13:47,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  37%|███▋      | 1620/4381 [43:19<1:13:47,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  37%|███▋      | 1630/4381 [43:33<1:13:27,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  37%|███▋      | 1630/4381 [43:33<1:13:27,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  37%|███▋      | 1640/4381 [43:54<1:13:19,  1.61s/it, loss=3.82, v_num=641]Epoch 2:  37%|███▋      | 1640/4381 [43:54<1:13:19,  1.61s/it, loss=3.84, v_num=641]Epoch 2:  38%|███▊      | 1650/4381 [44:08<1:13:01,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  38%|███▊      | 1650/4381 [44:08<1:13:01,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  38%|███▊      | 1660/4381 [44:24<1:12:44,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  38%|███▊      | 1660/4381 [44:24<1:12:44,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  38%|███▊      | 1670/4381 [44:40<1:12:29,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  38%|███▊      | 1670/4381 [44:40<1:12:29,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  38%|███▊      | 1680/4381 [44:56<1:12:13,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  38%|███▊      | 1680/4381 [44:56<1:12:13,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▊      | 1690/4381 [45:12<1:11:56,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▊      | 1690/4381 [45:12<1:11:56,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  39%|███▉      | 1700/4381 [45:29<1:11:41,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  39%|███▉      | 1700/4381 [45:29<1:11:41,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▉      | 1710/4381 [45:43<1:11:23,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▉      | 1710/4381 [45:43<1:11:23,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▉      | 1720/4381 [45:59<1:11:07,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  39%|███▉      | 1720/4381 [45:59<1:11:07,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  39%|███▉      | 1730/4381 [46:17<1:10:54,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  39%|███▉      | 1730/4381 [46:17<1:10:54,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  40%|███▉      | 1740/4381 [46:34<1:10:38,  1.60s/it, loss=3.87, v_num=641]Epoch 2:  40%|███▉      | 1740/4381 [46:34<1:10:38,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  40%|███▉      | 1750/4381 [46:48<1:10:19,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  40%|███▉      | 1750/4381 [46:48<1:10:19,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  40%|████      | 1760/4381 [47:02<1:10:00,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  40%|████      | 1760/4381 [47:02<1:10:00,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  40%|████      | 1770/4381 [47:17<1:09:43,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  40%|████      | 1770/4381 [47:17<1:09:43,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  41%|████      | 1780/4381 [47:32<1:09:25,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  41%|████      | 1780/4381 [47:32<1:09:25,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  41%|████      | 1790/4381 [47:47<1:09:07,  1.60s/it, loss=3.88, v_num=641]Epoch 2:  41%|████      | 1790/4381 [47:47<1:09:07,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  41%|████      | 1800/4381 [48:06<1:08:56,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  41%|████      | 1800/4381 [48:06<1:08:56,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  41%|████▏     | 1810/4381 [48:21<1:08:38,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  41%|████▏     | 1810/4381 [48:21<1:08:38,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  42%|████▏     | 1820/4381 [48:40<1:08:27,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  42%|████▏     | 1820/4381 [48:40<1:08:27,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  42%|████▏     | 1830/4381 [48:55<1:08:09,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  42%|████▏     | 1830/4381 [48:55<1:08:09,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  42%|████▏     | 1840/4381 [49:10<1:07:52,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  42%|████▏     | 1840/4381 [49:10<1:07:52,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  42%|████▏     | 1850/4381 [49:27<1:07:38,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  42%|████▏     | 1850/4381 [49:27<1:07:38,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  42%|████▏     | 1860/4381 [49:42<1:07:19,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  42%|████▏     | 1860/4381 [49:42<1:07:19,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  43%|████▎     | 1870/4381 [49:56<1:07:01,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  43%|████▎     | 1870/4381 [49:56<1:07:01,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  43%|████▎     | 1880/4381 [50:13<1:06:46,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  43%|████▎     | 1880/4381 [50:13<1:06:46,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  43%|████▎     | 1890/4381 [50:29<1:06:30,  1.60s/it, loss=3.86, v_num=641]Epoch 2:  43%|████▎     | 1890/4381 [50:29<1:06:30,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  43%|████▎     | 1900/4381 [50:44<1:06:13,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  43%|████▎     | 1900/4381 [50:44<1:06:13,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  44%|████▎     | 1910/4381 [51:01<1:05:58,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  44%|████▎     | 1910/4381 [51:01<1:05:58,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  44%|████▍     | 1920/4381 [51:16<1:05:40,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  44%|████▍     | 1920/4381 [51:16<1:05:40,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  44%|████▍     | 1930/4381 [51:31<1:05:24,  1.60s/it, loss=3.83, v_num=641]Epoch 2:  44%|████▍     | 1930/4381 [51:31<1:05:24,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  44%|████▍     | 1940/4381 [51:52<1:05:14,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  44%|████▍     | 1940/4381 [51:52<1:05:14,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  45%|████▍     | 1950/4381 [52:06<1:04:56,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▍     | 1950/4381 [52:06<1:04:56,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  45%|████▍     | 1960/4381 [52:22<1:04:40,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  45%|████▍     | 1960/4381 [52:22<1:04:40,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  45%|████▍     | 1970/4381 [52:41<1:04:27,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▍     | 1970/4381 [52:41<1:04:27,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▌     | 1980/4381 [52:55<1:04:09,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▌     | 1980/4381 [52:55<1:04:09,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▌     | 1990/4381 [53:09<1:03:50,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  45%|████▌     | 1990/4381 [53:09<1:03:50,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  46%|████▌     | 2000/4381 [53:27<1:03:36,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  46%|████▌     | 2000/4381 [53:27<1:03:36,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  46%|████▌     | 2010/4381 [53:40<1:03:16,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  46%|████▌     | 2010/4381 [53:40<1:03:16,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  46%|████▋     | 2030/4381 [54:12<1:02:45,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  46%|████▋     | 2030/4381 [54:12<1:02:45,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  47%|████▋     | 2040/4381 [54:27<1:02:28,  1.60s/it, loss=3.82, v_num=641]Epoch 2:  47%|████▋     | 2040/4381 [54:27<1:02:28,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  47%|████▋     | 2050/4381 [54:42<1:02:11,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  47%|████▋     | 2050/4381 [54:42<1:02:11,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  47%|████▋     | 2060/4381 [55:02<1:01:58,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  47%|████▋     | 2060/4381 [55:02<1:01:58,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  47%|████▋     | 2070/4381 [55:16<1:01:40,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  47%|████▋     | 2070/4381 [55:16<1:01:40,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  47%|████▋     | 2080/4381 [55:30<1:01:23,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  47%|████▋     | 2080/4381 [55:30<1:01:23,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  48%|████▊     | 2090/4381 [55:49<1:01:09,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  48%|████▊     | 2090/4381 [55:49<1:01:09,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  48%|████▊     | 2100/4381 [56:03<1:00:51,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  48%|████▊     | 2100/4381 [56:03<1:00:51,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  48%|████▊     | 2110/4381 [56:17<1:00:33,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  48%|████▊     | 2110/4381 [56:17<1:00:33,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  48%|████▊     | 2120/4381 [56:37<1:00:21,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  48%|████▊     | 2120/4381 [56:37<1:00:21,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  49%|████▊     | 2130/4381 [56:52<1:00:04,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  49%|████▊     | 2130/4381 [56:52<1:00:04,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  49%|████▉     | 2140/4381 [57:06<59:46,  1.60s/it, loss=3.75, v_num=641]  Epoch 2:  49%|████▉     | 2140/4381 [57:06<59:46,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  49%|████▉     | 2150/4381 [57:25<59:33,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  49%|████▉     | 2150/4381 [57:25<59:33,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  49%|████▉     | 2160/4381 [57:39<59:15,  1.60s/it, loss=3.81, v_num=641]Epoch 2:  49%|████▉     | 2160/4381 [57:39<59:15,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  50%|████▉     | 2170/4381 [57:53<58:57,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  50%|████▉     | 2170/4381 [57:53<58:57,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  50%|████▉     | 2180/4381 [58:10<58:42,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  50%|████▉     | 2180/4381 [58:10<58:42,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  50%|████▉     | 2190/4381 [58:26<58:26,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  50%|████▉     | 2190/4381 [58:26<58:26,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  50%|█████     | 2200/4381 [58:40<58:08,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  50%|█████     | 2200/4381 [58:40<58:08,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  50%|█████     | 2210/4381 [58:57<57:53,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  50%|█████     | 2210/4381 [58:57<57:53,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  51%|█████     | 2220/4381 [59:13<57:37,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  51%|█████     | 2220/4381 [59:13<57:37,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  51%|█████     | 2230/4381 [59:27<57:19,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  51%|█████     | 2230/4381 [59:27<57:19,  1.60s/it, loss=3.8, v_num=641] Epoch 2:  51%|█████     | 2240/4381 [59:42<57:02,  1.60s/it, loss=3.8, v_num=641]Epoch 2:  51%|█████     | 2240/4381 [59:42<57:02,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  51%|█████▏    | 2250/4381 [59:56<56:44,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  51%|█████▏    | 2250/4381 [59:56<56:44,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  52%|█████▏    | 2260/4381 [1:00:11<56:27,  1.60s/it, loss=3.78, v_num=641]Epoch 2:  52%|█████▏    | 2260/4381 [1:00:11<56:27,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  52%|█████▏    | 2270/4381 [1:00:31<56:15,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  52%|█████▏    | 2270/4381 [1:00:31<56:15,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  52%|█████▏    | 2280/4381 [1:00:42<55:55,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  52%|█████▏    | 2280/4381 [1:00:42<55:55,  1.60s/it, loss=3.74, v_num=641]Epoch 2:  52%|█████▏    | 2290/4381 [1:00:55<55:36,  1.60s/it, loss=3.74, v_num=641]Epoch 2:  52%|█████▏    | 2290/4381 [1:00:55<55:36,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  52%|█████▏    | 2300/4381 [1:01:15<55:23,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  52%|█████▏    | 2300/4381 [1:01:15<55:23,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  53%|█████▎    | 2310/4381 [1:01:28<55:05,  1.60s/it, loss=3.79, v_num=641]Epoch 2:  53%|█████▎    | 2310/4381 [1:01:28<55:05,  1.60s/it, loss=3.84, v_num=641]Epoch 2:  53%|█████▎    | 2320/4381 [1:01:41<54:46,  1.59s/it, loss=3.84, v_num=641]Epoch 2:  53%|█████▎    | 2320/4381 [1:01:41<54:46,  1.59s/it, loss=3.85, v_num=641]Epoch 2:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=3.85, v_num=641]Epoch 2:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  53%|█████▎    | 2340/4381 [1:02:15<54:16,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  53%|█████▎    | 2340/4381 [1:02:15<54:16,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▎    | 2350/4381 [1:02:29<53:59,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▎    | 2350/4381 [1:02:29<53:59,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▍    | 2360/4381 [1:02:47<53:45,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▍    | 2360/4381 [1:02:47<53:45,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▍    | 2370/4381 [1:03:02<53:28,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  54%|█████▍    | 2370/4381 [1:03:02<53:28,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  54%|█████▍    | 2380/4381 [1:03:18<53:12,  1.60s/it, loss=3.76, v_num=641]Epoch 2:  54%|█████▍    | 2380/4381 [1:03:18<53:12,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  55%|█████▍    | 2390/4381 [1:03:35<52:56,  1.60s/it, loss=3.77, v_num=641]Epoch 2:  55%|█████▍    | 2390/4381 [1:03:35<52:56,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  55%|█████▍    | 2400/4381 [1:03:50<52:40,  1.60s/it, loss=3.75, v_num=641]Epoch 2:  55%|█████▍    | 2400/4381 [1:03:50<52:40,  1.60s/it, loss=3.72, v_num=641]Epoch 2:  55%|█████▌    | 2410/4381 [1:04:04<52:22,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  55%|█████▌    | 2410/4381 [1:04:04<52:22,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  55%|█████▌    | 2420/4381 [1:04:21<52:07,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  55%|█████▌    | 2420/4381 [1:04:21<52:07,  1.60s/it, loss=3.73, v_num=641]Epoch 2:  55%|█████▌    | 2430/4381 [1:04:35<51:50,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  55%|█████▌    | 2430/4381 [1:04:35<51:50,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  56%|█████▌    | 2440/4381 [1:04:51<51:34,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  56%|█████▌    | 2440/4381 [1:04:51<51:34,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  56%|█████▌    | 2450/4381 [1:05:08<51:18,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  56%|█████▌    | 2450/4381 [1:05:08<51:18,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  56%|█████▌    | 2460/4381 [1:05:22<51:01,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  56%|█████▌    | 2460/4381 [1:05:22<51:01,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  56%|█████▋    | 2470/4381 [1:05:36<50:44,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  56%|█████▋    | 2470/4381 [1:05:36<50:44,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  57%|█████▋    | 2480/4381 [1:05:53<50:29,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  57%|█████▋    | 2480/4381 [1:05:53<50:29,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  57%|█████▋    | 2490/4381 [1:06:11<50:15,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  57%|█████▋    | 2490/4381 [1:06:11<50:15,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  57%|█████▋    | 2500/4381 [1:06:25<49:57,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  57%|█████▋    | 2500/4381 [1:06:25<49:57,  1.59s/it, loss=3.78, v_num=641]Epoch 2:  57%|█████▋    | 2510/4381 [1:06:41<49:41,  1.59s/it, loss=3.78, v_num=641]Epoch 2:  57%|█████▋    | 2510/4381 [1:06:41<49:41,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  58%|█████▊    | 2520/4381 [1:06:55<49:24,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  58%|█████▊    | 2520/4381 [1:06:55<49:24,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  58%|█████▊    | 2530/4381 [1:07:11<49:08,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  58%|█████▊    | 2530/4381 [1:07:11<49:08,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  58%|█████▊    | 2540/4381 [1:07:30<48:54,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  58%|█████▊    | 2540/4381 [1:07:30<48:54,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  58%|█████▊    | 2550/4381 [1:07:45<48:38,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  58%|█████▊    | 2550/4381 [1:07:45<48:38,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  58%|█████▊    | 2560/4381 [1:08:00<48:21,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  58%|█████▊    | 2560/4381 [1:08:00<48:21,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  59%|█████▊    | 2570/4381 [1:08:16<48:05,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  59%|█████▊    | 2570/4381 [1:08:16<48:05,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  59%|█████▉    | 2580/4381 [1:08:32<47:49,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  59%|█████▉    | 2580/4381 [1:08:32<47:49,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  59%|█████▉    | 2590/4381 [1:08:48<47:33,  1.59s/it, loss=3.73, v_num=641]Epoch 2:  59%|█████▉    | 2590/4381 [1:08:48<47:33,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  59%|█████▉    | 2600/4381 [1:09:08<47:20,  1.59s/it, loss=3.77, v_num=641]Epoch 2:  59%|█████▉    | 2600/4381 [1:09:08<47:20,  1.59s/it, loss=3.76, v_num=641]Epoch 2:  60%|█████▉    | 2610/4381 [1:09:23<47:04,  1.59s/it, loss=3.76, v_num=641]Epoch 2:  60%|█████▉    | 2610/4381 [1:09:23<47:04,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  60%|█████▉    | 2620/4381 [1:09:38<46:47,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  60%|█████▉    | 2620/4381 [1:09:38<46:47,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  60%|██████    | 2630/4381 [1:09:54<46:31,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  60%|██████    | 2630/4381 [1:09:54<46:31,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  60%|██████    | 2640/4381 [1:10:06<46:12,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  60%|██████    | 2640/4381 [1:10:06<46:12,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  60%|██████    | 2650/4381 [1:10:22<45:57,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  60%|██████    | 2650/4381 [1:10:22<45:57,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  61%|██████    | 2660/4381 [1:10:40<45:42,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  61%|██████    | 2660/4381 [1:10:40<45:42,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  61%|██████    | 2670/4381 [1:10:53<45:24,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  61%|██████    | 2670/4381 [1:10:53<45:24,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  61%|██████    | 2680/4381 [1:11:11<45:09,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  61%|██████    | 2680/4381 [1:11:11<45:09,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  61%|██████▏   | 2690/4381 [1:11:29<44:55,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  61%|██████▏   | 2690/4381 [1:11:29<44:55,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  62%|██████▏   | 2700/4381 [1:11:44<44:38,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  62%|██████▏   | 2700/4381 [1:11:44<44:38,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  62%|██████▏   | 2710/4381 [1:11:57<44:21,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  62%|██████▏   | 2710/4381 [1:11:57<44:21,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  62%|██████▏   | 2720/4381 [1:12:16<44:07,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  62%|██████▏   | 2720/4381 [1:12:16<44:07,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  62%|██████▏   | 2730/4381 [1:12:31<43:50,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  62%|██████▏   | 2730/4381 [1:12:31<43:50,  1.59s/it, loss=3.74, v_num=641]Epoch 2:  63%|██████▎   | 2740/4381 [1:12:45<43:33,  1.59s/it, loss=3.74, v_num=641]Epoch 2:  63%|██████▎   | 2740/4381 [1:12:45<43:33,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  63%|██████▎   | 2760/4381 [1:13:20<43:03,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  63%|██████▎   | 2760/4381 [1:13:20<43:03,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  63%|██████▎   | 2770/4381 [1:13:34<42:46,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  63%|██████▎   | 2770/4381 [1:13:34<42:46,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  63%|██████▎   | 2780/4381 [1:13:52<42:32,  1.59s/it, loss=3.75, v_num=641]Epoch 2:  63%|██████▎   | 2780/4381 [1:13:52<42:32,  1.59s/it, loss=3.74, v_num=641]Epoch 2:  64%|██████▎   | 2790/4381 [1:14:09<42:16,  1.59s/it, loss=3.74, v_num=641]Epoch 2:  64%|██████▎   | 2790/4381 [1:14:09<42:16,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  64%|██████▍   | 2800/4381 [1:14:21<41:58,  1.59s/it, loss=3.72, v_num=641]Epoch 2:  64%|██████▍   | 2800/4381 [1:14:21<41:58,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  64%|██████▍   | 2810/4381 [1:14:37<41:42,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  64%|██████▍   | 2810/4381 [1:14:37<41:42,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  64%|██████▍   | 2820/4381 [1:14:52<41:25,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  64%|██████▍   | 2820/4381 [1:14:52<41:25,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  65%|██████▍   | 2830/4381 [1:15:09<41:10,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  65%|██████▍   | 2830/4381 [1:15:09<41:10,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  65%|██████▍   | 2840/4381 [1:15:25<40:54,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  65%|██████▍   | 2840/4381 [1:15:25<40:54,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  65%|██████▌   | 2850/4381 [1:15:39<40:37,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  65%|██████▌   | 2850/4381 [1:15:39<40:37,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  66%|██████▌   | 2870/4381 [1:16:19<40:10,  1.60s/it, loss=3.66, v_num=641]Epoch 2:  66%|██████▌   | 2870/4381 [1:16:19<40:10,  1.60s/it, loss=3.68, v_num=641]Epoch 2:  66%|██████▌   | 2880/4381 [1:16:33<39:53,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  66%|██████▌   | 2880/4381 [1:16:33<39:53,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  66%|██████▌   | 2890/4381 [1:16:48<39:36,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  66%|██████▌   | 2890/4381 [1:16:48<39:36,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  66%|██████▌   | 2900/4381 [1:17:05<39:21,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  66%|██████▌   | 2900/4381 [1:17:05<39:21,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  66%|██████▋   | 2910/4381 [1:17:19<39:04,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  66%|██████▋   | 2910/4381 [1:17:19<39:04,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  67%|██████▋   | 2920/4381 [1:17:35<38:48,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  67%|██████▋   | 2920/4381 [1:17:35<38:48,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  67%|██████▋   | 2930/4381 [1:17:52<38:33,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  67%|██████▋   | 2930/4381 [1:17:52<38:33,  1.59s/it, loss=3.7, v_num=641] Epoch 2:  67%|██████▋   | 2940/4381 [1:18:07<38:16,  1.59s/it, loss=3.7, v_num=641]Epoch 2:  67%|██████▋   | 2940/4381 [1:18:07<38:16,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  67%|██████▋   | 2950/4381 [1:18:23<38:00,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  67%|██████▋   | 2950/4381 [1:18:23<38:00,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  68%|██████▊   | 2960/4381 [1:18:42<37:46,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  68%|██████▊   | 2960/4381 [1:18:42<37:46,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  68%|██████▊   | 2970/4381 [1:18:56<37:29,  1.59s/it, loss=3.71, v_num=641]Epoch 2:  68%|██████▊   | 2970/4381 [1:18:56<37:29,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  68%|██████▊   | 2980/4381 [1:19:11<37:13,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  68%|██████▊   | 2980/4381 [1:19:11<37:13,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  68%|██████▊   | 2990/4381 [1:19:27<36:57,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  68%|██████▊   | 2990/4381 [1:19:27<36:57,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  68%|██████▊   | 3000/4381 [1:19:45<36:42,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  68%|██████▊   | 3000/4381 [1:19:45<36:42,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  69%|██████▊   | 3010/4381 [1:19:59<36:25,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  69%|██████▊   | 3010/4381 [1:19:59<36:25,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  69%|██████▉   | 3020/4381 [1:20:19<36:11,  1.60s/it, loss=3.66, v_num=641]Epoch 2:  69%|██████▉   | 3020/4381 [1:20:19<36:11,  1.60s/it, loss=3.69, v_num=641]Epoch 2:  69%|██████▉   | 3030/4381 [1:20:31<35:53,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  69%|██████▉   | 3030/4381 [1:20:31<35:53,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  69%|██████▉   | 3040/4381 [1:20:45<35:36,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  69%|██████▉   | 3040/4381 [1:20:45<35:36,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  70%|██████▉   | 3050/4381 [1:21:04<35:22,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  70%|██████▉   | 3050/4381 [1:21:04<35:22,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  70%|██████▉   | 3060/4381 [1:21:19<35:05,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  70%|██████▉   | 3060/4381 [1:21:19<35:05,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  70%|███████   | 3070/4381 [1:21:33<34:49,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  70%|███████   | 3070/4381 [1:21:33<34:49,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  70%|███████   | 3080/4381 [1:21:52<34:34,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  70%|███████   | 3080/4381 [1:21:52<34:34,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  71%|███████   | 3090/4381 [1:22:06<34:17,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  71%|███████   | 3090/4381 [1:22:06<34:17,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████   | 3100/4381 [1:22:23<34:02,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████   | 3100/4381 [1:22:23<34:02,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  71%|███████   | 3110/4381 [1:22:40<33:46,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  71%|███████   | 3110/4381 [1:22:40<33:46,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████   | 3120/4381 [1:22:53<33:29,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████   | 3120/4381 [1:22:53<33:29,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████▏  | 3130/4381 [1:23:08<33:13,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  71%|███████▏  | 3130/4381 [1:23:08<33:13,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  72%|███████▏  | 3140/4381 [1:23:28<32:58,  1.59s/it, loss=3.68, v_num=641]Epoch 2:  72%|███████▏  | 3140/4381 [1:23:28<32:58,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  72%|███████▏  | 3150/4381 [1:23:43<32:42,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  72%|███████▏  | 3150/4381 [1:23:43<32:42,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  72%|███████▏  | 3160/4381 [1:23:55<32:25,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  72%|███████▏  | 3160/4381 [1:23:55<32:25,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  72%|███████▏  | 3170/4381 [1:24:15<32:10,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  72%|███████▏  | 3170/4381 [1:24:15<32:10,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  73%|███████▎  | 3180/4381 [1:24:29<31:54,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  73%|███████▎  | 3180/4381 [1:24:29<31:54,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  73%|███████▎  | 3190/4381 [1:24:44<31:37,  1.59s/it, loss=3.69, v_num=641]Epoch 2:  73%|███████▎  | 3190/4381 [1:24:44<31:37,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  73%|███████▎  | 3200/4381 [1:25:01<31:22,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  73%|███████▎  | 3200/4381 [1:25:01<31:22,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  73%|███████▎  | 3210/4381 [1:25:14<31:05,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  73%|███████▎  | 3210/4381 [1:25:14<31:05,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  73%|███████▎  | 3220/4381 [1:25:29<30:48,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  73%|███████▎  | 3220/4381 [1:25:29<30:48,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  74%|███████▎  | 3230/4381 [1:25:45<30:33,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  74%|███████▎  | 3230/4381 [1:25:45<30:33,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  74%|███████▍  | 3240/4381 [1:26:01<30:17,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  74%|███████▍  | 3240/4381 [1:26:01<30:17,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  74%|███████▍  | 3250/4381 [1:26:17<30:01,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  74%|███████▍  | 3250/4381 [1:26:17<30:01,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  74%|███████▍  | 3260/4381 [1:26:33<29:45,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  74%|███████▍  | 3260/4381 [1:26:33<29:45,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  75%|███████▍  | 3270/4381 [1:26:46<29:28,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  75%|███████▍  | 3270/4381 [1:26:46<29:28,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  75%|███████▍  | 3280/4381 [1:27:02<29:12,  1.59s/it, loss=3.67, v_num=641]Epoch 2:  75%|███████▍  | 3280/4381 [1:27:02<29:12,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  75%|███████▌  | 3290/4381 [1:27:17<28:56,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  75%|███████▌  | 3290/4381 [1:27:17<28:56,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  75%|███████▌  | 3300/4381 [1:27:32<28:40,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  75%|███████▌  | 3300/4381 [1:27:32<28:40,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  76%|███████▌  | 3310/4381 [1:27:47<28:23,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  76%|███████▌  | 3310/4381 [1:27:47<28:23,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  76%|███████▌  | 3320/4381 [1:28:04<28:08,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  76%|███████▌  | 3320/4381 [1:28:04<28:08,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  76%|███████▌  | 3330/4381 [1:28:18<27:51,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  76%|███████▌  | 3330/4381 [1:28:18<27:51,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  76%|███████▋  | 3350/4381 [1:28:51<27:20,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  76%|███████▋  | 3350/4381 [1:28:51<27:20,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  77%|███████▋  | 3360/4381 [1:29:02<27:03,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  77%|███████▋  | 3360/4381 [1:29:02<27:03,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  77%|███████▋  | 3370/4381 [1:29:14<26:45,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  77%|███████▋  | 3370/4381 [1:29:14<26:45,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  77%|███████▋  | 3380/4381 [1:29:31<26:30,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  77%|███████▋  | 3380/4381 [1:29:31<26:30,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  77%|███████▋  | 3390/4381 [1:29:47<26:14,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  77%|███████▋  | 3390/4381 [1:29:47<26:14,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  78%|███████▊  | 3400/4381 [1:30:01<25:58,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  78%|███████▊  | 3400/4381 [1:30:01<25:58,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  78%|███████▊  | 3410/4381 [1:30:17<25:42,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  78%|███████▊  | 3410/4381 [1:30:17<25:42,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  78%|███████▊  | 3420/4381 [1:30:34<25:26,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  78%|███████▊  | 3420/4381 [1:30:34<25:26,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  78%|███████▊  | 3430/4381 [1:30:49<25:10,  1.59s/it, loss=3.63, v_num=641]Epoch 2:  78%|███████▊  | 3430/4381 [1:30:49<25:10,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  79%|███████▊  | 3440/4381 [1:31:06<24:54,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  79%|███████▊  | 3440/4381 [1:31:06<24:54,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  79%|███████▊  | 3450/4381 [1:31:22<24:38,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  79%|███████▊  | 3450/4381 [1:31:22<24:38,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  79%|███████▉  | 3460/4381 [1:31:39<24:23,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  79%|███████▉  | 3460/4381 [1:31:39<24:23,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  79%|███████▉  | 3470/4381 [1:31:56<24:07,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  79%|███████▉  | 3470/4381 [1:31:56<24:07,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  79%|███████▉  | 3480/4381 [1:32:12<23:52,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  79%|███████▉  | 3480/4381 [1:32:12<23:52,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  80%|███████▉  | 3490/4381 [1:32:27<23:35,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  80%|███████▉  | 3490/4381 [1:32:27<23:35,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  80%|███████▉  | 3500/4381 [1:32:44<23:20,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  80%|███████▉  | 3500/4381 [1:32:44<23:20,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  80%|████████  | 3510/4381 [1:32:59<23:04,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  80%|████████  | 3510/4381 [1:32:59<23:04,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  80%|████████  | 3520/4381 [1:33:15<22:48,  1.59s/it, loss=3.66, v_num=641]Epoch 2:  80%|████████  | 3520/4381 [1:33:15<22:48,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  81%|████████  | 3530/4381 [1:33:33<22:32,  1.59s/it, loss=3.64, v_num=641]Epoch 2:  81%|████████  | 3530/4381 [1:33:33<22:32,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  81%|████████  | 3540/4381 [1:33:48<22:16,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  81%|████████  | 3540/4381 [1:33:48<22:16,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  81%|████████  | 3550/4381 [1:34:04<22:00,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  81%|████████  | 3550/4381 [1:34:04<22:00,  1.59s/it, loss=3.6, v_num=641] Epoch 2:  81%|████████▏ | 3560/4381 [1:34:21<21:45,  1.59s/it, loss=3.6, v_num=641]Epoch 2:  81%|████████▏ | 3560/4381 [1:34:21<21:45,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  81%|████████▏ | 3570/4381 [1:34:37<21:29,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  81%|████████▏ | 3570/4381 [1:34:37<21:29,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  82%|████████▏ | 3580/4381 [1:34:54<21:13,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  82%|████████▏ | 3580/4381 [1:34:54<21:13,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  82%|████████▏ | 3590/4381 [1:35:10<20:57,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  82%|████████▏ | 3590/4381 [1:35:10<20:57,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  82%|████████▏ | 3600/4381 [1:35:27<20:42,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  82%|████████▏ | 3600/4381 [1:35:27<20:42,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  82%|████████▏ | 3610/4381 [1:35:41<20:25,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  82%|████████▏ | 3610/4381 [1:35:41<20:25,  1.59s/it, loss=3.6, v_num=641] Epoch 2:  83%|████████▎ | 3620/4381 [1:36:00<20:10,  1.59s/it, loss=3.6, v_num=641]Epoch 2:  83%|████████▎ | 3620/4381 [1:36:00<20:10,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  83%|████████▎ | 3630/4381 [1:36:14<19:54,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  83%|████████▎ | 3630/4381 [1:36:14<19:54,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  83%|████████▎ | 3640/4381 [1:36:28<19:37,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  83%|████████▎ | 3640/4381 [1:36:28<19:37,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  83%|████████▎ | 3650/4381 [1:36:45<19:22,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  83%|████████▎ | 3650/4381 [1:36:45<19:22,  1.59s/it, loss=3.54, v_num=641]Epoch 2:  84%|████████▎ | 3660/4381 [1:37:00<19:06,  1.59s/it, loss=3.54, v_num=641]Epoch 2:  84%|████████▎ | 3660/4381 [1:37:00<19:06,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  84%|████████▍ | 3670/4381 [1:37:15<18:50,  1.59s/it, loss=3.58, v_num=641]Epoch 2:  84%|████████▍ | 3670/4381 [1:37:15<18:50,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  84%|████████▍ | 3680/4381 [1:37:35<18:35,  1.59s/it, loss=3.65, v_num=641]Epoch 2:  84%|████████▍ | 3680/4381 [1:37:35<18:35,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  84%|████████▍ | 3690/4381 [1:37:50<18:18,  1.59s/it, loss=3.61, v_num=641]Epoch 2:  84%|████████▍ | 3690/4381 [1:37:50<18:18,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  84%|████████▍ | 3700/4381 [1:38:05<18:02,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  84%|████████▍ | 3700/4381 [1:38:05<18:02,  1.59s/it, loss=3.6, v_num=641] Epoch 2:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=3.6, v_num=641]Epoch 2:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  85%|████████▍ | 3720/4381 [1:38:37<17:31,  1.59s/it, loss=3.57, v_num=641]Epoch 2:  85%|████████▍ | 3720/4381 [1:38:37<17:31,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  85%|████████▌ | 3730/4381 [1:38:50<17:14,  1.59s/it, loss=3.56, v_num=641]Epoch 2:  85%|████████▌ | 3730/4381 [1:38:50<17:14,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=3.59, v_num=641]Epoch 2:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=3.62, v_num=641]Epoch 2:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=3.62, v_num=641]Epoch 2:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=3.63, v_num=641]validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8096178770065308
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8074983954429626
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8151509761810303
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8132340312004089
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8137052655220032
Epoch 2:  86%|████████▌ | 3760/4381 [1:38:58<16:20,  1.58s/it, loss=3.63, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.0
valid accuracy: 0.8118821978569031
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.8144480586051941

Validating:   2%|▏         | 10/626 [00:03<03:25,  3.00it/s][AEpoch 2:  86%|████████▌ | 3770/4381 [1:39:01<16:02,  1.58s/it, loss=3.63, v_num=641]
Validating:   3%|▎         | 20/626 [00:04<02:12,  4.59it/s][AEpoch 2:  86%|████████▋ | 3780/4381 [1:39:03<15:44,  1.57s/it, loss=3.63, v_num=641]
Validating:   5%|▍         | 30/626 [00:07<02:24,  4.13it/s][AEpoch 2:  87%|████████▋ | 3790/4381 [1:39:05<15:26,  1.57s/it, loss=3.63, v_num=641]
Validating:   6%|▋         | 40/626 [00:08<02:00,  4.88it/s][AEpoch 2:  87%|████████▋ | 3800/4381 [1:39:07<15:09,  1.56s/it, loss=3.63, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:40,  5.74it/s][AEpoch 2:  87%|████████▋ | 3810/4381 [1:39:08<14:51,  1.56s/it, loss=3.63, v_num=641]
Validating:  10%|▉         | 60/626 [00:11<01:37,  5.80it/s][AEpoch 2:  87%|████████▋ | 3820/4381 [1:39:10<14:33,  1.56s/it, loss=3.63, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:48,  5.13it/s][AEpoch 2:  87%|████████▋ | 3830/4381 [1:39:12<14:16,  1.55s/it, loss=3.63, v_num=641]
Validating:  13%|█▎        | 80/626 [00:15<01:35,  5.74it/s][AEpoch 2:  88%|████████▊ | 3840/4381 [1:39:13<13:58,  1.55s/it, loss=3.63, v_num=641]
Validating:  14%|█▍        | 90/626 [00:16<01:29,  6.01it/s][AEpoch 2:  88%|████████▊ | 3850/4381 [1:39:15<13:41,  1.55s/it, loss=3.63, v_num=641]
Validating:  16%|█▌        | 100/626 [00:18<01:24,  6.22it/s][AEpoch 2:  88%|████████▊ | 3860/4381 [1:39:16<13:23,  1.54s/it, loss=3.63, v_num=641]
Validating:  18%|█▊        | 110/626 [00:20<01:28,  5.83it/s][AEpoch 2:  88%|████████▊ | 3870/4381 [1:39:18<13:06,  1.54s/it, loss=3.63, v_num=641]
Validating:  19%|█▉        | 120/626 [00:22<01:38,  5.14it/s][AEpoch 2:  89%|████████▊ | 3880/4381 [1:39:21<12:49,  1.54s/it, loss=3.63, v_num=641]
Validating:  21%|██        | 130/626 [00:24<01:31,  5.42it/s][AEpoch 2:  89%|████████▉ | 3890/4381 [1:39:22<12:32,  1.53s/it, loss=3.63, v_num=641]
Validating:  22%|██▏       | 140/626 [00:27<01:44,  4.67it/s][AEpoch 2:  89%|████████▉ | 3900/4381 [1:39:25<12:15,  1.53s/it, loss=3.63, v_num=641]
Validating:  24%|██▍       | 150/626 [00:28<01:33,  5.11it/s][AEpoch 2:  89%|████████▉ | 3910/4381 [1:39:27<11:58,  1.53s/it, loss=3.63, v_num=641]
Validating:  26%|██▌       | 160/626 [00:30<01:24,  5.53it/s][AEpoch 2:  89%|████████▉ | 3920/4381 [1:39:28<11:41,  1.52s/it, loss=3.63, v_num=641]
Validating:  27%|██▋       | 170/626 [00:32<01:25,  5.36it/s][AEpoch 2:  90%|████████▉ | 3930/4381 [1:39:30<11:25,  1.52s/it, loss=3.63, v_num=641]
Validating:  29%|██▉       | 180/626 [00:35<01:35,  4.66it/s][AEpoch 2:  90%|████████▉ | 3940/4381 [1:39:33<11:08,  1.52s/it, loss=3.63, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:35,  4.57it/s][AEpoch 2:  90%|█████████ | 3950/4381 [1:39:35<10:51,  1.51s/it, loss=3.63, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:40,  4.23it/s][AEpoch 2:  90%|█████████ | 3960/4381 [1:39:38<10:35,  1.51s/it, loss=3.63, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:32,  4.48it/s][AEpoch 2:  91%|█████████ | 3970/4381 [1:39:40<10:18,  1.51s/it, loss=3.63, v_num=641]
Validating:  35%|███▌      | 220/626 [00:44<01:26,  4.68it/s][AEpoch 2:  91%|█████████ | 3980/4381 [1:39:42<10:02,  1.50s/it, loss=3.63, v_num=641]
Validating:  37%|███▋      | 230/626 [00:46<01:23,  4.77it/s][AEpoch 2:  91%|█████████ | 3990/4381 [1:39:44<09:46,  1.50s/it, loss=3.63, v_num=641]
Validating:  38%|███▊      | 240/626 [00:47<01:17,  4.97it/s][AEpoch 2:  91%|█████████▏| 4000/4381 [1:39:46<09:30,  1.50s/it, loss=3.63, v_num=641]
Validating:  40%|███▉      | 250/626 [00:49<01:11,  5.28it/s][AEpoch 2:  92%|█████████▏| 4010/4381 [1:39:47<09:13,  1.49s/it, loss=3.63, v_num=641]
Validating:  42%|████▏     | 260/626 [00:51<01:12,  5.07it/s][AEpoch 2:  92%|█████████▏| 4020/4381 [1:39:50<08:57,  1.49s/it, loss=3.63, v_num=641]
Validating:  43%|████▎     | 270/626 [00:53<01:10,  5.04it/s][AEpoch 2:  92%|█████████▏| 4030/4381 [1:39:52<08:41,  1.49s/it, loss=3.63, v_num=641]
Validating:  45%|████▍     | 280/626 [00:55<01:10,  4.90it/s][AEpoch 2:  92%|█████████▏| 4040/4381 [1:39:54<08:25,  1.48s/it, loss=3.63, v_num=641]
Validating:  46%|████▋     | 290/626 [00:58<01:11,  4.67it/s][AEpoch 2:  92%|█████████▏| 4050/4381 [1:39:56<08:09,  1.48s/it, loss=3.63, v_num=641]
Validating:  48%|████▊     | 300/626 [00:59<01:00,  5.41it/s][AEpoch 2:  93%|█████████▎| 4060/4381 [1:39:57<07:54,  1.48s/it, loss=3.63, v_num=641]
Validating:  50%|████▉     | 310/626 [01:00<00:51,  6.11it/s][AEpoch 2:  93%|█████████▎| 4070/4381 [1:39:58<07:38,  1.47s/it, loss=3.63, v_num=641]
Validating:  51%|█████     | 320/626 [01:02<00:55,  5.47it/s][AEpoch 2:  93%|█████████▎| 4080/4381 [1:40:01<07:22,  1.47s/it, loss=3.63, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:04<00:53,  5.58it/s][AEpoch 2:  93%|█████████▎| 4090/4381 [1:40:02<07:06,  1.47s/it, loss=3.63, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:06<00:50,  5.65it/s][AEpoch 2:  94%|█████████▎| 4100/4381 [1:40:04<06:51,  1.46s/it, loss=3.63, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:08<00:55,  4.93it/s][AEpoch 2:  94%|█████████▍| 4110/4381 [1:40:07<06:36,  1.46s/it, loss=3.63, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:10<00:53,  4.99it/s][AEpoch 2:  94%|█████████▍| 4120/4381 [1:40:09<06:20,  1.46s/it, loss=3.63, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:11<00:43,  5.91it/s][AEpoch 2:  94%|█████████▍| 4130/4381 [1:40:10<06:05,  1.45s/it, loss=3.63, v_num=641]
Validating:  61%|██████    | 380/626 [01:13<00:41,  5.96it/s][AEpoch 2:  94%|█████████▍| 4140/4381 [1:40:11<05:49,  1.45s/it, loss=3.63, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:15<00:41,  5.69it/s][AEpoch 2:  95%|█████████▍| 4150/4381 [1:40:13<05:34,  1.45s/it, loss=3.63, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:16<00:38,  5.89it/s][AEpoch 2:  95%|█████████▍| 4160/4381 [1:40:15<05:19,  1.45s/it, loss=3.63, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:18<00:33,  6.36it/s][AEpoch 2:  95%|█████████▌| 4170/4381 [1:40:16<05:04,  1.44s/it, loss=3.63, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:20<00:38,  5.31it/s][AEpoch 2:  95%|█████████▌| 4180/4381 [1:40:19<04:49,  1.44s/it, loss=3.63, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:22<00:35,  5.56it/s][AEpoch 2:  96%|█████████▌| 4190/4381 [1:40:20<04:34,  1.44s/it, loss=3.63, v_num=641]
Validating:  70%|███████   | 440/626 [01:25<00:40,  4.54it/s][AEpoch 2:  96%|█████████▌| 4200/4381 [1:40:23<04:19,  1.43s/it, loss=3.63, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:27<00:39,  4.44it/s][AEpoch 2:  96%|█████████▌| 4210/4381 [1:40:26<04:04,  1.43s/it, loss=3.63, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:29<00:36,  4.58it/s][AEpoch 2:  96%|█████████▋| 4220/4381 [1:40:28<03:49,  1.43s/it, loss=3.63, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:32<00:34,  4.57it/s][AEpoch 2:  97%|█████████▋| 4230/4381 [1:40:30<03:35,  1.43s/it, loss=3.63, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:34<00:31,  4.57it/s][AEpoch 2:  97%|█████████▋| 4240/4381 [1:40:32<03:20,  1.42s/it, loss=3.63, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:36<00:30,  4.44it/s][AEpoch 2:  97%|█████████▋| 4250/4381 [1:40:35<03:05,  1.42s/it, loss=3.63, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:38<00:26,  4.77it/s][AEpoch 2:  97%|█████████▋| 4260/4381 [1:40:36<02:51,  1.42s/it, loss=3.63, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:40<00:25,  4.48it/s][AEpoch 2:  97%|█████████▋| 4270/4381 [1:40:39<02:36,  1.41s/it, loss=3.63, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:42<00:21,  5.03it/s][AEpoch 2:  98%|█████████▊| 4280/4381 [1:40:40<02:22,  1.41s/it, loss=3.63, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:44<00:18,  5.14it/s][AEpoch 2:  98%|█████████▊| 4290/4381 [1:40:42<02:08,  1.41s/it, loss=3.63, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:45<00:15,  5.57it/s][AEpoch 2:  98%|█████████▊| 4300/4381 [1:40:44<01:53,  1.41s/it, loss=3.63, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:47<00:13,  5.83it/s][AEpoch 2:  98%|█████████▊| 4310/4381 [1:40:45<01:39,  1.40s/it, loss=3.63, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:48<00:10,  6.08it/s][AEpoch 2:  99%|█████████▊| 4320/4381 [1:40:47<01:25,  1.40s/it, loss=3.63, v_num=641]
Validating:  91%|█████████ | 570/626 [01:50<00:09,  6.05it/s][AEpoch 2:  99%|█████████▉| 4330/4381 [1:40:48<01:11,  1.40s/it, loss=3.63, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:52<00:08,  5.46it/s][AEpoch 2:  99%|█████████▉| 4340/4381 [1:40:51<00:57,  1.39s/it, loss=3.63, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:53<00:06,  5.91it/s][AEpoch 2:  99%|█████████▉| 4350/4381 [1:40:52<00:43,  1.39s/it, loss=3.63, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:55<00:04,  6.07it/s][AEpoch 2: 100%|█████████▉| 4360/4381 [1:40:53<00:29,  1.39s/it, loss=3.63, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:56<00:02,  6.83it/s][AEpoch 2: 100%|█████████▉| 4370/4381 [1:40:54<00:15,  1.39s/it, loss=3.63, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:58<00:00,  6.37it/s][AEpoch 2: 100%|█████████▉| 4380/4381 [1:40:56<00:01,  1.38s/it, loss=3.63, v_num=641]
Validating: 100%|██████████| 626/626 [01:58<00:00,  7.00it/s][Avalidation_epoch_end
graph acc: 0.001597444089456869
valid accuracy: 0.8655656576156616
Epoch 2: 100%|██████████| 4381/4381 [1:40:58<00:00,  1.38s/it, loss=3.6, v_num=641] 
                                                             [AEpoch 2:   0%|          | 0/4381 [00:00<00:00, 11335.96it/s, loss=3.6, v_num=641]  Epoch 3:   0%|          | 0/4381 [00:00<00:01, 2972.58it/s, loss=3.6, v_num=641] Epoch 3:   0%|          | 0/4381 [00:16<20:01:56, 16.46s/it, loss=3.6, v_num=641]Epoch 3:   0%|          | 10/4381 [00:22<2:25:47,  2.00s/it, loss=3.6, v_num=641]Epoch 3:   0%|          | 10/4381 [00:22<2:25:47,  2.00s/it, loss=3.56, v_num=641]Epoch 3:   0%|          | 20/4381 [00:37<2:09:39,  1.78s/it, loss=3.56, v_num=641]Epoch 3:   0%|          | 20/4381 [00:37<2:09:39,  1.78s/it, loss=3.57, v_num=641]Epoch 3:   1%|          | 30/4381 [00:55<2:10:50,  1.80s/it, loss=3.57, v_num=641]Epoch 3:   1%|          | 30/4381 [00:55<2:10:50,  1.80s/it, loss=3.57, v_num=641]Epoch 3:   1%|          | 40/4381 [01:11<2:06:01,  1.74s/it, loss=3.57, v_num=641]Epoch 3:   1%|          | 40/4381 [01:11<2:06:01,  1.74s/it, loss=3.54, v_num=641]Epoch 3:   1%|          | 50/4381 [01:26<2:01:46,  1.69s/it, loss=3.54, v_num=641]Epoch 3:   1%|          | 50/4381 [01:26<2:01:46,  1.69s/it, loss=3.56, v_num=641]Epoch 3:   1%|▏         | 60/4381 [01:43<2:02:21,  1.70s/it, loss=3.56, v_num=641]Epoch 3:   1%|▏         | 60/4381 [01:43<2:02:21,  1.70s/it, loss=3.56, v_num=641]Epoch 3:   2%|▏         | 70/4381 [01:59<2:00:43,  1.68s/it, loss=3.56, v_num=641]Epoch 3:   2%|▏         | 70/4381 [01:59<2:00:43,  1.68s/it, loss=3.55, v_num=641]Epoch 3:   2%|▏         | 80/4381 [02:14<1:58:40,  1.66s/it, loss=3.55, v_num=641]Epoch 3:   2%|▏         | 80/4381 [02:14<1:58:40,  1.66s/it, loss=3.55, v_num=641]Epoch 3:   2%|▏         | 90/4381 [02:28<1:57:02,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   2%|▏         | 90/4381 [02:28<1:57:02,  1.64s/it, loss=3.56, v_num=641]Epoch 3:   2%|▏         | 100/4381 [02:44<1:56:10,  1.63s/it, loss=3.56, v_num=641]Epoch 3:   2%|▏         | 100/4381 [02:44<1:56:10,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 110/4381 [03:01<1:56:36,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 110/4381 [03:01<1:56:36,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 120/4381 [03:17<1:55:41,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 120/4381 [03:17<1:55:41,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 130/4381 [03:32<1:55:02,  1.62s/it, loss=3.55, v_num=641]Epoch 3:   3%|▎         | 130/4381 [03:32<1:55:04,  1.62s/it, loss=3.52, v_num=641]Epoch 3:   3%|▎         | 140/4381 [03:51<1:55:57,  1.64s/it, loss=3.52, v_num=641]Epoch 3:   3%|▎         | 140/4381 [03:51<1:55:58,  1.64s/it, loss=3.53, v_num=641]Epoch 3:   3%|▎         | 150/4381 [04:07<1:55:43,  1.64s/it, loss=3.53, v_num=641]Epoch 3:   3%|▎         | 150/4381 [04:07<1:55:43,  1.64s/it, loss=3.52, v_num=641]Epoch 3:   4%|▎         | 160/4381 [04:22<1:54:55,  1.63s/it, loss=3.52, v_num=641]Epoch 3:   4%|▎         | 160/4381 [04:22<1:54:55,  1.63s/it, loss=3.51, v_num=641]Epoch 3:   4%|▍         | 170/4381 [04:37<1:53:55,  1.62s/it, loss=3.51, v_num=641]Epoch 3:   4%|▍         | 170/4381 [04:37<1:53:55,  1.62s/it, loss=3.53, v_num=641]Epoch 3:   4%|▍         | 180/4381 [04:56<1:54:37,  1.64s/it, loss=3.53, v_num=641]Epoch 3:   4%|▍         | 180/4381 [04:56<1:54:37,  1.64s/it, loss=3.56, v_num=641]Epoch 3:   4%|▍         | 190/4381 [05:10<1:53:35,  1.63s/it, loss=3.56, v_num=641]Epoch 3:   4%|▍         | 190/4381 [05:10<1:53:35,  1.63s/it, loss=3.56, v_num=641]Epoch 3:   5%|▍         | 200/4381 [05:24<1:52:37,  1.62s/it, loss=3.56, v_num=641]Epoch 3:   5%|▍         | 200/4381 [05:24<1:52:37,  1.62s/it, loss=3.55, v_num=641]Epoch 3:   5%|▍         | 210/4381 [05:45<1:53:53,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   5%|▍         | 210/4381 [05:45<1:53:53,  1.64s/it, loss=3.5, v_num=641] Epoch 3:   5%|▌         | 220/4381 [06:01<1:53:29,  1.64s/it, loss=3.5, v_num=641]Epoch 3:   5%|▌         | 220/4381 [06:01<1:53:29,  1.64s/it, loss=3.5, v_num=641]Epoch 3:   5%|▌         | 230/4381 [06:16<1:52:39,  1.63s/it, loss=3.5, v_num=641]Epoch 3:   5%|▌         | 230/4381 [06:16<1:52:39,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   5%|▌         | 240/4381 [06:33<1:52:49,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   5%|▌         | 240/4381 [06:33<1:52:49,  1.63s/it, loss=3.56, v_num=641]Epoch 3:   6%|▌         | 250/4381 [06:52<1:53:13,  1.64s/it, loss=3.56, v_num=641]Epoch 3:   6%|▌         | 250/4381 [06:52<1:53:13,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   6%|▌         | 260/4381 [07:07<1:52:25,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   6%|▌         | 260/4381 [07:07<1:52:25,  1.64s/it, loss=3.51, v_num=641]Epoch 3:   6%|▌         | 270/4381 [07:26<1:52:51,  1.65s/it, loss=3.51, v_num=641]Epoch 3:   6%|▌         | 270/4381 [07:26<1:52:51,  1.65s/it, loss=3.55, v_num=641]Epoch 3:   6%|▋         | 280/4381 [07:43<1:52:38,  1.65s/it, loss=3.55, v_num=641]Epoch 3:   6%|▋         | 280/4381 [07:43<1:52:38,  1.65s/it, loss=3.55, v_num=641]Epoch 3:   7%|▋         | 290/4381 [07:56<1:51:41,  1.64s/it, loss=3.55, v_num=641]Epoch 3:   7%|▋         | 290/4381 [07:56<1:51:42,  1.64s/it, loss=3.5, v_num=641] Epoch 3:   7%|▋         | 300/4381 [08:15<1:51:56,  1.65s/it, loss=3.5, v_num=641]Epoch 3:   7%|▋         | 300/4381 [08:15<1:51:56,  1.65s/it, loss=3.5, v_num=641]Epoch 3:   7%|▋         | 310/4381 [08:30<1:51:19,  1.64s/it, loss=3.5, v_num=641]Epoch 3:   7%|▋         | 310/4381 [08:30<1:51:19,  1.64s/it, loss=3.54, v_num=641]Epoch 3:   7%|▋         | 320/4381 [08:44<1:50:29,  1.63s/it, loss=3.54, v_num=641]Epoch 3:   7%|▋         | 320/4381 [08:44<1:50:29,  1.63s/it, loss=3.58, v_num=641]Epoch 3:   8%|▊         | 330/4381 [09:01<1:50:27,  1.64s/it, loss=3.58, v_num=641]Epoch 3:   8%|▊         | 330/4381 [09:01<1:50:27,  1.64s/it, loss=3.57, v_num=641]Epoch 3:   8%|▊         | 340/4381 [09:16<1:49:56,  1.63s/it, loss=3.57, v_num=641]Epoch 3:   8%|▊         | 340/4381 [09:16<1:49:56,  1.63s/it, loss=3.53, v_num=641]Epoch 3:   8%|▊         | 350/4381 [09:32<1:49:29,  1.63s/it, loss=3.53, v_num=641]Epoch 3:   8%|▊         | 350/4381 [09:32<1:49:29,  1.63s/it, loss=3.53, v_num=641]Epoch 3:   8%|▊         | 360/4381 [09:48<1:49:11,  1.63s/it, loss=3.53, v_num=641]Epoch 3:   8%|▊         | 360/4381 [09:48<1:49:11,  1.63s/it, loss=3.54, v_num=641]Epoch 3:   8%|▊         | 370/4381 [10:02<1:48:33,  1.62s/it, loss=3.54, v_num=641]Epoch 3:   8%|▊         | 370/4381 [10:02<1:48:33,  1.62s/it, loss=3.53, v_num=641]Epoch 3:   9%|▊         | 380/4381 [10:16<1:47:53,  1.62s/it, loss=3.53, v_num=641]Epoch 3:   9%|▊         | 380/4381 [10:16<1:47:53,  1.62s/it, loss=3.55, v_num=641]Epoch 3:   9%|▉         | 390/4381 [10:36<1:48:20,  1.63s/it, loss=3.55, v_num=641]Epoch 3:   9%|▉         | 390/4381 [10:36<1:48:20,  1.63s/it, loss=3.54, v_num=641]Epoch 3:   9%|▉         | 400/4381 [10:49<1:47:31,  1.62s/it, loss=3.54, v_num=641]Epoch 3:   9%|▉         | 400/4381 [10:49<1:47:31,  1.62s/it, loss=3.52, v_num=641]Epoch 3:   9%|▉         | 410/4381 [11:06<1:47:15,  1.62s/it, loss=3.52, v_num=641]Epoch 3:   9%|▉         | 410/4381 [11:06<1:47:15,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  10%|▉         | 420/4381 [11:24<1:47:17,  1.63s/it, loss=3.49, v_num=641]Epoch 3:  10%|▉         | 420/4381 [11:24<1:47:17,  1.63s/it, loss=3.49, v_num=641]Epoch 3:  10%|▉         | 430/4381 [11:38<1:46:44,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  10%|▉         | 430/4381 [11:38<1:46:44,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  10%|█         | 440/4381 [11:53<1:46:19,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  10%|█         | 440/4381 [11:53<1:46:19,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  10%|█         | 450/4381 [12:12<1:46:26,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  10%|█         | 450/4381 [12:12<1:46:26,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  10%|█         | 460/4381 [12:27<1:46:00,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  10%|█         | 460/4381 [12:27<1:46:00,  1.62s/it, loss=3.5, v_num=641] Epoch 3:  11%|█         | 470/4381 [12:44<1:45:45,  1.62s/it, loss=3.5, v_num=641]Epoch 3:  11%|█         | 470/4381 [12:44<1:45:45,  1.62s/it, loss=3.5, v_num=641]Epoch 3:  11%|█         | 480/4381 [13:01<1:45:34,  1.62s/it, loss=3.5, v_num=641]Epoch 3:  11%|█         | 480/4381 [13:01<1:45:34,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  11%|█         | 490/4381 [13:16<1:45:12,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  11%|█         | 490/4381 [13:16<1:45:12,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  11%|█▏        | 500/4381 [13:34<1:45:07,  1.63s/it, loss=3.53, v_num=641]Epoch 3:  11%|█▏        | 500/4381 [13:34<1:45:07,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  12%|█▏        | 510/4381 [13:50<1:44:54,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  12%|█▏        | 510/4381 [13:50<1:44:54,  1.63s/it, loss=3.55, v_num=641]Epoch 3:  12%|█▏        | 520/4381 [14:07<1:44:42,  1.63s/it, loss=3.55, v_num=641]Epoch 3:  12%|█▏        | 520/4381 [14:07<1:44:42,  1.63s/it, loss=3.53, v_num=641]Epoch 3:  12%|█▏        | 530/4381 [14:22<1:44:17,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  12%|█▏        | 530/4381 [14:22<1:44:17,  1.62s/it, loss=3.46, v_num=641]Epoch 3:  12%|█▏        | 540/4381 [14:41<1:44:19,  1.63s/it, loss=3.46, v_num=641]Epoch 3:  12%|█▏        | 540/4381 [14:41<1:44:19,  1.63s/it, loss=3.5, v_num=641] Epoch 3:  13%|█▎        | 550/4381 [14:58<1:44:07,  1.63s/it, loss=3.5, v_num=641]Epoch 3:  13%|█▎        | 550/4381 [14:58<1:44:07,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  13%|█▎        | 560/4381 [15:13<1:43:42,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  13%|█▎        | 560/4381 [15:13<1:43:42,  1.63s/it, loss=3.5, v_num=641] Epoch 3:  13%|█▎        | 570/4381 [15:32<1:43:42,  1.63s/it, loss=3.5, v_num=641]Epoch 3:  13%|█▎        | 570/4381 [15:32<1:43:42,  1.63s/it, loss=3.53, v_num=641]Epoch 3:  13%|█▎        | 580/4381 [15:46<1:43:12,  1.63s/it, loss=3.53, v_num=641]Epoch 3:  13%|█▎        | 580/4381 [15:46<1:43:12,  1.63s/it, loss=3.49, v_num=641]Epoch 3:  13%|█▎        | 590/4381 [16:02<1:42:56,  1.63s/it, loss=3.49, v_num=641]Epoch 3:  13%|█▎        | 590/4381 [16:02<1:42:56,  1.63s/it, loss=3.46, v_num=641]Epoch 3:  14%|█▎        | 600/4381 [16:20<1:42:48,  1.63s/it, loss=3.46, v_num=641]Epoch 3:  14%|█▎        | 600/4381 [16:20<1:42:48,  1.63s/it, loss=3.5, v_num=641] Epoch 3:  14%|█▍        | 610/4381 [16:35<1:42:25,  1.63s/it, loss=3.5, v_num=641]Epoch 3:  14%|█▍        | 610/4381 [16:35<1:42:25,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  14%|█▍        | 620/4381 [16:49<1:41:51,  1.63s/it, loss=3.52, v_num=641]Epoch 3:  14%|█▍        | 620/4381 [16:49<1:41:51,  1.63s/it, loss=3.51, v_num=641]Epoch 3:  14%|█▍        | 630/4381 [17:05<1:41:35,  1.63s/it, loss=3.51, v_num=641]Epoch 3:  14%|█▍        | 630/4381 [17:05<1:41:35,  1.63s/it, loss=3.49, v_num=641]Epoch 3:  15%|█▍        | 640/4381 [17:20<1:41:12,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  15%|█▍        | 640/4381 [17:20<1:41:12,  1.62s/it, loss=3.51, v_num=641]Epoch 3:  15%|█▍        | 650/4381 [17:34<1:40:43,  1.62s/it, loss=3.51, v_num=641]Epoch 3:  15%|█▍        | 650/4381 [17:34<1:40:43,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  15%|█▌        | 660/4381 [17:52<1:40:37,  1.62s/it, loss=3.53, v_num=641]Epoch 3:  15%|█▌        | 660/4381 [17:52<1:40:37,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  15%|█▌        | 670/4381 [18:06<1:40:10,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  15%|█▌        | 670/4381 [18:06<1:40:10,  1.62s/it, loss=3.51, v_num=641]Epoch 3:  16%|█▌        | 680/4381 [18:21<1:39:47,  1.62s/it, loss=3.51, v_num=641]Epoch 3:  16%|█▌        | 680/4381 [18:21<1:39:47,  1.62s/it, loss=3.47, v_num=641]Epoch 3:  16%|█▌        | 690/4381 [18:39<1:39:38,  1.62s/it, loss=3.47, v_num=641]Epoch 3:  16%|█▌        | 690/4381 [18:39<1:39:38,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  16%|█▌        | 700/4381 [18:52<1:39:09,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  16%|█▌        | 700/4381 [18:52<1:39:09,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  16%|█▌        | 710/4381 [19:08<1:38:50,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  16%|█▌        | 710/4381 [19:08<1:38:50,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  16%|█▋        | 720/4381 [19:27<1:38:50,  1.62s/it, loss=3.49, v_num=641]Epoch 3:  16%|█▋        | 720/4381 [19:27<1:38:50,  1.62s/it, loss=3.48, v_num=641]Epoch 3:  17%|█▋        | 730/4381 [19:41<1:38:22,  1.62s/it, loss=3.48, v_num=641]Epoch 3:  17%|█▋        | 730/4381 [19:41<1:38:22,  1.62s/it, loss=3.52, v_num=641]Epoch 3:  17%|█▋        | 740/4381 [19:52<1:37:41,  1.61s/it, loss=3.52, v_num=641]Epoch 3:  17%|█▋        | 740/4381 [19:52<1:37:41,  1.61s/it, loss=3.51, v_num=641]Epoch 3:  17%|█▋        | 750/4381 [20:07<1:37:20,  1.61s/it, loss=3.51, v_num=641]Epoch 3:  17%|█▋        | 750/4381 [20:07<1:37:20,  1.61s/it, loss=3.47, v_num=641]Epoch 3:  17%|█▋        | 760/4381 [20:23<1:36:59,  1.61s/it, loss=3.47, v_num=641]Epoch 3:  17%|█▋        | 760/4381 [20:23<1:36:59,  1.61s/it, loss=3.46, v_num=641]Epoch 3:  18%|█▊        | 770/4381 [20:36<1:36:31,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  18%|█▊        | 770/4381 [20:36<1:36:31,  1.60s/it, loss=3.5, v_num=641] Epoch 3:  18%|█▊        | 780/4381 [20:53<1:36:18,  1.60s/it, loss=3.5, v_num=641]Epoch 3:  18%|█▊        | 780/4381 [20:53<1:36:18,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  18%|█▊        | 790/4381 [21:08<1:36:00,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  18%|█▊        | 790/4381 [21:08<1:36:00,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  18%|█▊        | 800/4381 [21:22<1:35:31,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  18%|█▊        | 800/4381 [21:22<1:35:31,  1.60s/it, loss=3.5, v_num=641] Epoch 3:  18%|█▊        | 810/4381 [21:40<1:35:25,  1.60s/it, loss=3.5, v_num=641]Epoch 3:  18%|█▊        | 810/4381 [21:40<1:35:25,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  19%|█▊        | 820/4381 [21:53<1:34:55,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  19%|█▊        | 820/4381 [21:53<1:34:55,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  19%|█▉        | 830/4381 [22:14<1:35:02,  1.61s/it, loss=3.48, v_num=641]Epoch 3:  19%|█▉        | 830/4381 [22:14<1:35:02,  1.61s/it, loss=3.5, v_num=641] Epoch 3:  19%|█▉        | 840/4381 [22:28<1:34:37,  1.60s/it, loss=3.5, v_num=641]Epoch 3:  19%|█▉        | 840/4381 [22:28<1:34:37,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  19%|█▉        | 850/4381 [22:44<1:34:20,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  19%|█▉        | 850/4381 [22:44<1:34:20,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  20%|█▉        | 860/4381 [23:00<1:34:06,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  20%|█▉        | 860/4381 [23:00<1:34:06,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  20%|█▉        | 870/4381 [23:16<1:33:50,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  20%|█▉        | 870/4381 [23:16<1:33:50,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  20%|██        | 880/4381 [23:30<1:33:23,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  20%|██        | 880/4381 [23:30<1:33:23,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  20%|██        | 890/4381 [23:47<1:33:12,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  20%|██        | 890/4381 [23:47<1:33:12,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  21%|██        | 900/4381 [24:04<1:33:02,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  21%|██        | 900/4381 [24:04<1:33:02,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  21%|██        | 910/4381 [24:20<1:32:43,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  21%|██        | 910/4381 [24:20<1:32:43,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  21%|██        | 920/4381 [24:34<1:32:22,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  21%|██        | 920/4381 [24:34<1:32:22,  1.60s/it, loss=3.51, v_num=641]Epoch 3:  21%|██        | 930/4381 [24:53<1:32:14,  1.60s/it, loss=3.51, v_num=641]Epoch 3:  21%|██        | 930/4381 [24:53<1:32:14,  1.60s/it, loss=3.51, v_num=641]Epoch 3:  21%|██▏       | 940/4381 [25:06<1:31:47,  1.60s/it, loss=3.51, v_num=641]Epoch 3:  21%|██▏       | 940/4381 [25:06<1:31:47,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  22%|██▏       | 950/4381 [25:21<1:31:29,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  22%|██▏       | 950/4381 [25:21<1:31:29,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  22%|██▏       | 960/4381 [25:36<1:31:11,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  22%|██▏       | 960/4381 [25:36<1:31:11,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  22%|██▏       | 970/4381 [25:49<1:30:43,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  22%|██▏       | 970/4381 [25:49<1:30:43,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  22%|██▏       | 980/4381 [26:07<1:30:33,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  22%|██▏       | 980/4381 [26:07<1:30:33,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  23%|██▎       | 990/4381 [26:21<1:30:11,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  23%|██▎       | 990/4381 [26:21<1:30:11,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  23%|██▎       | 1000/4381 [26:35<1:29:49,  1.59s/it, loss=3.47, v_num=641]Epoch 3:  23%|██▎       | 1000/4381 [26:35<1:29:49,  1.59s/it, loss=3.48, v_num=641]Epoch 3:  23%|██▎       | 1010/4381 [26:51<1:29:34,  1.59s/it, loss=3.48, v_num=641]Epoch 3:  23%|██▎       | 1010/4381 [26:51<1:29:34,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  23%|██▎       | 1020/4381 [27:10<1:29:27,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  23%|██▎       | 1020/4381 [27:10<1:29:27,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  24%|██▎       | 1030/4381 [27:22<1:28:59,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  24%|██▎       | 1030/4381 [27:22<1:28:59,  1.59s/it, loss=3.47, v_num=641]Epoch 3:  24%|██▎       | 1040/4381 [27:41<1:28:53,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  24%|██▎       | 1040/4381 [27:41<1:28:53,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  24%|██▍       | 1050/4381 [28:00<1:28:45,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  24%|██▍       | 1050/4381 [28:00<1:28:45,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  24%|██▍       | 1060/4381 [28:15<1:28:27,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  24%|██▍       | 1060/4381 [28:15<1:28:27,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  24%|██▍       | 1070/4381 [28:32<1:28:14,  1.60s/it, loss=3.48, v_num=641]Epoch 3:  24%|██▍       | 1070/4381 [28:32<1:28:14,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  25%|██▍       | 1080/4381 [28:50<1:28:03,  1.60s/it, loss=3.49, v_num=641]Epoch 3:  25%|██▍       | 1080/4381 [28:50<1:28:03,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  25%|██▍       | 1090/4381 [29:04<1:27:42,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  25%|██▍       | 1090/4381 [29:04<1:27:42,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  25%|██▌       | 1100/4381 [29:20<1:27:25,  1.60s/it, loss=3.46, v_num=641]Epoch 3:  25%|██▌       | 1100/4381 [29:20<1:27:25,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  25%|██▌       | 1110/4381 [29:35<1:27:08,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  25%|██▌       | 1110/4381 [29:35<1:27:08,  1.60s/it, loss=3.41, v_num=641]Epoch 3:  26%|██▌       | 1120/4381 [29:48<1:26:42,  1.60s/it, loss=3.41, v_num=641]Epoch 3:  26%|██▌       | 1120/4381 [29:48<1:26:42,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  26%|██▌       | 1130/4381 [30:07<1:26:34,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  26%|██▌       | 1130/4381 [30:07<1:26:34,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  26%|██▌       | 1140/4381 [30:21<1:26:13,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  26%|██▌       | 1140/4381 [30:21<1:26:13,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  26%|██▌       | 1150/4381 [30:36<1:25:54,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  26%|██▌       | 1150/4381 [30:36<1:25:54,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  26%|██▋       | 1160/4381 [30:53<1:25:42,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  26%|██▋       | 1160/4381 [30:53<1:25:42,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  27%|██▋       | 1170/4381 [31:08<1:25:23,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  27%|██▋       | 1170/4381 [31:08<1:25:23,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  27%|██▋       | 1180/4381 [31:25<1:25:09,  1.60s/it, loss=3.47, v_num=641]Epoch 3:  27%|██▋       | 1180/4381 [31:25<1:25:09,  1.60s/it, loss=3.42, v_num=641]Epoch 3:  27%|██▋       | 1190/4381 [31:39<1:24:49,  1.60s/it, loss=3.42, v_num=641]Epoch 3:  27%|██▋       | 1190/4381 [31:39<1:24:50,  1.60s/it, loss=3.43, v_num=641]Epoch 3:  27%|██▋       | 1200/4381 [31:55<1:24:32,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  27%|██▋       | 1200/4381 [31:55<1:24:32,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  28%|██▊       | 1210/4381 [32:09<1:24:13,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  28%|██▊       | 1210/4381 [32:09<1:24:13,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  28%|██▊       | 1220/4381 [32:25<1:23:57,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  28%|██▊       | 1220/4381 [32:25<1:23:57,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  28%|██▊       | 1230/4381 [32:43<1:23:46,  1.60s/it, loss=3.45, v_num=641]Epoch 3:  28%|██▊       | 1230/4381 [32:43<1:23:46,  1.60s/it, loss=3.44, v_num=641]Epoch 3:  28%|██▊       | 1240/4381 [32:59<1:23:29,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  28%|██▊       | 1240/4381 [32:59<1:23:29,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  29%|██▊       | 1250/4381 [33:12<1:23:05,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  29%|██▊       | 1250/4381 [33:12<1:23:05,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  29%|██▉       | 1260/4381 [33:28<1:22:51,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  29%|██▉       | 1260/4381 [33:28<1:22:51,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  29%|██▉       | 1270/4381 [33:42<1:22:30,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  29%|██▉       | 1270/4381 [33:42<1:22:30,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  29%|██▉       | 1280/4381 [33:58<1:22:15,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  29%|██▉       | 1280/4381 [33:58<1:22:15,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  29%|██▉       | 1290/4381 [34:18<1:22:09,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  29%|██▉       | 1290/4381 [34:18<1:22:09,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  30%|██▉       | 1300/4381 [34:34<1:21:53,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  30%|██▉       | 1300/4381 [34:34<1:21:53,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|██▉       | 1310/4381 [34:49<1:21:34,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|██▉       | 1310/4381 [34:49<1:21:34,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|███       | 1320/4381 [35:06<1:21:20,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|███       | 1320/4381 [35:06<1:21:20,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|███       | 1330/4381 [35:19<1:20:58,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  30%|███       | 1330/4381 [35:19<1:20:58,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  31%|███       | 1340/4381 [35:33<1:20:37,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  31%|███       | 1340/4381 [35:33<1:20:37,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  31%|███       | 1350/4381 [35:53<1:20:31,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  31%|███       | 1350/4381 [35:53<1:20:31,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███       | 1360/4381 [36:09<1:20:16,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███       | 1360/4381 [36:09<1:20:16,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███▏      | 1370/4381 [36:24<1:19:58,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███▏      | 1370/4381 [36:24<1:19:58,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███▏      | 1380/4381 [36:40<1:19:42,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  31%|███▏      | 1380/4381 [36:40<1:19:42,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  32%|███▏      | 1390/4381 [36:54<1:19:22,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  32%|███▏      | 1390/4381 [36:54<1:19:22,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  32%|███▏      | 1400/4381 [37:11<1:19:08,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  32%|███▏      | 1400/4381 [37:11<1:19:08,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  32%|███▏      | 1410/4381 [37:28<1:18:54,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  32%|███▏      | 1410/4381 [37:28<1:18:54,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  32%|███▏      | 1420/4381 [37:43<1:18:36,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  32%|███▏      | 1420/4381 [37:43<1:18:36,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  33%|███▎      | 1430/4381 [37:58<1:18:18,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  33%|███▎      | 1430/4381 [37:58<1:18:18,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  33%|███▎      | 1440/4381 [38:16<1:18:07,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  33%|███▎      | 1440/4381 [38:16<1:18:07,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  33%|███▎      | 1450/4381 [38:32<1:17:52,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  33%|███▎      | 1450/4381 [38:32<1:17:52,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  33%|███▎      | 1460/4381 [38:47<1:17:32,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  33%|███▎      | 1460/4381 [38:47<1:17:32,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  34%|███▎      | 1470/4381 [39:06<1:17:22,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  34%|███▎      | 1470/4381 [39:06<1:17:22,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  34%|███▍      | 1480/4381 [39:21<1:17:06,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  34%|███▍      | 1480/4381 [39:21<1:17:06,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  34%|███▍      | 1490/4381 [39:36<1:16:48,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  34%|███▍      | 1490/4381 [39:36<1:16:48,  1.59s/it, loss=3.5, v_num=641] Epoch 3:  34%|███▍      | 1500/4381 [39:51<1:16:30,  1.59s/it, loss=3.5, v_num=641]Epoch 3:  34%|███▍      | 1500/4381 [39:51<1:16:30,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  34%|███▍      | 1510/4381 [40:07<1:16:15,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  34%|███▍      | 1510/4381 [40:07<1:16:15,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  35%|███▍      | 1520/4381 [40:21<1:15:54,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  35%|███▍      | 1520/4381 [40:21<1:15:54,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  35%|███▍      | 1530/4381 [40:38<1:15:40,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  35%|███▍      | 1530/4381 [40:38<1:15:40,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  35%|███▌      | 1540/4381 [40:52<1:15:22,  1.59s/it, loss=3.45, v_num=641]Epoch 3:  35%|███▌      | 1540/4381 [40:52<1:15:22,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  35%|███▌      | 1550/4381 [41:05<1:15:00,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  35%|███▌      | 1550/4381 [41:05<1:15:00,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  36%|███▌      | 1560/4381 [41:23<1:14:47,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  36%|███▌      | 1560/4381 [41:23<1:14:47,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  36%|███▌      | 1570/4381 [41:39<1:14:32,  1.59s/it, loss=3.46, v_num=641]Epoch 3:  36%|███▌      | 1570/4381 [41:39<1:14:32,  1.59s/it, loss=3.47, v_num=641]Epoch 3:  36%|███▌      | 1580/4381 [41:56<1:14:17,  1.59s/it, loss=3.47, v_num=641]Epoch 3:  36%|███▌      | 1580/4381 [41:56<1:14:17,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  36%|███▋      | 1590/4381 [42:15<1:14:07,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  36%|███▋      | 1590/4381 [42:15<1:14:07,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  37%|███▋      | 1600/4381 [42:28<1:13:46,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  37%|███▋      | 1600/4381 [42:28<1:13:46,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  37%|███▋      | 1610/4381 [42:40<1:13:24,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  37%|███▋      | 1610/4381 [42:40<1:13:24,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  37%|███▋      | 1620/4381 [43:04<1:13:21,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  37%|███▋      | 1620/4381 [43:04<1:13:21,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  37%|███▋      | 1630/4381 [43:17<1:13:00,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  37%|███▋      | 1630/4381 [43:17<1:13:00,  1.59s/it, loss=3.4, v_num=641] Epoch 3:  37%|███▋      | 1640/4381 [43:30<1:12:40,  1.59s/it, loss=3.4, v_num=641]Epoch 3:  37%|███▋      | 1640/4381 [43:30<1:12:40,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  38%|███▊      | 1650/4381 [43:47<1:12:26,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  38%|███▊      | 1650/4381 [43:47<1:12:26,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  38%|███▊      | 1660/4381 [44:02<1:12:08,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  38%|███▊      | 1660/4381 [44:02<1:12:08,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  38%|███▊      | 1670/4381 [44:16<1:11:50,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  38%|███▊      | 1670/4381 [44:16<1:11:50,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  38%|███▊      | 1680/4381 [44:33<1:11:35,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  38%|███▊      | 1680/4381 [44:33<1:11:35,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  39%|███▊      | 1690/4381 [44:49<1:11:19,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  39%|███▊      | 1690/4381 [44:49<1:11:19,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  39%|███▉      | 1700/4381 [45:01<1:10:58,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  39%|███▉      | 1700/4381 [45:01<1:10:58,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  39%|███▉      | 1710/4381 [45:20<1:10:46,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  39%|███▉      | 1710/4381 [45:20<1:10:46,  1.59s/it, loss=3.4, v_num=641] Epoch 3:  39%|███▉      | 1720/4381 [45:35<1:10:29,  1.59s/it, loss=3.4, v_num=641]Epoch 3:  39%|███▉      | 1720/4381 [45:35<1:10:29,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  39%|███▉      | 1730/4381 [45:49<1:10:10,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  39%|███▉      | 1730/4381 [45:49<1:10:10,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  40%|███▉      | 1740/4381 [46:08<1:09:59,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  40%|███▉      | 1740/4381 [46:08<1:09:59,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  40%|███▉      | 1750/4381 [46:24<1:09:43,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  40%|███▉      | 1750/4381 [46:24<1:09:43,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  40%|████      | 1760/4381 [46:39<1:09:25,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  40%|████      | 1760/4381 [46:39<1:09:25,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  40%|████      | 1770/4381 [46:56<1:09:12,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  40%|████      | 1770/4381 [46:56<1:09:12,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  41%|████      | 1780/4381 [47:10<1:08:54,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  41%|████      | 1780/4381 [47:10<1:08:54,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  41%|████      | 1790/4381 [47:25<1:08:36,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  41%|████      | 1790/4381 [47:25<1:08:36,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  41%|████      | 1800/4381 [47:44<1:08:24,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  41%|████      | 1800/4381 [47:44<1:08:24,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  41%|████▏     | 1810/4381 [47:58<1:08:06,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  41%|████▏     | 1810/4381 [47:58<1:08:06,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  42%|████▏     | 1820/4381 [48:12<1:07:47,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  42%|████▏     | 1820/4381 [48:12<1:07:47,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  42%|████▏     | 1830/4381 [48:28<1:07:31,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  42%|████▏     | 1830/4381 [48:28<1:07:31,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  42%|████▏     | 1840/4381 [48:44<1:07:15,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  42%|████▏     | 1840/4381 [48:44<1:07:15,  1.59s/it, loss=3.35, v_num=641]Epoch 3:  42%|████▏     | 1850/4381 [48:59<1:06:59,  1.59s/it, loss=3.35, v_num=641]Epoch 3:  42%|████▏     | 1850/4381 [48:59<1:06:59,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  42%|████▏     | 1860/4381 [49:16<1:06:45,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  42%|████▏     | 1860/4381 [49:16<1:06:45,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  43%|████▎     | 1870/4381 [49:32<1:06:29,  1.59s/it, loss=3.41, v_num=641]Epoch 3:  43%|████▎     | 1870/4381 [49:32<1:06:29,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  43%|████▎     | 1880/4381 [49:46<1:06:10,  1.59s/it, loss=3.42, v_num=641]Epoch 3:  43%|████▎     | 1880/4381 [49:46<1:06:10,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  43%|████▎     | 1890/4381 [50:04<1:05:57,  1.59s/it, loss=3.43, v_num=641]Epoch 3:  43%|████▎     | 1890/4381 [50:04<1:05:57,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  43%|████▎     | 1900/4381 [50:20<1:05:42,  1.59s/it, loss=3.44, v_num=641]Epoch 3:  43%|████▎     | 1900/4381 [50:20<1:05:42,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  44%|████▎     | 1910/4381 [50:34<1:05:23,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  44%|████▎     | 1910/4381 [50:34<1:05:23,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  44%|████▍     | 1920/4381 [50:51<1:05:09,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  44%|████▍     | 1920/4381 [50:51<1:05:09,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  44%|████▍     | 1930/4381 [51:06<1:04:51,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  44%|████▍     | 1930/4381 [51:06<1:04:51,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  44%|████▍     | 1940/4381 [51:19<1:04:32,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  44%|████▍     | 1940/4381 [51:19<1:04:32,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  45%|████▍     | 1950/4381 [51:37<1:04:19,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  45%|████▍     | 1950/4381 [51:37<1:04:19,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  45%|████▍     | 1960/4381 [51:51<1:04:01,  1.59s/it, loss=3.38, v_num=641]Epoch 3:  45%|████▍     | 1960/4381 [51:51<1:04:01,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  45%|████▍     | 1970/4381 [52:07<1:03:46,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  45%|████▍     | 1970/4381 [52:07<1:03:46,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  45%|████▌     | 1980/4381 [52:23<1:03:29,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  45%|████▌     | 1980/4381 [52:23<1:03:29,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  45%|████▌     | 1990/4381 [52:38<1:03:13,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  45%|████▌     | 1990/4381 [52:38<1:03:13,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  46%|████▌     | 2000/4381 [52:55<1:02:58,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  46%|████▌     | 2000/4381 [52:55<1:02:58,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  46%|████▌     | 2010/4381 [53:12<1:02:43,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  46%|████▌     | 2010/4381 [53:12<1:02:43,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  46%|████▌     | 2020/4381 [53:25<1:02:25,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  46%|████▌     | 2020/4381 [53:25<1:02:25,  1.59s/it, loss=3.37, v_num=641]Epoch 3:  46%|████▋     | 2030/4381 [53:38<1:02:05,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  46%|████▋     | 2030/4381 [53:38<1:02:05,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  47%|████▋     | 2040/4381 [53:55<1:01:50,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  47%|████▋     | 2040/4381 [53:55<1:01:50,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  47%|████▋     | 2050/4381 [54:11<1:01:34,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  47%|████▋     | 2050/4381 [54:11<1:01:34,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  47%|████▋     | 2060/4381 [54:25<1:01:16,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  47%|████▋     | 2060/4381 [54:25<1:01:16,  1.58s/it, loss=3.4, v_num=641] Epoch 3:  47%|████▋     | 2070/4381 [54:42<1:01:02,  1.58s/it, loss=3.4, v_num=641]Epoch 3:  47%|████▋     | 2070/4381 [54:42<1:01:02,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  47%|████▋     | 2080/4381 [54:57<1:00:46,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  47%|████▋     | 2080/4381 [54:57<1:00:46,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  48%|████▊     | 2090/4381 [55:15<1:00:32,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  48%|████▊     | 2090/4381 [55:15<1:00:32,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  48%|████▊     | 2100/4381 [55:30<1:00:16,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  48%|████▊     | 2100/4381 [55:30<1:00:16,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  48%|████▊     | 2110/4381 [55:50<1:00:04,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  48%|████▊     | 2110/4381 [55:50<1:00:04,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  48%|████▊     | 2120/4381 [56:03<59:45,  1.59s/it, loss=3.34, v_num=641]  Epoch 3:  48%|████▊     | 2120/4381 [56:03<59:45,  1.59s/it, loss=3.4, v_num=641] Epoch 3:  49%|████▊     | 2130/4381 [56:20<59:30,  1.59s/it, loss=3.4, v_num=641]Epoch 3:  49%|████▊     | 2130/4381 [56:20<59:30,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  49%|████▉     | 2140/4381 [56:39<59:18,  1.59s/it, loss=3.39, v_num=641]Epoch 3:  49%|████▉     | 2140/4381 [56:39<59:18,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  49%|████▉     | 2150/4381 [56:52<58:59,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  49%|████▉     | 2150/4381 [56:52<58:59,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  49%|████▉     | 2160/4381 [57:06<58:41,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  49%|████▉     | 2160/4381 [57:06<58:41,  1.59s/it, loss=3.35, v_num=641]Epoch 3:  50%|████▉     | 2170/4381 [57:22<58:25,  1.59s/it, loss=3.35, v_num=641]Epoch 3:  50%|████▉     | 2170/4381 [57:22<58:25,  1.59s/it, loss=3.36, v_num=641]Epoch 3:  50%|████▉     | 2180/4381 [57:36<58:08,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  50%|████▉     | 2180/4381 [57:36<58:08,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  50%|████▉     | 2190/4381 [57:50<57:50,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  50%|████▉     | 2190/4381 [57:50<57:50,  1.58s/it, loss=3.4, v_num=641] Epoch 3:  50%|█████     | 2200/4381 [58:07<57:35,  1.58s/it, loss=3.4, v_num=641]Epoch 3:  50%|█████     | 2200/4381 [58:07<57:35,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  50%|█████     | 2210/4381 [58:21<57:18,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  50%|█████     | 2210/4381 [58:21<57:18,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  51%|█████     | 2220/4381 [58:34<56:59,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  51%|█████     | 2220/4381 [58:34<56:59,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  51%|█████     | 2230/4381 [58:53<56:46,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  51%|█████     | 2230/4381 [58:53<56:46,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  51%|█████     | 2240/4381 [59:06<56:28,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  51%|█████     | 2240/4381 [59:06<56:28,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  51%|█████▏    | 2250/4381 [59:22<56:12,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  51%|█████▏    | 2250/4381 [59:22<56:12,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  52%|█████▏    | 2260/4381 [59:40<55:59,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  52%|█████▏    | 2260/4381 [59:40<55:59,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  52%|█████▏    | 2270/4381 [59:54<55:40,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  52%|█████▏    | 2270/4381 [59:54<55:40,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  52%|█████▏    | 2280/4381 [1:00:12<55:27,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  52%|█████▏    | 2280/4381 [1:00:12<55:27,  1.58s/it, loss=3.41, v_num=641]Epoch 3:  52%|█████▏    | 2290/4381 [1:00:30<55:13,  1.58s/it, loss=3.41, v_num=641]Epoch 3:  52%|█████▏    | 2290/4381 [1:00:30<55:13,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  52%|█████▏    | 2300/4381 [1:00:42<54:54,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  52%|█████▏    | 2300/4381 [1:00:42<54:54,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  53%|█████▎    | 2310/4381 [1:00:59<54:39,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  53%|█████▎    | 2310/4381 [1:00:59<54:39,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  53%|█████▎    | 2320/4381 [1:01:14<54:23,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  53%|█████▎    | 2320/4381 [1:01:14<54:23,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  53%|█████▎    | 2330/4381 [1:01:30<54:06,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  53%|█████▎    | 2330/4381 [1:01:30<54:06,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  53%|█████▎    | 2340/4381 [1:01:46<53:51,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  53%|█████▎    | 2340/4381 [1:01:46<53:51,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  54%|█████▎    | 2350/4381 [1:02:04<53:37,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  54%|█████▎    | 2350/4381 [1:02:04<53:37,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  54%|█████▍    | 2360/4381 [1:02:18<53:20,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  54%|█████▍    | 2360/4381 [1:02:18<53:20,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  54%|█████▍    | 2370/4381 [1:02:32<53:03,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  54%|█████▍    | 2370/4381 [1:02:32<53:03,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  54%|█████▍    | 2380/4381 [1:02:49<52:47,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  54%|█████▍    | 2380/4381 [1:02:49<52:47,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  55%|█████▍    | 2390/4381 [1:03:03<52:30,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  55%|█████▍    | 2390/4381 [1:03:03<52:30,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  55%|█████▍    | 2400/4381 [1:03:22<52:17,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  55%|█████▍    | 2400/4381 [1:03:22<52:17,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  55%|█████▌    | 2410/4381 [1:03:35<51:59,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  55%|█████▌    | 2410/4381 [1:03:35<51:59,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  55%|█████▌    | 2420/4381 [1:03:48<51:41,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  55%|█████▌    | 2420/4381 [1:03:48<51:41,  1.58s/it, loss=3.39, v_num=641]Epoch 3:  55%|█████▌    | 2430/4381 [1:04:06<51:27,  1.58s/it, loss=3.39, v_num=641]Epoch 3:  55%|█████▌    | 2430/4381 [1:04:06<51:27,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  56%|█████▌    | 2440/4381 [1:04:20<51:10,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  56%|█████▌    | 2440/4381 [1:04:20<51:10,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  56%|█████▌    | 2450/4381 [1:04:35<50:53,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  56%|█████▌    | 2450/4381 [1:04:35<50:53,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  56%|█████▌    | 2460/4381 [1:04:53<50:39,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  56%|█████▌    | 2460/4381 [1:04:53<50:39,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  56%|█████▋    | 2470/4381 [1:05:10<50:24,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  56%|█████▋    | 2470/4381 [1:05:10<50:24,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  57%|█████▋    | 2480/4381 [1:05:25<50:07,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  57%|█████▋    | 2480/4381 [1:05:25<50:07,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  57%|█████▋    | 2490/4381 [1:05:44<49:54,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  57%|█████▋    | 2490/4381 [1:05:44<49:54,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  57%|█████▋    | 2500/4381 [1:05:58<49:36,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  57%|█████▋    | 2500/4381 [1:05:58<49:36,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  57%|█████▋    | 2510/4381 [1:06:13<49:20,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  57%|█████▋    | 2510/4381 [1:06:13<49:20,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2520/4381 [1:06:28<49:04,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2520/4381 [1:06:28<49:04,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  58%|█████▊    | 2530/4381 [1:06:45<48:49,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  58%|█████▊    | 2530/4381 [1:06:45<48:49,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2540/4381 [1:06:59<48:32,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2540/4381 [1:06:59<48:32,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2550/4381 [1:07:17<48:17,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  58%|█████▊    | 2550/4381 [1:07:17<48:17,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  58%|█████▊    | 2560/4381 [1:07:32<48:01,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  58%|█████▊    | 2560/4381 [1:07:32<48:01,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  59%|█████▊    | 2570/4381 [1:07:47<47:44,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  59%|█████▊    | 2570/4381 [1:07:47<47:44,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  59%|█████▉    | 2580/4381 [1:08:07<47:31,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  59%|█████▉    | 2580/4381 [1:08:07<47:31,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  59%|█████▉    | 2590/4381 [1:08:21<47:15,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  59%|█████▉    | 2590/4381 [1:08:21<47:15,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  59%|█████▉    | 2600/4381 [1:08:37<46:59,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  59%|█████▉    | 2600/4381 [1:08:37<46:59,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  60%|█████▉    | 2610/4381 [1:08:53<46:43,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  60%|█████▉    | 2610/4381 [1:08:53<46:43,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  60%|█████▉    | 2620/4381 [1:09:11<46:29,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  60%|█████▉    | 2620/4381 [1:09:11<46:29,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  60%|██████    | 2630/4381 [1:09:22<46:10,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  60%|██████    | 2630/4381 [1:09:22<46:10,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  60%|██████    | 2640/4381 [1:09:39<45:55,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  60%|██████    | 2640/4381 [1:09:39<45:55,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  60%|██████    | 2650/4381 [1:09:54<45:39,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  60%|██████    | 2650/4381 [1:09:54<45:39,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  61%|██████    | 2660/4381 [1:10:08<45:21,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  61%|██████    | 2660/4381 [1:10:08<45:21,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  61%|██████    | 2670/4381 [1:10:26<45:07,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  61%|██████    | 2670/4381 [1:10:26<45:07,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  61%|██████    | 2680/4381 [1:10:41<44:50,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  61%|██████    | 2680/4381 [1:10:41<44:50,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  61%|██████▏   | 2690/4381 [1:10:54<44:33,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  61%|██████▏   | 2690/4381 [1:10:54<44:33,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  62%|██████▏   | 2700/4381 [1:11:14<44:20,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  62%|██████▏   | 2700/4381 [1:11:14<44:20,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  62%|██████▏   | 2710/4381 [1:11:29<44:04,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  62%|██████▏   | 2710/4381 [1:11:29<44:04,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  62%|██████▏   | 2720/4381 [1:11:45<43:48,  1.58s/it, loss=3.38, v_num=641]Epoch 3:  62%|██████▏   | 2720/4381 [1:11:45<43:48,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  62%|██████▏   | 2730/4381 [1:12:05<43:34,  1.58s/it, loss=3.37, v_num=641]Epoch 3:  62%|██████▏   | 2730/4381 [1:12:05<43:34,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  63%|██████▎   | 2740/4381 [1:12:18<43:17,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  63%|██████▎   | 2740/4381 [1:12:18<43:17,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  63%|██████▎   | 2750/4381 [1:12:33<43:00,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  63%|██████▎   | 2750/4381 [1:12:33<43:00,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  63%|██████▎   | 2760/4381 [1:12:52<42:46,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  63%|██████▎   | 2760/4381 [1:12:52<42:46,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  63%|██████▎   | 2770/4381 [1:13:05<42:29,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  63%|██████▎   | 2770/4381 [1:13:05<42:29,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  63%|██████▎   | 2780/4381 [1:13:20<42:13,  1.58s/it, loss=3.35, v_num=641]Epoch 3:  63%|██████▎   | 2780/4381 [1:13:20<42:13,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  64%|██████▎   | 2790/4381 [1:13:38<41:58,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  64%|██████▎   | 2790/4381 [1:13:38<41:58,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  64%|██████▍   | 2800/4381 [1:13:51<41:41,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  64%|██████▍   | 2800/4381 [1:13:51<41:41,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  64%|██████▍   | 2810/4381 [1:14:06<41:24,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  64%|██████▍   | 2810/4381 [1:14:06<41:24,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  64%|██████▍   | 2820/4381 [1:14:26<41:11,  1.58s/it, loss=3.36, v_num=641]Epoch 3:  64%|██████▍   | 2820/4381 [1:14:26<41:11,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  65%|██████▍   | 2830/4381 [1:14:38<40:53,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  65%|██████▍   | 2830/4381 [1:14:38<40:53,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  65%|██████▍   | 2840/4381 [1:14:54<40:38,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  65%|██████▍   | 2840/4381 [1:14:54<40:38,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  65%|██████▌   | 2850/4381 [1:15:13<40:23,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  65%|██████▌   | 2850/4381 [1:15:13<40:23,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  65%|██████▌   | 2860/4381 [1:15:28<40:07,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  65%|██████▌   | 2860/4381 [1:15:28<40:07,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  66%|██████▌   | 2870/4381 [1:15:41<39:50,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  66%|██████▌   | 2870/4381 [1:15:41<39:50,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  66%|██████▌   | 2880/4381 [1:16:00<39:35,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  66%|██████▌   | 2880/4381 [1:16:00<39:35,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  66%|██████▌   | 2890/4381 [1:16:17<39:20,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  66%|██████▌   | 2890/4381 [1:16:17<39:20,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  66%|██████▌   | 2900/4381 [1:16:31<39:04,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  66%|██████▌   | 2900/4381 [1:16:31<39:04,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  66%|██████▋   | 2910/4381 [1:16:53<38:51,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  66%|██████▋   | 2910/4381 [1:16:53<38:51,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  67%|██████▋   | 2920/4381 [1:17:05<38:33,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  67%|██████▋   | 2920/4381 [1:17:05<38:33,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  67%|██████▋   | 2930/4381 [1:17:19<38:16,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  67%|██████▋   | 2930/4381 [1:17:19<38:16,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  67%|██████▋   | 2940/4381 [1:17:39<38:02,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  67%|██████▋   | 2940/4381 [1:17:39<38:02,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  67%|██████▋   | 2950/4381 [1:17:53<37:46,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  67%|██████▋   | 2950/4381 [1:17:53<37:46,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  68%|██████▊   | 2960/4381 [1:18:09<37:30,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  68%|██████▊   | 2960/4381 [1:18:09<37:30,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  68%|██████▊   | 2970/4381 [1:18:28<37:16,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  68%|██████▊   | 2970/4381 [1:18:28<37:16,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  68%|██████▊   | 2980/4381 [1:18:45<37:00,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  68%|██████▊   | 2980/4381 [1:18:45<37:00,  1.59s/it, loss=3.27, v_num=641]Epoch 3:  68%|██████▊   | 2990/4381 [1:19:00<36:44,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  68%|██████▊   | 2990/4381 [1:19:00<36:44,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  68%|██████▊   | 3000/4381 [1:19:15<36:28,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  68%|██████▊   | 3000/4381 [1:19:15<36:28,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  69%|██████▊   | 3010/4381 [1:19:29<36:11,  1.58s/it, loss=3.34, v_num=641]Epoch 3:  69%|██████▊   | 3010/4381 [1:19:29<36:11,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  69%|██████▉   | 3020/4381 [1:19:44<35:55,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  69%|██████▉   | 3020/4381 [1:19:44<35:55,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  69%|██████▉   | 3030/4381 [1:20:04<35:41,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  69%|██████▉   | 3030/4381 [1:20:04<35:41,  1.59s/it, loss=3.33, v_num=641]Epoch 3:  69%|██████▉   | 3040/4381 [1:20:17<35:24,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  69%|██████▉   | 3040/4381 [1:20:17<35:24,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  70%|██████▉   | 3050/4381 [1:20:32<35:08,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  70%|██████▉   | 3050/4381 [1:20:32<35:08,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  70%|██████▉   | 3060/4381 [1:20:49<34:52,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  70%|██████▉   | 3060/4381 [1:20:49<34:52,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  70%|███████   | 3070/4381 [1:21:03<34:36,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  70%|███████   | 3070/4381 [1:21:03<34:36,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  70%|███████   | 3080/4381 [1:21:18<34:19,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  70%|███████   | 3080/4381 [1:21:18<34:19,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  71%|███████   | 3090/4381 [1:21:36<34:05,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  71%|███████   | 3090/4381 [1:21:36<34:05,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  71%|███████   | 3100/4381 [1:21:50<33:48,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  71%|███████   | 3100/4381 [1:21:50<33:48,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  71%|███████   | 3110/4381 [1:22:07<33:33,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  71%|███████   | 3110/4381 [1:22:07<33:33,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  71%|███████   | 3120/4381 [1:22:24<33:17,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  71%|███████   | 3120/4381 [1:22:24<33:17,  1.58s/it, loss=3.3, v_num=641] Epoch 3:  71%|███████▏  | 3130/4381 [1:22:40<33:02,  1.58s/it, loss=3.3, v_num=641]Epoch 3:  71%|███████▏  | 3130/4381 [1:22:40<33:02,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  72%|███████▏  | 3140/4381 [1:22:56<32:46,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  72%|███████▏  | 3140/4381 [1:22:56<32:46,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  72%|███████▏  | 3150/4381 [1:23:14<32:31,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  72%|███████▏  | 3150/4381 [1:23:14<32:31,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  72%|███████▏  | 3160/4381 [1:23:29<32:15,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  72%|███████▏  | 3160/4381 [1:23:29<32:15,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  72%|███████▏  | 3170/4381 [1:23:43<31:58,  1.58s/it, loss=3.31, v_num=641]Epoch 3:  72%|███████▏  | 3170/4381 [1:23:43<31:58,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  73%|███████▎  | 3180/4381 [1:24:03<31:44,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  73%|███████▎  | 3180/4381 [1:24:03<31:44,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  73%|███████▎  | 3190/4381 [1:24:15<31:26,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  73%|███████▎  | 3190/4381 [1:24:15<31:26,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  73%|███████▎  | 3200/4381 [1:24:31<31:11,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  73%|███████▎  | 3200/4381 [1:24:31<31:11,  1.58s/it, loss=3.32, v_num=641]Epoch 3:  73%|███████▎  | 3210/4381 [1:24:50<30:56,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  73%|███████▎  | 3210/4381 [1:24:50<30:56,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  73%|███████▎  | 3220/4381 [1:25:05<30:40,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  73%|███████▎  | 3220/4381 [1:25:05<30:40,  1.59s/it, loss=3.33, v_num=641]Epoch 3:  74%|███████▎  | 3230/4381 [1:25:19<30:23,  1.58s/it, loss=3.33, v_num=641]Epoch 3:  74%|███████▎  | 3230/4381 [1:25:19<30:23,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  74%|███████▍  | 3240/4381 [1:25:34<30:07,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  74%|███████▍  | 3240/4381 [1:25:34<30:07,  1.58s/it, loss=3.25, v_num=641]Epoch 3:  74%|███████▍  | 3250/4381 [1:25:52<29:52,  1.58s/it, loss=3.25, v_num=641]Epoch 3:  74%|███████▍  | 3250/4381 [1:25:52<29:52,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  74%|███████▍  | 3260/4381 [1:26:07<29:36,  1.58s/it, loss=3.26, v_num=641]Epoch 3:  74%|███████▍  | 3260/4381 [1:26:07<29:36,  1.58s/it, loss=3.29, v_num=641]Epoch 3:  75%|███████▍  | 3270/4381 [1:26:26<29:21,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  75%|███████▍  | 3270/4381 [1:26:26<29:21,  1.59s/it, loss=3.33, v_num=641]Epoch 3:  75%|███████▍  | 3280/4381 [1:26:42<29:05,  1.59s/it, loss=3.33, v_num=641]Epoch 3:  75%|███████▍  | 3280/4381 [1:26:42<29:05,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  75%|███████▌  | 3290/4381 [1:26:58<28:49,  1.59s/it, loss=3.34, v_num=641]Epoch 3:  75%|███████▌  | 3290/4381 [1:26:58<28:49,  1.59s/it, loss=3.3, v_num=641] Epoch 3:  75%|███████▌  | 3300/4381 [1:27:15<28:34,  1.59s/it, loss=3.3, v_num=641]Epoch 3:  75%|███████▌  | 3300/4381 [1:27:15<28:34,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  76%|███████▌  | 3310/4381 [1:27:29<28:18,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  76%|███████▌  | 3310/4381 [1:27:29<28:18,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  76%|███████▌  | 3320/4381 [1:27:48<28:03,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  76%|███████▌  | 3320/4381 [1:27:48<28:03,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  76%|███████▌  | 3330/4381 [1:28:03<27:47,  1.59s/it, loss=3.31, v_num=641]Epoch 3:  76%|███████▌  | 3330/4381 [1:28:03<27:47,  1.59s/it, loss=3.3, v_num=641] Epoch 3:  76%|███████▌  | 3340/4381 [1:28:16<27:30,  1.59s/it, loss=3.3, v_num=641]Epoch 3:  76%|███████▌  | 3340/4381 [1:28:16<27:30,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  76%|███████▋  | 3350/4381 [1:28:38<27:16,  1.59s/it, loss=3.32, v_num=641]Epoch 3:  76%|███████▋  | 3350/4381 [1:28:38<27:16,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  77%|███████▋  | 3360/4381 [1:28:52<26:59,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  77%|███████▋  | 3360/4381 [1:28:52<26:59,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3370/4381 [1:29:07<26:43,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3370/4381 [1:29:07<26:43,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3380/4381 [1:29:24<26:28,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3380/4381 [1:29:24<26:28,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3390/4381 [1:29:38<26:11,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  77%|███████▋  | 3390/4381 [1:29:38<26:11,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  78%|███████▊  | 3400/4381 [1:29:50<25:54,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  78%|███████▊  | 3400/4381 [1:29:50<25:54,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  78%|███████▊  | 3410/4381 [1:30:09<25:40,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  78%|███████▊  | 3410/4381 [1:30:09<25:40,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  78%|███████▊  | 3420/4381 [1:30:26<25:24,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  78%|███████▊  | 3420/4381 [1:30:26<25:24,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  78%|███████▊  | 3430/4381 [1:30:41<25:08,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  78%|███████▊  | 3430/4381 [1:30:41<25:08,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  79%|███████▊  | 3440/4381 [1:30:55<24:51,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  79%|███████▊  | 3440/4381 [1:30:55<24:51,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  79%|███████▊  | 3450/4381 [1:31:09<24:35,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  79%|███████▊  | 3450/4381 [1:31:09<24:35,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  79%|███████▉  | 3460/4381 [1:31:26<24:19,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  79%|███████▉  | 3460/4381 [1:31:26<24:19,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  79%|███████▉  | 3470/4381 [1:31:42<24:04,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  79%|███████▉  | 3470/4381 [1:31:42<24:04,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  79%|███████▉  | 3480/4381 [1:32:00<23:48,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  79%|███████▉  | 3480/4381 [1:32:00<23:48,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  80%|███████▉  | 3490/4381 [1:32:14<23:32,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  80%|███████▉  | 3490/4381 [1:32:14<23:32,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  80%|███████▉  | 3500/4381 [1:32:34<23:17,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  80%|███████▉  | 3500/4381 [1:32:34<23:17,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  80%|████████  | 3510/4381 [1:32:48<23:01,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  80%|████████  | 3510/4381 [1:32:48<23:01,  1.59s/it, loss=3.3, v_num=641] Epoch 3:  80%|████████  | 3520/4381 [1:33:02<22:45,  1.59s/it, loss=3.3, v_num=641]Epoch 3:  80%|████████  | 3520/4381 [1:33:02<22:45,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  81%|████████  | 3530/4381 [1:33:22<22:30,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  81%|████████  | 3530/4381 [1:33:22<22:30,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  81%|████████  | 3540/4381 [1:33:37<22:14,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  81%|████████  | 3540/4381 [1:33:37<22:14,  1.59s/it, loss=3.27, v_num=641]Epoch 3:  81%|████████  | 3550/4381 [1:33:53<21:58,  1.59s/it, loss=3.27, v_num=641]Epoch 3:  81%|████████  | 3550/4381 [1:33:53<21:58,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  81%|████████▏ | 3560/4381 [1:34:10<21:42,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  81%|████████▏ | 3560/4381 [1:34:10<21:42,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  81%|████████▏ | 3570/4381 [1:34:27<21:27,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  81%|████████▏ | 3570/4381 [1:34:27<21:27,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  82%|████████▏ | 3580/4381 [1:34:40<21:10,  1.59s/it, loss=3.28, v_num=641]Epoch 3:  82%|████████▏ | 3580/4381 [1:34:40<21:10,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  82%|████████▏ | 3590/4381 [1:34:54<20:54,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  82%|████████▏ | 3590/4381 [1:34:54<20:54,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  82%|████████▏ | 3600/4381 [1:35:11<20:38,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  82%|████████▏ | 3600/4381 [1:35:11<20:38,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  82%|████████▏ | 3610/4381 [1:35:25<20:22,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  82%|████████▏ | 3610/4381 [1:35:25<20:22,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  83%|████████▎ | 3620/4381 [1:35:43<20:07,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  83%|████████▎ | 3620/4381 [1:35:43<20:07,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  83%|████████▎ | 3630/4381 [1:36:00<19:51,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  83%|████████▎ | 3630/4381 [1:36:00<19:51,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  83%|████████▎ | 3640/4381 [1:36:15<19:35,  1.59s/it, loss=3.24, v_num=641]Epoch 3:  83%|████████▎ | 3640/4381 [1:36:15<19:35,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  83%|████████▎ | 3650/4381 [1:36:31<19:19,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  83%|████████▎ | 3650/4381 [1:36:31<19:19,  1.59s/it, loss=3.27, v_num=641]Epoch 3:  84%|████████▎ | 3660/4381 [1:36:46<19:03,  1.59s/it, loss=3.27, v_num=641]Epoch 3:  84%|████████▎ | 3660/4381 [1:36:46<19:03,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  84%|████████▍ | 3670/4381 [1:37:02<18:47,  1.59s/it, loss=3.26, v_num=641]Epoch 3:  84%|████████▍ | 3670/4381 [1:37:02<18:47,  1.59s/it, loss=3.2, v_num=641] Epoch 3:  84%|████████▍ | 3680/4381 [1:37:17<18:31,  1.59s/it, loss=3.2, v_num=641]Epoch 3:  84%|████████▍ | 3680/4381 [1:37:17<18:31,  1.59s/it, loss=3.21, v_num=641]Epoch 3:  84%|████████▍ | 3690/4381 [1:37:32<18:15,  1.59s/it, loss=3.21, v_num=641]Epoch 3:  84%|████████▍ | 3690/4381 [1:37:32<18:15,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  84%|████████▍ | 3700/4381 [1:37:47<17:59,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  84%|████████▍ | 3700/4381 [1:37:47<17:59,  1.59s/it, loss=3.3, v_num=641] Epoch 3:  85%|████████▍ | 3710/4381 [1:38:04<17:43,  1.59s/it, loss=3.3, v_num=641]Epoch 3:  85%|████████▍ | 3710/4381 [1:38:04<17:43,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  85%|████████▍ | 3720/4381 [1:38:18<17:27,  1.59s/it, loss=3.29, v_num=641]Epoch 3:  85%|████████▍ | 3720/4381 [1:38:18<17:27,  1.59s/it, loss=3.25, v_num=641]Epoch 3:  85%|████████▌ | 3730/4381 [1:38:31<17:11,  1.58s/it, loss=3.25, v_num=641]Epoch 3:  85%|████████▌ | 3730/4381 [1:38:31<17:11,  1.58s/it, loss=3.22, v_num=641]Epoch 3:  85%|████████▌ | 3740/4381 [1:38:35<16:53,  1.58s/it, loss=3.22, v_num=641]Epoch 3:  85%|████████▌ | 3740/4381 [1:38:35<16:53,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  86%|████████▌ | 3750/4381 [1:38:38<16:35,  1.58s/it, loss=3.27, v_num=641]Epoch 3:  86%|████████▌ | 3750/4381 [1:38:38<16:35,  1.58s/it, loss=3.28, v_num=641]Epoch 3:  86%|████████▌ | 3760/4381 [1:38:39<16:17,  1.57s/it, loss=3.28, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.0
valid accuracy: 0.8609871864318848
validation_epoch_end
graph acc: 0.001597444089456869
valid accuracy: 0.8644555807113647
validation_epoch_end
graph acc: 0.003194888178913738
valid accuracy: 0.8655229210853577
validation_epoch_end
graph acc: 0.003194888178913738
valid accuracy: 0.8633360862731934
validation_epoch_end
graph acc: 0.003194888178913738
valid accuracy: 0.8660851120948792
validation_epoch_end
graph acc: 0.006389776357827476
valid accuracy: 0.865700900554657
validation_epoch_end
graph acc: 0.001597444089456869
valid accuracy: 0.8665192127227783

Validating:   2%|▏         | 10/626 [00:02<02:37,  3.92it/s][AEpoch 3:  86%|████████▌ | 3770/4381 [1:38:41<15:59,  1.57s/it, loss=3.28, v_num=641]
Validating:   3%|▎         | 20/626 [00:03<01:45,  5.76it/s][AEpoch 3:  86%|████████▋ | 3780/4381 [1:38:43<15:41,  1.57s/it, loss=3.28, v_num=641]
Validating:   5%|▍         | 30/626 [00:06<01:59,  5.01it/s][AEpoch 3:  87%|████████▋ | 3790/4381 [1:38:45<15:23,  1.56s/it, loss=3.28, v_num=641]
Validating:   6%|▋         | 40/626 [00:07<01:45,  5.54it/s][AEpoch 3:  87%|████████▋ | 3800/4381 [1:38:46<15:05,  1.56s/it, loss=3.28, v_num=641]
Validating:   8%|▊         | 50/626 [00:09<01:45,  5.47it/s][AEpoch 3:  87%|████████▋ | 3810/4381 [1:38:48<14:48,  1.56s/it, loss=3.28, v_num=641]
Validating:  10%|▉         | 60/626 [00:11<01:49,  5.17it/s][AEpoch 3:  87%|████████▋ | 3820/4381 [1:38:50<14:30,  1.55s/it, loss=3.28, v_num=641]
Validating:  11%|█         | 70/626 [00:13<01:42,  5.41it/s][AEpoch 3:  87%|████████▋ | 3830/4381 [1:38:52<14:13,  1.55s/it, loss=3.28, v_num=641]
Validating:  13%|█▎        | 80/626 [00:14<01:27,  6.22it/s][AEpoch 3:  88%|████████▊ | 3840/4381 [1:38:53<13:55,  1.54s/it, loss=3.28, v_num=641]
Validating:  14%|█▍        | 90/626 [00:15<01:24,  6.32it/s][AEpoch 3:  88%|████████▊ | 3850/4381 [1:38:55<13:38,  1.54s/it, loss=3.28, v_num=641]
Validating:  16%|█▌        | 100/626 [00:17<01:29,  5.87it/s][AEpoch 3:  88%|████████▊ | 3860/4381 [1:38:57<13:21,  1.54s/it, loss=3.28, v_num=641]
Validating:  18%|█▊        | 110/626 [00:20<01:37,  5.30it/s][AEpoch 3:  88%|████████▊ | 3870/4381 [1:38:59<13:04,  1.53s/it, loss=3.28, v_num=641]
Validating:  19%|█▉        | 120/626 [00:21<01:31,  5.52it/s][AEpoch 3:  89%|████████▊ | 3880/4381 [1:39:01<12:46,  1.53s/it, loss=3.28, v_num=641]
Validating:  21%|██        | 130/626 [00:22<01:20,  6.15it/s][AEpoch 3:  89%|████████▉ | 3890/4381 [1:39:02<12:29,  1.53s/it, loss=3.28, v_num=641]
Validating:  22%|██▏       | 140/626 [00:25<01:27,  5.56it/s][AEpoch 3:  89%|████████▉ | 3900/4381 [1:39:04<12:12,  1.52s/it, loss=3.28, v_num=641]
Validating:  24%|██▍       | 150/626 [00:27<01:29,  5.31it/s][AEpoch 3:  89%|████████▉ | 3910/4381 [1:39:06<11:56,  1.52s/it, loss=3.28, v_num=641]
Validating:  26%|██▌       | 160/626 [00:28<01:24,  5.55it/s][AEpoch 3:  89%|████████▉ | 3920/4381 [1:39:08<11:39,  1.52s/it, loss=3.28, v_num=641]
Validating:  27%|██▋       | 170/626 [00:31<01:30,  5.05it/s][AEpoch 3:  90%|████████▉ | 3930/4381 [1:39:10<11:22,  1.51s/it, loss=3.28, v_num=641]
Validating:  29%|██▉       | 180/626 [00:32<01:23,  5.31it/s][AEpoch 3:  90%|████████▉ | 3940/4381 [1:39:12<11:06,  1.51s/it, loss=3.28, v_num=641]
Validating:  30%|███       | 190/626 [00:35<01:26,  5.06it/s][AEpoch 3:  90%|█████████ | 3950/4381 [1:39:14<10:49,  1.51s/it, loss=3.28, v_num=641]
Validating:  32%|███▏      | 200/626 [00:36<01:22,  5.14it/s][AEpoch 3:  90%|█████████ | 3960/4381 [1:39:16<10:33,  1.50s/it, loss=3.28, v_num=641]
Validating:  34%|███▎      | 210/626 [00:38<01:17,  5.38it/s][AEpoch 3:  91%|█████████ | 3970/4381 [1:39:18<10:16,  1.50s/it, loss=3.28, v_num=641]
Validating:  35%|███▌      | 220/626 [00:40<01:18,  5.16it/s][AEpoch 3:  91%|█████████ | 3980/4381 [1:39:20<10:00,  1.50s/it, loss=3.28, v_num=641]
Validating:  37%|███▋      | 230/626 [00:42<01:13,  5.39it/s][AEpoch 3:  91%|█████████ | 3990/4381 [1:39:21<09:44,  1.49s/it, loss=3.28, v_num=641]
Validating:  38%|███▊      | 240/626 [00:44<01:12,  5.31it/s][AEpoch 3:  91%|█████████▏| 4000/4381 [1:39:23<09:27,  1.49s/it, loss=3.28, v_num=641]
Validating:  40%|███▉      | 250/626 [00:46<01:08,  5.47it/s][AEpoch 3:  92%|█████████▏| 4010/4381 [1:39:25<09:11,  1.49s/it, loss=3.28, v_num=641]
Validating:  42%|████▏     | 260/626 [00:48<01:12,  5.03it/s][AEpoch 3:  92%|█████████▏| 4020/4381 [1:39:27<08:55,  1.48s/it, loss=3.28, v_num=641]
Validating:  43%|████▎     | 270/626 [00:49<01:03,  5.62it/s][AEpoch 3:  92%|█████████▏| 4030/4381 [1:39:29<08:39,  1.48s/it, loss=3.28, v_num=641]
Validating:  45%|████▍     | 280/626 [00:53<01:19,  4.33it/s][AEpoch 3:  92%|█████████▏| 4040/4381 [1:39:32<08:24,  1.48s/it, loss=3.28, v_num=641]
Validating:  46%|████▋     | 290/626 [00:54<01:11,  4.70it/s][AEpoch 3:  92%|█████████▏| 4050/4381 [1:39:34<08:08,  1.47s/it, loss=3.28, v_num=641]
Validating:  48%|████▊     | 300/626 [00:56<01:02,  5.25it/s][AEpoch 3:  93%|█████████▎| 4060/4381 [1:39:35<07:52,  1.47s/it, loss=3.28, v_num=641]
Validating:  50%|████▉     | 310/626 [00:57<00:51,  6.13it/s][AEpoch 3:  93%|█████████▎| 4070/4381 [1:39:36<07:36,  1.47s/it, loss=3.28, v_num=641]
Validating:  51%|█████     | 320/626 [00:58<00:49,  6.19it/s][AEpoch 3:  93%|█████████▎| 4080/4381 [1:39:38<07:20,  1.46s/it, loss=3.28, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:00<00:47,  6.23it/s][AEpoch 3:  93%|█████████▎| 4090/4381 [1:39:39<07:05,  1.46s/it, loss=3.28, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:02<00:45,  6.30it/s][AEpoch 3:  94%|█████████▎| 4100/4381 [1:39:41<06:49,  1.46s/it, loss=3.28, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:04<00:51,  5.33it/s][AEpoch 3:  94%|█████████▍| 4110/4381 [1:39:43<06:34,  1.46s/it, loss=3.28, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:06<00:52,  5.04it/s][AEpoch 3:  94%|█████████▍| 4120/4381 [1:39:46<06:19,  1.45s/it, loss=3.28, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:08<00:47,  5.35it/s][AEpoch 3:  94%|█████████▍| 4130/4381 [1:39:47<06:03,  1.45s/it, loss=3.28, v_num=641]
Validating:  61%|██████    | 380/626 [01:09<00:42,  5.77it/s][AEpoch 3:  94%|█████████▍| 4140/4381 [1:39:49<05:48,  1.45s/it, loss=3.28, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:11<00:39,  6.04it/s][AEpoch 3:  95%|█████████▍| 4150/4381 [1:39:50<05:33,  1.44s/it, loss=3.28, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:12<00:32,  6.96it/s][AEpoch 3:  95%|█████████▍| 4160/4381 [1:39:51<05:18,  1.44s/it, loss=3.28, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:13<00:30,  7.19it/s][AEpoch 3:  95%|█████████▌| 4170/4381 [1:39:52<05:03,  1.44s/it, loss=3.28, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:16<00:37,  5.50it/s][AEpoch 3:  95%|█████████▌| 4180/4381 [1:39:55<04:48,  1.43s/it, loss=3.28, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:17<00:31,  6.18it/s][AEpoch 3:  96%|█████████▌| 4190/4381 [1:39:56<04:33,  1.43s/it, loss=3.28, v_num=641]
Validating:  70%|███████   | 440/626 [01:19<00:32,  5.72it/s][AEpoch 3:  96%|█████████▌| 4200/4381 [1:39:58<04:18,  1.43s/it, loss=3.28, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:21<00:30,  5.69it/s][AEpoch 3:  96%|█████████▌| 4210/4381 [1:40:00<04:03,  1.43s/it, loss=3.28, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:23<00:30,  5.43it/s][AEpoch 3:  96%|█████████▋| 4220/4381 [1:40:02<03:48,  1.42s/it, loss=3.28, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:25<00:30,  5.16it/s][AEpoch 3:  97%|█████████▋| 4230/4381 [1:40:04<03:34,  1.42s/it, loss=3.28, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:27<00:28,  5.05it/s][AEpoch 3:  97%|█████████▋| 4240/4381 [1:40:06<03:19,  1.42s/it, loss=3.28, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:29<00:27,  4.86it/s][AEpoch 3:  97%|█████████▋| 4250/4381 [1:40:09<03:05,  1.41s/it, loss=3.28, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:31<00:22,  5.57it/s][AEpoch 3:  97%|█████████▋| 4260/4381 [1:40:10<02:50,  1.41s/it, loss=3.28, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:33<00:24,  4.80it/s][AEpoch 3:  97%|█████████▋| 4270/4381 [1:40:13<02:36,  1.41s/it, loss=3.28, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:35<00:19,  5.35it/s][AEpoch 3:  98%|█████████▊| 4280/4381 [1:40:14<02:21,  1.40s/it, loss=3.28, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:36<00:17,  5.61it/s][AEpoch 3:  98%|█████████▊| 4290/4381 [1:40:16<02:07,  1.40s/it, loss=3.28, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:38<00:14,  5.94it/s][AEpoch 3:  98%|█████████▊| 4300/4381 [1:40:17<01:53,  1.40s/it, loss=3.28, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:39<00:12,  5.99it/s][AEpoch 3:  98%|█████████▊| 4310/4381 [1:40:19<01:39,  1.40s/it, loss=3.28, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:41<00:11,  5.98it/s][AEpoch 3:  99%|█████████▊| 4320/4381 [1:40:20<01:24,  1.39s/it, loss=3.28, v_num=641]
Validating:  91%|█████████ | 570/626 [01:42<00:08,  6.50it/s][AEpoch 3:  99%|█████████▉| 4330/4381 [1:40:22<01:10,  1.39s/it, loss=3.28, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:44<00:07,  5.95it/s][AEpoch 3:  99%|█████████▉| 4340/4381 [1:40:24<00:56,  1.39s/it, loss=3.28, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:46<00:05,  6.31it/s][AEpoch 3:  99%|█████████▉| 4350/4381 [1:40:25<00:42,  1.38s/it, loss=3.28, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:47<00:03,  6.91it/s][AEpoch 3: 100%|█████████▉| 4360/4381 [1:40:26<00:29,  1.38s/it, loss=3.28, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:48<00:02,  7.51it/s][AEpoch 3: 100%|█████████▉| 4370/4381 [1:40:27<00:15,  1.38s/it, loss=3.28, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:50<00:01,  5.94it/s][AEpoch 3: 100%|█████████▉| 4380/4381 [1:40:30<00:01,  1.38s/it, loss=3.28, v_num=641]
Validating: 100%|██████████| 626/626 [01:51<00:00,  6.73it/s][Avalidation_epoch_end
graph acc: 0.04792332268370607
valid accuracy: 0.9111989736557007
Epoch 3: 100%|██████████| 4381/4381 [1:40:39<00:00,  1.38s/it, loss=3.24, v_num=641]
                                                             [AEpoch 3:   0%|          | 0/4381 [00:00<00:00, 15196.75it/s, loss=3.24, v_num=641]  Epoch 4:   0%|          | 0/4381 [00:00<00:01, 3705.22it/s, loss=3.24, v_num=641] Epoch 4:   0%|          | 0/4381 [00:16<19:45:21, 16.23s/it, loss=3.24, v_num=641]Epoch 4:   0%|          | 10/4381 [00:21<2:24:19,  1.98s/it, loss=3.24, v_num=641]Epoch 4:   0%|          | 10/4381 [00:21<2:24:19,  1.98s/it, loss=3.21, v_num=641]Epoch 4:   0%|          | 20/4381 [00:36<2:07:09,  1.75s/it, loss=3.21, v_num=641]Epoch 4:   0%|          | 20/4381 [00:36<2:07:09,  1.75s/it, loss=3.2, v_num=641] Epoch 4:   1%|          | 30/4381 [00:53<2:06:11,  1.74s/it, loss=3.2, v_num=641]Epoch 4:   1%|          | 30/4381 [00:53<2:06:11,  1.74s/it, loss=3.21, v_num=641]Epoch 4:   1%|          | 40/4381 [01:09<2:02:57,  1.70s/it, loss=3.21, v_num=641]Epoch 4:   1%|          | 40/4381 [01:09<2:02:57,  1.70s/it, loss=3.22, v_num=641]Epoch 4:   1%|          | 50/4381 [01:26<2:01:45,  1.69s/it, loss=3.22, v_num=641]Epoch 4:   1%|          | 50/4381 [01:26<2:01:45,  1.69s/it, loss=3.24, v_num=641]Epoch 4:   1%|▏         | 60/4381 [01:42<2:00:52,  1.68s/it, loss=3.24, v_num=641]Epoch 4:   1%|▏         | 60/4381 [01:42<2:00:52,  1.68s/it, loss=3.22, v_num=641]Epoch 4:   2%|▏         | 70/4381 [01:58<1:59:32,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   2%|▏         | 70/4381 [01:58<1:59:32,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   2%|▏         | 80/4381 [02:13<1:58:22,  1.65s/it, loss=3.22, v_num=641]Epoch 4:   2%|▏         | 80/4381 [02:13<1:58:22,  1.65s/it, loss=3.25, v_num=641]Epoch 4:   2%|▏         | 90/4381 [02:30<1:58:23,  1.66s/it, loss=3.25, v_num=641]Epoch 4:   2%|▏         | 90/4381 [02:30<1:58:23,  1.66s/it, loss=3.21, v_num=641]Epoch 4:   2%|▏         | 100/4381 [02:48<1:58:52,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   2%|▏         | 100/4381 [02:48<1:58:53,  1.67s/it, loss=3.2, v_num=641] Epoch 4:   3%|▎         | 110/4381 [03:06<1:59:54,  1.68s/it, loss=3.2, v_num=641]Epoch 4:   3%|▎         | 110/4381 [03:06<1:59:54,  1.68s/it, loss=3.24, v_num=641]Epoch 4:   3%|▎         | 120/4381 [03:22<1:59:07,  1.68s/it, loss=3.24, v_num=641]Epoch 4:   3%|▎         | 120/4381 [03:22<1:59:07,  1.68s/it, loss=3.21, v_num=641]Epoch 4:   3%|▎         | 130/4381 [03:40<1:59:19,  1.68s/it, loss=3.21, v_num=641]Epoch 4:   3%|▎         | 130/4381 [03:40<1:59:19,  1.68s/it, loss=3.2, v_num=641] Epoch 4:   3%|▎         | 140/4381 [03:54<1:57:27,  1.66s/it, loss=3.2, v_num=641]Epoch 4:   3%|▎         | 140/4381 [03:54<1:57:27,  1.66s/it, loss=3.2, v_num=641]Epoch 4:   3%|▎         | 150/4381 [04:11<1:57:29,  1.67s/it, loss=3.2, v_num=641]Epoch 4:   3%|▎         | 150/4381 [04:11<1:57:29,  1.67s/it, loss=3.18, v_num=641]Epoch 4:   4%|▎         | 160/4381 [04:29<1:57:36,  1.67s/it, loss=3.18, v_num=641]Epoch 4:   4%|▎         | 160/4381 [04:29<1:57:36,  1.67s/it, loss=3.19, v_num=641]Epoch 4:   4%|▍         | 170/4381 [04:43<1:56:14,  1.66s/it, loss=3.19, v_num=641]Epoch 4:   4%|▍         | 170/4381 [04:43<1:56:14,  1.66s/it, loss=3.19, v_num=641]Epoch 4:   4%|▍         | 180/4381 [05:01<1:56:40,  1.67s/it, loss=3.19, v_num=641]Epoch 4:   4%|▍         | 180/4381 [05:01<1:56:40,  1.67s/it, loss=3.2, v_num=641] Epoch 4:   4%|▍         | 190/4381 [05:19<1:56:56,  1.67s/it, loss=3.2, v_num=641]Epoch 4:   4%|▍         | 190/4381 [05:19<1:56:56,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   5%|▍         | 200/4381 [05:36<1:56:31,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   5%|▍         | 200/4381 [05:36<1:56:31,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   5%|▍         | 210/4381 [05:51<1:55:41,  1.66s/it, loss=3.21, v_num=641]Epoch 4:   5%|▍         | 210/4381 [05:51<1:55:41,  1.66s/it, loss=3.24, v_num=641]Epoch 4:   5%|▌         | 220/4381 [06:07<1:55:27,  1.66s/it, loss=3.24, v_num=641]Epoch 4:   5%|▌         | 220/4381 [06:07<1:55:27,  1.66s/it, loss=3.23, v_num=641]Epoch 4:   5%|▌         | 230/4381 [06:25<1:55:26,  1.67s/it, loss=3.23, v_num=641]Epoch 4:   5%|▌         | 230/4381 [06:25<1:55:26,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   5%|▌         | 240/4381 [06:43<1:55:31,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   5%|▌         | 240/4381 [06:43<1:55:31,  1.67s/it, loss=3.25, v_num=641]Epoch 4:   6%|▌         | 250/4381 [07:00<1:55:26,  1.68s/it, loss=3.25, v_num=641]Epoch 4:   6%|▌         | 250/4381 [07:00<1:55:26,  1.68s/it, loss=3.27, v_num=641]Epoch 4:   6%|▌         | 260/4381 [07:15<1:54:42,  1.67s/it, loss=3.27, v_num=641]Epoch 4:   6%|▌         | 260/4381 [07:15<1:54:42,  1.67s/it, loss=3.26, v_num=641]Epoch 4:   6%|▌         | 270/4381 [07:31<1:54:14,  1.67s/it, loss=3.26, v_num=641]Epoch 4:   6%|▌         | 270/4381 [07:31<1:54:14,  1.67s/it, loss=3.24, v_num=641]Epoch 4:   6%|▋         | 280/4381 [07:50<1:54:23,  1.67s/it, loss=3.24, v_num=641]Epoch 4:   6%|▋         | 280/4381 [07:50<1:54:23,  1.67s/it, loss=3.26, v_num=641]Epoch 4:   7%|▋         | 290/4381 [08:06<1:53:59,  1.67s/it, loss=3.26, v_num=641]Epoch 4:   7%|▋         | 290/4381 [08:06<1:53:59,  1.67s/it, loss=3.22, v_num=641]Epoch 4:   7%|▋         | 300/4381 [08:21<1:53:16,  1.67s/it, loss=3.22, v_num=641]Epoch 4:   7%|▋         | 300/4381 [08:21<1:53:16,  1.67s/it, loss=3.21, v_num=641]Epoch 4:   7%|▋         | 310/4381 [08:42<1:53:56,  1.68s/it, loss=3.21, v_num=641]Epoch 4:   7%|▋         | 310/4381 [08:42<1:53:56,  1.68s/it, loss=3.23, v_num=641]Epoch 4:   7%|▋         | 320/4381 [08:55<1:52:54,  1.67s/it, loss=3.23, v_num=641]Epoch 4:   7%|▋         | 320/4381 [08:55<1:52:54,  1.67s/it, loss=3.25, v_num=641]Epoch 4:   8%|▊         | 330/4381 [09:10<1:52:16,  1.66s/it, loss=3.25, v_num=641]Epoch 4:   8%|▊         | 330/4381 [09:10<1:52:16,  1.66s/it, loss=3.28, v_num=641]Epoch 4:   8%|▊         | 340/4381 [09:29<1:52:25,  1.67s/it, loss=3.28, v_num=641]Epoch 4:   8%|▊         | 340/4381 [09:29<1:52:25,  1.67s/it, loss=3.24, v_num=641]Epoch 4:   8%|▊         | 350/4381 [09:43<1:51:44,  1.66s/it, loss=3.24, v_num=641]Epoch 4:   8%|▊         | 350/4381 [09:43<1:51:44,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   8%|▊         | 360/4381 [09:57<1:50:54,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   8%|▊         | 360/4381 [09:57<1:50:54,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   8%|▊         | 370/4381 [10:14<1:50:47,  1.66s/it, loss=3.22, v_num=641]Epoch 4:   8%|▊         | 370/4381 [10:14<1:50:47,  1.66s/it, loss=3.17, v_num=641]Epoch 4:   9%|▊         | 380/4381 [10:30<1:50:25,  1.66s/it, loss=3.17, v_num=641]Epoch 4:   9%|▊         | 380/4381 [10:30<1:50:25,  1.66s/it, loss=3.2, v_num=641] Epoch 4:   9%|▉         | 390/4381 [10:45<1:49:45,  1.65s/it, loss=3.2, v_num=641]Epoch 4:   9%|▉         | 390/4381 [10:45<1:49:45,  1.65s/it, loss=3.22, v_num=641]Epoch 4:   9%|▉         | 400/4381 [11:00<1:49:20,  1.65s/it, loss=3.22, v_num=641]Epoch 4:   9%|▉         | 400/4381 [11:00<1:49:20,  1.65s/it, loss=3.2, v_num=641] Epoch 4:   9%|▉         | 410/4381 [11:15<1:48:47,  1.64s/it, loss=3.2, v_num=641]Epoch 4:   9%|▉         | 410/4381 [11:15<1:48:47,  1.64s/it, loss=3.21, v_num=641]Epoch 4:  10%|▉         | 420/4381 [11:32<1:48:32,  1.64s/it, loss=3.21, v_num=641]Epoch 4:  10%|▉         | 420/4381 [11:32<1:48:32,  1.64s/it, loss=3.21, v_num=641]Epoch 4:  10%|▉         | 430/4381 [11:50<1:48:32,  1.65s/it, loss=3.21, v_num=641]Epoch 4:  10%|▉         | 430/4381 [11:50<1:48:32,  1.65s/it, loss=3.2, v_num=641] Epoch 4:  10%|█         | 440/4381 [12:06<1:48:09,  1.65s/it, loss=3.2, v_num=641]Epoch 4:  10%|█         | 440/4381 [12:06<1:48:09,  1.65s/it, loss=3.21, v_num=641]Epoch 4:  10%|█         | 450/4381 [12:22<1:47:51,  1.65s/it, loss=3.21, v_num=641]Epoch 4:  10%|█         | 450/4381 [12:22<1:47:51,  1.65s/it, loss=3.23, v_num=641]Epoch 4:  10%|█         | 460/4381 [12:39<1:47:39,  1.65s/it, loss=3.23, v_num=641]Epoch 4:  10%|█         | 460/4381 [12:39<1:47:39,  1.65s/it, loss=3.22, v_num=641]Epoch 4:  11%|█         | 470/4381 [12:53<1:47:05,  1.64s/it, loss=3.22, v_num=641]Epoch 4:  11%|█         | 470/4381 [12:53<1:47:05,  1.64s/it, loss=3.22, v_num=641]Epoch 4:  11%|█         | 480/4381 [13:07<1:46:30,  1.64s/it, loss=3.22, v_num=641]Epoch 4:  11%|█         | 480/4381 [13:07<1:46:30,  1.64s/it, loss=3.18, v_num=641]Epoch 4:  11%|█         | 490/4381 [13:24<1:46:16,  1.64s/it, loss=3.18, v_num=641]Epoch 4:  11%|█         | 490/4381 [13:24<1:46:16,  1.64s/it, loss=3.2, v_num=641] Epoch 4:  11%|█▏        | 500/4381 [13:40<1:45:56,  1.64s/it, loss=3.2, v_num=641]Epoch 4:  11%|█▏        | 500/4381 [13:40<1:45:56,  1.64s/it, loss=3.24, v_num=641]Epoch 4:  12%|█▏        | 510/4381 [13:55<1:45:28,  1.63s/it, loss=3.24, v_num=641]Epoch 4:  12%|█▏        | 510/4381 [13:55<1:45:28,  1.63s/it, loss=3.19, v_num=641]Epoch 4:  12%|█▏        | 520/4381 [14:09<1:44:52,  1.63s/it, loss=3.19, v_num=641]Epoch 4:  12%|█▏        | 520/4381 [14:09<1:44:52,  1.63s/it, loss=3.19, v_num=641]Epoch 4:  12%|█▏        | 530/4381 [14:23<1:44:20,  1.63s/it, loss=3.19, v_num=641]Epoch 4:  12%|█▏        | 530/4381 [14:23<1:44:20,  1.63s/it, loss=3.23, v_num=641]Epoch 4:  12%|█▏        | 540/4381 [14:39<1:44:03,  1.63s/it, loss=3.23, v_num=641]Epoch 4:  12%|█▏        | 540/4381 [14:39<1:44:03,  1.63s/it, loss=3.24, v_num=641]Epoch 4:  13%|█▎        | 550/4381 [14:54<1:43:41,  1.62s/it, loss=3.24, v_num=641]Epoch 4:  13%|█▎        | 550/4381 [14:54<1:43:41,  1.62s/it, loss=3.21, v_num=641]Epoch 4:  13%|█▎        | 560/4381 [15:09<1:43:14,  1.62s/it, loss=3.21, v_num=641]Epoch 4:  13%|█▎        | 560/4381 [15:09<1:43:14,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  13%|█▎        | 570/4381 [15:24<1:42:51,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  13%|█▎        | 570/4381 [15:24<1:42:51,  1.62s/it, loss=3.2, v_num=641] Epoch 4:  13%|█▎        | 580/4381 [15:42<1:42:44,  1.62s/it, loss=3.2, v_num=641]Epoch 4:  13%|█▎        | 580/4381 [15:42<1:42:44,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  13%|█▎        | 590/4381 [15:56<1:42:14,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  13%|█▎        | 590/4381 [15:56<1:42:14,  1.62s/it, loss=3.16, v_num=641]Epoch 4:  14%|█▎        | 600/4381 [16:14<1:42:13,  1.62s/it, loss=3.16, v_num=641]Epoch 4:  14%|█▎        | 600/4381 [16:14<1:42:13,  1.62s/it, loss=3.16, v_num=641]Epoch 4:  14%|█▍        | 610/4381 [16:30<1:41:56,  1.62s/it, loss=3.16, v_num=641]Epoch 4:  14%|█▍        | 610/4381 [16:30<1:41:56,  1.62s/it, loss=3.2, v_num=641] Epoch 4:  14%|█▍        | 620/4381 [16:46<1:41:34,  1.62s/it, loss=3.2, v_num=641]Epoch 4:  14%|█▍        | 620/4381 [16:46<1:41:34,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  14%|█▍        | 630/4381 [17:05<1:41:34,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  14%|█▍        | 630/4381 [17:05<1:41:34,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  15%|█▍        | 640/4381 [17:21<1:41:18,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  15%|█▍        | 640/4381 [17:21<1:41:18,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  15%|█▍        | 650/4381 [17:36<1:40:52,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  15%|█▍        | 650/4381 [17:36<1:40:52,  1.62s/it, loss=3.2, v_num=641] Epoch 4:  15%|█▌        | 660/4381 [17:52<1:40:39,  1.62s/it, loss=3.2, v_num=641]Epoch 4:  15%|█▌        | 660/4381 [17:52<1:40:39,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  15%|█▌        | 670/4381 [18:08<1:40:21,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  15%|█▌        | 670/4381 [18:08<1:40:21,  1.62s/it, loss=3.23, v_num=641]Epoch 4:  16%|█▌        | 680/4381 [18:24<1:40:05,  1.62s/it, loss=3.23, v_num=641]Epoch 4:  16%|█▌        | 680/4381 [18:24<1:40:05,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  16%|█▌        | 690/4381 [18:42<1:39:58,  1.63s/it, loss=3.19, v_num=641]Epoch 4:  16%|█▌        | 690/4381 [18:42<1:39:58,  1.63s/it, loss=3.17, v_num=641]Epoch 4:  16%|█▌        | 700/4381 [18:56<1:39:30,  1.62s/it, loss=3.17, v_num=641]Epoch 4:  16%|█▌        | 700/4381 [18:56<1:39:30,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  16%|█▌        | 710/4381 [19:11<1:39:05,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  16%|█▌        | 710/4381 [19:11<1:39:05,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  16%|█▋        | 720/4381 [19:30<1:39:04,  1.62s/it, loss=3.22, v_num=641]Epoch 4:  16%|█▋        | 720/4381 [19:30<1:39:04,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  17%|█▋        | 730/4381 [19:43<1:38:33,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  17%|█▋        | 730/4381 [19:43<1:38:33,  1.62s/it, loss=3.17, v_num=641]Epoch 4:  17%|█▋        | 740/4381 [19:59<1:38:13,  1.62s/it, loss=3.17, v_num=641]Epoch 4:  17%|█▋        | 740/4381 [19:59<1:38:13,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  17%|█▋        | 750/4381 [20:18<1:38:10,  1.62s/it, loss=3.19, v_num=641]Epoch 4:  17%|█▋        | 750/4381 [20:18<1:38:10,  1.62s/it, loss=3.21, v_num=641]Epoch 4:  17%|█▋        | 760/4381 [20:34<1:37:53,  1.62s/it, loss=3.21, v_num=641]Epoch 4:  17%|█▋        | 760/4381 [20:34<1:37:53,  1.62s/it, loss=3.17, v_num=641]Epoch 4:  18%|█▊        | 770/4381 [20:45<1:37:14,  1.62s/it, loss=3.17, v_num=641]Epoch 4:  18%|█▊        | 770/4381 [20:45<1:37:14,  1.62s/it, loss=3.15, v_num=641]Epoch 4:  18%|█▊        | 780/4381 [21:02<1:37:03,  1.62s/it, loss=3.15, v_num=641]Epoch 4:  18%|█▊        | 780/4381 [21:02<1:37:03,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  18%|█▊        | 790/4381 [21:17<1:36:41,  1.62s/it, loss=3.18, v_num=641]Epoch 4:  18%|█▊        | 790/4381 [21:17<1:36:41,  1.62s/it, loss=3.2, v_num=641] Epoch 4:  18%|█▊        | 800/4381 [21:33<1:36:23,  1.62s/it, loss=3.2, v_num=641]Epoch 4:  18%|█▊        | 800/4381 [21:33<1:36:23,  1.62s/it, loss=3.21, v_num=641]Epoch 4:  18%|█▊        | 810/4381 [21:49<1:36:06,  1.61s/it, loss=3.21, v_num=641]Epoch 4:  18%|█▊        | 810/4381 [21:49<1:36:06,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  19%|█▊        | 820/4381 [22:05<1:35:50,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  19%|█▊        | 820/4381 [22:05<1:35:50,  1.61s/it, loss=3.14, v_num=641]Epoch 4:  19%|█▉        | 830/4381 [22:18<1:35:20,  1.61s/it, loss=3.14, v_num=641]Epoch 4:  19%|█▉        | 830/4381 [22:18<1:35:20,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  19%|█▉        | 840/4381 [22:37<1:35:15,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  19%|█▉        | 840/4381 [22:37<1:35:15,  1.61s/it, loss=3.2, v_num=641] Epoch 4:  19%|█▉        | 850/4381 [22:51<1:34:52,  1.61s/it, loss=3.2, v_num=641]Epoch 4:  19%|█▉        | 850/4381 [22:51<1:34:52,  1.61s/it, loss=3.23, v_num=641]Epoch 4:  20%|█▉        | 860/4381 [23:05<1:34:24,  1.61s/it, loss=3.23, v_num=641]Epoch 4:  20%|█▉        | 860/4381 [23:05<1:34:24,  1.61s/it, loss=3.21, v_num=641]Epoch 4:  20%|█▉        | 870/4381 [23:22<1:34:13,  1.61s/it, loss=3.21, v_num=641]Epoch 4:  20%|█▉        | 870/4381 [23:22<1:34:13,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  20%|██        | 880/4381 [23:39<1:34:02,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  20%|██        | 880/4381 [23:39<1:34:02,  1.61s/it, loss=3.2, v_num=641] Epoch 4:  20%|██        | 890/4381 [23:53<1:33:38,  1.61s/it, loss=3.2, v_num=641]Epoch 4:  20%|██        | 890/4381 [23:53<1:33:38,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  21%|██        | 900/4381 [24:10<1:33:24,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  21%|██        | 900/4381 [24:10<1:33:24,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  21%|██        | 910/4381 [24:26<1:33:07,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  21%|██        | 910/4381 [24:26<1:33:07,  1.61s/it, loss=3.19, v_num=641]Epoch 4:  21%|██        | 920/4381 [24:42<1:32:52,  1.61s/it, loss=3.19, v_num=641]Epoch 4:  21%|██        | 920/4381 [24:42<1:32:52,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  21%|██        | 930/4381 [25:01<1:32:45,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  21%|██        | 930/4381 [25:01<1:32:45,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  21%|██▏       | 940/4381 [25:16<1:32:23,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  21%|██▏       | 940/4381 [25:16<1:32:23,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  22%|██▏       | 950/4381 [25:31<1:32:05,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  22%|██▏       | 950/4381 [25:31<1:32:05,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 960/4381 [25:48<1:31:51,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 960/4381 [25:48<1:31:51,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 970/4381 [26:00<1:31:21,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 970/4381 [26:00<1:31:21,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 980/4381 [26:13<1:30:56,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  22%|██▏       | 980/4381 [26:13<1:30:56,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  23%|██▎       | 990/4381 [26:31<1:30:47,  1.61s/it, loss=3.15, v_num=641]Epoch 4:  23%|██▎       | 990/4381 [26:31<1:30:47,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1000/4381 [26:46<1:30:24,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1000/4381 [26:46<1:30:24,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1010/4381 [27:01<1:30:06,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1010/4381 [27:01<1:30:06,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1020/4381 [27:18<1:29:53,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  23%|██▎       | 1020/4381 [27:18<1:29:53,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  24%|██▎       | 1030/4381 [27:32<1:29:32,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  24%|██▎       | 1030/4381 [27:32<1:29:32,  1.60s/it, loss=3.21, v_num=641]Epoch 4:  24%|██▎       | 1040/4381 [27:46<1:29:08,  1.60s/it, loss=3.21, v_num=641]Epoch 4:  24%|██▎       | 1040/4381 [27:46<1:29:08,  1.60s/it, loss=3.2, v_num=641] Epoch 4:  24%|██▍       | 1050/4381 [28:04<1:28:57,  1.60s/it, loss=3.2, v_num=641]Epoch 4:  24%|██▍       | 1050/4381 [28:04<1:28:57,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  24%|██▍       | 1060/4381 [28:20<1:28:41,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  24%|██▍       | 1060/4381 [28:20<1:28:41,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  24%|██▍       | 1070/4381 [28:38<1:28:32,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  24%|██▍       | 1070/4381 [28:38<1:28:32,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  25%|██▍       | 1080/4381 [28:55<1:28:19,  1.61s/it, loss=3.16, v_num=641]Epoch 4:  25%|██▍       | 1080/4381 [28:55<1:28:19,  1.61s/it, loss=3.21, v_num=641]Epoch 4:  25%|██▍       | 1090/4381 [29:10<1:28:00,  1.60s/it, loss=3.21, v_num=641]Epoch 4:  25%|██▍       | 1090/4381 [29:10<1:28:00,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  25%|██▌       | 1100/4381 [29:24<1:27:39,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  25%|██▌       | 1100/4381 [29:24<1:27:39,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  25%|██▌       | 1110/4381 [29:45<1:27:37,  1.61s/it, loss=3.22, v_num=641]Epoch 4:  25%|██▌       | 1110/4381 [29:45<1:27:37,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  26%|██▌       | 1120/4381 [30:01<1:27:20,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  26%|██▌       | 1120/4381 [30:01<1:27:20,  1.61s/it, loss=3.15, v_num=641]Epoch 4:  26%|██▌       | 1130/4381 [30:16<1:27:02,  1.61s/it, loss=3.15, v_num=641]Epoch 4:  26%|██▌       | 1130/4381 [30:16<1:27:02,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  26%|██▌       | 1140/4381 [30:34<1:26:51,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  26%|██▌       | 1140/4381 [30:34<1:26:51,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  26%|██▌       | 1150/4381 [30:47<1:26:25,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  26%|██▌       | 1150/4381 [30:47<1:26:25,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  26%|██▋       | 1160/4381 [31:01<1:26:04,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  26%|██▋       | 1160/4381 [31:01<1:26:04,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1170/4381 [31:16<1:25:46,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1170/4381 [31:16<1:25:46,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1180/4381 [31:35<1:25:37,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1180/4381 [31:35<1:25:37,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1190/4381 [31:51<1:25:22,  1.61s/it, loss=3.17, v_num=641]Epoch 4:  27%|██▋       | 1190/4381 [31:51<1:25:22,  1.61s/it, loss=3.15, v_num=641]Epoch 4:  27%|██▋       | 1200/4381 [32:07<1:25:05,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  27%|██▋       | 1200/4381 [32:07<1:25:05,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  28%|██▊       | 1210/4381 [32:23<1:24:48,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  28%|██▊       | 1210/4381 [32:23<1:24:48,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  28%|██▊       | 1220/4381 [32:35<1:24:23,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  28%|██▊       | 1220/4381 [32:35<1:24:23,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  28%|██▊       | 1230/4381 [32:53<1:24:10,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  28%|██▊       | 1230/4381 [32:53<1:24:10,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  28%|██▊       | 1240/4381 [33:12<1:24:01,  1.61s/it, loss=3.18, v_num=641]Epoch 4:  28%|██▊       | 1240/4381 [33:12<1:24:01,  1.61s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▊       | 1250/4381 [33:28<1:23:45,  1.61s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▊       | 1250/4381 [33:28<1:23:45,  1.61s/it, loss=3.2, v_num=641] Epoch 4:  29%|██▉       | 1260/4381 [33:41<1:23:23,  1.60s/it, loss=3.2, v_num=641]Epoch 4:  29%|██▉       | 1260/4381 [33:41<1:23:23,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▉       | 1270/4381 [33:56<1:23:04,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▉       | 1270/4381 [33:56<1:23:04,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▉       | 1280/4381 [34:12<1:22:48,  1.60s/it, loss=3.22, v_num=641]Epoch 4:  29%|██▉       | 1280/4381 [34:12<1:22:48,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  29%|██▉       | 1290/4381 [34:30<1:22:36,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  29%|██▉       | 1290/4381 [34:30<1:22:36,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  30%|██▉       | 1300/4381 [34:47<1:22:24,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  30%|██▉       | 1300/4381 [34:47<1:22:24,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  30%|██▉       | 1310/4381 [35:02<1:22:04,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  30%|██▉       | 1310/4381 [35:02<1:22:04,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  30%|███       | 1320/4381 [35:15<1:21:42,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  30%|███       | 1320/4381 [35:15<1:21:42,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  30%|███       | 1330/4381 [35:33<1:21:30,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  30%|███       | 1330/4381 [35:33<1:21:30,  1.60s/it, loss=3.1, v_num=641] Epoch 4:  31%|███       | 1340/4381 [35:47<1:21:09,  1.60s/it, loss=3.1, v_num=641]Epoch 4:  31%|███       | 1340/4381 [35:47<1:21:09,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  31%|███       | 1350/4381 [36:02<1:20:52,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  31%|███       | 1350/4381 [36:02<1:20:52,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  31%|███       | 1360/4381 [36:22<1:20:43,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  31%|███       | 1360/4381 [36:22<1:20:43,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  31%|███▏      | 1370/4381 [36:37<1:20:25,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  31%|███▏      | 1370/4381 [36:37<1:20:25,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  31%|███▏      | 1380/4381 [36:51<1:20:05,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  31%|███▏      | 1380/4381 [36:51<1:20:05,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  32%|███▏      | 1390/4381 [37:10<1:19:55,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  32%|███▏      | 1390/4381 [37:10<1:19:55,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  32%|███▏      | 1400/4381 [37:25<1:19:37,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  32%|███▏      | 1400/4381 [37:25<1:19:37,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  32%|███▏      | 1410/4381 [37:37<1:19:13,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  32%|███▏      | 1410/4381 [37:37<1:19:13,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  32%|███▏      | 1420/4381 [37:55<1:19:02,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  32%|███▏      | 1420/4381 [37:55<1:19:02,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  33%|███▎      | 1430/4381 [38:09<1:18:40,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  33%|███▎      | 1430/4381 [38:09<1:18:40,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  33%|███▎      | 1440/4381 [38:23<1:18:20,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  33%|███▎      | 1440/4381 [38:23<1:18:20,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  33%|███▎      | 1450/4381 [38:42<1:18:10,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  33%|███▎      | 1450/4381 [38:42<1:18:10,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  33%|███▎      | 1460/4381 [38:56<1:17:51,  1.60s/it, loss=3.12, v_num=641]Epoch 4:  33%|███▎      | 1460/4381 [38:56<1:17:51,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  34%|███▎      | 1470/4381 [39:11<1:17:32,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  34%|███▎      | 1470/4381 [39:11<1:17:32,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  34%|███▍      | 1480/4381 [39:27<1:17:16,  1.60s/it, loss=3.13, v_num=641]Epoch 4:  34%|███▍      | 1480/4381 [39:27<1:17:16,  1.60s/it, loss=3.09, v_num=641]Epoch 4:  34%|███▍      | 1490/4381 [39:42<1:17:00,  1.60s/it, loss=3.09, v_num=641]Epoch 4:  34%|███▍      | 1490/4381 [39:42<1:17:00,  1.60s/it, loss=3.09, v_num=641]Epoch 4:  34%|███▍      | 1500/4381 [39:57<1:16:41,  1.60s/it, loss=3.09, v_num=641]Epoch 4:  34%|███▍      | 1500/4381 [39:57<1:16:41,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  34%|███▍      | 1510/4381 [40:13<1:16:25,  1.60s/it, loss=3.17, v_num=641]Epoch 4:  34%|███▍      | 1510/4381 [40:13<1:16:25,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  35%|███▍      | 1520/4381 [40:27<1:16:05,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  35%|███▍      | 1520/4381 [40:27<1:16:05,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  35%|███▍      | 1530/4381 [40:43<1:15:50,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  35%|███▍      | 1530/4381 [40:43<1:15:50,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  35%|███▌      | 1540/4381 [41:01<1:15:38,  1.60s/it, loss=3.19, v_num=641]Epoch 4:  35%|███▌      | 1540/4381 [41:01<1:15:38,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  35%|███▌      | 1550/4381 [41:18<1:15:23,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  35%|███▌      | 1550/4381 [41:18<1:15:23,  1.60s/it, loss=3.14, v_num=641]Epoch 4:  36%|███▌      | 1560/4381 [41:31<1:15:02,  1.60s/it, loss=3.14, v_num=641]Epoch 4:  36%|███▌      | 1560/4381 [41:31<1:15:02,  1.60s/it, loss=3.14, v_num=641]Epoch 4:  36%|███▌      | 1570/4381 [41:45<1:14:42,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  36%|███▌      | 1570/4381 [41:45<1:14:42,  1.59s/it, loss=3.16, v_num=641]Epoch 4:  36%|███▌      | 1580/4381 [42:01<1:14:26,  1.59s/it, loss=3.16, v_num=641]Epoch 4:  36%|███▌      | 1580/4381 [42:01<1:14:26,  1.59s/it, loss=3.16, v_num=641]Epoch 4:  36%|███▋      | 1590/4381 [42:20<1:14:16,  1.60s/it, loss=3.16, v_num=641]Epoch 4:  36%|███▋      | 1590/4381 [42:20<1:14:16,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1600/4381 [42:34<1:13:57,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1600/4381 [42:34<1:13:57,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1610/4381 [42:46<1:13:34,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1610/4381 [42:46<1:13:34,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  37%|███▋      | 1620/4381 [43:03<1:13:19,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  37%|███▋      | 1620/4381 [43:03<1:13:19,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1630/4381 [43:21<1:13:08,  1.60s/it, loss=3.15, v_num=641]Epoch 4:  37%|███▋      | 1630/4381 [43:21<1:13:08,  1.60s/it, loss=3.18, v_num=641]Epoch 4:  37%|███▋      | 1640/4381 [43:36<1:12:50,  1.59s/it, loss=3.18, v_num=641]Epoch 4:  37%|███▋      | 1640/4381 [43:36<1:12:50,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  38%|███▊      | 1650/4381 [43:48<1:12:28,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  38%|███▊      | 1650/4381 [43:48<1:12:28,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  38%|███▊      | 1660/4381 [44:06<1:12:14,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  38%|███▊      | 1660/4381 [44:06<1:12:14,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  38%|███▊      | 1670/4381 [44:20<1:11:56,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  38%|███▊      | 1670/4381 [44:20<1:11:56,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  38%|███▊      | 1680/4381 [44:38<1:11:44,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  38%|███▊      | 1680/4381 [44:38<1:11:44,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  39%|███▊      | 1690/4381 [44:54<1:11:28,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  39%|███▊      | 1690/4381 [44:54<1:11:28,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  39%|███▉      | 1700/4381 [45:08<1:11:08,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  39%|███▉      | 1700/4381 [45:08<1:11:08,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  39%|███▉      | 1710/4381 [45:24<1:10:53,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  39%|███▉      | 1710/4381 [45:24<1:10:53,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  39%|███▉      | 1720/4381 [45:42<1:10:40,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  39%|███▉      | 1720/4381 [45:42<1:10:40,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  39%|███▉      | 1730/4381 [45:56<1:10:21,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  39%|███▉      | 1730/4381 [45:56<1:10:21,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  40%|███▉      | 1740/4381 [46:14<1:10:08,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  40%|███▉      | 1740/4381 [46:14<1:10:08,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  40%|███▉      | 1750/4381 [46:31<1:09:55,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  40%|███▉      | 1750/4381 [46:31<1:09:55,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  40%|████      | 1760/4381 [46:46<1:09:36,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  40%|████      | 1760/4381 [46:46<1:09:36,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  40%|████      | 1770/4381 [47:04<1:09:23,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  40%|████      | 1770/4381 [47:04<1:09:23,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  41%|████      | 1780/4381 [47:18<1:09:05,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  41%|████      | 1780/4381 [47:18<1:09:05,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  41%|████      | 1790/4381 [47:34<1:08:49,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  41%|████      | 1790/4381 [47:34<1:08:49,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  41%|████      | 1800/4381 [47:51<1:08:34,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  41%|████      | 1800/4381 [47:51<1:08:34,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  41%|████▏     | 1810/4381 [48:06<1:08:17,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  41%|████▏     | 1810/4381 [48:06<1:08:17,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  42%|████▏     | 1820/4381 [48:17<1:07:55,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  42%|████▏     | 1820/4381 [48:17<1:07:55,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  42%|████▏     | 1830/4381 [48:36<1:07:43,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  42%|████▏     | 1830/4381 [48:36<1:07:43,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  42%|████▏     | 1840/4381 [48:51<1:07:25,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  42%|████▏     | 1840/4381 [48:51<1:07:25,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  42%|████▏     | 1850/4381 [49:04<1:07:06,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  42%|████▏     | 1850/4381 [49:04<1:07:06,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  42%|████▏     | 1860/4381 [49:23<1:06:54,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  42%|████▏     | 1860/4381 [49:23<1:06:54,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  43%|████▎     | 1870/4381 [49:38<1:06:37,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  43%|████▎     | 1870/4381 [49:38<1:06:37,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  43%|████▎     | 1880/4381 [49:51<1:06:17,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  43%|████▎     | 1880/4381 [49:51<1:06:17,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  43%|████▎     | 1890/4381 [50:09<1:06:04,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  43%|████▎     | 1890/4381 [50:09<1:06:04,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  43%|████▎     | 1900/4381 [50:22<1:05:45,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  43%|████▎     | 1900/4381 [50:22<1:05:45,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  44%|████▎     | 1910/4381 [50:38<1:05:29,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  44%|████▎     | 1910/4381 [50:38<1:05:29,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  44%|████▍     | 1920/4381 [50:55<1:05:14,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  44%|████▍     | 1920/4381 [50:55<1:05:14,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  44%|████▍     | 1930/4381 [51:11<1:04:58,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  44%|████▍     | 1930/4381 [51:11<1:04:58,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  44%|████▍     | 1940/4381 [51:27<1:04:42,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  44%|████▍     | 1940/4381 [51:27<1:04:42,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  45%|████▍     | 1950/4381 [51:45<1:04:29,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  45%|████▍     | 1950/4381 [51:45<1:04:29,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  45%|████▍     | 1960/4381 [51:57<1:04:09,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  45%|████▍     | 1960/4381 [51:57<1:04:09,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  45%|████▍     | 1970/4381 [52:10<1:03:49,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  45%|████▍     | 1970/4381 [52:10<1:03:49,  1.59s/it, loss=3.19, v_num=641]Epoch 4:  45%|████▌     | 1980/4381 [52:30<1:03:38,  1.59s/it, loss=3.19, v_num=641]Epoch 4:  45%|████▌     | 1980/4381 [52:30<1:03:38,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  45%|████▌     | 1990/4381 [52:45<1:03:22,  1.59s/it, loss=3.17, v_num=641]Epoch 4:  45%|████▌     | 1990/4381 [52:45<1:03:22,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  46%|████▌     | 2000/4381 [53:00<1:03:04,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  46%|████▌     | 2000/4381 [53:00<1:03:04,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  46%|████▌     | 2010/4381 [53:21<1:02:54,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  46%|████▌     | 2010/4381 [53:21<1:02:54,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  46%|████▌     | 2020/4381 [53:33<1:02:33,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  46%|████▌     | 2020/4381 [53:33<1:02:33,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  46%|████▋     | 2030/4381 [53:46<1:02:15,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  46%|████▋     | 2030/4381 [53:46<1:02:15,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  47%|████▋     | 2040/4381 [54:01<1:01:58,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  47%|████▋     | 2040/4381 [54:01<1:01:58,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  47%|████▋     | 2050/4381 [54:17<1:01:42,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  47%|████▋     | 2050/4381 [54:17<1:01:42,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  47%|████▋     | 2060/4381 [54:33<1:01:26,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  47%|████▋     | 2060/4381 [54:33<1:01:26,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  47%|████▋     | 2070/4381 [54:50<1:01:11,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  47%|████▋     | 2070/4381 [54:50<1:01:11,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  47%|████▋     | 2080/4381 [55:06<1:00:56,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  47%|████▋     | 2080/4381 [55:06<1:00:56,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  48%|████▊     | 2090/4381 [55:22<1:00:40,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  48%|████▊     | 2090/4381 [55:22<1:00:40,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  48%|████▊     | 2100/4381 [55:40<1:00:26,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  48%|████▊     | 2100/4381 [55:40<1:00:26,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  48%|████▊     | 2110/4381 [55:55<1:00:09,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  48%|████▊     | 2110/4381 [55:55<1:00:09,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  48%|████▊     | 2120/4381 [56:10<59:53,  1.59s/it, loss=3.13, v_num=641]  Epoch 4:  48%|████▊     | 2120/4381 [56:10<59:53,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  49%|████▊     | 2130/4381 [56:30<59:41,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  49%|████▊     | 2130/4381 [56:30<59:41,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  49%|████▉     | 2140/4381 [56:47<59:26,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  49%|████▉     | 2140/4381 [56:47<59:26,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  49%|████▉     | 2150/4381 [57:01<59:08,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  49%|████▉     | 2150/4381 [57:01<59:08,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  49%|████▉     | 2160/4381 [57:18<58:54,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  49%|████▉     | 2160/4381 [57:18<58:54,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  50%|████▉     | 2170/4381 [57:33<58:37,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  50%|████▉     | 2170/4381 [57:33<58:37,  1.59s/it, loss=3.06, v_num=641]Epoch 4:  50%|████▉     | 2180/4381 [57:46<58:18,  1.59s/it, loss=3.06, v_num=641]Epoch 4:  50%|████▉     | 2180/4381 [57:46<58:18,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  50%|████▉     | 2190/4381 [58:06<58:06,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  50%|████▉     | 2190/4381 [58:06<58:06,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  50%|█████     | 2200/4381 [58:22<57:50,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  50%|█████     | 2200/4381 [58:22<57:50,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  50%|█████     | 2210/4381 [58:36<57:32,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  50%|█████     | 2210/4381 [58:36<57:32,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  51%|█████     | 2220/4381 [58:54<57:19,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  51%|█████     | 2220/4381 [58:54<57:19,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  51%|█████     | 2230/4381 [59:09<57:02,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  51%|█████     | 2230/4381 [59:09<57:02,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  51%|█████     | 2240/4381 [59:23<56:44,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  51%|█████     | 2240/4381 [59:23<56:44,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  51%|█████▏    | 2250/4381 [59:37<56:26,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  51%|█████▏    | 2250/4381 [59:37<56:26,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  52%|█████▏    | 2260/4381 [59:52<56:09,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  52%|█████▏    | 2260/4381 [59:52<56:09,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  52%|█████▏    | 2270/4381 [1:00:05<55:51,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  52%|█████▏    | 2270/4381 [1:00:05<55:51,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  52%|█████▏    | 2280/4381 [1:00:22<55:36,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  52%|█████▏    | 2280/4381 [1:00:22<55:36,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  52%|█████▏    | 2290/4381 [1:00:38<55:21,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  52%|█████▏    | 2290/4381 [1:00:38<55:21,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  52%|█████▏    | 2300/4381 [1:00:54<55:05,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  52%|█████▏    | 2300/4381 [1:00:54<55:05,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  53%|█████▎    | 2310/4381 [1:01:13<54:51,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  53%|█████▎    | 2310/4381 [1:01:13<54:51,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  53%|█████▎    | 2320/4381 [1:01:29<54:35,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  53%|█████▎    | 2320/4381 [1:01:29<54:35,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  53%|█████▎    | 2330/4381 [1:01:43<54:18,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  53%|█████▎    | 2330/4381 [1:01:43<54:18,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  53%|█████▎    | 2340/4381 [1:02:03<54:06,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  53%|█████▎    | 2340/4381 [1:02:03<54:06,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  54%|█████▎    | 2350/4381 [1:02:17<53:48,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  54%|█████▎    | 2350/4381 [1:02:17<53:48,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  54%|█████▍    | 2360/4381 [1:02:31<53:30,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  54%|█████▍    | 2360/4381 [1:02:31<53:30,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  54%|█████▍    | 2370/4381 [1:02:48<53:15,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  54%|█████▍    | 2370/4381 [1:02:48<53:15,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  54%|█████▍    | 2380/4381 [1:03:02<52:58,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  54%|█████▍    | 2380/4381 [1:03:02<52:58,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  55%|█████▍    | 2390/4381 [1:03:16<52:41,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  55%|█████▍    | 2390/4381 [1:03:16<52:41,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  55%|█████▍    | 2400/4381 [1:03:35<52:28,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  55%|█████▍    | 2400/4381 [1:03:35<52:28,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  55%|█████▌    | 2410/4381 [1:03:49<52:10,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  55%|█████▌    | 2410/4381 [1:03:49<52:10,  1.59s/it, loss=3.16, v_num=641]Epoch 4:  55%|█████▌    | 2420/4381 [1:04:04<51:54,  1.59s/it, loss=3.16, v_num=641]Epoch 4:  55%|█████▌    | 2420/4381 [1:04:04<51:54,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  55%|█████▌    | 2430/4381 [1:04:19<51:37,  1.59s/it, loss=3.15, v_num=641]Epoch 4:  55%|█████▌    | 2430/4381 [1:04:19<51:37,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  56%|█████▌    | 2440/4381 [1:04:33<51:20,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  56%|█████▌    | 2440/4381 [1:04:33<51:20,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  56%|█████▌    | 2450/4381 [1:04:46<51:01,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  56%|█████▌    | 2450/4381 [1:04:46<51:01,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  56%|█████▌    | 2460/4381 [1:05:02<50:45,  1.59s/it, loss=3.14, v_num=641]Epoch 4:  56%|█████▌    | 2460/4381 [1:05:02<50:45,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  56%|█████▋    | 2470/4381 [1:05:18<50:30,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  56%|█████▋    | 2470/4381 [1:05:18<50:30,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  57%|█████▋    | 2480/4381 [1:05:33<50:13,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  57%|█████▋    | 2480/4381 [1:05:33<50:13,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  57%|█████▋    | 2490/4381 [1:05:51<49:59,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  57%|█████▋    | 2490/4381 [1:05:51<49:59,  1.59s/it, loss=3.08, v_num=641]Epoch 4:  57%|█████▋    | 2500/4381 [1:06:07<49:44,  1.59s/it, loss=3.08, v_num=641]Epoch 4:  57%|█████▋    | 2500/4381 [1:06:07<49:44,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  57%|█████▋    | 2510/4381 [1:06:22<49:27,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  57%|█████▋    | 2510/4381 [1:06:22<49:27,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  58%|█████▊    | 2520/4381 [1:06:40<49:13,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2520/4381 [1:06:40<49:13,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2530/4381 [1:06:57<48:57,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2530/4381 [1:06:57<48:57,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2540/4381 [1:07:10<48:40,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2540/4381 [1:07:10<48:40,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2550/4381 [1:07:28<48:25,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  58%|█████▊    | 2550/4381 [1:07:28<48:25,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  58%|█████▊    | 2560/4381 [1:07:42<48:08,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  58%|█████▊    | 2560/4381 [1:07:42<48:08,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  59%|█████▊    | 2570/4381 [1:07:57<47:51,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  59%|█████▊    | 2570/4381 [1:07:57<47:51,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  59%|█████▉    | 2580/4381 [1:08:15<47:38,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  59%|█████▉    | 2580/4381 [1:08:15<47:38,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  59%|█████▉    | 2590/4381 [1:08:29<47:20,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  59%|█████▉    | 2590/4381 [1:08:29<47:20,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  59%|█████▉    | 2600/4381 [1:08:42<47:03,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  59%|█████▉    | 2600/4381 [1:08:42<47:03,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  60%|█████▉    | 2610/4381 [1:09:01<46:49,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  60%|█████▉    | 2610/4381 [1:09:01<46:49,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  60%|█████▉    | 2620/4381 [1:09:17<46:33,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  60%|█████▉    | 2620/4381 [1:09:17<46:33,  1.59s/it, loss=3.12, v_num=641]Epoch 4:  60%|██████    | 2630/4381 [1:09:29<46:14,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  60%|██████    | 2630/4381 [1:09:29<46:14,  1.58s/it, loss=3.13, v_num=641]Epoch 4:  60%|██████    | 2640/4381 [1:09:47<46:00,  1.59s/it, loss=3.13, v_num=641]Epoch 4:  60%|██████    | 2640/4381 [1:09:47<46:00,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  60%|██████    | 2650/4381 [1:10:03<45:44,  1.59s/it, loss=3.11, v_num=641]Epoch 4:  60%|██████    | 2650/4381 [1:10:03<45:44,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  61%|██████    | 2660/4381 [1:10:15<45:26,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  61%|██████    | 2660/4381 [1:10:15<45:26,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  61%|██████    | 2670/4381 [1:10:33<45:12,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  61%|██████    | 2670/4381 [1:10:33<45:12,  1.59s/it, loss=3.1, v_num=641] Epoch 4:  61%|██████    | 2680/4381 [1:10:48<44:55,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  61%|██████    | 2680/4381 [1:10:48<44:55,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  61%|██████▏   | 2690/4381 [1:11:05<44:40,  1.59s/it, loss=3.1, v_num=641]Epoch 4:  61%|██████▏   | 2690/4381 [1:11:05<44:40,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  62%|██████▏   | 2700/4381 [1:11:22<44:25,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  62%|██████▏   | 2700/4381 [1:11:22<44:25,  1.59s/it, loss=3.09, v_num=641]Epoch 4:  62%|██████▏   | 2710/4381 [1:11:34<44:07,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  62%|██████▏   | 2710/4381 [1:11:34<44:07,  1.58s/it, loss=3.1, v_num=641] Epoch 4:  62%|██████▏   | 2720/4381 [1:11:48<43:49,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  62%|██████▏   | 2720/4381 [1:11:48<43:49,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  62%|██████▏   | 2730/4381 [1:12:06<43:35,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  62%|██████▏   | 2730/4381 [1:12:06<43:35,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  63%|██████▎   | 2740/4381 [1:12:21<43:19,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  63%|██████▎   | 2740/4381 [1:12:21<43:19,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  63%|██████▎   | 2750/4381 [1:12:34<43:01,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  63%|██████▎   | 2750/4381 [1:12:34<43:01,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  63%|██████▎   | 2760/4381 [1:12:56<42:49,  1.59s/it, loss=3.08, v_num=641]Epoch 4:  63%|██████▎   | 2760/4381 [1:12:56<42:49,  1.59s/it, loss=3.07, v_num=641]Epoch 4:  63%|██████▎   | 2770/4381 [1:13:09<42:32,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  63%|██████▎   | 2770/4381 [1:13:09<42:32,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  63%|██████▎   | 2780/4381 [1:13:22<42:14,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  63%|██████▎   | 2780/4381 [1:13:22<42:14,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  64%|██████▎   | 2790/4381 [1:13:42<42:00,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  64%|██████▎   | 2790/4381 [1:13:42<42:00,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  64%|██████▍   | 2800/4381 [1:13:56<41:44,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  64%|██████▍   | 2800/4381 [1:13:56<41:44,  1.58s/it, loss=3.13, v_num=641]Epoch 4:  64%|██████▍   | 2810/4381 [1:14:10<41:27,  1.58s/it, loss=3.13, v_num=641]Epoch 4:  64%|██████▍   | 2810/4381 [1:14:10<41:27,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  64%|██████▍   | 2820/4381 [1:14:29<41:13,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  64%|██████▍   | 2820/4381 [1:14:29<41:13,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  65%|██████▍   | 2830/4381 [1:14:45<40:57,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  65%|██████▍   | 2830/4381 [1:14:45<40:57,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  65%|██████▍   | 2840/4381 [1:14:59<40:40,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  65%|██████▍   | 2840/4381 [1:14:59<40:40,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  65%|██████▌   | 2850/4381 [1:15:17<40:26,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  65%|██████▌   | 2850/4381 [1:15:17<40:26,  1.58s/it, loss=3.1, v_num=641] Epoch 4:  65%|██████▌   | 2860/4381 [1:15:30<40:08,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  65%|██████▌   | 2860/4381 [1:15:30<40:08,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  66%|██████▌   | 2870/4381 [1:15:45<39:52,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  66%|██████▌   | 2870/4381 [1:15:45<39:52,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  66%|██████▌   | 2880/4381 [1:16:02<39:37,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  66%|██████▌   | 2880/4381 [1:16:02<39:37,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  66%|██████▌   | 2890/4381 [1:16:18<39:21,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  66%|██████▌   | 2890/4381 [1:16:18<39:21,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  66%|██████▌   | 2900/4381 [1:16:33<39:05,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  66%|██████▌   | 2900/4381 [1:16:33<39:05,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  66%|██████▋   | 2910/4381 [1:16:50<38:49,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  66%|██████▋   | 2910/4381 [1:16:50<38:49,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  67%|██████▋   | 2920/4381 [1:17:04<38:33,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  67%|██████▋   | 2920/4381 [1:17:04<38:33,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  67%|██████▋   | 2930/4381 [1:17:21<38:17,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  67%|██████▋   | 2930/4381 [1:17:21<38:17,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  67%|██████▋   | 2940/4381 [1:17:39<38:02,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  67%|██████▋   | 2940/4381 [1:17:39<38:02,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  67%|██████▋   | 2950/4381 [1:17:54<37:46,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  67%|██████▋   | 2950/4381 [1:17:54<37:46,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  68%|██████▊   | 2960/4381 [1:18:09<37:30,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  68%|██████▊   | 2960/4381 [1:18:09<37:30,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  68%|██████▊   | 2970/4381 [1:18:22<37:13,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  68%|██████▊   | 2970/4381 [1:18:22<37:13,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  68%|██████▊   | 2980/4381 [1:18:37<36:57,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  68%|██████▊   | 2980/4381 [1:18:37<36:57,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  68%|██████▊   | 2990/4381 [1:18:52<36:41,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  68%|██████▊   | 2990/4381 [1:18:52<36:41,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  68%|██████▊   | 3000/4381 [1:19:08<36:25,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  68%|██████▊   | 3000/4381 [1:19:08<36:25,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  69%|██████▊   | 3010/4381 [1:19:23<36:08,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  69%|██████▊   | 3010/4381 [1:19:23<36:08,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  69%|██████▉   | 3020/4381 [1:19:39<35:53,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  69%|██████▉   | 3020/4381 [1:19:39<35:53,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  69%|██████▉   | 3030/4381 [1:19:58<35:38,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  69%|██████▉   | 3030/4381 [1:19:58<35:38,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  69%|██████▉   | 3040/4381 [1:20:13<35:22,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  69%|██████▉   | 3040/4381 [1:20:13<35:22,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  70%|██████▉   | 3050/4381 [1:20:30<35:07,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  70%|██████▉   | 3050/4381 [1:20:30<35:07,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  70%|██████▉   | 3060/4381 [1:20:45<34:51,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  70%|██████▉   | 3060/4381 [1:20:45<34:51,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  70%|███████   | 3070/4381 [1:21:00<34:34,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  70%|███████   | 3070/4381 [1:21:00<34:34,  1.58s/it, loss=3.03, v_num=641]Epoch 4:  70%|███████   | 3080/4381 [1:21:13<34:17,  1.58s/it, loss=3.03, v_num=641]Epoch 4:  70%|███████   | 3080/4381 [1:21:13<34:17,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  71%|███████   | 3090/4381 [1:21:27<34:01,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  71%|███████   | 3090/4381 [1:21:27<34:01,  1.58s/it, loss=3.1, v_num=641] Epoch 4:  71%|███████   | 3100/4381 [1:21:43<33:45,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  71%|███████   | 3100/4381 [1:21:43<33:45,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  71%|███████   | 3110/4381 [1:22:01<33:30,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  71%|███████   | 3110/4381 [1:22:01<33:30,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  71%|███████   | 3120/4381 [1:22:16<33:14,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  71%|███████   | 3120/4381 [1:22:16<33:14,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  71%|███████▏  | 3130/4381 [1:22:32<32:58,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  71%|███████▏  | 3130/4381 [1:22:32<32:58,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  72%|███████▏  | 3140/4381 [1:22:48<32:43,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  72%|███████▏  | 3140/4381 [1:22:48<32:43,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  72%|███████▏  | 3150/4381 [1:23:07<32:28,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  72%|███████▏  | 3150/4381 [1:23:07<32:28,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  72%|███████▏  | 3160/4381 [1:23:19<32:11,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  72%|███████▏  | 3160/4381 [1:23:19<32:11,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  72%|███████▏  | 3170/4381 [1:23:33<31:54,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  72%|███████▏  | 3170/4381 [1:23:33<31:54,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  73%|███████▎  | 3180/4381 [1:23:51<31:39,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  73%|███████▎  | 3180/4381 [1:23:51<31:39,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  73%|███████▎  | 3190/4381 [1:24:06<31:23,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  73%|███████▎  | 3190/4381 [1:24:06<31:23,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  73%|███████▎  | 3200/4381 [1:24:23<31:08,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  73%|███████▎  | 3200/4381 [1:24:23<31:08,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  73%|███████▎  | 3210/4381 [1:24:41<30:52,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  73%|███████▎  | 3210/4381 [1:24:41<30:52,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  73%|███████▎  | 3220/4381 [1:24:57<30:37,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  73%|███████▎  | 3220/4381 [1:24:57<30:37,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  74%|███████▎  | 3230/4381 [1:25:11<30:21,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  74%|███████▎  | 3230/4381 [1:25:11<30:21,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  74%|███████▍  | 3240/4381 [1:25:29<30:05,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  74%|███████▍  | 3240/4381 [1:25:29<30:05,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  74%|███████▍  | 3250/4381 [1:25:45<29:49,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  74%|███████▍  | 3250/4381 [1:25:45<29:49,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  74%|███████▍  | 3260/4381 [1:25:59<29:33,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  74%|███████▍  | 3260/4381 [1:25:59<29:33,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  75%|███████▍  | 3270/4381 [1:26:16<29:18,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  75%|███████▍  | 3270/4381 [1:26:16<29:18,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  75%|███████▍  | 3280/4381 [1:26:31<29:01,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  75%|███████▍  | 3280/4381 [1:26:31<29:01,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  75%|███████▌  | 3290/4381 [1:26:44<28:45,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  75%|███████▌  | 3290/4381 [1:26:44<28:45,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  75%|███████▌  | 3300/4381 [1:27:02<28:30,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  75%|███████▌  | 3300/4381 [1:27:02<28:30,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  76%|███████▌  | 3310/4381 [1:27:16<28:13,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  76%|███████▌  | 3310/4381 [1:27:16<28:13,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  76%|███████▌  | 3320/4381 [1:27:32<27:58,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  76%|███████▌  | 3320/4381 [1:27:32<27:58,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  76%|███████▌  | 3330/4381 [1:27:48<27:42,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  76%|███████▌  | 3330/4381 [1:27:48<27:42,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  76%|███████▌  | 3340/4381 [1:28:02<27:26,  1.58s/it, loss=3.12, v_num=641]Epoch 4:  76%|███████▌  | 3340/4381 [1:28:02<27:26,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  76%|███████▋  | 3350/4381 [1:28:19<27:10,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  76%|███████▋  | 3350/4381 [1:28:19<27:10,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  77%|███████▋  | 3360/4381 [1:28:33<26:54,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  77%|███████▋  | 3360/4381 [1:28:33<26:54,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  77%|███████▋  | 3370/4381 [1:28:50<26:38,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  77%|███████▋  | 3370/4381 [1:28:50<26:38,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  77%|███████▋  | 3380/4381 [1:29:04<26:22,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  77%|███████▋  | 3380/4381 [1:29:04<26:22,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  77%|███████▋  | 3390/4381 [1:29:21<26:06,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  77%|███████▋  | 3390/4381 [1:29:21<26:06,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  78%|███████▊  | 3400/4381 [1:29:35<25:50,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  78%|███████▊  | 3400/4381 [1:29:35<25:50,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  78%|███████▊  | 3410/4381 [1:29:51<25:34,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  78%|███████▊  | 3410/4381 [1:29:51<25:34,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  78%|███████▊  | 3420/4381 [1:30:09<25:19,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  78%|███████▊  | 3420/4381 [1:30:09<25:19,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  78%|███████▊  | 3430/4381 [1:30:24<25:03,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  78%|███████▊  | 3430/4381 [1:30:24<25:03,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  79%|███████▊  | 3440/4381 [1:30:40<24:47,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  79%|███████▊  | 3440/4381 [1:30:40<24:47,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  79%|███████▊  | 3450/4381 [1:30:58<24:32,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  79%|███████▊  | 3450/4381 [1:30:58<24:32,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  79%|███████▉  | 3460/4381 [1:31:15<24:17,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  79%|███████▉  | 3460/4381 [1:31:15<24:17,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  79%|███████▉  | 3470/4381 [1:31:30<24:01,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  79%|███████▉  | 3470/4381 [1:31:30<24:01,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  79%|███████▉  | 3480/4381 [1:31:49<23:46,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  79%|███████▉  | 3480/4381 [1:31:49<23:46,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  80%|███████▉  | 3490/4381 [1:32:06<23:30,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  80%|███████▉  | 3490/4381 [1:32:06<23:30,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  80%|███████▉  | 3500/4381 [1:32:21<23:14,  1.58s/it, loss=3.11, v_num=641]Epoch 4:  80%|███████▉  | 3500/4381 [1:32:21<23:14,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  80%|████████  | 3510/4381 [1:32:38<22:58,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  80%|████████  | 3510/4381 [1:32:38<22:58,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  80%|████████  | 3520/4381 [1:32:54<22:43,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  80%|████████  | 3520/4381 [1:32:54<22:43,  1.58s/it, loss=3.1, v_num=641] Epoch 4:  81%|████████  | 3530/4381 [1:33:07<22:26,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  81%|████████  | 3530/4381 [1:33:07<22:26,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  81%|████████  | 3540/4381 [1:33:24<22:11,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  81%|████████  | 3540/4381 [1:33:24<22:11,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  81%|████████  | 3550/4381 [1:33:38<21:54,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  81%|████████  | 3550/4381 [1:33:38<21:54,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  81%|████████▏ | 3560/4381 [1:33:52<21:38,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  81%|████████▏ | 3560/4381 [1:33:52<21:38,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  81%|████████▏ | 3570/4381 [1:34:10<21:23,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  81%|████████▏ | 3570/4381 [1:34:10<21:23,  1.58s/it, loss=3.1, v_num=641] Epoch 4:  82%|████████▏ | 3580/4381 [1:34:25<21:07,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  82%|████████▏ | 3580/4381 [1:34:25<21:07,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  82%|████████▏ | 3590/4381 [1:34:38<20:50,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  82%|████████▏ | 3590/4381 [1:34:38<20:50,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  82%|████████▏ | 3600/4381 [1:34:56<20:35,  1.58s/it, loss=3.1, v_num=641]Epoch 4:  82%|████████▏ | 3600/4381 [1:34:56<20:35,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  82%|████████▏ | 3610/4381 [1:35:08<20:18,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  82%|████████▏ | 3610/4381 [1:35:08<20:18,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  83%|████████▎ | 3620/4381 [1:35:23<20:02,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  83%|████████▎ | 3620/4381 [1:35:23<20:02,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  83%|████████▎ | 3630/4381 [1:35:40<19:47,  1.58s/it, loss=3.02, v_num=641]Epoch 4:  83%|████████▎ | 3630/4381 [1:35:40<19:47,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  83%|████████▎ | 3640/4381 [1:35:53<19:30,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  83%|████████▎ | 3640/4381 [1:35:53<19:30,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  83%|████████▎ | 3650/4381 [1:36:08<19:14,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  83%|████████▎ | 3650/4381 [1:36:08<19:14,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  84%|████████▎ | 3660/4381 [1:36:26<18:59,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  84%|████████▎ | 3660/4381 [1:36:26<18:59,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  84%|████████▍ | 3670/4381 [1:36:40<18:43,  1.58s/it, loss=3.06, v_num=641]Epoch 4:  84%|████████▍ | 3670/4381 [1:36:40<18:43,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  84%|████████▍ | 3680/4381 [1:36:55<18:27,  1.58s/it, loss=3.07, v_num=641]Epoch 4:  84%|████████▍ | 3680/4381 [1:36:55<18:27,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  84%|████████▍ | 3690/4381 [1:37:13<18:12,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  84%|████████▍ | 3690/4381 [1:37:13<18:12,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  84%|████████▍ | 3700/4381 [1:37:30<17:56,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  84%|████████▍ | 3700/4381 [1:37:30<17:56,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  85%|████████▍ | 3710/4381 [1:37:45<17:40,  1.58s/it, loss=3.09, v_num=641]Epoch 4:  85%|████████▍ | 3710/4381 [1:37:45<17:40,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  85%|████████▍ | 3720/4381 [1:38:01<17:24,  1.58s/it, loss=3.08, v_num=641]Epoch 4:  85%|████████▍ | 3720/4381 [1:38:01<17:24,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  85%|████████▌ | 3730/4381 [1:38:13<17:08,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  85%|████████▌ | 3730/4381 [1:38:13<17:08,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  85%|████████▌ | 3740/4381 [1:38:20<16:50,  1.58s/it, loss=3.04, v_num=641]Epoch 4:  85%|████████▌ | 3740/4381 [1:38:20<16:50,  1.58s/it, loss=3.05, v_num=641]Epoch 4:  86%|████████▌ | 3750/4381 [1:38:23<16:33,  1.57s/it, loss=3.05, v_num=641]Epoch 4:  86%|████████▌ | 3750/4381 [1:38:23<16:33,  1.57s/it, loss=3.04, v_num=641]validation_epoch_end
graph acc: 0.020766773162939296
valid accuracy: 0.9074534177780151
validation_epoch_end
graph acc: 0.023961661341853034
valid accuracy: 0.9054505228996277
validation_epoch_end
graph acc: 0.02875399361022364
valid accuracy: 0.9115514159202576
validation_epoch_end
graph acc: 0.019169329073482427
valid accuracy: 0.9097614884376526
validation_epoch_end
graph acc: 0.038338658146964855
valid accuracy: 0.9099736213684082
validation_epoch_end
graph acc: 0.02875399361022364
valid accuracy: 0.9104680418968201
validation_epoch_end
graph acc: 0.03354632587859425
valid accuracy: 0.9112445712089539
Epoch 4:  86%|████████▌ | 3760/4381 [1:38:25<16:15,  1.57s/it, loss=3.04, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][A
Validating:   2%|▏         | 10/626 [00:03<03:51,  2.66it/s][AEpoch 4:  86%|████████▌ | 3770/4381 [1:38:28<15:57,  1.57s/it, loss=3.04, v_num=641]
Validating:   3%|▎         | 20/626 [00:04<02:12,  4.56it/s][AEpoch 4:  86%|████████▋ | 3780/4381 [1:38:29<15:39,  1.56s/it, loss=3.04, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:36,  3.80it/s][AEpoch 4:  87%|████████▋ | 3790/4381 [1:38:33<15:21,  1.56s/it, loss=3.04, v_num=641]
Validating:   6%|▋         | 40/626 [00:09<02:17,  4.26it/s][AEpoch 4:  87%|████████▋ | 3800/4381 [1:38:35<15:04,  1.56s/it, loss=3.04, v_num=641]
Validating:   8%|▊         | 50/626 [00:11<01:58,  4.85it/s][AEpoch 4:  87%|████████▋ | 3810/4381 [1:38:36<14:46,  1.55s/it, loss=3.04, v_num=641]
Validating:  10%|▉         | 60/626 [00:13<02:00,  4.71it/s][AEpoch 4:  87%|████████▋ | 3820/4381 [1:38:38<14:29,  1.55s/it, loss=3.04, v_num=641]
Validating:  11%|█         | 70/626 [00:15<01:50,  5.04it/s][AEpoch 4:  87%|████████▋ | 3830/4381 [1:38:40<14:11,  1.55s/it, loss=3.04, v_num=641]
Validating:  13%|█▎        | 80/626 [00:16<01:35,  5.75it/s][AEpoch 4:  88%|████████▊ | 3840/4381 [1:38:41<13:54,  1.54s/it, loss=3.04, v_num=641]
Validating:  14%|█▍        | 90/626 [00:18<01:32,  5.77it/s][AEpoch 4:  88%|████████▊ | 3850/4381 [1:38:43<13:36,  1.54s/it, loss=3.04, v_num=641]
Validating:  16%|█▌        | 100/626 [00:20<01:35,  5.52it/s][AEpoch 4:  88%|████████▊ | 3860/4381 [1:38:45<13:19,  1.53s/it, loss=3.04, v_num=641]
Validating:  18%|█▊        | 110/626 [00:22<01:36,  5.33it/s][AEpoch 4:  88%|████████▊ | 3870/4381 [1:38:47<13:02,  1.53s/it, loss=3.04, v_num=641]
Validating:  19%|█▉        | 120/626 [00:24<01:32,  5.48it/s][AEpoch 4:  89%|████████▊ | 3880/4381 [1:38:49<12:45,  1.53s/it, loss=3.04, v_num=641]
Validating:  21%|██        | 130/626 [00:25<01:28,  5.60it/s][AEpoch 4:  89%|████████▉ | 3890/4381 [1:38:50<12:28,  1.52s/it, loss=3.04, v_num=641]
Validating:  22%|██▏       | 140/626 [00:28<01:36,  5.04it/s][AEpoch 4:  89%|████████▉ | 3900/4381 [1:38:53<12:11,  1.52s/it, loss=3.04, v_num=641]
Validating:  24%|██▍       | 150/626 [00:30<01:36,  4.91it/s][AEpoch 4:  89%|████████▉ | 3910/4381 [1:38:55<11:54,  1.52s/it, loss=3.04, v_num=641]
Validating:  26%|██▌       | 160/626 [00:31<01:21,  5.72it/s][AEpoch 4:  89%|████████▉ | 3920/4381 [1:38:56<11:37,  1.51s/it, loss=3.04, v_num=641]
Validating:  27%|██▋       | 170/626 [00:33<01:24,  5.38it/s][AEpoch 4:  90%|████████▉ | 3930/4381 [1:38:58<11:21,  1.51s/it, loss=3.04, v_num=641]
Validating:  29%|██▉       | 180/626 [00:34<01:14,  6.03it/s][AEpoch 4:  90%|████████▉ | 3940/4381 [1:38:59<11:04,  1.51s/it, loss=3.04, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:24,  5.17it/s][AEpoch 4:  90%|█████████ | 3950/4381 [1:39:02<10:48,  1.50s/it, loss=3.04, v_num=641]
Validating:  32%|███▏      | 200/626 [00:39<01:26,  4.92it/s][AEpoch 4:  90%|█████████ | 3960/4381 [1:39:04<10:31,  1.50s/it, loss=3.04, v_num=641]
Validating:  34%|███▎      | 210/626 [00:41<01:16,  5.41it/s][AEpoch 4:  91%|█████████ | 3970/4381 [1:39:06<10:15,  1.50s/it, loss=3.04, v_num=641]
Validating:  35%|███▌      | 220/626 [00:43<01:21,  4.97it/s][AEpoch 4:  91%|█████████ | 3980/4381 [1:39:08<09:59,  1.49s/it, loss=3.04, v_num=641]
Validating:  37%|███▋      | 230/626 [00:44<01:14,  5.32it/s][AEpoch 4:  91%|█████████ | 3990/4381 [1:39:10<09:42,  1.49s/it, loss=3.04, v_num=641]
Validating:  38%|███▊      | 240/626 [00:46<01:11,  5.39it/s][AEpoch 4:  91%|█████████▏| 4000/4381 [1:39:11<09:26,  1.49s/it, loss=3.04, v_num=641]
Validating:  40%|███▉      | 250/626 [00:49<01:13,  5.08it/s][AEpoch 4:  92%|█████████▏| 4010/4381 [1:39:14<09:10,  1.48s/it, loss=3.04, v_num=641]
Validating:  42%|████▏     | 260/626 [00:51<01:15,  4.83it/s][AEpoch 4:  92%|█████████▏| 4020/4381 [1:39:16<08:54,  1.48s/it, loss=3.04, v_num=641]
Validating:  43%|████▎     | 270/626 [00:53<01:10,  5.07it/s][AEpoch 4:  92%|█████████▏| 4030/4381 [1:39:18<08:38,  1.48s/it, loss=3.04, v_num=641]
Validating:  45%|████▍     | 280/626 [00:56<01:19,  4.36it/s][AEpoch 4:  92%|█████████▏| 4040/4381 [1:39:21<08:23,  1.48s/it, loss=3.04, v_num=641]
Validating:  46%|████▋     | 290/626 [00:58<01:16,  4.37it/s][AEpoch 4:  92%|█████████▏| 4050/4381 [1:39:23<08:07,  1.47s/it, loss=3.04, v_num=641]
Validating:  48%|████▊     | 300/626 [00:59<01:05,  4.98it/s][AEpoch 4:  93%|█████████▎| 4060/4381 [1:39:24<07:51,  1.47s/it, loss=3.04, v_num=641]
Validating:  50%|████▉     | 310/626 [01:01<01:01,  5.10it/s][AEpoch 4:  93%|█████████▎| 4070/4381 [1:39:26<07:35,  1.47s/it, loss=3.04, v_num=641]
Validating:  51%|█████     | 320/626 [01:02<00:54,  5.61it/s][AEpoch 4:  93%|█████████▎| 4080/4381 [1:39:28<07:20,  1.46s/it, loss=3.04, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:04<00:48,  6.07it/s][AEpoch 4:  93%|█████████▎| 4090/4381 [1:39:29<07:04,  1.46s/it, loss=3.04, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:05<00:47,  6.01it/s][AEpoch 4:  94%|█████████▎| 4100/4381 [1:39:31<06:49,  1.46s/it, loss=3.04, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:08<00:56,  4.88it/s][AEpoch 4:  94%|█████████▍| 4110/4381 [1:39:34<06:33,  1.45s/it, loss=3.04, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:10<00:51,  5.20it/s][AEpoch 4:  94%|█████████▍| 4120/4381 [1:39:35<06:18,  1.45s/it, loss=3.04, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:11<00:40,  6.36it/s][AEpoch 4:  94%|█████████▍| 4130/4381 [1:39:36<06:03,  1.45s/it, loss=3.04, v_num=641]
Validating:  61%|██████    | 380/626 [01:13<00:41,  5.87it/s][AEpoch 4:  94%|█████████▍| 4140/4381 [1:39:38<05:47,  1.44s/it, loss=3.04, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:14<00:37,  6.29it/s][AEpoch 4:  95%|█████████▍| 4150/4381 [1:39:39<05:32,  1.44s/it, loss=3.04, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:15<00:34,  6.62it/s][AEpoch 4:  95%|█████████▍| 4160/4381 [1:39:41<05:17,  1.44s/it, loss=3.04, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:17<00:31,  6.95it/s][AEpoch 4:  95%|█████████▌| 4170/4381 [1:39:42<05:02,  1.43s/it, loss=3.04, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:19<00:37,  5.54it/s][AEpoch 4:  95%|█████████▌| 4180/4381 [1:39:45<04:47,  1.43s/it, loss=3.04, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:21<00:34,  5.76it/s][AEpoch 4:  96%|█████████▌| 4190/4381 [1:39:46<04:32,  1.43s/it, loss=3.04, v_num=641]
Validating:  70%|███████   | 440/626 [01:23<00:32,  5.70it/s][AEpoch 4:  96%|█████████▌| 4200/4381 [1:39:48<04:18,  1.43s/it, loss=3.04, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:24<00:29,  5.90it/s][AEpoch 4:  96%|█████████▌| 4210/4381 [1:39:49<04:03,  1.42s/it, loss=3.04, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:26<00:28,  5.82it/s][AEpoch 4:  96%|█████████▋| 4220/4381 [1:39:51<03:48,  1.42s/it, loss=3.04, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:28<00:29,  5.26it/s][AEpoch 4:  97%|█████████▋| 4230/4381 [1:39:54<03:33,  1.42s/it, loss=3.04, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:30<00:27,  5.25it/s][AEpoch 4:  97%|█████████▋| 4240/4381 [1:39:55<03:19,  1.41s/it, loss=3.04, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:34<00:32,  4.21it/s][AEpoch 4:  97%|█████████▋| 4250/4381 [1:39:59<03:04,  1.41s/it, loss=3.04, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:35<00:24,  5.21it/s][AEpoch 4:  97%|█████████▋| 4260/4381 [1:40:00<02:50,  1.41s/it, loss=3.04, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:38<00:25,  4.53it/s][AEpoch 4:  97%|█████████▋| 4270/4381 [1:40:03<02:36,  1.41s/it, loss=3.04, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:39<00:20,  5.07it/s][AEpoch 4:  98%|█████████▊| 4280/4381 [1:40:04<02:21,  1.40s/it, loss=3.04, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:41<00:19,  5.05it/s][AEpoch 4:  98%|█████████▊| 4290/4381 [1:40:06<02:07,  1.40s/it, loss=3.04, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:43<00:15,  5.40it/s][AEpoch 4:  98%|█████████▊| 4300/4381 [1:40:08<01:53,  1.40s/it, loss=3.04, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:44<00:13,  5.66it/s][AEpoch 4:  98%|█████████▊| 4310/4381 [1:40:09<01:38,  1.39s/it, loss=3.04, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:46<00:11,  5.65it/s][AEpoch 4:  99%|█████████▊| 4320/4381 [1:40:11<01:24,  1.39s/it, loss=3.04, v_num=641]
Validating:  91%|█████████ | 570/626 [01:47<00:09,  5.93it/s][AEpoch 4:  99%|█████████▉| 4330/4381 [1:40:12<01:10,  1.39s/it, loss=3.04, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:49<00:07,  5.94it/s][AEpoch 4:  99%|█████████▉| 4340/4381 [1:40:14<00:56,  1.39s/it, loss=3.04, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:50<00:05,  6.62it/s][AEpoch 4:  99%|█████████▉| 4350/4381 [1:40:15<00:42,  1.38s/it, loss=3.04, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:52<00:03,  6.67it/s][AEpoch 4: 100%|█████████▉| 4360/4381 [1:40:17<00:28,  1.38s/it, loss=3.04, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:53<00:02,  6.96it/s][AEpoch 4: 100%|█████████▉| 4370/4381 [1:40:18<00:15,  1.38s/it, loss=3.04, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:55<00:01,  5.78it/s][AEpoch 4: 100%|█████████▉| 4380/4381 [1:40:20<00:01,  1.37s/it, loss=3.04, v_num=641]
Validating: 100%|██████████| 626/626 [01:56<00:00,  6.55it/s][Avalidation_epoch_end
graph acc: 0.09105431309904154
valid accuracy: 0.9322195649147034
Epoch 4: 100%|██████████| 4381/4381 [1:40:25<00:00,  1.37s/it, loss=3.03, v_num=641]
                                                             [AEpoch 4:   0%|          | 0/4381 [00:00<00:00, 10894.30it/s, loss=3.03, v_num=641]  Epoch 5:   0%|          | 0/4381 [00:00<00:01, 2916.76it/s, loss=3.03, v_num=641] Epoch 5:   0%|          | 0/4381 [00:10<12:32:45, 10.31s/it, loss=3.03, v_num=641]Epoch 5:   0%|          | 10/4381 [00:19<2:12:14,  1.82s/it, loss=3.03, v_num=641]Epoch 5:   0%|          | 10/4381 [00:19<2:12:14,  1.82s/it, loss=3.03, v_num=641]Epoch 5:   0%|          | 20/4381 [00:38<2:12:06,  1.82s/it, loss=3.03, v_num=641]Epoch 5:   0%|          | 20/4381 [00:38<2:12:07,  1.82s/it, loss=3, v_num=641]   Epoch 5:   1%|          | 30/4381 [00:55<2:10:26,  1.80s/it, loss=3, v_num=641]Epoch 5:   1%|          | 30/4381 [00:55<2:10:26,  1.80s/it, loss=3.01, v_num=641]Epoch 5:   1%|          | 40/4381 [01:12<2:08:03,  1.77s/it, loss=3.01, v_num=641]Epoch 5:   1%|          | 40/4381 [01:12<2:08:03,  1.77s/it, loss=2.99, v_num=641]Epoch 5:   1%|          | 50/4381 [01:26<2:01:52,  1.69s/it, loss=2.99, v_num=641]Epoch 5:   1%|          | 50/4381 [01:26<2:01:52,  1.69s/it, loss=3.02, v_num=641]Epoch 5:   1%|▏         | 60/4381 [01:48<2:08:31,  1.78s/it, loss=3.02, v_num=641]Epoch 5:   1%|▏         | 60/4381 [01:48<2:08:31,  1.78s/it, loss=3.03, v_num=641]Epoch 5:   2%|▏         | 70/4381 [02:05<2:07:03,  1.77s/it, loss=3.03, v_num=641]Epoch 5:   2%|▏         | 70/4381 [02:05<2:07:03,  1.77s/it, loss=2.96, v_num=641]Epoch 5:   2%|▏         | 80/4381 [02:21<2:05:09,  1.75s/it, loss=2.96, v_num=641]Epoch 5:   2%|▏         | 80/4381 [02:21<2:05:09,  1.75s/it, loss=2.96, v_num=641]Epoch 5:   2%|▏         | 90/4381 [02:39<2:05:31,  1.76s/it, loss=2.96, v_num=641]Epoch 5:   2%|▏         | 90/4381 [02:39<2:05:31,  1.76s/it, loss=2.98, v_num=641]Epoch 5:   2%|▏         | 100/4381 [02:54<2:03:30,  1.73s/it, loss=2.98, v_num=641]Epoch 5:   2%|▏         | 100/4381 [02:54<2:03:30,  1.73s/it, loss=2.99, v_num=641]Epoch 5:   3%|▎         | 110/4381 [03:10<2:02:07,  1.72s/it, loss=2.99, v_num=641]Epoch 5:   3%|▎         | 110/4381 [03:10<2:02:07,  1.72s/it, loss=3.02, v_num=641]Epoch 5:   3%|▎         | 120/4381 [03:28<2:02:35,  1.73s/it, loss=3.02, v_num=641]Epoch 5:   3%|▎         | 120/4381 [03:28<2:02:35,  1.73s/it, loss=3.04, v_num=641]Epoch 5:   3%|▎         | 130/4381 [03:47<2:02:58,  1.74s/it, loss=3.04, v_num=641]Epoch 5:   3%|▎         | 130/4381 [03:47<2:02:58,  1.74s/it, loss=3, v_num=641]   Epoch 5:   3%|▎         | 140/4381 [04:01<2:01:18,  1.72s/it, loss=3, v_num=641]Epoch 5:   3%|▎         | 140/4381 [04:01<2:01:18,  1.72s/it, loss=2.99, v_num=641]Epoch 5:   3%|▎         | 150/4381 [04:18<2:00:50,  1.71s/it, loss=2.99, v_num=641]Epoch 5:   3%|▎         | 150/4381 [04:18<2:00:50,  1.71s/it, loss=3.02, v_num=641]Epoch 5:   4%|▎         | 160/4381 [04:34<1:59:51,  1.70s/it, loss=3.02, v_num=641]Epoch 5:   4%|▎         | 160/4381 [04:34<1:59:51,  1.70s/it, loss=3.02, v_num=641]Epoch 5:   4%|▍         | 170/4381 [04:47<1:58:03,  1.68s/it, loss=3.02, v_num=641]Epoch 5:   4%|▍         | 170/4381 [04:47<1:58:04,  1.68s/it, loss=3, v_num=641]   Epoch 5:   4%|▍         | 180/4381 [05:05<1:58:14,  1.69s/it, loss=3, v_num=641]Epoch 5:   4%|▍         | 180/4381 [05:05<1:58:14,  1.69s/it, loss=2.99, v_num=641]Epoch 5:   4%|▍         | 190/4381 [05:19<1:56:52,  1.67s/it, loss=2.99, v_num=641]Epoch 5:   4%|▍         | 190/4381 [05:19<1:56:52,  1.67s/it, loss=3.04, v_num=641]Epoch 5:   5%|▍         | 200/4381 [05:32<1:55:24,  1.66s/it, loss=3.04, v_num=641]Epoch 5:   5%|▍         | 200/4381 [05:32<1:55:24,  1.66s/it, loss=3.06, v_num=641]Epoch 5:   5%|▍         | 210/4381 [05:52<1:56:02,  1.67s/it, loss=3.06, v_num=641]Epoch 5:   5%|▍         | 210/4381 [05:52<1:56:02,  1.67s/it, loss=3.03, v_num=641]Epoch 5:   5%|▌         | 220/4381 [06:06<1:54:51,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   5%|▌         | 220/4381 [06:06<1:54:51,  1.66s/it, loss=3.01, v_num=641]Epoch 5:   5%|▌         | 230/4381 [06:24<1:55:16,  1.67s/it, loss=3.01, v_num=641]Epoch 5:   5%|▌         | 230/4381 [06:24<1:55:16,  1.67s/it, loss=2.99, v_num=641]Epoch 5:   5%|▌         | 240/4381 [06:42<1:55:19,  1.67s/it, loss=2.99, v_num=641]Epoch 5:   5%|▌         | 240/4381 [06:42<1:55:19,  1.67s/it, loss=3.03, v_num=641]Epoch 5:   6%|▌         | 250/4381 [06:56<1:54:13,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   6%|▌         | 250/4381 [06:56<1:54:13,  1.66s/it, loss=3.02, v_num=641]Epoch 5:   6%|▌         | 260/4381 [07:11<1:53:39,  1.65s/it, loss=3.02, v_num=641]Epoch 5:   6%|▌         | 260/4381 [07:11<1:53:39,  1.65s/it, loss=2.99, v_num=641]Epoch 5:   6%|▌         | 270/4381 [07:30<1:53:54,  1.66s/it, loss=2.99, v_num=641]Epoch 5:   6%|▌         | 270/4381 [07:30<1:53:54,  1.66s/it, loss=2.98, v_num=641]Epoch 5:   6%|▋         | 280/4381 [07:47<1:53:49,  1.67s/it, loss=2.98, v_num=641]Epoch 5:   6%|▋         | 280/4381 [07:47<1:53:49,  1.67s/it, loss=2.97, v_num=641]Epoch 5:   7%|▋         | 290/4381 [08:05<1:53:42,  1.67s/it, loss=2.97, v_num=641]Epoch 5:   7%|▋         | 290/4381 [08:05<1:53:42,  1.67s/it, loss=3, v_num=641]   Epoch 5:   7%|▋         | 300/4381 [08:20<1:53:07,  1.66s/it, loss=3, v_num=641]Epoch 5:   7%|▋         | 300/4381 [08:20<1:53:07,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   7%|▋         | 310/4381 [08:36<1:52:45,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   7%|▋         | 310/4381 [08:36<1:52:45,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   7%|▋         | 320/4381 [08:52<1:52:14,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   7%|▋         | 320/4381 [08:52<1:52:14,  1.66s/it, loss=3.02, v_num=641]Epoch 5:   8%|▊         | 330/4381 [09:08<1:51:49,  1.66s/it, loss=3.02, v_num=641]Epoch 5:   8%|▊         | 330/4381 [09:08<1:51:49,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 340/4381 [09:25<1:51:43,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 340/4381 [09:25<1:51:43,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 350/4381 [09:42<1:51:29,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 350/4381 [09:42<1:51:29,  1.66s/it, loss=3.01, v_num=641]Epoch 5:   8%|▊         | 360/4381 [09:56<1:50:42,  1.65s/it, loss=3.01, v_num=641]Epoch 5:   8%|▊         | 360/4381 [09:56<1:50:42,  1.65s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 370/4381 [10:15<1:50:50,  1.66s/it, loss=3.03, v_num=641]Epoch 5:   8%|▊         | 370/4381 [10:15<1:50:50,  1.66s/it, loss=3.01, v_num=641]Epoch 5:   9%|▊         | 380/4381 [10:28<1:49:56,  1.65s/it, loss=3.01, v_num=641]Epoch 5:   9%|▊         | 380/4381 [10:28<1:49:56,  1.65s/it, loss=2.98, v_num=641]Epoch 5:   9%|▉         | 390/4381 [10:42<1:49:15,  1.64s/it, loss=2.98, v_num=641]Epoch 5:   9%|▉         | 390/4381 [10:42<1:49:15,  1.64s/it, loss=2.98, v_num=641]Epoch 5:   9%|▉         | 400/4381 [11:00<1:49:20,  1.65s/it, loss=2.98, v_num=641]Epoch 5:   9%|▉         | 400/4381 [11:00<1:49:20,  1.65s/it, loss=2.95, v_num=641]Epoch 5:   9%|▉         | 410/4381 [11:15<1:48:41,  1.64s/it, loss=2.95, v_num=641]Epoch 5:   9%|▉         | 410/4381 [11:15<1:48:41,  1.64s/it, loss=3.02, v_num=641]Epoch 5:  10%|▉         | 420/4381 [11:30<1:48:14,  1.64s/it, loss=3.02, v_num=641]Epoch 5:  10%|▉         | 420/4381 [11:30<1:48:14,  1.64s/it, loss=3.06, v_num=641]Epoch 5:  10%|▉         | 430/4381 [11:48<1:48:14,  1.64s/it, loss=3.06, v_num=641]Epoch 5:  10%|▉         | 430/4381 [11:48<1:48:14,  1.64s/it, loss=3.01, v_num=641]Epoch 5:  10%|█         | 440/4381 [12:04<1:47:56,  1.64s/it, loss=3.01, v_num=641]Epoch 5:  10%|█         | 440/4381 [12:04<1:47:56,  1.64s/it, loss=3.01, v_num=641]Epoch 5:  10%|█         | 450/4381 [12:19<1:47:22,  1.64s/it, loss=3.01, v_num=641]Epoch 5:  10%|█         | 450/4381 [12:19<1:47:22,  1.64s/it, loss=3, v_num=641]   Epoch 5:  10%|█         | 460/4381 [12:35<1:47:05,  1.64s/it, loss=3, v_num=641]Epoch 5:  10%|█         | 460/4381 [12:35<1:47:05,  1.64s/it, loss=2.98, v_num=641]Epoch 5:  11%|█         | 470/4381 [12:51<1:46:50,  1.64s/it, loss=2.98, v_num=641]Epoch 5:  11%|█         | 470/4381 [12:51<1:46:50,  1.64s/it, loss=2.97, v_num=641]Epoch 5:  11%|█         | 480/4381 [13:04<1:46:06,  1.63s/it, loss=2.97, v_num=641]Epoch 5:  11%|█         | 480/4381 [13:04<1:46:06,  1.63s/it, loss=3.01, v_num=641]Epoch 5:  11%|█         | 490/4381 [13:24<1:46:12,  1.64s/it, loss=3.01, v_num=641]Epoch 5:  11%|█         | 490/4381 [13:24<1:46:12,  1.64s/it, loss=3.02, v_num=641]Epoch 5:  11%|█▏        | 500/4381 [13:38<1:45:38,  1.63s/it, loss=3.02, v_num=641]Epoch 5:  11%|█▏        | 500/4381 [13:38<1:45:38,  1.63s/it, loss=3.03, v_num=641]Epoch 5:  12%|█▏        | 510/4381 [13:51<1:45:00,  1.63s/it, loss=3.03, v_num=641]Epoch 5:  12%|█▏        | 510/4381 [13:51<1:45:00,  1.63s/it, loss=3.06, v_num=641]Epoch 5:  12%|█▏        | 520/4381 [14:10<1:45:01,  1.63s/it, loss=3.06, v_num=641]Epoch 5:  12%|█▏        | 520/4381 [14:10<1:45:01,  1.63s/it, loss=3.06, v_num=641]Epoch 5:  12%|█▏        | 530/4381 [14:24<1:44:31,  1.63s/it, loss=3.06, v_num=641]Epoch 5:  12%|█▏        | 530/4381 [14:24<1:44:31,  1.63s/it, loss=3.05, v_num=641]Epoch 5:  12%|█▏        | 540/4381 [14:41<1:44:16,  1.63s/it, loss=3.05, v_num=641]Epoch 5:  12%|█▏        | 540/4381 [14:41<1:44:16,  1.63s/it, loss=2.99, v_num=641]Epoch 5:  13%|█▎        | 550/4381 [14:57<1:43:58,  1.63s/it, loss=2.99, v_num=641]Epoch 5:  13%|█▎        | 550/4381 [14:57<1:43:58,  1.63s/it, loss=2.97, v_num=641]Epoch 5:  13%|█▎        | 560/4381 [15:10<1:43:23,  1.62s/it, loss=2.97, v_num=641]Epoch 5:  13%|█▎        | 560/4381 [15:10<1:43:23,  1.62s/it, loss=2.98, v_num=641]Epoch 5:  13%|█▎        | 570/4381 [15:27<1:43:10,  1.62s/it, loss=2.98, v_num=641]Epoch 5:  13%|█▎        | 570/4381 [15:27<1:43:10,  1.62s/it, loss=3.03, v_num=641]Epoch 5:  13%|█▎        | 580/4381 [15:46<1:43:12,  1.63s/it, loss=3.03, v_num=641]Epoch 5:  13%|█▎        | 580/4381 [15:46<1:43:12,  1.63s/it, loss=3.03, v_num=641]Epoch 5:  13%|█▎        | 590/4381 [16:00<1:42:39,  1.62s/it, loss=3.03, v_num=641]Epoch 5:  13%|█▎        | 590/4381 [16:00<1:42:39,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  14%|█▎        | 600/4381 [16:14<1:42:13,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  14%|█▎        | 600/4381 [16:14<1:42:13,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  14%|█▍        | 610/4381 [16:34<1:42:16,  1.63s/it, loss=3.01, v_num=641]Epoch 5:  14%|█▍        | 610/4381 [16:34<1:42:16,  1.63s/it, loss=3, v_num=641]   Epoch 5:  14%|█▍        | 620/4381 [16:48<1:41:45,  1.62s/it, loss=3, v_num=641]Epoch 5:  14%|█▍        | 620/4381 [16:48<1:41:45,  1.62s/it, loss=3, v_num=641]Epoch 5:  14%|█▍        | 630/4381 [17:04<1:41:27,  1.62s/it, loss=3, v_num=641]Epoch 5:  14%|█▍        | 630/4381 [17:04<1:41:27,  1.62s/it, loss=3.02, v_num=641]Epoch 5:  15%|█▍        | 640/4381 [17:22<1:41:21,  1.63s/it, loss=3.02, v_num=641]Epoch 5:  15%|█▍        | 640/4381 [17:22<1:41:21,  1.63s/it, loss=3.01, v_num=641]Epoch 5:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  15%|█▌        | 660/4381 [17:48<1:40:15,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  15%|█▌        | 660/4381 [17:48<1:40:15,  1.62s/it, loss=2.99, v_num=641]Epoch 5:  15%|█▌        | 670/4381 [18:03<1:39:52,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  15%|█▌        | 670/4381 [18:03<1:39:52,  1.61s/it, loss=3, v_num=641]   Epoch 5:  16%|█▌        | 680/4381 [18:19<1:39:35,  1.61s/it, loss=3, v_num=641]Epoch 5:  16%|█▌        | 680/4381 [18:19<1:39:35,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  16%|█▌        | 690/4381 [18:33<1:39:07,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  16%|█▌        | 690/4381 [18:33<1:39:07,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  16%|█▌        | 700/4381 [18:51<1:39:01,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  16%|█▌        | 700/4381 [18:51<1:39:01,  1.61s/it, loss=3, v_num=641]   Epoch 5:  16%|█▌        | 710/4381 [19:09<1:38:52,  1.62s/it, loss=3, v_num=641]Epoch 5:  16%|█▌        | 710/4381 [19:09<1:38:52,  1.62s/it, loss=3, v_num=641]Epoch 5:  16%|█▋        | 720/4381 [19:22<1:38:21,  1.61s/it, loss=3, v_num=641]Epoch 5:  16%|█▋        | 720/4381 [19:22<1:38:21,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  17%|█▋        | 730/4381 [19:37<1:37:59,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  17%|█▋        | 730/4381 [19:37<1:37:59,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  17%|█▋        | 740/4381 [19:52<1:37:38,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  17%|█▋        | 740/4381 [19:52<1:37:38,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  17%|█▋        | 750/4381 [20:07<1:37:17,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  17%|█▋        | 750/4381 [20:07<1:37:17,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  17%|█▋        | 760/4381 [20:24<1:37:05,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  17%|█▋        | 760/4381 [20:24<1:37:05,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  18%|█▊        | 770/4381 [20:39<1:36:42,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  18%|█▊        | 770/4381 [20:39<1:36:42,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  18%|█▊        | 780/4381 [20:57<1:36:35,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  18%|█▊        | 780/4381 [20:57<1:36:36,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  18%|█▊        | 790/4381 [21:14<1:36:27,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  18%|█▊        | 790/4381 [21:14<1:36:27,  1.61s/it, loss=3, v_num=641]   Epoch 5:  18%|█▊        | 800/4381 [21:29<1:36:04,  1.61s/it, loss=3, v_num=641]Epoch 5:  18%|█▊        | 800/4381 [21:29<1:36:04,  1.61s/it, loss=3, v_num=641]Epoch 5:  18%|█▊        | 810/4381 [21:44<1:35:45,  1.61s/it, loss=3, v_num=641]Epoch 5:  18%|█▊        | 810/4381 [21:44<1:35:45,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  19%|█▊        | 820/4381 [22:01<1:35:30,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  19%|█▊        | 820/4381 [22:01<1:35:31,  1.61s/it, loss=3.03, v_num=641]Epoch 5:  19%|█▉        | 830/4381 [22:18<1:35:20,  1.61s/it, loss=3.03, v_num=641]Epoch 5:  19%|█▉        | 830/4381 [22:18<1:35:20,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  19%|█▉        | 840/4381 [22:35<1:35:07,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  19%|█▉        | 840/4381 [22:35<1:35:07,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  19%|█▉        | 850/4381 [22:55<1:35:07,  1.62s/it, loss=2.97, v_num=641]Epoch 5:  19%|█▉        | 850/4381 [22:55<1:35:07,  1.62s/it, loss=3.01, v_num=641]Epoch 5:  20%|█▉        | 860/4381 [23:08<1:34:39,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  20%|█▉        | 860/4381 [23:08<1:34:39,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  20%|█▉        | 870/4381 [23:23<1:34:17,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  20%|█▉        | 870/4381 [23:23<1:34:17,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  20%|██        | 880/4381 [23:40<1:34:04,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  20%|██        | 880/4381 [23:40<1:34:04,  1.61s/it, loss=2.95, v_num=641]Epoch 5:  20%|██        | 890/4381 [23:54<1:33:39,  1.61s/it, loss=2.95, v_num=641]Epoch 5:  20%|██        | 890/4381 [23:54<1:33:39,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  21%|██        | 900/4381 [24:12<1:33:30,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  21%|██        | 900/4381 [24:12<1:33:30,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  21%|██        | 910/4381 [24:27<1:33:12,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  21%|██        | 910/4381 [24:27<1:33:12,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  21%|██        | 920/4381 [24:42<1:32:52,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  21%|██        | 920/4381 [24:42<1:32:52,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  21%|██        | 930/4381 [24:57<1:32:29,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  21%|██        | 930/4381 [24:57<1:32:29,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  21%|██▏       | 940/4381 [25:15<1:32:22,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  21%|██▏       | 940/4381 [25:15<1:32:22,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  22%|██▏       | 950/4381 [25:29<1:31:57,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  22%|██▏       | 950/4381 [25:29<1:31:57,  1.61s/it, loss=3, v_num=641]   Epoch 5:  22%|██▏       | 960/4381 [25:43<1:31:33,  1.61s/it, loss=3, v_num=641]Epoch 5:  22%|██▏       | 960/4381 [25:43<1:31:33,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  22%|██▏       | 970/4381 [25:59<1:31:18,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  22%|██▏       | 970/4381 [25:59<1:31:18,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  22%|██▏       | 980/4381 [26:14<1:30:58,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  22%|██▏       | 980/4381 [26:14<1:30:58,  1.61s/it, loss=3, v_num=641]   Epoch 5:  23%|██▎       | 990/4381 [26:34<1:30:55,  1.61s/it, loss=3, v_num=641]Epoch 5:  23%|██▎       | 990/4381 [26:34<1:30:55,  1.61s/it, loss=3, v_num=641]Epoch 5:  23%|██▎       | 1000/4381 [26:49<1:30:36,  1.61s/it, loss=3, v_num=641]Epoch 5:  23%|██▎       | 1000/4381 [26:49<1:30:36,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  23%|██▎       | 1010/4381 [27:05<1:30:18,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  23%|██▎       | 1010/4381 [27:05<1:30:18,  1.61s/it, loss=3, v_num=641]   Epoch 5:  23%|██▎       | 1020/4381 [27:23<1:30:09,  1.61s/it, loss=3, v_num=641]Epoch 5:  23%|██▎       | 1020/4381 [27:23<1:30:09,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  24%|██▎       | 1030/4381 [27:37<1:29:46,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  24%|██▎       | 1030/4381 [27:37<1:29:46,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  24%|██▎       | 1040/4381 [27:53<1:29:29,  1.61s/it, loss=2.99, v_num=641]Epoch 5:  24%|██▎       | 1040/4381 [27:53<1:29:29,  1.61s/it, loss=3, v_num=641]   Epoch 5:  24%|██▍       | 1050/4381 [28:11<1:29:20,  1.61s/it, loss=3, v_num=641]Epoch 5:  24%|██▍       | 1050/4381 [28:11<1:29:20,  1.61s/it, loss=3.04, v_num=641]Epoch 5:  24%|██▍       | 1060/4381 [28:26<1:29:01,  1.61s/it, loss=3.04, v_num=641]Epoch 5:  24%|██▍       | 1060/4381 [28:26<1:29:01,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  24%|██▍       | 1070/4381 [28:41<1:28:43,  1.61s/it, loss=3.02, v_num=641]Epoch 5:  24%|██▍       | 1070/4381 [28:41<1:28:43,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  25%|██▍       | 1080/4381 [28:56<1:28:24,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  25%|██▍       | 1080/4381 [28:56<1:28:24,  1.61s/it, loss=2.94, v_num=641]Epoch 5:  25%|██▍       | 1090/4381 [29:13<1:28:08,  1.61s/it, loss=2.94, v_num=641]Epoch 5:  25%|██▍       | 1090/4381 [29:13<1:28:08,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  25%|██▌       | 1100/4381 [29:28<1:27:50,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  25%|██▌       | 1100/4381 [29:28<1:27:50,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  25%|██▌       | 1110/4381 [29:48<1:27:46,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  25%|██▌       | 1110/4381 [29:48<1:27:46,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  26%|██▌       | 1120/4381 [30:02<1:27:22,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  26%|██▌       | 1120/4381 [30:02<1:27:22,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  26%|██▌       | 1130/4381 [30:18<1:27:07,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  26%|██▌       | 1130/4381 [30:18<1:27:07,  1.61s/it, loss=2.94, v_num=641]Epoch 5:  26%|██▌       | 1140/4381 [30:36<1:26:57,  1.61s/it, loss=2.94, v_num=641]Epoch 5:  26%|██▌       | 1140/4381 [30:36<1:26:57,  1.61s/it, loss=2.93, v_num=641]Epoch 5:  26%|██▌       | 1150/4381 [30:52<1:26:40,  1.61s/it, loss=2.93, v_num=641]Epoch 5:  26%|██▌       | 1150/4381 [30:52<1:26:40,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  26%|██▋       | 1160/4381 [31:06<1:26:17,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  26%|██▋       | 1160/4381 [31:06<1:26:17,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  27%|██▋       | 1170/4381 [31:25<1:26:09,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  27%|██▋       | 1170/4381 [31:25<1:26:09,  1.61s/it, loss=3, v_num=641]   Epoch 5:  27%|██▋       | 1180/4381 [31:39<1:25:49,  1.61s/it, loss=3, v_num=641]Epoch 5:  27%|██▋       | 1180/4381 [31:39<1:25:49,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  27%|██▋       | 1190/4381 [31:52<1:25:22,  1.61s/it, loss=3.01, v_num=641]Epoch 5:  27%|██▋       | 1190/4381 [31:52<1:25:22,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  27%|██▋       | 1200/4381 [32:10<1:25:12,  1.61s/it, loss=2.96, v_num=641]Epoch 5:  27%|██▋       | 1200/4381 [32:10<1:25:12,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  28%|██▊       | 1210/4381 [32:23<1:24:48,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  28%|██▊       | 1210/4381 [32:23<1:24:48,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  28%|██▊       | 1220/4381 [32:36<1:24:24,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  28%|██▊       | 1220/4381 [32:36<1:24:24,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  28%|██▊       | 1230/4381 [32:55<1:24:17,  1.61s/it, loss=2.98, v_num=641]Epoch 5:  28%|██▊       | 1230/4381 [32:55<1:24:17,  1.61s/it, loss=2.97, v_num=641]Epoch 5:  28%|██▊       | 1240/4381 [33:09<1:23:54,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  28%|██▊       | 1240/4381 [33:09<1:23:54,  1.60s/it, loss=3, v_num=641]   Epoch 5:  29%|██▊       | 1250/4381 [33:26<1:23:40,  1.60s/it, loss=3, v_num=641]Epoch 5:  29%|██▊       | 1250/4381 [33:26<1:23:40,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  29%|██▉       | 1260/4381 [33:45<1:23:33,  1.61s/it, loss=2.95, v_num=641]Epoch 5:  29%|██▉       | 1260/4381 [33:45<1:23:33,  1.61s/it, loss=2.93, v_num=641]Epoch 5:  29%|██▉       | 1270/4381 [33:59<1:23:12,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  29%|██▉       | 1270/4381 [33:59<1:23:12,  1.60s/it, loss=3, v_num=641]   Epoch 5:  29%|██▉       | 1280/4381 [34:13<1:22:50,  1.60s/it, loss=3, v_num=641]Epoch 5:  29%|██▉       | 1280/4381 [34:13<1:22:50,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  29%|██▉       | 1290/4381 [34:30<1:22:37,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  29%|██▉       | 1290/4381 [34:30<1:22:37,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  30%|██▉       | 1300/4381 [34:47<1:22:22,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  30%|██▉       | 1300/4381 [34:47<1:22:22,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  30%|██▉       | 1310/4381 [35:04<1:22:08,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  30%|██▉       | 1310/4381 [35:04<1:22:08,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  30%|███       | 1320/4381 [35:21<1:21:55,  1.61s/it, loss=3.03, v_num=641]Epoch 5:  30%|███       | 1320/4381 [35:21<1:21:55,  1.61s/it, loss=3, v_num=641]   Epoch 5:  30%|███       | 1330/4381 [35:36<1:21:37,  1.61s/it, loss=3, v_num=641]Epoch 5:  30%|███       | 1330/4381 [35:36<1:21:37,  1.61s/it, loss=3, v_num=641]Epoch 5:  31%|███       | 1340/4381 [35:50<1:21:17,  1.60s/it, loss=3, v_num=641]Epoch 5:  31%|███       | 1340/4381 [35:50<1:21:17,  1.60s/it, loss=3, v_num=641]Epoch 5:  31%|███       | 1350/4381 [36:06<1:20:59,  1.60s/it, loss=3, v_num=641]Epoch 5:  31%|███       | 1350/4381 [36:06<1:20:59,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  31%|███       | 1360/4381 [36:20<1:20:39,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  31%|███       | 1360/4381 [36:20<1:20:39,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  31%|███▏      | 1370/4381 [36:34<1:20:19,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  31%|███▏      | 1370/4381 [36:34<1:20:19,  1.60s/it, loss=3, v_num=641]   Epoch 5:  31%|███▏      | 1380/4381 [36:50<1:20:03,  1.60s/it, loss=3, v_num=641]Epoch 5:  31%|███▏      | 1380/4381 [36:50<1:20:03,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  32%|███▏      | 1390/4381 [37:03<1:19:40,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  32%|███▏      | 1390/4381 [37:03<1:19:40,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  32%|███▏      | 1400/4381 [37:18<1:19:23,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  32%|███▏      | 1400/4381 [37:18<1:19:23,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  32%|███▏      | 1410/4381 [37:39<1:19:16,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  32%|███▏      | 1410/4381 [37:39<1:19:16,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  32%|███▏      | 1420/4381 [37:52<1:18:56,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  32%|███▏      | 1420/4381 [37:52<1:18:56,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  33%|███▎      | 1430/4381 [38:08<1:18:39,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  33%|███▎      | 1430/4381 [38:08<1:18:39,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  33%|███▎      | 1440/4381 [38:24<1:18:23,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  33%|███▎      | 1440/4381 [38:24<1:18:23,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  33%|███▎      | 1450/4381 [38:42<1:18:11,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  33%|███▎      | 1450/4381 [38:42<1:18:11,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  33%|███▎      | 1460/4381 [38:57<1:17:53,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  33%|███▎      | 1460/4381 [38:57<1:17:53,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  34%|███▎      | 1470/4381 [39:16<1:17:42,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  34%|███▎      | 1470/4381 [39:16<1:17:42,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  34%|███▍      | 1480/4381 [39:34<1:17:30,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  34%|███▍      | 1480/4381 [39:34<1:17:30,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  34%|███▍      | 1490/4381 [39:49<1:17:12,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  34%|███▍      | 1490/4381 [39:49<1:17:12,  1.60s/it, loss=3, v_num=641]   Epoch 5:  34%|███▍      | 1500/4381 [40:04<1:16:55,  1.60s/it, loss=3, v_num=641]Epoch 5:  34%|███▍      | 1500/4381 [40:04<1:16:55,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  34%|███▍      | 1510/4381 [40:20<1:16:39,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  34%|███▍      | 1510/4381 [40:20<1:16:39,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  35%|███▍      | 1520/4381 [40:35<1:16:21,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  35%|███▍      | 1520/4381 [40:35<1:16:21,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  35%|███▍      | 1530/4381 [40:56<1:16:13,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  35%|███▍      | 1530/4381 [40:56<1:16:13,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  35%|███▌      | 1540/4381 [41:08<1:15:51,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  35%|███▌      | 1540/4381 [41:08<1:15:51,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  35%|███▌      | 1550/4381 [41:24<1:15:34,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  35%|███▌      | 1550/4381 [41:24<1:15:34,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  36%|███▌      | 1560/4381 [41:38<1:15:15,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  36%|███▌      | 1560/4381 [41:38<1:15:15,  1.60s/it, loss=3, v_num=641]   Epoch 5:  36%|███▌      | 1570/4381 [41:57<1:15:04,  1.60s/it, loss=3, v_num=641]Epoch 5:  36%|███▌      | 1570/4381 [41:57<1:15:04,  1.60s/it, loss=3, v_num=641]Epoch 5:  36%|███▌      | 1580/4381 [42:13<1:14:48,  1.60s/it, loss=3, v_num=641]Epoch 5:  36%|███▌      | 1580/4381 [42:13<1:14:48,  1.60s/it, loss=3, v_num=641]Epoch 5:  36%|███▋      | 1590/4381 [42:27<1:14:29,  1.60s/it, loss=3, v_num=641]Epoch 5:  36%|███▋      | 1590/4381 [42:27<1:14:29,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  37%|███▋      | 1600/4381 [42:45<1:14:16,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  37%|███▋      | 1600/4381 [42:45<1:14:16,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  37%|███▋      | 1610/4381 [43:00<1:13:58,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  37%|███▋      | 1610/4381 [43:00<1:13:58,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  37%|███▋      | 1620/4381 [43:16<1:13:42,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  37%|███▋      | 1620/4381 [43:16<1:13:42,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  37%|███▋      | 1630/4381 [43:33<1:13:28,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  37%|███▋      | 1630/4381 [43:33<1:13:28,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  37%|███▋      | 1640/4381 [43:48<1:13:10,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  37%|███▋      | 1640/4381 [43:48<1:13:10,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  38%|███▊      | 1650/4381 [44:02<1:12:50,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  38%|███▊      | 1650/4381 [44:02<1:12:50,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  38%|███▊      | 1660/4381 [44:21<1:12:40,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  38%|███▊      | 1660/4381 [44:21<1:12:40,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  38%|███▊      | 1670/4381 [44:34<1:12:19,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  38%|███▊      | 1670/4381 [44:34<1:12:19,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  38%|███▊      | 1680/4381 [44:51<1:12:05,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  38%|███▊      | 1680/4381 [44:51<1:12:05,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  39%|███▊      | 1690/4381 [45:08<1:11:50,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  39%|███▊      | 1690/4381 [45:08<1:11:50,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  39%|███▉      | 1700/4381 [45:22<1:11:31,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  39%|███▉      | 1700/4381 [45:22<1:11:31,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  39%|███▉      | 1710/4381 [45:38<1:11:15,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  39%|███▉      | 1710/4381 [45:38<1:11:15,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  39%|███▉      | 1720/4381 [45:57<1:11:03,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  39%|███▉      | 1720/4381 [45:57<1:11:03,  1.60s/it, loss=3.04, v_num=641]Epoch 5:  39%|███▉      | 1730/4381 [46:12<1:10:46,  1.60s/it, loss=3.04, v_num=641]Epoch 5:  39%|███▉      | 1730/4381 [46:12<1:10:46,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  40%|███▉      | 1740/4381 [46:27<1:10:28,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  40%|███▉      | 1740/4381 [46:27<1:10:28,  1.60s/it, loss=3, v_num=641]   Epoch 5:  40%|███▉      | 1750/4381 [46:42<1:10:10,  1.60s/it, loss=3, v_num=641]Epoch 5:  40%|███▉      | 1750/4381 [46:42<1:10:10,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  40%|████      | 1760/4381 [46:56<1:09:52,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  40%|████      | 1760/4381 [46:56<1:09:52,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  40%|████      | 1770/4381 [47:17<1:09:42,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  40%|████      | 1770/4381 [47:17<1:09:42,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  41%|████      | 1780/4381 [47:32<1:09:25,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  41%|████      | 1780/4381 [47:32<1:09:25,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  41%|████      | 1790/4381 [47:45<1:09:05,  1.60s/it, loss=3.02, v_num=641]Epoch 5:  41%|████      | 1790/4381 [47:45<1:09:05,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  41%|████      | 1800/4381 [48:02<1:08:50,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  41%|████      | 1800/4381 [48:02<1:08:50,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  41%|████▏     | 1810/4381 [48:17<1:08:33,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  41%|████▏     | 1810/4381 [48:17<1:08:33,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  42%|████▏     | 1820/4381 [48:32<1:08:15,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  42%|████▏     | 1820/4381 [48:32<1:08:15,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  42%|████▏     | 1830/4381 [48:48<1:08:00,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  42%|████▏     | 1830/4381 [48:48<1:08:00,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  42%|████▏     | 1840/4381 [49:06<1:07:46,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  42%|████▏     | 1840/4381 [49:06<1:07:46,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  42%|████▏     | 1850/4381 [49:22<1:07:30,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  42%|████▏     | 1850/4381 [49:22<1:07:30,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  42%|████▏     | 1860/4381 [49:39<1:07:15,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  42%|████▏     | 1860/4381 [49:39<1:07:15,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  43%|████▎     | 1870/4381 [49:53<1:06:56,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  43%|████▎     | 1870/4381 [49:53<1:06:56,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  43%|████▎     | 1880/4381 [50:08<1:06:40,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  43%|████▎     | 1880/4381 [50:08<1:06:40,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  43%|████▎     | 1890/4381 [50:27<1:06:27,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  43%|████▎     | 1890/4381 [50:27<1:06:27,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  43%|████▎     | 1900/4381 [50:42<1:06:11,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  43%|████▎     | 1900/4381 [50:42<1:06:11,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  44%|████▎     | 1910/4381 [50:56<1:05:51,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  44%|████▎     | 1910/4381 [50:56<1:05:51,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  44%|████▍     | 1920/4381 [51:14<1:05:38,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  44%|████▍     | 1920/4381 [51:14<1:05:38,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  44%|████▍     | 1930/4381 [51:28<1:05:20,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  44%|████▍     | 1930/4381 [51:28<1:05:20,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  44%|████▍     | 1940/4381 [51:45<1:05:05,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  44%|████▍     | 1940/4381 [51:45<1:05:05,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  45%|████▍     | 1950/4381 [52:03<1:04:51,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  45%|████▍     | 1950/4381 [52:03<1:04:51,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  45%|████▍     | 1960/4381 [52:17<1:04:33,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  45%|████▍     | 1960/4381 [52:17<1:04:33,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  45%|████▍     | 1970/4381 [52:32<1:04:16,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  45%|████▍     | 1970/4381 [52:32<1:04:16,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  45%|████▌     | 1980/4381 [52:51<1:04:04,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  45%|████▌     | 1980/4381 [52:51<1:04:04,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  45%|████▌     | 1990/4381 [53:04<1:03:44,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  45%|████▌     | 1990/4381 [53:04<1:03:44,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  46%|████▌     | 2000/4381 [53:21<1:03:28,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  46%|████▌     | 2000/4381 [53:21<1:03:28,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  46%|████▌     | 2010/4381 [53:38<1:03:15,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  46%|████▌     | 2010/4381 [53:38<1:03:15,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  46%|████▌     | 2020/4381 [53:52<1:02:56,  1.60s/it, loss=2.92, v_num=641]Epoch 5:  46%|████▌     | 2020/4381 [53:52<1:02:56,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  47%|████▋     | 2040/4381 [54:26<1:02:27,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  47%|████▋     | 2040/4381 [54:26<1:02:27,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  47%|████▋     | 2050/4381 [54:41<1:02:09,  1.60s/it, loss=2.99, v_num=641]Epoch 5:  47%|████▋     | 2050/4381 [54:41<1:02:09,  1.60s/it, loss=3, v_num=641]   Epoch 5:  47%|████▋     | 2060/4381 [54:56<1:01:52,  1.60s/it, loss=3, v_num=641]Epoch 5:  47%|████▋     | 2060/4381 [54:56<1:01:52,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  47%|████▋     | 2070/4381 [55:10<1:01:34,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  47%|████▋     | 2070/4381 [55:10<1:01:34,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  47%|████▋     | 2080/4381 [55:25<1:01:17,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  47%|████▋     | 2080/4381 [55:25<1:01:17,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  48%|████▊     | 2090/4381 [55:39<1:00:59,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  48%|████▊     | 2090/4381 [55:39<1:00:59,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  48%|████▊     | 2100/4381 [55:55<1:00:43,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  48%|████▊     | 2100/4381 [55:55<1:00:43,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  48%|████▊     | 2110/4381 [56:10<1:00:26,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  48%|████▊     | 2110/4381 [56:10<1:00:26,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  48%|████▊     | 2120/4381 [56:25<1:00:09,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  48%|████▊     | 2120/4381 [56:25<1:00:09,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  49%|████▊     | 2130/4381 [56:42<59:54,  1.60s/it, loss=2.94, v_num=641]  Epoch 5:  49%|████▊     | 2130/4381 [56:42<59:54,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  49%|████▉     | 2140/4381 [56:56<59:36,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  49%|████▉     | 2140/4381 [56:56<59:36,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  49%|████▉     | 2150/4381 [57:13<59:21,  1.60s/it, loss=3.01, v_num=641]Epoch 5:  49%|████▉     | 2150/4381 [57:13<59:21,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  50%|████▉     | 2170/4381 [57:43<58:47,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  50%|████▉     | 2170/4381 [57:43<58:47,  1.60s/it, loss=3, v_num=641]   Epoch 5:  50%|████▉     | 2180/4381 [57:56<58:28,  1.59s/it, loss=3, v_num=641]Epoch 5:  50%|████▉     | 2180/4381 [57:56<58:28,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  50%|████▉     | 2190/4381 [58:16<58:16,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  50%|████▉     | 2190/4381 [58:16<58:16,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  50%|█████     | 2200/4381 [58:31<57:59,  1.60s/it, loss=2.97, v_num=641]Epoch 5:  50%|█████     | 2200/4381 [58:31<57:59,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  50%|█████     | 2210/4381 [58:47<57:43,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  50%|█████     | 2210/4381 [58:47<57:43,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  51%|█████     | 2220/4381 [59:09<57:33,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  51%|█████     | 2220/4381 [59:09<57:33,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  51%|█████     | 2230/4381 [59:22<57:14,  1.60s/it, loss=2.93, v_num=641]Epoch 5:  51%|█████     | 2230/4381 [59:22<57:14,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  51%|█████     | 2240/4381 [59:35<56:55,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  51%|█████     | 2240/4381 [59:35<56:55,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  51%|█████▏    | 2250/4381 [59:53<56:42,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  51%|█████▏    | 2250/4381 [59:53<56:42,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  52%|█████▏    | 2260/4381 [1:00:11<56:27,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  52%|█████▏    | 2260/4381 [1:00:11<56:27,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  52%|█████▏    | 2270/4381 [1:00:26<56:11,  1.60s/it, loss=2.96, v_num=641]Epoch 5:  52%|█████▏    | 2270/4381 [1:00:26<56:11,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  52%|█████▏    | 2280/4381 [1:00:42<55:55,  1.60s/it, loss=2.95, v_num=641]Epoch 5:  52%|█████▏    | 2280/4381 [1:00:42<55:55,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  52%|█████▏    | 2290/4381 [1:00:58<55:39,  1.60s/it, loss=2.94, v_num=641]Epoch 5:  52%|█████▏    | 2290/4381 [1:00:58<55:39,  1.60s/it, loss=3, v_num=641]   Epoch 5:  52%|█████▏    | 2300/4381 [1:01:11<55:20,  1.60s/it, loss=3, v_num=641]Epoch 5:  52%|█████▏    | 2300/4381 [1:01:11<55:20,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  53%|█████▎    | 2310/4381 [1:01:27<55:04,  1.60s/it, loss=3.03, v_num=641]Epoch 5:  53%|█████▎    | 2310/4381 [1:01:27<55:04,  1.60s/it, loss=2.98, v_num=641]Epoch 5:  53%|█████▎    | 2320/4381 [1:01:40<54:46,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  53%|█████▎    | 2320/4381 [1:01:40<54:46,  1.59s/it, loss=3, v_num=641]   Epoch 5:  53%|█████▎    | 2330/4381 [1:01:55<54:29,  1.59s/it, loss=3, v_num=641]Epoch 5:  53%|█████▎    | 2330/4381 [1:01:55<54:29,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  53%|█████▎    | 2340/4381 [1:02:10<54:12,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  53%|█████▎    | 2340/4381 [1:02:10<54:12,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  54%|█████▎    | 2350/4381 [1:02:24<53:55,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  54%|█████▎    | 2350/4381 [1:02:24<53:55,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  54%|█████▍    | 2360/4381 [1:02:38<53:37,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  54%|█████▍    | 2360/4381 [1:02:38<53:37,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  54%|█████▍    | 2370/4381 [1:02:55<53:22,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  54%|█████▍    | 2370/4381 [1:02:55<53:22,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  54%|█████▍    | 2380/4381 [1:03:10<53:05,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  54%|█████▍    | 2380/4381 [1:03:10<53:05,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  55%|█████▍    | 2390/4381 [1:03:28<52:51,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  55%|█████▍    | 2390/4381 [1:03:28<52:51,  1.59s/it, loss=3, v_num=641]   Epoch 5:  55%|█████▍    | 2400/4381 [1:03:42<52:33,  1.59s/it, loss=3, v_num=641]Epoch 5:  55%|█████▍    | 2400/4381 [1:03:42<52:33,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2410/4381 [1:03:59<52:19,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2410/4381 [1:03:59<52:19,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2420/4381 [1:04:16<52:03,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2420/4381 [1:04:16<52:03,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2430/4381 [1:04:33<51:48,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  55%|█████▌    | 2430/4381 [1:04:33<51:48,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  56%|█████▌    | 2440/4381 [1:04:47<51:31,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  56%|█████▌    | 2440/4381 [1:04:47<51:31,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  56%|█████▌    | 2450/4381 [1:05:02<51:14,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  56%|█████▌    | 2450/4381 [1:05:02<51:14,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  56%|█████▌    | 2460/4381 [1:05:20<50:59,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  56%|█████▌    | 2460/4381 [1:05:20<50:59,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  56%|█████▋    | 2470/4381 [1:05:34<50:42,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  56%|█████▋    | 2470/4381 [1:05:34<50:42,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  57%|█████▋    | 2480/4381 [1:05:51<50:27,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  57%|█████▋    | 2480/4381 [1:05:51<50:27,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  57%|█████▋    | 2490/4381 [1:06:06<50:11,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  57%|█████▋    | 2490/4381 [1:06:06<50:11,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  57%|█████▋    | 2500/4381 [1:06:19<49:53,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  57%|█████▋    | 2500/4381 [1:06:19<49:53,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  57%|█████▋    | 2510/4381 [1:06:35<49:36,  1.59s/it, loss=3.01, v_num=641]Epoch 5:  57%|█████▋    | 2510/4381 [1:06:35<49:36,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  58%|█████▊    | 2520/4381 [1:06:54<49:23,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  58%|█████▊    | 2520/4381 [1:06:54<49:23,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  58%|█████▊    | 2530/4381 [1:07:08<49:06,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  58%|█████▊    | 2530/4381 [1:07:08<49:06,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  58%|█████▊    | 2540/4381 [1:07:22<48:48,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  58%|█████▊    | 2540/4381 [1:07:22<48:48,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  58%|█████▊    | 2550/4381 [1:07:41<48:35,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  58%|█████▊    | 2550/4381 [1:07:41<48:35,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  58%|█████▊    | 2560/4381 [1:07:54<48:17,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  58%|█████▊    | 2560/4381 [1:07:54<48:17,  1.59s/it, loss=2.99, v_num=641]Epoch 5:  59%|█████▊    | 2570/4381 [1:08:07<47:59,  1.59s/it, loss=2.99, v_num=641]Epoch 5:  59%|█████▊    | 2570/4381 [1:08:07<47:59,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  59%|█████▉    | 2580/4381 [1:08:27<47:46,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  59%|█████▉    | 2580/4381 [1:08:27<47:46,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  59%|█████▉    | 2590/4381 [1:08:42<47:29,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  59%|█████▉    | 2590/4381 [1:08:42<47:29,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  59%|█████▉    | 2600/4381 [1:08:54<47:11,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  59%|█████▉    | 2600/4381 [1:08:54<47:11,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  60%|█████▉    | 2610/4381 [1:09:14<46:57,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  60%|█████▉    | 2610/4381 [1:09:14<46:57,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  60%|█████▉    | 2620/4381 [1:09:28<46:40,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  60%|█████▉    | 2620/4381 [1:09:28<46:40,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  60%|██████    | 2630/4381 [1:09:44<46:25,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  60%|██████    | 2630/4381 [1:09:44<46:25,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  60%|██████    | 2640/4381 [1:09:59<46:08,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  60%|██████    | 2640/4381 [1:09:59<46:08,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  60%|██████    | 2650/4381 [1:10:15<45:52,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  60%|██████    | 2650/4381 [1:10:15<45:52,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  61%|██████    | 2660/4381 [1:10:30<45:36,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  61%|██████    | 2660/4381 [1:10:30<45:36,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  61%|██████    | 2670/4381 [1:10:48<45:21,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  61%|██████    | 2670/4381 [1:10:48<45:21,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  61%|██████    | 2680/4381 [1:11:02<45:04,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  61%|██████    | 2680/4381 [1:11:02<45:04,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  61%|██████▏   | 2690/4381 [1:11:18<44:48,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  61%|██████▏   | 2690/4381 [1:11:18<44:48,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  62%|██████▏   | 2700/4381 [1:11:35<44:33,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  62%|██████▏   | 2700/4381 [1:11:35<44:33,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  62%|██████▏   | 2710/4381 [1:11:50<44:16,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  62%|██████▏   | 2710/4381 [1:11:50<44:16,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  62%|██████▏   | 2720/4381 [1:12:05<44:00,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  62%|██████▏   | 2720/4381 [1:12:05<44:00,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  62%|██████▏   | 2730/4381 [1:12:22<43:45,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  62%|██████▏   | 2730/4381 [1:12:22<43:45,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  63%|██████▎   | 2740/4381 [1:12:35<43:27,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  63%|██████▎   | 2740/4381 [1:12:35<43:27,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2750/4381 [1:12:51<43:11,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2750/4381 [1:12:51<43:11,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2760/4381 [1:13:09<42:56,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2760/4381 [1:13:09<42:56,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  63%|██████▎   | 2780/4381 [1:13:38<42:23,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  63%|██████▎   | 2780/4381 [1:13:38<42:23,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  64%|██████▎   | 2790/4381 [1:13:56<42:08,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  64%|██████▎   | 2790/4381 [1:13:56<42:08,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  64%|██████▍   | 2800/4381 [1:14:11<41:52,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  64%|██████▍   | 2800/4381 [1:14:11<41:52,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  64%|██████▍   | 2810/4381 [1:14:26<41:36,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  64%|██████▍   | 2810/4381 [1:14:26<41:36,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  64%|██████▍   | 2820/4381 [1:14:45<41:21,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  64%|██████▍   | 2820/4381 [1:14:45<41:21,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  65%|██████▍   | 2830/4381 [1:14:58<41:04,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  65%|██████▍   | 2830/4381 [1:14:58<41:04,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  65%|██████▍   | 2840/4381 [1:15:13<40:48,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  65%|██████▍   | 2840/4381 [1:15:13<40:48,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  65%|██████▌   | 2850/4381 [1:15:32<40:33,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  65%|██████▌   | 2850/4381 [1:15:32<40:33,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  65%|██████▌   | 2860/4381 [1:15:47<40:17,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  65%|██████▌   | 2860/4381 [1:15:47<40:17,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  66%|██████▌   | 2870/4381 [1:16:03<40:01,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  66%|██████▌   | 2870/4381 [1:16:03<40:01,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  66%|██████▌   | 2880/4381 [1:16:21<39:47,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  66%|██████▌   | 2880/4381 [1:16:21<39:47,  1.59s/it, loss=2.99, v_num=641]Epoch 5:  66%|██████▌   | 2890/4381 [1:16:36<39:30,  1.59s/it, loss=2.99, v_num=641]Epoch 5:  66%|██████▌   | 2890/4381 [1:16:36<39:30,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  66%|██████▌   | 2900/4381 [1:16:50<39:13,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  66%|██████▌   | 2900/4381 [1:16:50<39:13,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  66%|██████▋   | 2910/4381 [1:17:08<38:59,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  66%|██████▋   | 2910/4381 [1:17:08<38:59,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  67%|██████▋   | 2920/4381 [1:17:21<38:41,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  67%|██████▋   | 2920/4381 [1:17:21<38:41,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  67%|██████▋   | 2930/4381 [1:17:35<38:24,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  67%|██████▋   | 2930/4381 [1:17:35<38:24,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  67%|██████▋   | 2940/4381 [1:17:54<38:10,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  67%|██████▋   | 2940/4381 [1:17:54<38:10,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  67%|██████▋   | 2950/4381 [1:18:09<37:54,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  67%|██████▋   | 2950/4381 [1:18:09<37:54,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  68%|██████▊   | 2960/4381 [1:18:26<37:38,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  68%|██████▊   | 2960/4381 [1:18:26<37:38,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  68%|██████▊   | 2970/4381 [1:18:44<37:23,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  68%|██████▊   | 2970/4381 [1:18:44<37:23,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  68%|██████▊   | 2980/4381 [1:18:55<37:05,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  68%|██████▊   | 2980/4381 [1:18:55<37:05,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  68%|██████▊   | 2990/4381 [1:19:10<36:49,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  68%|██████▊   | 2990/4381 [1:19:10<36:49,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  68%|██████▊   | 3000/4381 [1:19:27<36:33,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  68%|██████▊   | 3000/4381 [1:19:27<36:33,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  69%|██████▊   | 3010/4381 [1:19:42<36:17,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  69%|██████▊   | 3010/4381 [1:19:42<36:17,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  69%|██████▉   | 3020/4381 [1:19:56<36:00,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  69%|██████▉   | 3020/4381 [1:19:56<36:00,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  69%|██████▉   | 3030/4381 [1:20:14<35:45,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  69%|██████▉   | 3030/4381 [1:20:14<35:45,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  69%|██████▉   | 3040/4381 [1:20:29<35:29,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  69%|██████▉   | 3040/4381 [1:20:29<35:29,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  70%|██████▉   | 3050/4381 [1:20:42<35:12,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  70%|██████▉   | 3050/4381 [1:20:42<35:12,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  70%|██████▉   | 3060/4381 [1:20:58<34:56,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  70%|██████▉   | 3060/4381 [1:20:58<34:56,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  70%|███████   | 3070/4381 [1:21:16<34:41,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  70%|███████   | 3070/4381 [1:21:16<34:41,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  70%|███████   | 3080/4381 [1:21:30<34:24,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  70%|███████   | 3080/4381 [1:21:30<34:24,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  71%|███████   | 3090/4381 [1:21:47<34:09,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  71%|███████   | 3090/4381 [1:21:47<34:09,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  71%|███████   | 3100/4381 [1:22:00<33:52,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  71%|███████   | 3100/4381 [1:22:00<33:52,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  71%|███████   | 3110/4381 [1:22:15<33:36,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  71%|███████   | 3110/4381 [1:22:15<33:36,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  71%|███████   | 3120/4381 [1:22:32<33:20,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  71%|███████   | 3120/4381 [1:22:32<33:20,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  71%|███████▏  | 3130/4381 [1:22:50<33:05,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  71%|███████▏  | 3130/4381 [1:22:50<33:05,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  72%|███████▏  | 3140/4381 [1:23:05<32:49,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  72%|███████▏  | 3140/4381 [1:23:05<32:49,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3150/4381 [1:23:22<32:34,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3150/4381 [1:23:22<32:34,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3160/4381 [1:23:35<32:17,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3160/4381 [1:23:35<32:17,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3170/4381 [1:23:52<32:01,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  72%|███████▏  | 3170/4381 [1:23:52<32:01,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  73%|███████▎  | 3180/4381 [1:24:11<31:47,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  73%|███████▎  | 3180/4381 [1:24:11<31:47,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  73%|███████▎  | 3190/4381 [1:24:27<31:31,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  73%|███████▎  | 3190/4381 [1:24:27<31:31,  1.59s/it, loss=3, v_num=641]   Epoch 5:  73%|███████▎  | 3200/4381 [1:24:41<31:14,  1.59s/it, loss=3, v_num=641]Epoch 5:  73%|███████▎  | 3200/4381 [1:24:41<31:14,  1.59s/it, loss=3, v_num=641]Epoch 5:  73%|███████▎  | 3210/4381 [1:24:55<30:58,  1.59s/it, loss=3, v_num=641]Epoch 5:  73%|███████▎  | 3210/4381 [1:24:55<30:58,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  73%|███████▎  | 3220/4381 [1:25:14<30:43,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  73%|███████▎  | 3220/4381 [1:25:14<30:43,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  74%|███████▎  | 3230/4381 [1:25:29<30:27,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  74%|███████▎  | 3230/4381 [1:25:29<30:27,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  74%|███████▍  | 3240/4381 [1:25:43<30:10,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  74%|███████▍  | 3240/4381 [1:25:43<30:10,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  74%|███████▍  | 3250/4381 [1:26:02<29:55,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  74%|███████▍  | 3250/4381 [1:26:02<29:55,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  74%|███████▍  | 3260/4381 [1:26:16<29:39,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  74%|███████▍  | 3260/4381 [1:26:16<29:39,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  75%|███████▍  | 3270/4381 [1:26:31<29:23,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  75%|███████▍  | 3270/4381 [1:26:31<29:23,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▍  | 3280/4381 [1:26:50<29:08,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▍  | 3280/4381 [1:26:50<29:08,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▌  | 3290/4381 [1:27:03<28:51,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▌  | 3290/4381 [1:27:03<28:51,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▌  | 3300/4381 [1:27:19<28:35,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  75%|███████▌  | 3300/4381 [1:27:19<28:35,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  76%|███████▌  | 3310/4381 [1:27:37<28:20,  1.59s/it, loss=2.97, v_num=641]Epoch 5:  76%|███████▌  | 3310/4381 [1:27:37<28:20,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  76%|███████▌  | 3320/4381 [1:27:52<28:04,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  76%|███████▌  | 3320/4381 [1:27:52<28:04,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  76%|███████▌  | 3330/4381 [1:28:10<27:49,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  76%|███████▌  | 3330/4381 [1:28:10<27:49,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  76%|███████▌  | 3340/4381 [1:28:26<27:33,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  76%|███████▌  | 3340/4381 [1:28:26<27:33,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  76%|███████▋  | 3350/4381 [1:28:39<27:16,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  76%|███████▋  | 3350/4381 [1:28:39<27:16,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  77%|███████▋  | 3360/4381 [1:28:58<27:01,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  77%|███████▋  | 3360/4381 [1:28:58<27:01,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  77%|███████▋  | 3370/4381 [1:29:15<26:46,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  77%|███████▋  | 3370/4381 [1:29:15<26:46,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  77%|███████▋  | 3380/4381 [1:29:29<26:29,  1.59s/it, loss=2.94, v_num=641]Epoch 5:  77%|███████▋  | 3380/4381 [1:29:29<26:29,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  77%|███████▋  | 3390/4381 [1:29:44<26:13,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  77%|███████▋  | 3390/4381 [1:29:44<26:13,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  78%|███████▊  | 3400/4381 [1:29:59<25:57,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  78%|███████▊  | 3400/4381 [1:29:59<25:57,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  78%|███████▊  | 3410/4381 [1:30:17<25:42,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  78%|███████▊  | 3410/4381 [1:30:17<25:42,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  78%|███████▊  | 3420/4381 [1:30:32<25:26,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  78%|███████▊  | 3420/4381 [1:30:32<25:26,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  78%|███████▊  | 3430/4381 [1:30:50<25:10,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  78%|███████▊  | 3430/4381 [1:30:50<25:10,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  79%|███████▊  | 3440/4381 [1:31:07<24:55,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  79%|███████▊  | 3440/4381 [1:31:07<24:55,  1.59s/it, loss=2.88, v_num=641]Epoch 5:  79%|███████▊  | 3450/4381 [1:31:22<24:39,  1.59s/it, loss=2.88, v_num=641]Epoch 5:  79%|███████▊  | 3450/4381 [1:31:22<24:39,  1.59s/it, loss=2.88, v_num=641]Epoch 5:  79%|███████▉  | 3460/4381 [1:31:40<24:23,  1.59s/it, loss=2.88, v_num=641]Epoch 5:  79%|███████▉  | 3460/4381 [1:31:40<24:23,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  79%|███████▉  | 3470/4381 [1:31:55<24:07,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  79%|███████▉  | 3470/4381 [1:31:55<24:07,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  79%|███████▉  | 3480/4381 [1:32:12<23:51,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  79%|███████▉  | 3480/4381 [1:32:12<23:51,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  80%|███████▉  | 3490/4381 [1:32:27<23:35,  1.59s/it, loss=2.89, v_num=641]Epoch 5:  80%|███████▉  | 3490/4381 [1:32:27<23:35,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  80%|███████▉  | 3500/4381 [1:32:44<23:20,  1.59s/it, loss=2.93, v_num=641]Epoch 5:  80%|███████▉  | 3500/4381 [1:32:44<23:20,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  80%|████████  | 3510/4381 [1:33:02<23:04,  1.59s/it, loss=2.98, v_num=641]Epoch 5:  80%|████████  | 3510/4381 [1:33:02<23:04,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  80%|████████  | 3520/4381 [1:33:15<22:48,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  80%|████████  | 3520/4381 [1:33:15<22:48,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  81%|████████  | 3530/4381 [1:33:30<22:32,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  81%|████████  | 3530/4381 [1:33:30<22:32,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  81%|████████  | 3540/4381 [1:33:49<22:16,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  81%|████████  | 3540/4381 [1:33:49<22:16,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  81%|████████  | 3550/4381 [1:34:04<22:00,  1.59s/it, loss=2.96, v_num=641]Epoch 5:  81%|████████  | 3550/4381 [1:34:04<22:00,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  81%|████████▏ | 3560/4381 [1:34:19<21:44,  1.59s/it, loss=2.95, v_num=641]Epoch 5:  81%|████████▏ | 3560/4381 [1:34:19<21:44,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  82%|████████▏ | 3580/4381 [1:34:50<21:12,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  82%|████████▏ | 3580/4381 [1:34:50<21:12,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  82%|████████▏ | 3590/4381 [1:35:06<20:57,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  82%|████████▏ | 3590/4381 [1:35:06<20:57,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  82%|████████▏ | 3600/4381 [1:35:20<20:40,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  82%|████████▏ | 3600/4381 [1:35:20<20:40,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  82%|████████▏ | 3610/4381 [1:35:39<20:25,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  82%|████████▏ | 3610/4381 [1:35:39<20:25,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  83%|████████▎ | 3620/4381 [1:35:54<20:09,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3620/4381 [1:35:54<20:09,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3630/4381 [1:36:09<19:53,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3630/4381 [1:36:09<19:53,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3640/4381 [1:36:27<19:37,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3640/4381 [1:36:27<19:37,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3650/4381 [1:36:41<19:21,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  83%|████████▎ | 3650/4381 [1:36:41<19:21,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  84%|████████▎ | 3660/4381 [1:36:56<19:05,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  84%|████████▎ | 3660/4381 [1:36:56<19:05,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  84%|████████▍ | 3670/4381 [1:37:15<18:50,  1.59s/it, loss=2.87, v_num=641]Epoch 5:  84%|████████▍ | 3670/4381 [1:37:15<18:50,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  84%|████████▍ | 3680/4381 [1:37:28<18:33,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  84%|████████▍ | 3680/4381 [1:37:28<18:33,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  84%|████████▍ | 3690/4381 [1:37:41<18:17,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  84%|████████▍ | 3690/4381 [1:37:41<18:17,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  84%|████████▍ | 3700/4381 [1:37:58<18:01,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  84%|████████▍ | 3700/4381 [1:37:58<18:01,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  85%|████████▍ | 3710/4381 [1:38:12<17:45,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  85%|████████▍ | 3710/4381 [1:38:12<17:45,  1.59s/it, loss=2.9, v_num=641] Epoch 5:  85%|████████▍ | 3720/4381 [1:38:31<17:30,  1.59s/it, loss=2.9, v_num=641]Epoch 5:  85%|████████▍ | 3720/4381 [1:38:31<17:30,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  85%|████████▌ | 3730/4381 [1:38:41<17:13,  1.59s/it, loss=2.91, v_num=641]Epoch 5:  85%|████████▌ | 3730/4381 [1:38:41<17:13,  1.59s/it, loss=2.92, v_num=641]Epoch 5:  85%|████████▌ | 3740/4381 [1:38:46<16:55,  1.58s/it, loss=2.92, v_num=641]Epoch 5:  85%|████████▌ | 3740/4381 [1:38:46<16:55,  1.58s/it, loss=2.9, v_num=641] Epoch 5:  86%|████████▌ | 3750/4381 [1:38:49<16:37,  1.58s/it, loss=2.9, v_num=641]Epoch 5:  86%|████████▌ | 3750/4381 [1:38:49<16:37,  1.58s/it, loss=2.92, v_num=641]validation_epoch_end
graph acc: 0.07188498402555911
valid accuracy: 0.9269041419029236
validation_epoch_end
graph acc: 0.08466453674121406
valid accuracy: 0.9313852787017822
validation_epoch_end
graph acc: 0.08306709265175719
valid accuracy: 0.9316808581352234
validation_epoch_end
graph acc: 0.07667731629392971
valid accuracy: 0.9329686760902405
validation_epoch_end
graph acc: 0.08306709265175719
valid accuracy: 0.9324729442596436
Epoch 5:  86%|████████▌ | 3760/4381 [1:38:51<16:19,  1.58s/it, loss=2.92, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.07987220447284345
valid accuracy: 0.9278446435928345
validation_epoch_end
graph acc: 0.10862619808306709
valid accuracy: 0.9302515387535095

Validating:   2%|▏         | 10/626 [00:03<03:46,  2.73it/s][AEpoch 5:  86%|████████▌ | 3770/4381 [1:38:54<16:01,  1.57s/it, loss=2.92, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:29,  4.06it/s][AEpoch 5:  86%|████████▋ | 3780/4381 [1:38:56<15:43,  1.57s/it, loss=2.92, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:33,  3.87it/s][AEpoch 5:  87%|████████▋ | 3790/4381 [1:38:59<15:25,  1.57s/it, loss=2.92, v_num=641]
Validating:   6%|▋         | 40/626 [00:09<02:11,  4.44it/s][AEpoch 5:  87%|████████▋ | 3800/4381 [1:39:00<15:08,  1.56s/it, loss=2.92, v_num=641]
Validating:   8%|▊         | 50/626 [00:11<01:51,  5.15it/s][AEpoch 5:  87%|████████▋ | 3810/4381 [1:39:02<14:50,  1.56s/it, loss=2.92, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:47,  5.28it/s][AEpoch 5:  87%|████████▋ | 3820/4381 [1:39:03<14:32,  1.56s/it, loss=2.92, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:47,  5.16it/s][AEpoch 5:  87%|████████▋ | 3830/4381 [1:39:05<14:15,  1.55s/it, loss=2.92, v_num=641]
Validating:  13%|█▎        | 80/626 [00:16<01:44,  5.23it/s][AEpoch 5:  88%|████████▊ | 3840/4381 [1:39:07<13:57,  1.55s/it, loss=2.92, v_num=641]
Validating:  14%|█▍        | 90/626 [00:18<01:30,  5.90it/s][AEpoch 5:  88%|████████▊ | 3850/4381 [1:39:09<13:40,  1.54s/it, loss=2.92, v_num=641]
Validating:  16%|█▌        | 100/626 [00:20<01:36,  5.45it/s][AEpoch 5:  88%|████████▊ | 3860/4381 [1:39:11<13:23,  1.54s/it, loss=2.92, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:32,  5.57it/s][AEpoch 5:  88%|████████▊ | 3870/4381 [1:39:12<13:05,  1.54s/it, loss=2.92, v_num=641]
Validating:  19%|█▉        | 120/626 [00:23<01:27,  5.80it/s][AEpoch 5:  89%|████████▊ | 3880/4381 [1:39:14<12:48,  1.53s/it, loss=2.92, v_num=641]
Validating:  21%|██        | 130/626 [00:24<01:20,  6.14it/s][AEpoch 5:  89%|████████▉ | 3890/4381 [1:39:15<12:31,  1.53s/it, loss=2.92, v_num=641]
Validating:  22%|██▏       | 140/626 [00:27<01:35,  5.12it/s][AEpoch 5:  89%|████████▉ | 3900/4381 [1:39:18<12:14,  1.53s/it, loss=2.92, v_num=641]
Validating:  24%|██▍       | 150/626 [00:29<01:33,  5.09it/s][AEpoch 5:  89%|████████▉ | 3910/4381 [1:39:20<11:57,  1.52s/it, loss=2.92, v_num=641]
Validating:  26%|██▌       | 160/626 [00:30<01:22,  5.63it/s][AEpoch 5:  89%|████████▉ | 3920/4381 [1:39:21<11:40,  1.52s/it, loss=2.92, v_num=641]
Validating:  27%|██▋       | 170/626 [00:33<01:28,  5.14it/s][AEpoch 5:  90%|████████▉ | 3930/4381 [1:39:24<11:24,  1.52s/it, loss=2.92, v_num=641]
Validating:  29%|██▉       | 180/626 [00:34<01:19,  5.60it/s][AEpoch 5:  90%|████████▉ | 3940/4381 [1:39:25<11:07,  1.51s/it, loss=2.92, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:25,  5.11it/s][AEpoch 5:  90%|█████████ | 3950/4381 [1:39:28<10:51,  1.51s/it, loss=2.92, v_num=641]
Validating:  32%|███▏      | 200/626 [00:39<01:27,  4.86it/s][AEpoch 5:  90%|█████████ | 3960/4381 [1:39:30<10:34,  1.51s/it, loss=2.92, v_num=641]
Validating:  34%|███▎      | 210/626 [00:41<01:22,  5.05it/s][AEpoch 5:  91%|█████████ | 3970/4381 [1:39:32<10:18,  1.50s/it, loss=2.92, v_num=641]
Validating:  35%|███▌      | 220/626 [00:43<01:19,  5.11it/s][AEpoch 5:  91%|█████████ | 3980/4381 [1:39:34<10:01,  1.50s/it, loss=2.92, v_num=641]
Validating:  37%|███▋      | 230/626 [00:44<01:12,  5.47it/s][AEpoch 5:  91%|█████████ | 3990/4381 [1:39:35<09:45,  1.50s/it, loss=2.92, v_num=641]
Validating:  38%|███▊      | 240/626 [00:46<01:08,  5.62it/s][AEpoch 5:  91%|█████████▏| 4000/4381 [1:39:37<09:29,  1.49s/it, loss=2.92, v_num=641]
Validating:  40%|███▉      | 250/626 [00:48<01:08,  5.50it/s][AEpoch 5:  92%|█████████▏| 4010/4381 [1:39:39<09:13,  1.49s/it, loss=2.92, v_num=641]
Validating:  42%|████▏     | 260/626 [00:50<01:11,  5.10it/s][AEpoch 5:  92%|█████████▏| 4020/4381 [1:39:41<08:57,  1.49s/it, loss=2.92, v_num=641]
Validating:  43%|████▎     | 270/626 [00:51<01:01,  5.82it/s][AEpoch 5:  92%|█████████▏| 4030/4381 [1:39:42<08:40,  1.48s/it, loss=2.92, v_num=641]
Validating:  45%|████▍     | 280/626 [00:54<01:14,  4.66it/s][AEpoch 5:  92%|█████████▏| 4040/4381 [1:39:45<08:25,  1.48s/it, loss=2.92, v_num=641]
Validating:  46%|████▋     | 290/626 [00:56<01:11,  4.73it/s][AEpoch 5:  92%|█████████▏| 4050/4381 [1:39:47<08:09,  1.48s/it, loss=2.92, v_num=641]
Validating:  48%|████▊     | 300/626 [00:58<01:02,  5.18it/s][AEpoch 5:  93%|█████████▎| 4060/4381 [1:39:49<07:53,  1.47s/it, loss=2.92, v_num=641]
Validating:  50%|████▉     | 310/626 [00:59<00:53,  5.93it/s][AEpoch 5:  93%|█████████▎| 4070/4381 [1:39:50<07:37,  1.47s/it, loss=2.92, v_num=641]
Validating:  51%|█████     | 320/626 [01:01<00:53,  5.70it/s][AEpoch 5:  93%|█████████▎| 4080/4381 [1:39:52<07:21,  1.47s/it, loss=2.92, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:02<00:47,  6.28it/s][AEpoch 5:  93%|█████████▎| 4090/4381 [1:39:53<07:06,  1.47s/it, loss=2.92, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:04<00:45,  6.27it/s][AEpoch 5:  94%|█████████▎| 4100/4381 [1:39:55<06:50,  1.46s/it, loss=2.92, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:07<00:56,  4.88it/s][AEpoch 5:  94%|█████████▍| 4110/4381 [1:39:58<06:35,  1.46s/it, loss=2.92, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:09<00:54,  4.87it/s][AEpoch 5:  94%|█████████▍| 4120/4381 [1:40:00<06:20,  1.46s/it, loss=2.92, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:10<00:47,  5.41it/s][AEpoch 5:  94%|█████████▍| 4130/4381 [1:40:01<06:04,  1.45s/it, loss=2.92, v_num=641]
Validating:  61%|██████    | 380/626 [01:11<00:41,  5.94it/s][AEpoch 5:  94%|█████████▍| 4140/4381 [1:40:02<05:49,  1.45s/it, loss=2.92, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:13<00:36,  6.40it/s][AEpoch 5:  95%|█████████▍| 4150/4381 [1:40:04<05:34,  1.45s/it, loss=2.92, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:14<00:32,  7.02it/s][AEpoch 5:  95%|█████████▍| 4160/4381 [1:40:05<05:18,  1.44s/it, loss=2.92, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:15<00:30,  7.11it/s][AEpoch 5:  95%|█████████▌| 4170/4381 [1:40:06<05:03,  1.44s/it, loss=2.92, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:18<00:39,  5.26it/s][AEpoch 5:  95%|█████████▌| 4180/4381 [1:40:09<04:48,  1.44s/it, loss=2.92, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:20<00:35,  5.59it/s][AEpoch 5:  96%|█████████▌| 4190/4381 [1:40:11<04:33,  1.43s/it, loss=2.92, v_num=641]
Validating:  70%|███████   | 440/626 [01:22<00:34,  5.37it/s][AEpoch 5:  96%|█████████▌| 4200/4381 [1:40:13<04:19,  1.43s/it, loss=2.92, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:23<00:30,  5.69it/s][AEpoch 5:  96%|█████████▌| 4210/4381 [1:40:14<04:04,  1.43s/it, loss=2.92, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:25<00:30,  5.51it/s][AEpoch 5:  96%|█████████▋| 4220/4381 [1:40:16<03:49,  1.43s/it, loss=2.92, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:27<00:27,  5.78it/s][AEpoch 5:  97%|█████████▋| 4230/4381 [1:40:18<03:34,  1.42s/it, loss=2.92, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:29<00:26,  5.52it/s][AEpoch 5:  97%|█████████▋| 4240/4381 [1:40:20<03:20,  1.42s/it, loss=2.92, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:31<00:26,  5.06it/s][AEpoch 5:  97%|█████████▋| 4250/4381 [1:40:22<03:05,  1.42s/it, loss=2.92, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:33<00:23,  5.34it/s][AEpoch 5:  97%|█████████▋| 4260/4381 [1:40:24<02:51,  1.41s/it, loss=2.92, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:35<00:23,  4.93it/s][AEpoch 5:  97%|█████████▋| 4270/4381 [1:40:26<02:36,  1.41s/it, loss=2.92, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:37<00:19,  5.34it/s][AEpoch 5:  98%|█████████▊| 4280/4381 [1:40:28<02:22,  1.41s/it, loss=2.92, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:39<00:18,  5.24it/s][AEpoch 5:  98%|█████████▊| 4290/4381 [1:40:30<02:07,  1.41s/it, loss=2.92, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:40<00:14,  5.80it/s][AEpoch 5:  98%|█████████▊| 4300/4381 [1:40:31<01:53,  1.40s/it, loss=2.92, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:41<00:11,  6.39it/s][AEpoch 5:  98%|█████████▊| 4310/4381 [1:40:32<01:39,  1.40s/it, loss=2.92, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:42<00:09,  6.98it/s][AEpoch 5:  99%|█████████▊| 4320/4381 [1:40:33<01:25,  1.40s/it, loss=2.92, v_num=641]
Validating:  91%|█████████ | 570/626 [01:44<00:07,  7.32it/s][AEpoch 5:  99%|█████████▉| 4330/4381 [1:40:35<01:11,  1.39s/it, loss=2.92, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:46<00:07,  6.41it/s][AEpoch 5:  99%|█████████▉| 4340/4381 [1:40:37<00:57,  1.39s/it, loss=2.92, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:47<00:05,  6.67it/s][AEpoch 5:  99%|█████████▉| 4350/4381 [1:40:38<00:43,  1.39s/it, loss=2.92, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:48<00:03,  6.97it/s][AEpoch 5: 100%|█████████▉| 4360/4381 [1:40:39<00:29,  1.38s/it, loss=2.92, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:50<00:02,  6.38it/s][AEpoch 5: 100%|█████████▉| 4370/4381 [1:40:41<00:15,  1.38s/it, loss=2.92, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:52<00:01,  5.93it/s][AEpoch 5: 100%|█████████▉| 4380/4381 [1:40:43<00:01,  1.38s/it, loss=2.92, v_num=641]
Validating: 100%|██████████| 626/626 [01:53<00:00,  6.47it/s][Avalidation_epoch_end
graph acc: 0.12460063897763578
valid accuracy: 0.9469914436340332
Epoch 5: 100%|██████████| 4381/4381 [1:40:50<00:00,  1.38s/it, loss=2.95, v_num=641]
                                                             [AEpoch 5:   0%|          | 0/4381 [00:00<00:00, 10810.06it/s, loss=2.95, v_num=641]  Epoch 6:   0%|          | 0/4381 [00:00<00:01, 3057.07it/s, loss=2.95, v_num=641] Epoch 6:   0%|          | 0/4381 [00:18<22:50:20, 18.77s/it, loss=2.95, v_num=641]Epoch 6:   0%|          | 10/4381 [00:21<2:20:37,  1.93s/it, loss=2.95, v_num=641]Epoch 6:   0%|          | 10/4381 [00:21<2:20:37,  1.93s/it, loss=2.93, v_num=641]Epoch 6:   0%|          | 20/4381 [00:34<1:59:57,  1.65s/it, loss=2.93, v_num=641]Epoch 6:   0%|          | 20/4381 [00:34<1:59:57,  1.65s/it, loss=2.87, v_num=641]Epoch 6:   1%|          | 30/4381 [00:53<2:04:18,  1.71s/it, loss=2.87, v_num=641]Epoch 6:   1%|          | 30/4381 [00:53<2:04:18,  1.71s/it, loss=2.88, v_num=641]Epoch 6:   1%|          | 40/4381 [01:08<2:01:44,  1.68s/it, loss=2.88, v_num=641]Epoch 6:   1%|          | 40/4381 [01:09<2:01:49,  1.68s/it, loss=2.89, v_num=641]Epoch 6:   1%|          | 50/4381 [01:25<2:00:23,  1.67s/it, loss=2.89, v_num=641]Epoch 6:   1%|          | 50/4381 [01:25<2:00:23,  1.67s/it, loss=2.83, v_num=641]Epoch 6:   1%|▏         | 60/4381 [01:43<2:01:50,  1.69s/it, loss=2.83, v_num=641]Epoch 6:   1%|▏         | 60/4381 [01:43<2:01:50,  1.69s/it, loss=2.8, v_num=641] Epoch 6:   2%|▏         | 70/4381 [02:00<2:01:54,  1.70s/it, loss=2.8, v_num=641]Epoch 6:   2%|▏         | 70/4381 [02:00<2:01:54,  1.70s/it, loss=2.82, v_num=641]Epoch 6:   2%|▏         | 80/4381 [02:16<2:00:25,  1.68s/it, loss=2.82, v_num=641]Epoch 6:   2%|▏         | 80/4381 [02:16<2:00:25,  1.68s/it, loss=2.86, v_num=641]Epoch 6:   2%|▏         | 90/4381 [02:32<1:59:27,  1.67s/it, loss=2.86, v_num=641]Epoch 6:   2%|▏         | 90/4381 [02:32<1:59:27,  1.67s/it, loss=2.85, v_num=641]Epoch 6:   2%|▏         | 100/4381 [02:50<2:00:38,  1.69s/it, loss=2.85, v_num=641]Epoch 6:   2%|▏         | 100/4381 [02:50<2:00:38,  1.69s/it, loss=2.83, v_num=641]Epoch 6:   3%|▎         | 110/4381 [03:09<2:01:38,  1.71s/it, loss=2.83, v_num=641]Epoch 6:   3%|▎         | 110/4381 [03:09<2:01:38,  1.71s/it, loss=2.84, v_num=641]Epoch 6:   3%|▎         | 120/4381 [03:24<2:00:18,  1.69s/it, loss=2.84, v_num=641]Epoch 6:   3%|▎         | 120/4381 [03:24<2:00:18,  1.69s/it, loss=2.86, v_num=641]Epoch 6:   3%|▎         | 130/4381 [03:41<2:00:03,  1.69s/it, loss=2.86, v_num=641]Epoch 6:   3%|▎         | 130/4381 [03:41<2:00:03,  1.69s/it, loss=2.88, v_num=641]Epoch 6:   3%|▎         | 140/4381 [03:57<1:58:57,  1.68s/it, loss=2.88, v_num=641]Epoch 6:   3%|▎         | 140/4381 [03:57<1:58:57,  1.68s/it, loss=2.87, v_num=641]Epoch 6:   3%|▎         | 150/4381 [04:13<1:58:10,  1.68s/it, loss=2.87, v_num=641]Epoch 6:   3%|▎         | 150/4381 [04:13<1:58:10,  1.68s/it, loss=2.86, v_num=641]Epoch 6:   4%|▎         | 160/4381 [04:30<1:58:17,  1.68s/it, loss=2.86, v_num=641]Epoch 6:   4%|▎         | 160/4381 [04:30<1:58:17,  1.68s/it, loss=2.85, v_num=641]Epoch 6:   4%|▍         | 170/4381 [04:51<1:59:48,  1.71s/it, loss=2.85, v_num=641]Epoch 6:   4%|▍         | 170/4381 [04:51<1:59:48,  1.71s/it, loss=2.83, v_num=641]Epoch 6:   4%|▍         | 180/4381 [05:09<1:59:51,  1.71s/it, loss=2.83, v_num=641]Epoch 6:   4%|▍         | 180/4381 [05:09<1:59:51,  1.71s/it, loss=2.85, v_num=641]Epoch 6:   4%|▍         | 190/4381 [05:24<1:58:38,  1.70s/it, loss=2.85, v_num=641]Epoch 6:   4%|▍         | 190/4381 [05:24<1:58:38,  1.70s/it, loss=2.89, v_num=641]Epoch 6:   5%|▍         | 200/4381 [05:39<1:57:41,  1.69s/it, loss=2.89, v_num=641]Epoch 6:   5%|▍         | 200/4381 [05:39<1:57:41,  1.69s/it, loss=2.88, v_num=641]Epoch 6:   5%|▍         | 210/4381 [05:59<1:58:19,  1.70s/it, loss=2.88, v_num=641]Epoch 6:   5%|▍         | 210/4381 [05:59<1:58:19,  1.70s/it, loss=2.87, v_num=641]Epoch 6:   5%|▌         | 220/4381 [06:11<1:56:43,  1.68s/it, loss=2.87, v_num=641]Epoch 6:   5%|▌         | 220/4381 [06:11<1:56:43,  1.68s/it, loss=2.86, v_num=641]Epoch 6:   5%|▌         | 230/4381 [06:27<1:56:06,  1.68s/it, loss=2.86, v_num=641]Epoch 6:   5%|▌         | 230/4381 [06:27<1:56:06,  1.68s/it, loss=2.85, v_num=641]Epoch 6:   5%|▌         | 240/4381 [06:45<1:56:00,  1.68s/it, loss=2.85, v_num=641]Epoch 6:   5%|▌         | 240/4381 [06:45<1:56:00,  1.68s/it, loss=2.87, v_num=641]Epoch 6:   6%|▌         | 250/4381 [07:03<1:56:02,  1.69s/it, loss=2.87, v_num=641]Epoch 6:   6%|▌         | 250/4381 [07:03<1:56:02,  1.69s/it, loss=2.88, v_num=641]Epoch 6:   6%|▌         | 260/4381 [07:17<1:55:09,  1.68s/it, loss=2.88, v_num=641]Epoch 6:   6%|▌         | 260/4381 [07:17<1:55:09,  1.68s/it, loss=2.89, v_num=641]Epoch 6:   6%|▌         | 270/4381 [07:31<1:54:11,  1.67s/it, loss=2.89, v_num=641]Epoch 6:   6%|▌         | 270/4381 [07:31<1:54:11,  1.67s/it, loss=2.87, v_num=641]Epoch 6:   6%|▋         | 280/4381 [07:49<1:54:08,  1.67s/it, loss=2.87, v_num=641]Epoch 6:   6%|▋         | 280/4381 [07:49<1:54:08,  1.67s/it, loss=2.88, v_num=641]Epoch 6:   7%|▋         | 290/4381 [08:02<1:53:08,  1.66s/it, loss=2.88, v_num=641]Epoch 6:   7%|▋         | 290/4381 [08:02<1:53:08,  1.66s/it, loss=2.88, v_num=641]Epoch 6:   7%|▋         | 300/4381 [08:17<1:52:28,  1.65s/it, loss=2.88, v_num=641]Epoch 6:   7%|▋         | 300/4381 [08:17<1:52:28,  1.65s/it, loss=2.89, v_num=641]Epoch 6:   7%|▋         | 310/4381 [08:35<1:52:28,  1.66s/it, loss=2.89, v_num=641]Epoch 6:   7%|▋         | 310/4381 [08:35<1:52:28,  1.66s/it, loss=2.84, v_num=641]Epoch 6:   7%|▋         | 320/4381 [08:49<1:51:44,  1.65s/it, loss=2.84, v_num=641]Epoch 6:   7%|▋         | 320/4381 [08:49<1:51:44,  1.65s/it, loss=2.82, v_num=641]Epoch 6:   8%|▊         | 330/4381 [09:04<1:51:10,  1.65s/it, loss=2.82, v_num=641]Epoch 6:   8%|▊         | 330/4381 [09:04<1:51:10,  1.65s/it, loss=2.83, v_num=641]Epoch 6:   8%|▊         | 340/4381 [09:21<1:50:59,  1.65s/it, loss=2.83, v_num=641]Epoch 6:   8%|▊         | 340/4381 [09:21<1:50:59,  1.65s/it, loss=2.86, v_num=641]Epoch 6:   8%|▊         | 350/4381 [09:38<1:50:40,  1.65s/it, loss=2.86, v_num=641]Epoch 6:   8%|▊         | 350/4381 [09:38<1:50:40,  1.65s/it, loss=2.9, v_num=641] Epoch 6:   8%|▊         | 360/4381 [09:53<1:50:12,  1.64s/it, loss=2.9, v_num=641]Epoch 6:   8%|▊         | 360/4381 [09:53<1:50:12,  1.64s/it, loss=2.87, v_num=641]Epoch 6:   8%|▊         | 370/4381 [10:12<1:50:23,  1.65s/it, loss=2.87, v_num=641]Epoch 6:   8%|▊         | 370/4381 [10:12<1:50:23,  1.65s/it, loss=2.87, v_num=641]Epoch 6:   9%|▊         | 380/4381 [10:27<1:49:51,  1.65s/it, loss=2.87, v_num=641]Epoch 6:   9%|▊         | 380/4381 [10:27<1:49:51,  1.65s/it, loss=2.9, v_num=641] Epoch 6:   9%|▉         | 390/4381 [10:43<1:49:30,  1.65s/it, loss=2.9, v_num=641]Epoch 6:   9%|▉         | 390/4381 [10:43<1:49:30,  1.65s/it, loss=2.91, v_num=641]Epoch 6:   9%|▉         | 400/4381 [11:02<1:49:41,  1.65s/it, loss=2.91, v_num=641]Epoch 6:   9%|▉         | 400/4381 [11:02<1:49:41,  1.65s/it, loss=2.89, v_num=641]Epoch 6:   9%|▉         | 410/4381 [11:17<1:49:10,  1.65s/it, loss=2.89, v_num=641]Epoch 6:   9%|▉         | 410/4381 [11:17<1:49:10,  1.65s/it, loss=2.91, v_num=641]Epoch 6:  10%|▉         | 420/4381 [11:32<1:48:39,  1.65s/it, loss=2.91, v_num=641]Epoch 6:  10%|▉         | 420/4381 [11:32<1:48:39,  1.65s/it, loss=2.89, v_num=641]Epoch 6:  10%|▉         | 430/4381 [11:50<1:48:33,  1.65s/it, loss=2.89, v_num=641]Epoch 6:  10%|▉         | 430/4381 [11:50<1:48:33,  1.65s/it, loss=2.89, v_num=641]Epoch 6:  10%|█         | 440/4381 [12:06<1:48:07,  1.65s/it, loss=2.89, v_num=641]Epoch 6:  10%|█         | 440/4381 [12:06<1:48:07,  1.65s/it, loss=2.92, v_num=641]Epoch 6:  10%|█         | 450/4381 [12:21<1:47:39,  1.64s/it, loss=2.92, v_num=641]Epoch 6:  10%|█         | 450/4381 [12:21<1:47:39,  1.64s/it, loss=2.87, v_num=641]Epoch 6:  10%|█         | 460/4381 [12:39<1:47:37,  1.65s/it, loss=2.87, v_num=641]Epoch 6:  10%|█         | 460/4381 [12:39<1:47:37,  1.65s/it, loss=2.86, v_num=641]Epoch 6:  11%|█         | 470/4381 [12:53<1:47:03,  1.64s/it, loss=2.86, v_num=641]Epoch 6:  11%|█         | 470/4381 [12:53<1:47:03,  1.64s/it, loss=2.91, v_num=641]Epoch 6:  11%|█         | 480/4381 [13:08<1:46:36,  1.64s/it, loss=2.91, v_num=641]Epoch 6:  11%|█         | 480/4381 [13:08<1:46:36,  1.64s/it, loss=2.94, v_num=641]Epoch 6:  11%|█         | 490/4381 [13:25<1:46:24,  1.64s/it, loss=2.94, v_num=641]Epoch 6:  11%|█         | 490/4381 [13:25<1:46:24,  1.64s/it, loss=2.92, v_num=641]Epoch 6:  11%|█▏        | 500/4381 [13:40<1:45:55,  1.64s/it, loss=2.92, v_num=641]Epoch 6:  11%|█▏        | 500/4381 [13:40<1:45:55,  1.64s/it, loss=2.89, v_num=641]Epoch 6:  12%|█▏        | 510/4381 [13:54<1:45:25,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  12%|█▏        | 510/4381 [13:54<1:45:25,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 520/4381 [14:11<1:45:07,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 520/4381 [14:11<1:45:07,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 530/4381 [14:29<1:45:03,  1.64s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 530/4381 [14:29<1:45:03,  1.64s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 540/4381 [14:43<1:44:35,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  12%|█▏        | 540/4381 [14:43<1:44:35,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  13%|█▎        | 550/4381 [14:59<1:44:15,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  13%|█▎        | 550/4381 [14:59<1:44:15,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  13%|█▎        | 560/4381 [15:17<1:44:09,  1.64s/it, loss=2.89, v_num=641]Epoch 6:  13%|█▎        | 560/4381 [15:17<1:44:09,  1.64s/it, loss=2.88, v_num=641]Epoch 6:  13%|█▎        | 570/4381 [15:33<1:43:53,  1.64s/it, loss=2.88, v_num=641]Epoch 6:  13%|█▎        | 570/4381 [15:33<1:43:53,  1.64s/it, loss=2.88, v_num=641]Epoch 6:  13%|█▎        | 580/4381 [15:48<1:43:23,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  13%|█▎        | 580/4381 [15:48<1:43:23,  1.63s/it, loss=2.86, v_num=641]Epoch 6:  13%|█▎        | 590/4381 [16:06<1:43:16,  1.63s/it, loss=2.86, v_num=641]Epoch 6:  13%|█▎        | 590/4381 [16:06<1:43:16,  1.63s/it, loss=2.86, v_num=641]Epoch 6:  14%|█▎        | 600/4381 [16:21<1:42:57,  1.63s/it, loss=2.86, v_num=641]Epoch 6:  14%|█▎        | 600/4381 [16:21<1:42:57,  1.63s/it, loss=2.85, v_num=641]Epoch 6:  14%|█▍        | 610/4381 [16:36<1:42:28,  1.63s/it, loss=2.85, v_num=641]Epoch 6:  14%|█▍        | 610/4381 [16:36<1:42:28,  1.63s/it, loss=2.87, v_num=641]Epoch 6:  14%|█▍        | 620/4381 [16:53<1:42:16,  1.63s/it, loss=2.87, v_num=641]Epoch 6:  14%|█▍        | 620/4381 [16:53<1:42:16,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  14%|█▍        | 630/4381 [17:08<1:41:54,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  14%|█▍        | 630/4381 [17:08<1:41:54,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  15%|█▍        | 640/4381 [17:22<1:41:25,  1.63s/it, loss=2.89, v_num=641]Epoch 6:  15%|█▍        | 640/4381 [17:22<1:41:25,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  15%|█▍        | 650/4381 [17:41<1:41:22,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  15%|█▍        | 650/4381 [17:41<1:41:22,  1.63s/it, loss=2.87, v_num=641]Epoch 6:  15%|█▌        | 660/4381 [17:58<1:41:10,  1.63s/it, loss=2.87, v_num=641]Epoch 6:  15%|█▌        | 660/4381 [17:58<1:41:10,  1.63s/it, loss=2.85, v_num=641]Epoch 6:  15%|█▌        | 670/4381 [18:10<1:40:32,  1.63s/it, loss=2.85, v_num=641]Epoch 6:  15%|█▌        | 670/4381 [18:10<1:40:32,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  16%|█▌        | 680/4381 [18:28<1:40:26,  1.63s/it, loss=2.88, v_num=641]Epoch 6:  16%|█▌        | 680/4381 [18:28<1:40:26,  1.63s/it, loss=2.91, v_num=641]Epoch 6:  16%|█▌        | 690/4381 [18:44<1:40:09,  1.63s/it, loss=2.91, v_num=641]Epoch 6:  16%|█▌        | 690/4381 [18:44<1:40:09,  1.63s/it, loss=2.9, v_num=641] Epoch 6:  16%|█▌        | 700/4381 [18:57<1:39:34,  1.62s/it, loss=2.9, v_num=641]Epoch 6:  16%|█▌        | 700/4381 [18:57<1:39:34,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  16%|█▌        | 710/4381 [19:14<1:39:22,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  16%|█▌        | 710/4381 [19:14<1:39:22,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  16%|█▋        | 720/4381 [19:28<1:38:54,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  16%|█▋        | 720/4381 [19:28<1:38:54,  1.62s/it, loss=2.85, v_num=641]Epoch 6:  17%|█▋        | 730/4381 [19:45<1:38:38,  1.62s/it, loss=2.85, v_num=641]Epoch 6:  17%|█▋        | 730/4381 [19:45<1:38:38,  1.62s/it, loss=2.85, v_num=641]Epoch 6:  17%|█▋        | 740/4381 [20:00<1:38:17,  1.62s/it, loss=2.85, v_num=641]Epoch 6:  17%|█▋        | 740/4381 [20:00<1:38:17,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  17%|█▋        | 750/4381 [20:15<1:37:57,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  17%|█▋        | 750/4381 [20:15<1:37:57,  1.62s/it, loss=2.9, v_num=641] Epoch 6:  17%|█▋        | 760/4381 [20:32<1:37:42,  1.62s/it, loss=2.9, v_num=641]Epoch 6:  17%|█▋        | 760/4381 [20:32<1:37:42,  1.62s/it, loss=2.9, v_num=641]Epoch 6:  18%|█▊        | 770/4381 [20:48<1:37:28,  1.62s/it, loss=2.9, v_num=641]Epoch 6:  18%|█▊        | 770/4381 [20:48<1:37:28,  1.62s/it, loss=2.81, v_num=641]Epoch 6:  18%|█▊        | 780/4381 [21:02<1:37:00,  1.62s/it, loss=2.81, v_num=641]Epoch 6:  18%|█▊        | 780/4381 [21:02<1:37:00,  1.62s/it, loss=2.82, v_num=641]Epoch 6:  18%|█▊        | 790/4381 [21:18<1:36:44,  1.62s/it, loss=2.82, v_num=641]Epoch 6:  18%|█▊        | 790/4381 [21:18<1:36:44,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  18%|█▊        | 800/4381 [21:37<1:36:42,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  18%|█▊        | 800/4381 [21:37<1:36:42,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  18%|█▊        | 810/4381 [21:53<1:36:21,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  18%|█▊        | 810/4381 [21:53<1:36:21,  1.62s/it, loss=2.9, v_num=641] Epoch 6:  19%|█▊        | 820/4381 [22:09<1:36:05,  1.62s/it, loss=2.9, v_num=641]Epoch 6:  19%|█▊        | 820/4381 [22:09<1:36:05,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  19%|█▉        | 830/4381 [22:28<1:36:02,  1.62s/it, loss=2.87, v_num=641]Epoch 6:  19%|█▉        | 830/4381 [22:28<1:36:02,  1.62s/it, loss=2.84, v_num=641]Epoch 6:  19%|█▉        | 840/4381 [22:43<1:35:40,  1.62s/it, loss=2.84, v_num=641]Epoch 6:  19%|█▉        | 840/4381 [22:43<1:35:40,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  19%|█▉        | 850/4381 [22:57<1:35:14,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  19%|█▉        | 850/4381 [22:57<1:35:14,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  20%|█▉        | 860/4381 [23:14<1:35:04,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  20%|█▉        | 860/4381 [23:14<1:35:04,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  20%|█▉        | 870/4381 [23:30<1:34:44,  1.62s/it, loss=2.88, v_num=641]Epoch 6:  20%|█▉        | 870/4381 [23:30<1:34:44,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  20%|██        | 880/4381 [23:46<1:34:29,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  20%|██        | 880/4381 [23:46<1:34:29,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  20%|██        | 890/4381 [24:05<1:34:22,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  20%|██        | 890/4381 [24:05<1:34:22,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  21%|██        | 900/4381 [24:18<1:33:56,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  21%|██        | 900/4381 [24:18<1:33:56,  1.62s/it, loss=2.82, v_num=641]Epoch 6:  21%|██        | 910/4381 [24:32<1:33:31,  1.62s/it, loss=2.82, v_num=641]Epoch 6:  21%|██        | 910/4381 [24:32<1:33:31,  1.62s/it, loss=2.83, v_num=641]Epoch 6:  21%|██        | 920/4381 [24:51<1:33:26,  1.62s/it, loss=2.83, v_num=641]Epoch 6:  21%|██        | 920/4381 [24:51<1:33:26,  1.62s/it, loss=2.83, v_num=641]Epoch 6:  21%|██        | 930/4381 [25:06<1:33:03,  1.62s/it, loss=2.83, v_num=641]Epoch 6:  21%|██        | 930/4381 [25:06<1:33:03,  1.62s/it, loss=2.86, v_num=641]Epoch 6:  21%|██▏       | 940/4381 [25:19<1:32:34,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  21%|██▏       | 940/4381 [25:19<1:32:34,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  22%|██▏       | 950/4381 [25:35<1:32:18,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  22%|██▏       | 950/4381 [25:35<1:32:18,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  22%|██▏       | 960/4381 [25:50<1:31:58,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  22%|██▏       | 960/4381 [25:50<1:31:58,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  22%|██▏       | 970/4381 [26:07<1:31:47,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  22%|██▏       | 970/4381 [26:07<1:31:47,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  22%|██▏       | 980/4381 [26:21<1:31:24,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  22%|██▏       | 980/4381 [26:21<1:31:24,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  23%|██▎       | 990/4381 [26:39<1:31:12,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  23%|██▎       | 990/4381 [26:39<1:31:12,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  23%|██▎       | 1000/4381 [26:53<1:30:50,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  23%|██▎       | 1000/4381 [26:53<1:30:50,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  23%|██▎       | 1010/4381 [27:10<1:30:36,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  23%|██▎       | 1010/4381 [27:10<1:30:36,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  23%|██▎       | 1020/4381 [27:27<1:30:23,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  23%|██▎       | 1020/4381 [27:27<1:30:23,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▎       | 1030/4381 [27:42<1:30:04,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▎       | 1030/4381 [27:42<1:30:04,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▎       | 1040/4381 [27:57<1:29:42,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▎       | 1040/4381 [27:57<1:29:42,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▍       | 1050/4381 [28:14<1:29:30,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  24%|██▍       | 1050/4381 [28:14<1:29:30,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  24%|██▍       | 1060/4381 [28:29<1:29:09,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  24%|██▍       | 1060/4381 [28:29<1:29:09,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  24%|██▍       | 1070/4381 [28:44<1:28:51,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  24%|██▍       | 1070/4381 [28:44<1:28:51,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  25%|██▍       | 1080/4381 [29:03<1:28:43,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  25%|██▍       | 1080/4381 [29:03<1:28:43,  1.61s/it, loss=2.92, v_num=641]Epoch 6:  25%|██▍       | 1090/4381 [29:19<1:28:26,  1.61s/it, loss=2.92, v_num=641]Epoch 6:  25%|██▍       | 1090/4381 [29:19<1:28:26,  1.61s/it, loss=2.91, v_num=641]Epoch 6:  25%|██▌       | 1100/4381 [29:33<1:28:05,  1.61s/it, loss=2.91, v_num=641]Epoch 6:  25%|██▌       | 1100/4381 [29:33<1:28:05,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  25%|██▌       | 1110/4381 [29:52<1:27:56,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  25%|██▌       | 1110/4381 [29:52<1:27:56,  1.61s/it, loss=2.84, v_num=641]Epoch 6:  26%|██▌       | 1120/4381 [30:08<1:27:39,  1.61s/it, loss=2.84, v_num=641]Epoch 6:  26%|██▌       | 1120/4381 [30:08<1:27:39,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  26%|██▌       | 1130/4381 [30:21<1:27:17,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  26%|██▌       | 1130/4381 [30:21<1:27:17,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  26%|██▌       | 1140/4381 [30:40<1:27:08,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  26%|██▌       | 1140/4381 [30:40<1:27:08,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  26%|██▌       | 1150/4381 [30:54<1:26:46,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  26%|██▌       | 1150/4381 [30:54<1:26:46,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  26%|██▋       | 1160/4381 [31:08<1:26:24,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  26%|██▋       | 1160/4381 [31:08<1:26:24,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  27%|██▋       | 1170/4381 [31:27<1:26:14,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  27%|██▋       | 1170/4381 [31:27<1:26:14,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  27%|██▋       | 1180/4381 [31:41<1:25:54,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  27%|██▋       | 1180/4381 [31:41<1:25:54,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  27%|██▋       | 1190/4381 [31:56<1:25:34,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  27%|██▋       | 1190/4381 [31:56<1:25:34,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  27%|██▋       | 1200/4381 [32:13<1:25:20,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  27%|██▋       | 1200/4381 [32:13<1:25:20,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  28%|██▊       | 1210/4381 [32:29<1:25:03,  1.61s/it, loss=2.85, v_num=641]Epoch 6:  28%|██▊       | 1210/4381 [32:29<1:25:03,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  28%|██▊       | 1220/4381 [32:44<1:24:44,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  28%|██▊       | 1220/4381 [32:44<1:24:44,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  28%|██▊       | 1230/4381 [33:00<1:24:30,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  28%|██▊       | 1230/4381 [33:00<1:24:30,  1.61s/it, loss=2.83, v_num=641]Epoch 6:  28%|██▊       | 1240/4381 [33:16<1:24:12,  1.61s/it, loss=2.83, v_num=641]Epoch 6:  28%|██▊       | 1240/4381 [33:16<1:24:12,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  29%|██▊       | 1250/4381 [33:30<1:23:53,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  29%|██▊       | 1250/4381 [33:30<1:23:53,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  29%|██▉       | 1260/4381 [33:45<1:23:31,  1.61s/it, loss=2.89, v_num=641]Epoch 6:  29%|██▉       | 1260/4381 [33:45<1:23:31,  1.61s/it, loss=2.88, v_num=641]Epoch 6:  29%|██▉       | 1270/4381 [33:58<1:23:08,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  29%|██▉       | 1270/4381 [33:58<1:23:08,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  29%|██▉       | 1280/4381 [34:12<1:22:48,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  29%|██▉       | 1280/4381 [34:12<1:22:48,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  29%|██▉       | 1290/4381 [34:27<1:22:29,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  29%|██▉       | 1290/4381 [34:27<1:22:29,  1.60s/it, loss=2.91, v_num=641]Epoch 6:  30%|██▉       | 1300/4381 [34:43<1:22:14,  1.60s/it, loss=2.91, v_num=641]Epoch 6:  30%|██▉       | 1300/4381 [34:43<1:22:14,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  30%|██▉       | 1310/4381 [34:58<1:21:54,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  30%|██▉       | 1310/4381 [34:58<1:21:54,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  30%|███       | 1320/4381 [35:18<1:21:50,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  30%|███       | 1320/4381 [35:18<1:21:50,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  30%|███       | 1330/4381 [35:32<1:21:29,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  30%|███       | 1330/4381 [35:32<1:21:29,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1340/4381 [35:48<1:21:11,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1340/4381 [35:48<1:21:11,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1350/4381 [36:06<1:21:01,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1350/4381 [36:06<1:21:01,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1360/4381 [36:22<1:20:44,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███       | 1360/4381 [36:22<1:20:44,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███▏      | 1370/4381 [36:37<1:20:27,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  31%|███▏      | 1370/4381 [36:37<1:20:27,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  31%|███▏      | 1380/4381 [36:54<1:20:13,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  31%|███▏      | 1380/4381 [36:54<1:20:13,  1.60s/it, loss=2.9, v_num=641] Epoch 6:  32%|███▏      | 1390/4381 [37:09<1:19:53,  1.60s/it, loss=2.9, v_num=641]Epoch 6:  32%|███▏      | 1390/4381 [37:09<1:19:53,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  32%|███▏      | 1400/4381 [37:24<1:19:35,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  32%|███▏      | 1400/4381 [37:24<1:19:35,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  32%|███▏      | 1410/4381 [37:42<1:19:24,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  32%|███▏      | 1410/4381 [37:42<1:19:24,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  32%|███▏      | 1420/4381 [37:58<1:19:08,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  32%|███▏      | 1420/4381 [37:58<1:19:08,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  33%|███▎      | 1430/4381 [38:10<1:18:44,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  33%|███▎      | 1430/4381 [38:10<1:18:44,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  33%|███▎      | 1440/4381 [38:27<1:18:29,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  33%|███▎      | 1440/4381 [38:27<1:18:29,  1.60s/it, loss=2.92, v_num=641]Epoch 6:  33%|███▎      | 1450/4381 [38:43<1:18:12,  1.60s/it, loss=2.92, v_num=641]Epoch 6:  33%|███▎      | 1450/4381 [38:43<1:18:12,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  33%|███▎      | 1460/4381 [39:00<1:17:59,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  33%|███▎      | 1460/4381 [39:00<1:17:59,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  34%|███▎      | 1470/4381 [39:17<1:17:45,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  34%|███▎      | 1470/4381 [39:17<1:17:45,  1.60s/it, loss=2.78, v_num=641]Epoch 6:  34%|███▍      | 1480/4381 [39:33<1:17:29,  1.60s/it, loss=2.78, v_num=641]Epoch 6:  34%|███▍      | 1480/4381 [39:33<1:17:29,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  34%|███▍      | 1490/4381 [39:53<1:17:21,  1.61s/it, loss=2.83, v_num=641]Epoch 6:  34%|███▍      | 1490/4381 [39:53<1:17:21,  1.61s/it, loss=2.87, v_num=641]Epoch 6:  34%|███▍      | 1500/4381 [40:07<1:17:00,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  34%|███▍      | 1500/4381 [40:07<1:17:00,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  34%|███▍      | 1510/4381 [40:21<1:16:40,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  34%|███▍      | 1510/4381 [40:21<1:16:40,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  35%|███▍      | 1520/4381 [40:42<1:16:34,  1.61s/it, loss=2.84, v_num=641]Epoch 6:  35%|███▍      | 1520/4381 [40:42<1:16:34,  1.61s/it, loss=2.86, v_num=641]Epoch 6:  35%|███▍      | 1530/4381 [40:56<1:16:14,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  35%|███▍      | 1530/4381 [40:56<1:16:14,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  35%|███▌      | 1540/4381 [41:11<1:15:55,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  35%|███▌      | 1540/4381 [41:11<1:15:55,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  36%|███▌      | 1560/4381 [41:42<1:15:22,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  36%|███▌      | 1560/4381 [41:42<1:15:22,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  36%|███▌      | 1570/4381 [41:58<1:15:07,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  36%|███▌      | 1570/4381 [41:58<1:15:07,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  36%|███▌      | 1580/4381 [42:14<1:14:50,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  36%|███▌      | 1580/4381 [42:14<1:14:50,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  36%|███▋      | 1590/4381 [42:29<1:14:32,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  36%|███▋      | 1590/4381 [42:29<1:14:32,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  37%|███▋      | 1600/4381 [42:45<1:14:16,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  37%|███▋      | 1600/4381 [42:45<1:14:16,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  37%|███▋      | 1610/4381 [43:02<1:14:01,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  37%|███▋      | 1610/4381 [43:02<1:14:01,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  37%|███▋      | 1620/4381 [43:17<1:13:44,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  37%|███▋      | 1620/4381 [43:17<1:13:44,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  37%|███▋      | 1630/4381 [43:30<1:13:23,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  37%|███▋      | 1630/4381 [43:30<1:13:23,  1.60s/it, loss=2.8, v_num=641] Epoch 6:  37%|███▋      | 1640/4381 [43:48<1:13:09,  1.60s/it, loss=2.8, v_num=641]Epoch 6:  37%|███▋      | 1640/4381 [43:48<1:13:09,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  38%|███▊      | 1650/4381 [44:02<1:12:51,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  38%|███▊      | 1650/4381 [44:02<1:12:51,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  38%|███▊      | 1660/4381 [44:16<1:12:31,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  38%|███▊      | 1660/4381 [44:16<1:12:31,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  38%|███▊      | 1670/4381 [44:35<1:12:20,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  38%|███▊      | 1670/4381 [44:35<1:12:20,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  38%|███▊      | 1680/4381 [44:49<1:12:01,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  38%|███▊      | 1680/4381 [44:49<1:12:01,  1.60s/it, loss=2.9, v_num=641] Epoch 6:  39%|███▊      | 1690/4381 [45:02<1:11:40,  1.60s/it, loss=2.9, v_num=641]Epoch 6:  39%|███▊      | 1690/4381 [45:02<1:11:40,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  39%|███▉      | 1700/4381 [45:25<1:11:35,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  39%|███▉      | 1700/4381 [45:25<1:11:35,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  39%|███▉      | 1710/4381 [45:41<1:11:19,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  39%|███▉      | 1710/4381 [45:41<1:11:19,  1.60s/it, loss=2.8, v_num=641] Epoch 6:  39%|███▉      | 1720/4381 [45:56<1:11:02,  1.60s/it, loss=2.8, v_num=641]Epoch 6:  39%|███▉      | 1720/4381 [45:56<1:11:02,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  39%|███▉      | 1730/4381 [46:13<1:10:47,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  39%|███▉      | 1730/4381 [46:13<1:10:47,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  40%|███▉      | 1740/4381 [46:29<1:10:31,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  40%|███▉      | 1740/4381 [46:29<1:10:31,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  40%|███▉      | 1750/4381 [46:44<1:10:14,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  40%|███▉      | 1750/4381 [46:44<1:10:14,  1.60s/it, loss=2.8, v_num=641] Epoch 6:  40%|████      | 1760/4381 [47:01<1:09:59,  1.60s/it, loss=2.8, v_num=641]Epoch 6:  40%|████      | 1760/4381 [47:01<1:09:59,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  40%|████      | 1770/4381 [47:19<1:09:45,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  40%|████      | 1770/4381 [47:19<1:09:45,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  41%|████      | 1780/4381 [47:32<1:09:26,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  41%|████      | 1780/4381 [47:32<1:09:26,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  41%|████      | 1790/4381 [47:48<1:09:10,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  41%|████      | 1790/4381 [47:48<1:09:10,  1.60s/it, loss=2.91, v_num=641]Epoch 6:  41%|████      | 1800/4381 [48:02<1:08:50,  1.60s/it, loss=2.91, v_num=641]Epoch 6:  41%|████      | 1800/4381 [48:02<1:08:50,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  41%|████▏     | 1810/4381 [48:16<1:08:32,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  41%|████▏     | 1810/4381 [48:16<1:08:32,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  42%|████▏     | 1820/4381 [48:37<1:08:22,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  42%|████▏     | 1820/4381 [48:37<1:08:22,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  42%|████▏     | 1830/4381 [48:52<1:08:06,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  42%|████▏     | 1830/4381 [48:52<1:08:06,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  42%|████▏     | 1840/4381 [49:08<1:07:49,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  42%|████▏     | 1840/4381 [49:08<1:07:49,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  42%|████▏     | 1850/4381 [49:25<1:07:35,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  42%|████▏     | 1850/4381 [49:25<1:07:35,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  42%|████▏     | 1860/4381 [49:40<1:07:16,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  42%|████▏     | 1860/4381 [49:40<1:07:16,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  43%|████▎     | 1870/4381 [49:52<1:06:56,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  43%|████▎     | 1870/4381 [49:52<1:06:56,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  43%|████▎     | 1880/4381 [50:08<1:06:39,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  43%|████▎     | 1880/4381 [50:08<1:06:39,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  43%|████▎     | 1890/4381 [50:25<1:06:25,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  43%|████▎     | 1890/4381 [50:25<1:06:25,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  43%|████▎     | 1900/4381 [50:39<1:06:07,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  43%|████▎     | 1900/4381 [50:39<1:06:07,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  44%|████▎     | 1910/4381 [50:58<1:05:54,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  44%|████▎     | 1910/4381 [50:58<1:05:54,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  44%|████▍     | 1920/4381 [51:12<1:05:36,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  44%|████▍     | 1920/4381 [51:12<1:05:36,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  44%|████▍     | 1930/4381 [51:29<1:05:21,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  44%|████▍     | 1930/4381 [51:29<1:05:21,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  44%|████▍     | 1940/4381 [51:41<1:05:00,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  44%|████▍     | 1940/4381 [51:41<1:05:00,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▍     | 1950/4381 [51:59<1:04:47,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▍     | 1950/4381 [51:59<1:04:47,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  45%|████▍     | 1960/4381 [52:12<1:04:26,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  45%|████▍     | 1960/4381 [52:12<1:04:26,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▍     | 1970/4381 [52:27<1:04:09,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▍     | 1970/4381 [52:27<1:04:09,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▌     | 1980/4381 [52:44<1:03:55,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▌     | 1980/4381 [52:44<1:03:55,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▌     | 1990/4381 [52:58<1:03:37,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  45%|████▌     | 1990/4381 [52:58<1:03:37,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▌     | 2000/4381 [53:15<1:03:22,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▌     | 2000/4381 [53:15<1:03:22,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▌     | 2010/4381 [53:30<1:03:04,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▌     | 2010/4381 [53:30<1:03:04,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  46%|████▌     | 2020/4381 [53:45<1:02:47,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  46%|████▌     | 2020/4381 [53:45<1:02:47,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▋     | 2030/4381 [54:03<1:02:34,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  46%|████▋     | 2030/4381 [54:03<1:02:34,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  47%|████▋     | 2040/4381 [54:18<1:02:17,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  47%|████▋     | 2040/4381 [54:18<1:02:17,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  47%|████▋     | 2050/4381 [54:33<1:02:00,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  47%|████▋     | 2050/4381 [54:33<1:02:00,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  47%|████▋     | 2060/4381 [54:52<1:01:47,  1.60s/it, loss=2.89, v_num=641]Epoch 6:  47%|████▋     | 2060/4381 [54:52<1:01:47,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  47%|████▋     | 2070/4381 [55:06<1:01:29,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  47%|████▋     | 2070/4381 [55:06<1:01:29,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  47%|████▋     | 2080/4381 [55:19<1:01:10,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  47%|████▋     | 2080/4381 [55:19<1:01:10,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  48%|████▊     | 2090/4381 [55:38<1:00:57,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  48%|████▊     | 2090/4381 [55:38<1:00:57,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  48%|████▊     | 2100/4381 [55:54<1:00:41,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  48%|████▊     | 2100/4381 [55:54<1:00:41,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  48%|████▊     | 2110/4381 [56:09<1:00:24,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  48%|████▊     | 2110/4381 [56:09<1:00:24,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  48%|████▊     | 2120/4381 [56:27<1:00:10,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  48%|████▊     | 2120/4381 [56:27<1:00:10,  1.60s/it, loss=2.8, v_num=641] Epoch 6:  49%|████▊     | 2130/4381 [56:42<59:54,  1.60s/it, loss=2.8, v_num=641]  Epoch 6:  49%|████▊     | 2130/4381 [56:42<59:54,  1.60s/it, loss=2.8, v_num=641]Epoch 6:  49%|████▉     | 2140/4381 [56:56<59:36,  1.60s/it, loss=2.8, v_num=641]Epoch 6:  49%|████▉     | 2140/4381 [56:56<59:36,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  49%|████▉     | 2150/4381 [57:16<59:24,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  49%|████▉     | 2150/4381 [57:16<59:24,  1.60s/it, loss=2.9, v_num=641] Epoch 6:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.9, v_num=641]Epoch 6:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  50%|████▉     | 2170/4381 [57:44<58:48,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  50%|████▉     | 2170/4381 [57:44<58:48,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  50%|████▉     | 2180/4381 [57:59<58:31,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  50%|████▉     | 2180/4381 [57:59<58:31,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  50%|████▉     | 2190/4381 [58:16<58:16,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  50%|████▉     | 2190/4381 [58:16<58:16,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  50%|█████     | 2200/4381 [58:30<57:58,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  50%|█████     | 2200/4381 [58:30<57:58,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  50%|█████     | 2210/4381 [58:46<57:42,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  50%|█████     | 2210/4381 [58:46<57:42,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  51%|█████     | 2220/4381 [59:05<57:29,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  51%|█████     | 2220/4381 [59:05<57:29,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  51%|█████     | 2230/4381 [59:18<57:10,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  51%|█████     | 2230/4381 [59:18<57:10,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  51%|█████     | 2240/4381 [59:35<56:55,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  51%|█████     | 2240/4381 [59:35<56:55,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  51%|█████▏    | 2250/4381 [59:51<56:39,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  51%|█████▏    | 2250/4381 [59:51<56:39,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  52%|█████▏    | 2260/4381 [1:00:05<56:22,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  52%|█████▏    | 2260/4381 [1:00:05<56:22,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  52%|█████▏    | 2270/4381 [1:00:22<56:07,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  52%|█████▏    | 2270/4381 [1:00:22<56:07,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  52%|█████▏    | 2280/4381 [1:00:39<55:51,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  52%|█████▏    | 2280/4381 [1:00:39<55:51,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  52%|█████▏    | 2290/4381 [1:00:54<55:35,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  52%|█████▏    | 2290/4381 [1:00:54<55:35,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  52%|█████▏    | 2300/4381 [1:01:10<55:19,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  52%|█████▏    | 2300/4381 [1:01:10<55:19,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  53%|█████▎    | 2310/4381 [1:01:29<55:06,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  53%|█████▎    | 2310/4381 [1:01:29<55:06,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  53%|█████▎    | 2320/4381 [1:01:46<54:51,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  53%|█████▎    | 2320/4381 [1:01:46<54:51,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  53%|█████▎    | 2340/4381 [1:02:17<54:18,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  53%|█████▎    | 2340/4381 [1:02:17<54:18,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  54%|█████▎    | 2350/4381 [1:02:32<54:01,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  54%|█████▎    | 2350/4381 [1:02:32<54:01,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  54%|█████▍    | 2360/4381 [1:02:48<53:46,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  54%|█████▍    | 2360/4381 [1:02:48<53:46,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  54%|█████▍    | 2370/4381 [1:03:08<53:33,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  54%|█████▍    | 2370/4381 [1:03:08<53:33,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  54%|█████▍    | 2380/4381 [1:03:23<53:16,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  54%|█████▍    | 2380/4381 [1:03:23<53:16,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▍    | 2390/4381 [1:03:36<52:58,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▍    | 2390/4381 [1:03:36<52:58,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▍    | 2400/4381 [1:03:52<52:42,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▍    | 2400/4381 [1:03:52<52:42,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  55%|█████▌    | 2410/4381 [1:04:07<52:25,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  55%|█████▌    | 2410/4381 [1:04:07<52:25,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  55%|█████▌    | 2420/4381 [1:04:21<52:08,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  55%|█████▌    | 2420/4381 [1:04:21<52:08,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▌    | 2430/4381 [1:04:40<51:54,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  55%|█████▌    | 2430/4381 [1:04:40<51:54,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  56%|█████▌    | 2440/4381 [1:04:54<51:36,  1.60s/it, loss=2.88, v_num=641]Epoch 6:  56%|█████▌    | 2440/4381 [1:04:54<51:36,  1.60s/it, loss=2.91, v_num=641]Epoch 6:  56%|█████▌    | 2450/4381 [1:05:07<51:18,  1.59s/it, loss=2.91, v_num=641]Epoch 6:  56%|█████▌    | 2450/4381 [1:05:07<51:18,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  56%|█████▌    | 2460/4381 [1:05:26<51:04,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  56%|█████▌    | 2460/4381 [1:05:26<51:04,  1.60s/it, loss=2.79, v_num=641]Epoch 6:  56%|█████▋    | 2470/4381 [1:05:42<50:49,  1.60s/it, loss=2.79, v_num=641]Epoch 6:  56%|█████▋    | 2470/4381 [1:05:42<50:49,  1.60s/it, loss=2.78, v_num=641]Epoch 6:  57%|█████▋    | 2480/4381 [1:05:58<50:33,  1.60s/it, loss=2.78, v_num=641]Epoch 6:  57%|█████▋    | 2480/4381 [1:05:58<50:33,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  57%|█████▋    | 2490/4381 [1:06:12<50:15,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  57%|█████▋    | 2490/4381 [1:06:12<50:15,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  57%|█████▋    | 2500/4381 [1:06:28<49:59,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  57%|█████▋    | 2500/4381 [1:06:28<49:59,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  57%|█████▋    | 2510/4381 [1:06:48<49:46,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  57%|█████▋    | 2510/4381 [1:06:48<49:46,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  58%|█████▊    | 2520/4381 [1:07:03<49:30,  1.60s/it, loss=2.86, v_num=641]Epoch 6:  58%|█████▊    | 2520/4381 [1:07:03<49:30,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  58%|█████▊    | 2530/4381 [1:07:18<49:13,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  58%|█████▊    | 2530/4381 [1:07:18<49:13,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  58%|█████▊    | 2540/4381 [1:07:36<48:59,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  58%|█████▊    | 2540/4381 [1:07:36<48:59,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  58%|█████▊    | 2550/4381 [1:07:52<48:42,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  58%|█████▊    | 2550/4381 [1:07:52<48:42,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  59%|█████▊    | 2570/4381 [1:08:25<48:11,  1.60s/it, loss=2.82, v_num=641]Epoch 6:  59%|█████▊    | 2570/4381 [1:08:25<48:11,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  59%|█████▉    | 2580/4381 [1:08:40<47:55,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  59%|█████▉    | 2580/4381 [1:08:40<47:55,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  59%|█████▉    | 2590/4381 [1:08:55<47:38,  1.60s/it, loss=2.87, v_num=641]Epoch 6:  59%|█████▉    | 2590/4381 [1:08:55<47:38,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  59%|█████▉    | 2600/4381 [1:09:13<47:23,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  59%|█████▉    | 2600/4381 [1:09:13<47:23,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|█████▉    | 2610/4381 [1:09:27<47:06,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|█████▉    | 2610/4381 [1:09:27<47:06,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|█████▉    | 2620/4381 [1:09:42<46:50,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|█████▉    | 2620/4381 [1:09:42<46:50,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|██████    | 2630/4381 [1:10:01<46:35,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|██████    | 2630/4381 [1:10:01<46:35,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  60%|██████    | 2640/4381 [1:10:14<46:18,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  60%|██████    | 2640/4381 [1:10:14<46:18,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|██████    | 2650/4381 [1:10:29<46:01,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  60%|██████    | 2650/4381 [1:10:29<46:01,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████    | 2660/4381 [1:10:46<45:46,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████    | 2660/4381 [1:10:46<45:46,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████    | 2670/4381 [1:11:01<45:29,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████    | 2670/4381 [1:11:01<45:29,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  61%|██████    | 2680/4381 [1:11:18<45:14,  1.60s/it, loss=2.84, v_num=641]Epoch 6:  61%|██████    | 2680/4381 [1:11:18<45:14,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████▏   | 2690/4381 [1:11:36<44:59,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  61%|██████▏   | 2690/4381 [1:11:36<44:59,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  62%|██████▏   | 2700/4381 [1:11:51<44:43,  1.60s/it, loss=2.85, v_num=641]Epoch 6:  62%|██████▏   | 2700/4381 [1:11:51<44:43,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  62%|██████▏   | 2710/4381 [1:12:04<44:25,  1.60s/it, loss=2.83, v_num=641]Epoch 6:  62%|██████▏   | 2710/4381 [1:12:04<44:25,  1.60s/it, loss=2.81, v_num=641]Epoch 6:  62%|██████▏   | 2720/4381 [1:12:19<44:08,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  62%|██████▏   | 2720/4381 [1:12:19<44:08,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  62%|██████▏   | 2730/4381 [1:12:33<43:51,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  62%|██████▏   | 2730/4381 [1:12:33<43:51,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  63%|██████▎   | 2740/4381 [1:12:46<43:34,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  63%|██████▎   | 2740/4381 [1:12:46<43:34,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  63%|██████▎   | 2760/4381 [1:13:17<43:01,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  63%|██████▎   | 2760/4381 [1:13:17<43:01,  1.59s/it, loss=2.88, v_num=641]Epoch 6:  63%|██████▎   | 2770/4381 [1:13:33<42:45,  1.59s/it, loss=2.88, v_num=641]Epoch 6:  63%|██████▎   | 2770/4381 [1:13:33<42:45,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  63%|██████▎   | 2780/4381 [1:13:50<42:30,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  63%|██████▎   | 2780/4381 [1:13:50<42:30,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  64%|██████▎   | 2790/4381 [1:14:07<42:15,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  64%|██████▎   | 2790/4381 [1:14:07<42:15,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  64%|██████▍   | 2800/4381 [1:14:21<41:58,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  64%|██████▍   | 2800/4381 [1:14:21<41:58,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  64%|██████▍   | 2810/4381 [1:14:39<41:43,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  64%|██████▍   | 2810/4381 [1:14:39<41:43,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  64%|██████▍   | 2820/4381 [1:14:53<41:26,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  64%|██████▍   | 2820/4381 [1:14:53<41:26,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▍   | 2830/4381 [1:15:09<41:10,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▍   | 2830/4381 [1:15:09<41:10,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  65%|██████▍   | 2840/4381 [1:15:27<40:55,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  65%|██████▍   | 2840/4381 [1:15:27<40:55,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▌   | 2850/4381 [1:15:40<40:38,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▌   | 2850/4381 [1:15:40<40:38,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▌   | 2860/4381 [1:15:57<40:22,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  65%|██████▌   | 2860/4381 [1:15:57<40:22,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  66%|██████▌   | 2870/4381 [1:16:16<40:08,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  66%|██████▌   | 2870/4381 [1:16:16<40:08,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▌   | 2880/4381 [1:16:30<39:51,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▌   | 2880/4381 [1:16:30<39:51,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▌   | 2890/4381 [1:16:46<39:35,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▌   | 2890/4381 [1:16:46<39:35,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  66%|██████▌   | 2900/4381 [1:17:05<39:21,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  66%|██████▌   | 2900/4381 [1:17:05<39:21,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▋   | 2910/4381 [1:17:20<39:05,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  66%|██████▋   | 2910/4381 [1:17:20<39:05,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  67%|██████▋   | 2920/4381 [1:17:34<38:47,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  67%|██████▋   | 2920/4381 [1:17:34<38:47,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  67%|██████▋   | 2930/4381 [1:17:51<38:32,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  67%|██████▋   | 2930/4381 [1:17:51<38:32,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  67%|██████▋   | 2940/4381 [1:18:05<38:15,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  67%|██████▋   | 2940/4381 [1:18:05<38:15,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  67%|██████▋   | 2950/4381 [1:18:21<37:59,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  67%|██████▋   | 2950/4381 [1:18:21<37:59,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  68%|██████▊   | 2960/4381 [1:18:40<37:45,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  68%|██████▊   | 2960/4381 [1:18:40<37:45,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  68%|██████▊   | 2970/4381 [1:18:55<37:28,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  68%|██████▊   | 2970/4381 [1:18:55<37:28,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  68%|██████▊   | 2980/4381 [1:19:09<37:12,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  68%|██████▊   | 2980/4381 [1:19:09<37:12,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  68%|██████▊   | 2990/4381 [1:19:24<36:55,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  68%|██████▊   | 2990/4381 [1:19:24<36:55,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  68%|██████▊   | 3000/4381 [1:19:39<36:39,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  68%|██████▊   | 3000/4381 [1:19:39<36:39,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  69%|██████▊   | 3010/4381 [1:19:52<36:22,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  69%|██████▊   | 3010/4381 [1:19:52<36:22,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  69%|██████▉   | 3020/4381 [1:20:10<36:07,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  69%|██████▉   | 3020/4381 [1:20:10<36:07,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  69%|██████▉   | 3030/4381 [1:20:23<35:50,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  69%|██████▉   | 3030/4381 [1:20:23<35:50,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  69%|██████▉   | 3040/4381 [1:20:37<35:33,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  69%|██████▉   | 3040/4381 [1:20:37<35:33,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  70%|██████▉   | 3050/4381 [1:20:59<35:19,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  70%|██████▉   | 3050/4381 [1:20:59<35:19,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  70%|██████▉   | 3060/4381 [1:21:16<35:04,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  70%|██████▉   | 3060/4381 [1:21:16<35:04,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  70%|███████   | 3070/4381 [1:21:29<34:47,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  70%|███████   | 3070/4381 [1:21:29<34:47,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  70%|███████   | 3080/4381 [1:21:50<34:33,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  70%|███████   | 3080/4381 [1:21:50<34:33,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  71%|███████   | 3090/4381 [1:22:05<34:17,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  71%|███████   | 3090/4381 [1:22:05<34:17,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  71%|███████   | 3100/4381 [1:22:20<34:00,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  71%|███████   | 3100/4381 [1:22:20<34:00,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  71%|███████   | 3110/4381 [1:22:40<33:46,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  71%|███████   | 3110/4381 [1:22:40<33:46,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  71%|███████   | 3120/4381 [1:22:54<33:29,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  71%|███████   | 3120/4381 [1:22:54<33:29,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  71%|███████▏  | 3130/4381 [1:23:08<33:13,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  71%|███████▏  | 3130/4381 [1:23:08<33:13,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  72%|███████▏  | 3140/4381 [1:23:28<32:58,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  72%|███████▏  | 3140/4381 [1:23:28<32:58,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  72%|███████▏  | 3150/4381 [1:23:42<32:42,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  72%|███████▏  | 3150/4381 [1:23:42<32:42,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  72%|███████▏  | 3160/4381 [1:23:58<32:26,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  72%|███████▏  | 3160/4381 [1:23:58<32:26,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  72%|███████▏  | 3170/4381 [1:24:16<32:11,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  72%|███████▏  | 3170/4381 [1:24:16<32:11,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  73%|███████▎  | 3180/4381 [1:24:31<31:54,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  73%|███████▎  | 3180/4381 [1:24:31<31:54,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  73%|███████▎  | 3190/4381 [1:24:46<31:38,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  73%|███████▎  | 3190/4381 [1:24:46<31:38,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  73%|███████▎  | 3200/4381 [1:25:03<31:22,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  73%|███████▎  | 3200/4381 [1:25:03<31:22,  1.59s/it, loss=2.89, v_num=641]Epoch 6:  73%|███████▎  | 3210/4381 [1:25:17<31:06,  1.59s/it, loss=2.89, v_num=641]Epoch 6:  73%|███████▎  | 3210/4381 [1:25:17<31:06,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  73%|███████▎  | 3220/4381 [1:25:32<30:50,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  73%|███████▎  | 3220/4381 [1:25:32<30:50,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  74%|███████▎  | 3230/4381 [1:25:49<30:34,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  74%|███████▎  | 3230/4381 [1:25:49<30:34,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  74%|███████▍  | 3240/4381 [1:26:03<30:17,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  74%|███████▍  | 3240/4381 [1:26:03<30:17,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  74%|███████▍  | 3250/4381 [1:26:19<30:02,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  74%|███████▍  | 3250/4381 [1:26:19<30:02,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  74%|███████▍  | 3260/4381 [1:26:39<29:47,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  74%|███████▍  | 3260/4381 [1:26:39<29:47,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  75%|███████▍  | 3270/4381 [1:26:55<29:31,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  75%|███████▍  | 3270/4381 [1:26:55<29:31,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  75%|███████▍  | 3280/4381 [1:27:08<29:14,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  75%|███████▍  | 3280/4381 [1:27:08<29:14,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  75%|███████▌  | 3290/4381 [1:27:27<28:59,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  75%|███████▌  | 3290/4381 [1:27:27<28:59,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  75%|███████▌  | 3300/4381 [1:27:41<28:42,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  75%|███████▌  | 3300/4381 [1:27:41<28:42,  1.59s/it, loss=2.78, v_num=641]Epoch 6:  76%|███████▌  | 3310/4381 [1:27:55<28:26,  1.59s/it, loss=2.78, v_num=641]Epoch 6:  76%|███████▌  | 3310/4381 [1:27:55<28:26,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  76%|███████▌  | 3320/4381 [1:28:13<28:11,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  76%|███████▌  | 3320/4381 [1:28:13<28:11,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  76%|███████▌  | 3330/4381 [1:28:28<27:54,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  76%|███████▌  | 3330/4381 [1:28:28<27:54,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  76%|███████▌  | 3340/4381 [1:28:43<27:38,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  76%|███████▌  | 3340/4381 [1:28:43<27:38,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  76%|███████▋  | 3350/4381 [1:29:01<27:23,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  76%|███████▋  | 3350/4381 [1:29:01<27:23,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  77%|███████▋  | 3360/4381 [1:29:15<27:06,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  77%|███████▋  | 3360/4381 [1:29:15<27:06,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  77%|███████▋  | 3370/4381 [1:29:29<26:50,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  77%|███████▋  | 3370/4381 [1:29:29<26:50,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  77%|███████▋  | 3380/4381 [1:29:46<26:34,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  77%|███████▋  | 3380/4381 [1:29:46<26:34,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  77%|███████▋  | 3390/4381 [1:29:59<26:18,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  77%|███████▋  | 3390/4381 [1:29:59<26:18,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  78%|███████▊  | 3400/4381 [1:30:13<26:01,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  78%|███████▊  | 3400/4381 [1:30:13<26:01,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  78%|███████▊  | 3410/4381 [1:30:33<25:46,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  78%|███████▊  | 3410/4381 [1:30:33<25:46,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  78%|███████▊  | 3420/4381 [1:30:47<25:30,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  78%|███████▊  | 3420/4381 [1:30:47<25:30,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  78%|███████▊  | 3430/4381 [1:31:01<25:13,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  78%|███████▊  | 3430/4381 [1:31:01<25:13,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  79%|███████▊  | 3440/4381 [1:31:20<24:58,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  79%|███████▊  | 3440/4381 [1:31:20<24:58,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  79%|███████▊  | 3450/4381 [1:31:34<24:42,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  79%|███████▊  | 3450/4381 [1:31:34<24:42,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  79%|███████▉  | 3460/4381 [1:31:49<24:26,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  79%|███████▉  | 3460/4381 [1:31:49<24:26,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  79%|███████▉  | 3470/4381 [1:32:06<24:10,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  79%|███████▉  | 3470/4381 [1:32:06<24:10,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  79%|███████▉  | 3480/4381 [1:32:22<23:54,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  79%|███████▉  | 3480/4381 [1:32:22<23:54,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  80%|███████▉  | 3490/4381 [1:32:35<23:37,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  80%|███████▉  | 3490/4381 [1:32:35<23:37,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  80%|███████▉  | 3500/4381 [1:32:52<23:22,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  80%|███████▉  | 3500/4381 [1:32:52<23:22,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  80%|████████  | 3510/4381 [1:33:05<23:05,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  80%|████████  | 3510/4381 [1:33:05<23:05,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  80%|████████  | 3520/4381 [1:33:21<22:49,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  80%|████████  | 3520/4381 [1:33:21<22:49,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  81%|████████  | 3530/4381 [1:33:38<22:34,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  81%|████████  | 3530/4381 [1:33:38<22:34,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  81%|████████  | 3540/4381 [1:33:53<22:17,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  81%|████████  | 3540/4381 [1:33:53<22:17,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  81%|████████  | 3550/4381 [1:34:07<22:01,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  81%|████████  | 3550/4381 [1:34:07<22:01,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  81%|████████▏ | 3560/4381 [1:34:26<21:46,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  81%|████████▏ | 3560/4381 [1:34:26<21:46,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  81%|████████▏ | 3570/4381 [1:34:43<21:30,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  81%|████████▏ | 3570/4381 [1:34:43<21:30,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  82%|████████▏ | 3580/4381 [1:34:58<21:14,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  82%|████████▏ | 3580/4381 [1:34:58<21:14,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  82%|████████▏ | 3590/4381 [1:35:14<20:58,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  82%|████████▏ | 3590/4381 [1:35:14<20:58,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  82%|████████▏ | 3600/4381 [1:35:28<20:42,  1.59s/it, loss=2.87, v_num=641]Epoch 6:  82%|████████▏ | 3600/4381 [1:35:28<20:42,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  82%|████████▏ | 3610/4381 [1:35:43<20:26,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  82%|████████▏ | 3610/4381 [1:35:43<20:26,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3620/4381 [1:36:02<20:11,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3620/4381 [1:36:02<20:11,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3630/4381 [1:36:20<19:55,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3630/4381 [1:36:20<19:55,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  83%|████████▎ | 3640/4381 [1:36:33<19:39,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  83%|████████▎ | 3640/4381 [1:36:33<19:39,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3650/4381 [1:36:52<19:23,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  83%|████████▎ | 3650/4381 [1:36:52<19:23,  1.59s/it, loss=2.78, v_num=641]Epoch 6:  84%|████████▎ | 3660/4381 [1:37:08<19:07,  1.59s/it, loss=2.78, v_num=641]Epoch 6:  84%|████████▎ | 3660/4381 [1:37:08<19:07,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  84%|████████▍ | 3670/4381 [1:37:23<18:51,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  84%|████████▍ | 3670/4381 [1:37:23<18:51,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  84%|████████▍ | 3680/4381 [1:37:41<18:36,  1.59s/it, loss=2.81, v_num=641]Epoch 6:  84%|████████▍ | 3680/4381 [1:37:41<18:36,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  84%|████████▍ | 3690/4381 [1:37:55<18:19,  1.59s/it, loss=2.79, v_num=641]Epoch 6:  84%|████████▍ | 3690/4381 [1:37:55<18:19,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  84%|████████▍ | 3700/4381 [1:38:11<18:04,  1.59s/it, loss=2.85, v_num=641]Epoch 6:  84%|████████▍ | 3700/4381 [1:38:11<18:04,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  85%|████████▍ | 3710/4381 [1:38:29<17:48,  1.59s/it, loss=2.86, v_num=641]Epoch 6:  85%|████████▍ | 3710/4381 [1:38:29<17:48,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  85%|████████▍ | 3720/4381 [1:38:42<17:32,  1.59s/it, loss=2.84, v_num=641]Epoch 6:  85%|████████▍ | 3720/4381 [1:38:42<17:32,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  85%|████████▌ | 3730/4381 [1:38:55<17:15,  1.59s/it, loss=2.83, v_num=641]Epoch 6:  85%|████████▌ | 3730/4381 [1:38:55<17:15,  1.59s/it, loss=2.8, v_num=641] Epoch 6:  85%|████████▌ | 3740/4381 [1:38:58<16:57,  1.59s/it, loss=2.8, v_num=641]Epoch 6:  85%|████████▌ | 3740/4381 [1:38:58<16:57,  1.59s/it, loss=2.82, v_num=641]Epoch 6:  86%|████████▌ | 3750/4381 [1:39:01<16:39,  1.58s/it, loss=2.82, v_num=641]Epoch 6:  86%|████████▌ | 3750/4381 [1:39:01<16:39,  1.58s/it, loss=2.84, v_num=641]Epoch 6:  86%|████████▌ | 3760/4381 [1:39:02<16:21,  1.58s/it, loss=2.84, v_num=641]
Validating: 0it [00:00, ?it/s][Avalidation_epoch_end
graph acc: 0.14536741214057508
valid accuracy: 0.9424583911895752

Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.12779552715654952
valid accuracy: 0.9447780847549438
validation_epoch_end
graph acc: 0.14696485623003194
valid accuracy: 0.9465906620025635
validation_epoch_end
graph acc: 0.12939297124600638
valid accuracy: 0.9459190964698792
validation_epoch_end
graph acc: 0.1182108626198083
valid accuracy: 0.9411789178848267
validation_epoch_end
graph acc: 0.14057507987220447
valid accuracy: 0.9465744495391846
validation_epoch_end
graph acc: 0.14376996805111822
valid accuracy: 0.9413356781005859

Validating:   2%|▏         | 10/626 [00:04<04:51,  2.11it/s][AEpoch 6:  86%|████████▌ | 3770/4381 [1:39:07<16:03,  1.58s/it, loss=2.84, v_num=641]
Validating:   3%|▎         | 20/626 [00:06<02:47,  3.62it/s][AEpoch 6:  86%|████████▋ | 3780/4381 [1:39:08<15:45,  1.57s/it, loss=2.84, v_num=641]
Validating:   5%|▍         | 30/626 [00:09<02:51,  3.48it/s][AEpoch 6:  87%|████████▋ | 3790/4381 [1:39:11<15:27,  1.57s/it, loss=2.84, v_num=641]
Validating:   6%|▋         | 40/626 [00:10<02:23,  4.08it/s][AEpoch 6:  87%|████████▋ | 3800/4381 [1:39:13<15:10,  1.57s/it, loss=2.84, v_num=641]
Validating:   8%|▊         | 50/626 [00:12<02:03,  4.68it/s][AEpoch 6:  87%|████████▋ | 3810/4381 [1:39:15<14:52,  1.56s/it, loss=2.84, v_num=641]
Validating:  10%|▉         | 60/626 [00:14<01:54,  4.93it/s][AEpoch 6:  87%|████████▋ | 3820/4381 [1:39:16<14:34,  1.56s/it, loss=2.84, v_num=641]
Validating:  11%|█         | 70/626 [00:16<02:02,  4.52it/s][AEpoch 6:  87%|████████▋ | 3830/4381 [1:39:19<14:17,  1.56s/it, loss=2.84, v_num=641]
Validating:  13%|█▎        | 80/626 [00:18<01:44,  5.21it/s][AEpoch 6:  88%|████████▊ | 3840/4381 [1:39:20<13:59,  1.55s/it, loss=2.84, v_num=641]
Validating:  14%|█▍        | 90/626 [00:19<01:37,  5.47it/s][AEpoch 6:  88%|████████▊ | 3850/4381 [1:39:22<13:42,  1.55s/it, loss=2.84, v_num=641]
Validating:  16%|█▌        | 100/626 [00:22<01:56,  4.52it/s][AEpoch 6:  88%|████████▊ | 3860/4381 [1:39:25<13:24,  1.55s/it, loss=2.84, v_num=641]
Validating:  18%|█▊        | 110/626 [00:24<01:49,  4.72it/s][AEpoch 6:  88%|████████▊ | 3870/4381 [1:39:27<13:07,  1.54s/it, loss=2.84, v_num=641]
Validating:  19%|█▉        | 120/626 [00:27<01:49,  4.64it/s][AEpoch 6:  89%|████████▊ | 3880/4381 [1:39:29<12:50,  1.54s/it, loss=2.84, v_num=641]
Validating:  21%|██        | 130/626 [00:28<01:37,  5.10it/s][AEpoch 6:  89%|████████▉ | 3890/4381 [1:39:31<12:33,  1.53s/it, loss=2.84, v_num=641]
Validating:  22%|██▏       | 140/626 [00:31<01:45,  4.62it/s][AEpoch 6:  89%|████████▉ | 3900/4381 [1:39:33<12:16,  1.53s/it, loss=2.84, v_num=641]
Validating:  24%|██▍       | 150/626 [00:33<01:43,  4.58it/s][AEpoch 6:  89%|████████▉ | 3910/4381 [1:39:36<11:59,  1.53s/it, loss=2.84, v_num=641]
Validating:  26%|██▌       | 160/626 [00:35<01:34,  4.91it/s][AEpoch 6:  89%|████████▉ | 3920/4381 [1:39:37<11:42,  1.52s/it, loss=2.84, v_num=641]
Validating:  27%|██▋       | 170/626 [00:37<01:39,  4.58it/s][AEpoch 6:  90%|████████▉ | 3930/4381 [1:39:40<11:26,  1.52s/it, loss=2.84, v_num=641]
Validating:  29%|██▉       | 180/626 [00:39<01:36,  4.64it/s][AEpoch 6:  90%|████████▉ | 3940/4381 [1:39:42<11:09,  1.52s/it, loss=2.84, v_num=641]
Validating:  30%|███       | 190/626 [00:41<01:24,  5.16it/s][AEpoch 6:  90%|█████████ | 3950/4381 [1:39:43<10:52,  1.51s/it, loss=2.84, v_num=641]
Validating:  32%|███▏      | 200/626 [00:43<01:26,  4.90it/s][AEpoch 6:  90%|█████████ | 3960/4381 [1:39:46<10:36,  1.51s/it, loss=2.84, v_num=641]
Validating:  34%|███▎      | 210/626 [00:45<01:26,  4.79it/s][AEpoch 6:  91%|█████████ | 3970/4381 [1:39:48<10:19,  1.51s/it, loss=2.84, v_num=641]
Validating:  35%|███▌      | 220/626 [00:48<01:31,  4.45it/s][AEpoch 6:  91%|█████████ | 3980/4381 [1:39:50<10:03,  1.50s/it, loss=2.84, v_num=641]
Validating:  37%|███▋      | 230/626 [00:50<01:29,  4.44it/s][AEpoch 6:  91%|█████████ | 3990/4381 [1:39:53<09:47,  1.50s/it, loss=2.84, v_num=641]
Validating:  38%|███▊      | 240/626 [00:52<01:20,  4.80it/s][AEpoch 6:  91%|█████████▏| 4000/4381 [1:39:54<09:30,  1.50s/it, loss=2.84, v_num=641]
Validating:  40%|███▉      | 250/626 [00:54<01:22,  4.56it/s][AEpoch 6:  92%|█████████▏| 4010/4381 [1:39:57<09:14,  1.50s/it, loss=2.84, v_num=641]
Validating:  42%|████▏     | 260/626 [00:56<01:17,  4.72it/s][AEpoch 6:  92%|█████████▏| 4020/4381 [1:39:59<08:58,  1.49s/it, loss=2.84, v_num=641]
Validating:  43%|████▎     | 270/626 [00:57<01:06,  5.33it/s][AEpoch 6:  92%|█████████▏| 4030/4381 [1:40:00<08:42,  1.49s/it, loss=2.84, v_num=641]
Validating:  45%|████▍     | 280/626 [01:01<01:17,  4.44it/s][AEpoch 6:  92%|█████████▏| 4040/4381 [1:40:03<08:26,  1.49s/it, loss=2.84, v_num=641]
Validating:  46%|████▋     | 290/626 [01:03<01:15,  4.47it/s][AEpoch 6:  92%|█████████▏| 4050/4381 [1:40:05<08:10,  1.48s/it, loss=2.84, v_num=641]
Validating:  48%|████▊     | 300/626 [01:04<01:04,  5.03it/s][AEpoch 6:  93%|█████████▎| 4060/4381 [1:40:07<07:54,  1.48s/it, loss=2.84, v_num=641]
Validating:  50%|████▉     | 310/626 [01:05<00:55,  5.69it/s][AEpoch 6:  93%|█████████▎| 4070/4381 [1:40:08<07:39,  1.48s/it, loss=2.84, v_num=641]
Validating:  51%|█████     | 320/626 [01:08<00:57,  5.30it/s][AEpoch 6:  93%|█████████▎| 4080/4381 [1:40:10<07:23,  1.47s/it, loss=2.84, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:09<00:51,  5.77it/s][AEpoch 6:  93%|█████████▎| 4090/4381 [1:40:12<07:07,  1.47s/it, loss=2.84, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:11<00:49,  5.82it/s][AEpoch 6:  94%|█████████▎| 4100/4381 [1:40:13<06:52,  1.47s/it, loss=2.84, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:13<00:56,  4.91it/s][AEpoch 6:  94%|█████████▍| 4110/4381 [1:40:16<06:36,  1.46s/it, loss=2.84, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:15<00:53,  4.98it/s][AEpoch 6:  94%|█████████▍| 4120/4381 [1:40:18<06:21,  1.46s/it, loss=2.84, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:16<00:43,  5.91it/s][AEpoch 6:  94%|█████████▍| 4130/4381 [1:40:19<06:05,  1.46s/it, loss=2.84, v_num=641]
Validating:  61%|██████    | 380/626 [01:19<00:47,  5.22it/s][AEpoch 6:  94%|█████████▍| 4140/4381 [1:40:21<05:50,  1.45s/it, loss=2.84, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:20<00:38,  6.12it/s][AEpoch 6:  95%|█████████▍| 4150/4381 [1:40:22<05:35,  1.45s/it, loss=2.84, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:21<00:34,  6.58it/s][AEpoch 6:  95%|█████████▍| 4160/4381 [1:40:24<05:19,  1.45s/it, loss=2.84, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:22<00:30,  6.99it/s][AEpoch 6:  95%|█████████▌| 4170/4381 [1:40:25<05:04,  1.44s/it, loss=2.84, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:25<00:39,  5.21it/s][AEpoch 6:  95%|█████████▌| 4180/4381 [1:40:28<04:49,  1.44s/it, loss=2.84, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:27<00:36,  5.42it/s][AEpoch 6:  96%|█████████▌| 4190/4381 [1:40:30<04:34,  1.44s/it, loss=2.84, v_num=641]
Validating:  70%|███████   | 440/626 [01:29<00:33,  5.53it/s][AEpoch 6:  96%|█████████▌| 4200/4381 [1:40:31<04:19,  1.44s/it, loss=2.84, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:31<00:32,  5.34it/s][AEpoch 6:  96%|█████████▌| 4210/4381 [1:40:33<04:05,  1.43s/it, loss=2.84, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:32<00:30,  5.53it/s][AEpoch 6:  96%|█████████▋| 4220/4381 [1:40:35<03:50,  1.43s/it, loss=2.84, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:35<00:31,  5.00it/s][AEpoch 6:  97%|█████████▋| 4230/4381 [1:40:37<03:35,  1.43s/it, loss=2.84, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:37<00:28,  5.21it/s][AEpoch 6:  97%|█████████▋| 4240/4381 [1:40:39<03:20,  1.42s/it, loss=2.84, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:40<00:30,  4.44it/s][AEpoch 6:  97%|█████████▋| 4250/4381 [1:40:42<03:06,  1.42s/it, loss=2.84, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:41<00:23,  5.30it/s][AEpoch 6:  97%|█████████▋| 4260/4381 [1:40:43<02:51,  1.42s/it, loss=2.84, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:44<00:26,  4.46it/s][AEpoch 6:  97%|█████████▋| 4270/4381 [1:40:46<02:37,  1.42s/it, loss=2.84, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:45<00:21,  5.01it/s][AEpoch 6:  98%|█████████▊| 4280/4381 [1:40:48<02:22,  1.41s/it, loss=2.84, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:47<00:19,  4.92it/s][AEpoch 6:  98%|█████████▊| 4290/4381 [1:40:50<02:08,  1.41s/it, loss=2.84, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:49<00:17,  4.89it/s][AEpoch 6:  98%|█████████▊| 4300/4381 [1:40:52<01:53,  1.41s/it, loss=2.84, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:51<00:14,  5.24it/s][AEpoch 6:  98%|█████████▊| 4310/4381 [1:40:53<01:39,  1.40s/it, loss=2.84, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:52<00:11,  5.86it/s][AEpoch 6:  99%|█████████▊| 4320/4381 [1:40:55<01:25,  1.40s/it, loss=2.84, v_num=641]
Validating:  91%|█████████ | 570/626 [01:54<00:09,  6.14it/s][AEpoch 6:  99%|█████████▉| 4330/4381 [1:40:56<01:11,  1.40s/it, loss=2.84, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:56<00:08,  5.42it/s][AEpoch 6:  99%|█████████▉| 4340/4381 [1:40:59<00:57,  1.40s/it, loss=2.84, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:58<00:06,  5.60it/s][AEpoch 6:  99%|█████████▉| 4350/4381 [1:41:00<00:43,  1.39s/it, loss=2.84, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:58<00:03,  6.93it/s][AEpoch 6: 100%|█████████▉| 4360/4381 [1:41:01<00:29,  1.39s/it, loss=2.84, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:59<00:01,  8.46it/s][AEpoch 6: 100%|█████████▉| 4370/4381 [1:41:01<00:15,  1.39s/it, loss=2.84, v_num=641]
Validating:  99%|█████████▉| 620/626 [02:00<00:00,  8.70it/s][AEpoch 6: 100%|█████████▉| 4380/4381 [1:41:02<00:01,  1.38s/it, loss=2.84, v_num=641]
Validating: 100%|██████████| 626/626 [02:00<00:00, 10.25it/s][Avalidation_epoch_end
graph acc: 0.16134185303514376
valid accuracy: 0.9550043344497681
Epoch 6: 100%|██████████| 4381/4381 [1:41:04<00:00,  1.38s/it, loss=2.83, v_num=641]
                                                             [AEpoch 6:   0%|          | 0/4381 [00:00<00:00, 9754.20it/s, loss=2.83, v_num=641]   Epoch 7:   0%|          | 0/4381 [00:00<00:01, 2563.76it/s, loss=2.83, v_num=641]Epoch 7:   0%|          | 0/4381 [00:13<16:54:54, 13.90s/it, loss=2.83, v_num=641]Epoch 7:   0%|          | 10/4381 [00:21<2:22:09,  1.95s/it, loss=2.83, v_num=641]Epoch 7:   0%|          | 10/4381 [00:21<2:22:09,  1.95s/it, loss=2.8, v_num=641] Epoch 7:   0%|          | 20/4381 [00:38<2:13:22,  1.84s/it, loss=2.8, v_num=641]Epoch 7:   0%|          | 20/4381 [00:38<2:13:22,  1.84s/it, loss=2.78, v_num=641]Epoch 7:   1%|          | 30/4381 [01:00<2:22:21,  1.96s/it, loss=2.78, v_num=641]Epoch 7:   1%|          | 30/4381 [01:00<2:22:21,  1.96s/it, loss=2.78, v_num=641]Epoch 7:   1%|          | 40/4381 [01:15<2:13:53,  1.85s/it, loss=2.78, v_num=641]Epoch 7:   1%|          | 40/4381 [01:15<2:13:53,  1.85s/it, loss=2.8, v_num=641] Epoch 7:   1%|          | 50/4381 [01:33<2:11:48,  1.83s/it, loss=2.8, v_num=641]Epoch 7:   1%|          | 50/4381 [01:33<2:11:48,  1.83s/it, loss=2.78, v_num=641]Epoch 7:   1%|▏         | 60/4381 [01:47<2:06:45,  1.76s/it, loss=2.78, v_num=641]Epoch 7:   1%|▏         | 60/4381 [01:47<2:06:45,  1.76s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 70/4381 [02:04<2:06:04,  1.75s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 70/4381 [02:04<2:06:04,  1.75s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 80/4381 [02:21<2:05:31,  1.75s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 80/4381 [02:21<2:05:31,  1.75s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 90/4381 [02:37<2:03:36,  1.73s/it, loss=2.76, v_num=641]Epoch 7:   2%|▏         | 90/4381 [02:37<2:03:36,  1.73s/it, loss=2.77, v_num=641]Epoch 7:   2%|▏         | 100/4381 [02:53<2:02:33,  1.72s/it, loss=2.77, v_num=641]Epoch 7:   2%|▏         | 100/4381 [02:53<2:02:33,  1.72s/it, loss=2.76, v_num=641]Epoch 7:   3%|▎         | 110/4381 [03:09<2:01:29,  1.71s/it, loss=2.76, v_num=641]Epoch 7:   3%|▎         | 110/4381 [03:09<2:01:29,  1.71s/it, loss=2.77, v_num=641]Epoch 7:   3%|▎         | 120/4381 [03:26<2:01:24,  1.71s/it, loss=2.77, v_num=641]Epoch 7:   3%|▎         | 120/4381 [03:26<2:01:24,  1.71s/it, loss=2.79, v_num=641]Epoch 7:   3%|▎         | 130/4381 [03:40<1:59:27,  1.69s/it, loss=2.79, v_num=641]Epoch 7:   3%|▎         | 130/4381 [03:40<1:59:27,  1.69s/it, loss=2.78, v_num=641]Epoch 7:   3%|▎         | 140/4381 [03:55<1:58:05,  1.67s/it, loss=2.78, v_num=641]Epoch 7:   3%|▎         | 140/4381 [03:55<1:58:05,  1.67s/it, loss=2.79, v_num=641]Epoch 7:   3%|▎         | 150/4381 [04:12<1:57:59,  1.67s/it, loss=2.79, v_num=641]Epoch 7:   3%|▎         | 150/4381 [04:12<1:57:59,  1.67s/it, loss=2.79, v_num=641]Epoch 7:   4%|▎         | 160/4381 [04:26<1:56:32,  1.66s/it, loss=2.79, v_num=641]Epoch 7:   4%|▎         | 160/4381 [04:26<1:56:32,  1.66s/it, loss=2.78, v_num=641]Epoch 7:   4%|▍         | 170/4381 [04:44<1:56:52,  1.67s/it, loss=2.78, v_num=641]Epoch 7:   4%|▍         | 170/4381 [04:44<1:56:52,  1.67s/it, loss=2.81, v_num=641]Epoch 7:   4%|▍         | 180/4381 [04:59<1:56:02,  1.66s/it, loss=2.81, v_num=641]Epoch 7:   4%|▍         | 180/4381 [04:59<1:56:02,  1.66s/it, loss=2.83, v_num=641]Epoch 7:   4%|▍         | 190/4381 [05:16<1:55:49,  1.66s/it, loss=2.83, v_num=641]Epoch 7:   4%|▍         | 190/4381 [05:16<1:55:49,  1.66s/it, loss=2.8, v_num=641] Epoch 7:   5%|▍         | 200/4381 [05:37<1:56:55,  1.68s/it, loss=2.8, v_num=641]Epoch 7:   5%|▍         | 200/4381 [05:37<1:56:55,  1.68s/it, loss=2.78, v_num=641]Epoch 7:   5%|▍         | 210/4381 [05:51<1:55:56,  1.67s/it, loss=2.78, v_num=641]Epoch 7:   5%|▍         | 210/4381 [05:51<1:55:56,  1.67s/it, loss=2.77, v_num=641]Epoch 7:   5%|▌         | 220/4381 [06:08<1:55:30,  1.67s/it, loss=2.77, v_num=641]Epoch 7:   5%|▌         | 220/4381 [06:08<1:55:30,  1.67s/it, loss=2.76, v_num=641]Epoch 7:   5%|▌         | 230/4381 [06:26<1:55:53,  1.68s/it, loss=2.76, v_num=641]Epoch 7:   5%|▌         | 230/4381 [06:26<1:55:53,  1.68s/it, loss=2.77, v_num=641]Epoch 7:   5%|▌         | 240/4381 [06:42<1:55:24,  1.67s/it, loss=2.77, v_num=641]Epoch 7:   5%|▌         | 240/4381 [06:42<1:55:24,  1.67s/it, loss=2.75, v_num=641]Epoch 7:   6%|▌         | 250/4381 [06:57<1:54:37,  1.66s/it, loss=2.75, v_num=641]Epoch 7:   6%|▌         | 250/4381 [06:57<1:54:37,  1.66s/it, loss=2.76, v_num=641]Epoch 7:   6%|▌         | 260/4381 [07:16<1:54:53,  1.67s/it, loss=2.76, v_num=641]Epoch 7:   6%|▌         | 260/4381 [07:16<1:54:53,  1.67s/it, loss=2.81, v_num=641]Epoch 7:   6%|▌         | 270/4381 [07:31<1:54:15,  1.67s/it, loss=2.81, v_num=641]Epoch 7:   6%|▌         | 270/4381 [07:31<1:54:15,  1.67s/it, loss=2.75, v_num=641]Epoch 7:   6%|▋         | 280/4381 [07:48<1:54:02,  1.67s/it, loss=2.75, v_num=641]Epoch 7:   6%|▋         | 280/4381 [07:48<1:54:02,  1.67s/it, loss=2.73, v_num=641]Epoch 7:   7%|▋         | 290/4381 [08:07<1:54:10,  1.67s/it, loss=2.73, v_num=641]Epoch 7:   7%|▋         | 290/4381 [08:07<1:54:10,  1.67s/it, loss=2.76, v_num=641]Epoch 7:   7%|▋         | 300/4381 [08:21<1:53:17,  1.67s/it, loss=2.76, v_num=641]Epoch 7:   7%|▋         | 300/4381 [08:21<1:53:17,  1.67s/it, loss=2.78, v_num=641]Epoch 7:   7%|▋         | 310/4381 [08:37<1:52:52,  1.66s/it, loss=2.78, v_num=641]Epoch 7:   7%|▋         | 310/4381 [08:37<1:52:52,  1.66s/it, loss=2.75, v_num=641]Epoch 7:   7%|▋         | 320/4381 [08:56<1:53:01,  1.67s/it, loss=2.75, v_num=641]Epoch 7:   7%|▋         | 320/4381 [08:56<1:53:01,  1.67s/it, loss=2.74, v_num=641]Epoch 7:   8%|▊         | 330/4381 [09:10<1:52:12,  1.66s/it, loss=2.74, v_num=641]Epoch 7:   8%|▊         | 330/4381 [09:10<1:52:12,  1.66s/it, loss=2.79, v_num=641]Epoch 7:   8%|▊         | 340/4381 [09:25<1:51:36,  1.66s/it, loss=2.79, v_num=641]Epoch 7:   8%|▊         | 340/4381 [09:25<1:51:36,  1.66s/it, loss=2.79, v_num=641]Epoch 7:   8%|▊         | 350/4381 [09:43<1:51:44,  1.66s/it, loss=2.79, v_num=641]Epoch 7:   8%|▊         | 350/4381 [09:43<1:51:44,  1.66s/it, loss=2.76, v_num=641]Epoch 7:   8%|▊         | 360/4381 [09:57<1:50:56,  1.66s/it, loss=2.76, v_num=641]Epoch 7:   8%|▊         | 360/4381 [09:57<1:50:56,  1.66s/it, loss=2.75, v_num=641]Epoch 7:   8%|▊         | 370/4381 [10:11<1:50:06,  1.65s/it, loss=2.75, v_num=641]Epoch 7:   8%|▊         | 370/4381 [10:11<1:50:06,  1.65s/it, loss=2.77, v_num=641]Epoch 7:   9%|▊         | 380/4381 [10:29<1:50:07,  1.65s/it, loss=2.77, v_num=641]Epoch 7:   9%|▊         | 380/4381 [10:29<1:50:07,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 390/4381 [10:43<1:49:32,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 390/4381 [10:43<1:49:32,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 400/4381 [11:03<1:49:43,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 400/4381 [11:03<1:49:43,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 410/4381 [11:19<1:49:24,  1.65s/it, loss=2.76, v_num=641]Epoch 7:   9%|▉         | 410/4381 [11:19<1:49:24,  1.65s/it, loss=2.76, v_num=641]Epoch 7:  10%|▉         | 420/4381 [11:36<1:49:08,  1.65s/it, loss=2.76, v_num=641]Epoch 7:  10%|▉         | 420/4381 [11:36<1:49:08,  1.65s/it, loss=2.72, v_num=641]Epoch 7:  10%|▉         | 430/4381 [11:54<1:49:07,  1.66s/it, loss=2.72, v_num=641]Epoch 7:  10%|▉         | 430/4381 [11:54<1:49:07,  1.66s/it, loss=2.72, v_num=641]Epoch 7:  10%|█         | 440/4381 [12:08<1:48:32,  1.65s/it, loss=2.72, v_num=641]Epoch 7:  10%|█         | 440/4381 [12:08<1:48:32,  1.65s/it, loss=2.76, v_num=641]Epoch 7:  10%|█         | 450/4381 [12:23<1:47:59,  1.65s/it, loss=2.76, v_num=641]Epoch 7:  10%|█         | 450/4381 [12:23<1:47:59,  1.65s/it, loss=2.74, v_num=641]Epoch 7:  10%|█         | 460/4381 [12:40<1:47:50,  1.65s/it, loss=2.74, v_num=641]Epoch 7:  10%|█         | 460/4381 [12:40<1:47:50,  1.65s/it, loss=2.75, v_num=641]Epoch 7:  11%|█         | 470/4381 [12:57<1:47:35,  1.65s/it, loss=2.75, v_num=641]Epoch 7:  11%|█         | 470/4381 [12:57<1:47:35,  1.65s/it, loss=2.75, v_num=641]Epoch 7:  11%|█         | 480/4381 [13:10<1:46:51,  1.64s/it, loss=2.75, v_num=641]Epoch 7:  11%|█         | 480/4381 [13:10<1:46:51,  1.64s/it, loss=2.77, v_num=641]Epoch 7:  11%|█         | 490/4381 [13:28<1:46:46,  1.65s/it, loss=2.77, v_num=641]Epoch 7:  11%|█         | 490/4381 [13:28<1:46:46,  1.65s/it, loss=2.8, v_num=641] Epoch 7:  11%|█▏        | 500/4381 [13:46<1:46:42,  1.65s/it, loss=2.8, v_num=641]Epoch 7:  11%|█▏        | 500/4381 [13:46<1:46:42,  1.65s/it, loss=2.74, v_num=641]Epoch 7:  12%|█▏        | 510/4381 [13:59<1:46:00,  1.64s/it, loss=2.74, v_num=641]Epoch 7:  12%|█▏        | 510/4381 [13:59<1:46:00,  1.64s/it, loss=2.75, v_num=641]Epoch 7:  12%|█▏        | 520/4381 [14:16<1:45:46,  1.64s/it, loss=2.75, v_num=641]Epoch 7:  12%|█▏        | 520/4381 [14:16<1:45:46,  1.64s/it, loss=2.76, v_num=641]Epoch 7:  12%|█▏        | 530/4381 [14:30<1:45:10,  1.64s/it, loss=2.76, v_num=641]Epoch 7:  12%|█▏        | 530/4381 [14:30<1:45:10,  1.64s/it, loss=2.77, v_num=641]Epoch 7:  12%|█▏        | 540/4381 [14:43<1:44:32,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  12%|█▏        | 540/4381 [14:43<1:44:32,  1.63s/it, loss=2.79, v_num=641]Epoch 7:  13%|█▎        | 550/4381 [15:01<1:44:29,  1.64s/it, loss=2.79, v_num=641]Epoch 7:  13%|█▎        | 550/4381 [15:01<1:44:29,  1.64s/it, loss=2.79, v_num=641]Epoch 7:  13%|█▎        | 560/4381 [15:15<1:43:53,  1.63s/it, loss=2.79, v_num=641]Epoch 7:  13%|█▎        | 560/4381 [15:15<1:43:53,  1.63s/it, loss=2.8, v_num=641] Epoch 7:  13%|█▎        | 570/4381 [15:33<1:43:48,  1.63s/it, loss=2.8, v_num=641]Epoch 7:  13%|█▎        | 570/4381 [15:33<1:43:48,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  13%|█▎        | 580/4381 [15:49<1:43:34,  1.64s/it, loss=2.77, v_num=641]Epoch 7:  13%|█▎        | 580/4381 [15:49<1:43:34,  1.64s/it, loss=2.76, v_num=641]Epoch 7:  13%|█▎        | 590/4381 [16:05<1:43:12,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  13%|█▎        | 590/4381 [16:05<1:43:12,  1.63s/it, loss=2.74, v_num=641]Epoch 7:  14%|█▎        | 600/4381 [16:20<1:42:50,  1.63s/it, loss=2.74, v_num=641]Epoch 7:  14%|█▎        | 600/4381 [16:20<1:42:50,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  14%|█▍        | 610/4381 [16:35<1:42:24,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  14%|█▍        | 610/4381 [16:35<1:42:24,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  14%|█▍        | 620/4381 [16:52<1:42:12,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  14%|█▍        | 620/4381 [16:52<1:42:12,  1.63s/it, loss=2.8, v_num=641] Epoch 7:  14%|█▍        | 630/4381 [17:09<1:41:57,  1.63s/it, loss=2.8, v_num=641]Epoch 7:  14%|█▍        | 630/4381 [17:09<1:41:57,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  15%|█▍        | 640/4381 [17:24<1:41:34,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  15%|█▍        | 640/4381 [17:24<1:41:34,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  15%|█▍        | 650/4381 [17:43<1:41:33,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  15%|█▍        | 650/4381 [17:43<1:41:33,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  15%|█▌        | 660/4381 [17:57<1:41:05,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  15%|█▌        | 660/4381 [17:57<1:41:05,  1.63s/it, loss=2.8, v_num=641] Epoch 7:  15%|█▌        | 670/4381 [18:13<1:40:46,  1.63s/it, loss=2.8, v_num=641]Epoch 7:  15%|█▌        | 670/4381 [18:13<1:40:46,  1.63s/it, loss=2.79, v_num=641]Epoch 7:  16%|█▌        | 680/4381 [18:30<1:40:36,  1.63s/it, loss=2.79, v_num=641]Epoch 7:  16%|█▌        | 680/4381 [18:30<1:40:36,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  16%|█▌        | 690/4381 [18:46<1:40:15,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  16%|█▌        | 690/4381 [18:46<1:40:15,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  16%|█▌        | 700/4381 [19:03<1:40:03,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  16%|█▌        | 700/4381 [19:03<1:40:03,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  16%|█▌        | 710/4381 [19:18<1:39:43,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  16%|█▌        | 710/4381 [19:18<1:39:43,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  16%|█▋        | 720/4381 [19:35<1:39:30,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  16%|█▋        | 720/4381 [19:35<1:39:30,  1.63s/it, loss=2.74, v_num=641]Epoch 7:  17%|█▋        | 730/4381 [19:50<1:39:05,  1.63s/it, loss=2.74, v_num=641]Epoch 7:  17%|█▋        | 730/4381 [19:50<1:39:05,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  17%|█▋        | 740/4381 [20:12<1:39:17,  1.64s/it, loss=2.73, v_num=641]Epoch 7:  17%|█▋        | 740/4381 [20:12<1:39:17,  1.64s/it, loss=2.74, v_num=641]Epoch 7:  17%|█▋        | 750/4381 [20:25<1:38:44,  1.63s/it, loss=2.74, v_num=641]Epoch 7:  17%|█▋        | 750/4381 [20:25<1:38:44,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  17%|█▋        | 760/4381 [20:40<1:38:24,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  17%|█▋        | 760/4381 [20:40<1:38:24,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  18%|█▊        | 770/4381 [20:58<1:38:14,  1.63s/it, loss=2.77, v_num=641]Epoch 7:  18%|█▊        | 770/4381 [20:58<1:38:14,  1.63s/it, loss=2.8, v_num=641] Epoch 7:  18%|█▊        | 780/4381 [21:12<1:37:49,  1.63s/it, loss=2.8, v_num=641]Epoch 7:  18%|█▊        | 780/4381 [21:12<1:37:49,  1.63s/it, loss=2.82, v_num=641]Epoch 7:  18%|█▊        | 790/4381 [21:26<1:37:21,  1.63s/it, loss=2.82, v_num=641]Epoch 7:  18%|█▊        | 790/4381 [21:26<1:37:21,  1.63s/it, loss=2.81, v_num=641]Epoch 7:  18%|█▊        | 800/4381 [21:44<1:37:12,  1.63s/it, loss=2.81, v_num=641]Epoch 7:  18%|█▊        | 800/4381 [21:44<1:37:12,  1.63s/it, loss=2.78, v_num=641]Epoch 7:  18%|█▊        | 810/4381 [21:59<1:36:51,  1.63s/it, loss=2.78, v_num=641]Epoch 7:  18%|█▊        | 810/4381 [21:59<1:36:51,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  19%|█▊        | 820/4381 [22:15<1:36:33,  1.63s/it, loss=2.73, v_num=641]Epoch 7:  19%|█▊        | 820/4381 [22:15<1:36:33,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  19%|█▉        | 830/4381 [22:34<1:36:28,  1.63s/it, loss=2.76, v_num=641]Epoch 7:  19%|█▉        | 830/4381 [22:34<1:36:28,  1.63s/it, loss=2.78, v_num=641]Epoch 7:  19%|█▉        | 840/4381 [22:47<1:35:57,  1.63s/it, loss=2.78, v_num=641]Epoch 7:  19%|█▉        | 840/4381 [22:47<1:35:57,  1.63s/it, loss=2.78, v_num=641]Epoch 7:  19%|█▉        | 850/4381 [23:01<1:35:32,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  19%|█▉        | 850/4381 [23:01<1:35:32,  1.62s/it, loss=2.8, v_num=641] Epoch 7:  20%|█▉        | 860/4381 [23:19<1:35:21,  1.63s/it, loss=2.8, v_num=641]Epoch 7:  20%|█▉        | 860/4381 [23:19<1:35:21,  1.63s/it, loss=2.79, v_num=641]Epoch 7:  20%|█▉        | 870/4381 [23:33<1:34:57,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  20%|█▉        | 870/4381 [23:33<1:34:57,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  20%|██        | 880/4381 [23:47<1:34:32,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  20%|██        | 880/4381 [23:47<1:34:32,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  20%|██        | 890/4381 [24:04<1:34:18,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  20%|██        | 890/4381 [24:04<1:34:18,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 900/4381 [24:19<1:33:59,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 900/4381 [24:19<1:33:59,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 910/4381 [24:34<1:33:36,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 910/4381 [24:34<1:33:36,  1.62s/it, loss=2.81, v_num=641]Epoch 7:  21%|██        | 920/4381 [24:51<1:33:23,  1.62s/it, loss=2.81, v_num=641]Epoch 7:  21%|██        | 920/4381 [24:51<1:33:23,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 930/4381 [25:05<1:33:00,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██        | 930/4381 [25:05<1:33:00,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  21%|██▏       | 940/4381 [25:19<1:32:36,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  21%|██▏       | 940/4381 [25:19<1:32:36,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  22%|██▏       | 950/4381 [25:36<1:32:24,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  22%|██▏       | 950/4381 [25:36<1:32:24,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 960/4381 [25:55<1:32:18,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 960/4381 [25:55<1:32:18,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 970/4381 [26:11<1:32:00,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 970/4381 [26:11<1:32:00,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 980/4381 [26:25<1:31:35,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  22%|██▏       | 980/4381 [26:25<1:31:35,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  23%|██▎       | 990/4381 [26:42<1:31:24,  1.62s/it, loss=2.77, v_num=641]Epoch 7:  23%|██▎       | 990/4381 [26:42<1:31:24,  1.62s/it, loss=2.8, v_num=641] Epoch 7:  23%|██▎       | 1000/4381 [26:57<1:31:02,  1.62s/it, loss=2.8, v_num=641]Epoch 7:  23%|██▎       | 1000/4381 [26:57<1:31:02,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  23%|██▎       | 1010/4381 [27:14<1:30:50,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  23%|██▎       | 1010/4381 [27:14<1:30:50,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  23%|██▎       | 1020/4381 [27:30<1:30:34,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  23%|██▎       | 1020/4381 [27:30<1:30:34,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  24%|██▎       | 1030/4381 [27:44<1:30:11,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  24%|██▎       | 1030/4381 [27:44<1:30:11,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  24%|██▎       | 1040/4381 [27:58<1:29:46,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  24%|██▎       | 1040/4381 [27:58<1:29:46,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  24%|██▍       | 1050/4381 [28:17<1:29:38,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  24%|██▍       | 1050/4381 [28:17<1:29:38,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  24%|██▍       | 1060/4381 [28:31<1:29:17,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  24%|██▍       | 1060/4381 [28:31<1:29:17,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  24%|██▍       | 1070/4381 [28:49<1:29:07,  1.62s/it, loss=2.74, v_num=641]Epoch 7:  24%|██▍       | 1070/4381 [28:49<1:29:07,  1.62s/it, loss=2.79, v_num=641]Epoch 7:  25%|██▍       | 1080/4381 [29:05<1:28:48,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  25%|██▍       | 1080/4381 [29:05<1:28:48,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  25%|██▍       | 1090/4381 [29:20<1:28:29,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  25%|██▍       | 1090/4381 [29:20<1:28:29,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  25%|██▌       | 1100/4381 [29:37<1:28:17,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  25%|██▌       | 1100/4381 [29:37<1:28:17,  1.61s/it, loss=2.8, v_num=641] Epoch 7:  25%|██▌       | 1110/4381 [29:53<1:28:01,  1.61s/it, loss=2.8, v_num=641]Epoch 7:  25%|██▌       | 1110/4381 [29:53<1:28:01,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1120/4381 [30:11<1:27:49,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1120/4381 [30:11<1:27:49,  1.62s/it, loss=2.75, v_num=641]Epoch 7:  26%|██▌       | 1130/4381 [30:28<1:27:37,  1.62s/it, loss=2.75, v_num=641]Epoch 7:  26%|██▌       | 1130/4381 [30:28<1:27:37,  1.62s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1140/4381 [30:41<1:27:10,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1140/4381 [30:41<1:27:10,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1150/4381 [30:54<1:26:44,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  26%|██▌       | 1150/4381 [30:54<1:26:44,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  26%|██▋       | 1160/4381 [31:10<1:26:29,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  26%|██▋       | 1160/4381 [31:10<1:26:29,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  27%|██▋       | 1170/4381 [31:27<1:26:16,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  27%|██▋       | 1170/4381 [31:27<1:26:16,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  27%|██▋       | 1180/4381 [31:44<1:26:01,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  27%|██▋       | 1180/4381 [31:44<1:26:01,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  27%|██▋       | 1190/4381 [32:02<1:25:51,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  27%|██▋       | 1190/4381 [32:02<1:25:51,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  27%|██▋       | 1200/4381 [32:18<1:25:33,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  27%|██▋       | 1200/4381 [32:18<1:25:33,  1.61s/it, loss=2.85, v_num=641]Epoch 7:  28%|██▊       | 1210/4381 [32:32<1:25:13,  1.61s/it, loss=2.85, v_num=641]Epoch 7:  28%|██▊       | 1210/4381 [32:32<1:25:13,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  28%|██▊       | 1220/4381 [32:50<1:25:00,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  28%|██▊       | 1220/4381 [32:50<1:25:00,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  28%|██▊       | 1230/4381 [33:04<1:24:40,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  28%|██▊       | 1230/4381 [33:04<1:24:40,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  28%|██▊       | 1240/4381 [33:19<1:24:21,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  28%|██▊       | 1240/4381 [33:19<1:24:21,  1.61s/it, loss=2.75, v_num=641]Epoch 7:  29%|██▊       | 1250/4381 [33:36<1:24:07,  1.61s/it, loss=2.75, v_num=641]Epoch 7:  29%|██▊       | 1250/4381 [33:36<1:24:07,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  29%|██▉       | 1260/4381 [33:49<1:23:43,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  29%|██▉       | 1260/4381 [33:49<1:23:43,  1.61s/it, loss=2.8, v_num=641] Epoch 7:  29%|██▉       | 1270/4381 [34:03<1:23:22,  1.61s/it, loss=2.8, v_num=641]Epoch 7:  29%|██▉       | 1270/4381 [34:03<1:23:22,  1.61s/it, loss=2.84, v_num=641]Epoch 7:  29%|██▉       | 1280/4381 [34:21<1:23:09,  1.61s/it, loss=2.84, v_num=641]Epoch 7:  29%|██▉       | 1280/4381 [34:21<1:23:09,  1.61s/it, loss=2.8, v_num=641] Epoch 7:  29%|██▉       | 1290/4381 [34:36<1:22:51,  1.61s/it, loss=2.8, v_num=641]Epoch 7:  29%|██▉       | 1290/4381 [34:36<1:22:51,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  30%|██▉       | 1300/4381 [34:52<1:22:34,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  30%|██▉       | 1300/4381 [34:52<1:22:34,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  30%|██▉       | 1310/4381 [35:10<1:22:24,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  30%|██▉       | 1310/4381 [35:10<1:22:24,  1.61s/it, loss=2.84, v_num=641]Epoch 7:  30%|███       | 1320/4381 [35:25<1:22:04,  1.61s/it, loss=2.84, v_num=641]Epoch 7:  30%|███       | 1320/4381 [35:25<1:22:04,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  30%|███       | 1330/4381 [35:39<1:21:44,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  30%|███       | 1330/4381 [35:39<1:21:44,  1.61s/it, loss=2.73, v_num=641]Epoch 7:  31%|███       | 1340/4381 [35:56<1:21:30,  1.61s/it, loss=2.73, v_num=641]Epoch 7:  31%|███       | 1340/4381 [35:56<1:21:30,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  31%|███       | 1350/4381 [36:10<1:21:09,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  31%|███       | 1350/4381 [36:10<1:21:09,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  31%|███       | 1360/4381 [36:27<1:20:56,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  31%|███       | 1360/4381 [36:27<1:20:56,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  31%|███▏      | 1370/4381 [36:44<1:20:40,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  31%|███▏      | 1370/4381 [36:44<1:20:40,  1.61s/it, loss=2.75, v_num=641]Epoch 7:  31%|███▏      | 1380/4381 [36:57<1:20:17,  1.61s/it, loss=2.75, v_num=641]Epoch 7:  31%|███▏      | 1380/4381 [36:57<1:20:17,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  32%|███▏      | 1390/4381 [37:11<1:19:57,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  32%|███▏      | 1390/4381 [37:11<1:19:57,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  32%|███▏      | 1400/4381 [37:27<1:19:42,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  32%|███▏      | 1400/4381 [37:27<1:19:42,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  32%|███▏      | 1410/4381 [37:43<1:19:26,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  32%|███▏      | 1410/4381 [37:43<1:19:26,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  32%|███▏      | 1420/4381 [37:59<1:19:08,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  32%|███▏      | 1420/4381 [37:59<1:19:08,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  33%|███▎      | 1430/4381 [38:12<1:18:48,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  33%|███▎      | 1430/4381 [38:12<1:18:48,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  33%|███▎      | 1440/4381 [38:28<1:18:31,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  33%|███▎      | 1440/4381 [38:28<1:18:31,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  33%|███▎      | 1450/4381 [38:44<1:18:16,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  33%|███▎      | 1450/4381 [38:44<1:18:16,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  33%|███▎      | 1460/4381 [39:02<1:18:03,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  33%|███▎      | 1460/4381 [39:02<1:18:03,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  34%|███▎      | 1470/4381 [39:20<1:17:51,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  34%|███▎      | 1470/4381 [39:20<1:17:51,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  34%|███▍      | 1480/4381 [39:32<1:17:27,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  34%|███▍      | 1480/4381 [39:32<1:17:27,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  34%|███▍      | 1490/4381 [39:47<1:17:09,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  34%|███▍      | 1490/4381 [39:47<1:17:09,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  34%|███▍      | 1500/4381 [40:08<1:17:02,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  34%|███▍      | 1500/4381 [40:08<1:17:02,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  34%|███▍      | 1510/4381 [40:24<1:16:46,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  34%|███▍      | 1510/4381 [40:24<1:16:46,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  35%|███▍      | 1520/4381 [40:40<1:16:30,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  35%|███▍      | 1520/4381 [40:40<1:16:30,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  35%|███▍      | 1530/4381 [40:57<1:16:16,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  35%|███▍      | 1530/4381 [40:57<1:16:16,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  35%|███▌      | 1540/4381 [41:13<1:15:59,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  35%|███▌      | 1540/4381 [41:13<1:15:59,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  35%|███▌      | 1550/4381 [41:27<1:15:39,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  35%|███▌      | 1550/4381 [41:27<1:15:39,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  36%|███▌      | 1560/4381 [41:46<1:15:29,  1.61s/it, loss=2.73, v_num=641]Epoch 7:  36%|███▌      | 1560/4381 [41:46<1:15:29,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  36%|███▌      | 1570/4381 [42:01<1:15:12,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  36%|███▌      | 1570/4381 [42:01<1:15:12,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  36%|███▌      | 1580/4381 [42:16<1:14:54,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  36%|███▌      | 1580/4381 [42:16<1:14:54,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  36%|███▋      | 1590/4381 [42:36<1:14:43,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  36%|███▋      | 1590/4381 [42:36<1:14:43,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  37%|███▋      | 1600/4381 [42:49<1:14:24,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  37%|███▋      | 1600/4381 [42:49<1:14:24,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  37%|███▋      | 1610/4381 [43:06<1:14:08,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  37%|███▋      | 1610/4381 [43:06<1:14:08,  1.61s/it, loss=2.8, v_num=641] Epoch 7:  37%|███▋      | 1620/4381 [43:22<1:13:53,  1.61s/it, loss=2.8, v_num=641]Epoch 7:  37%|███▋      | 1620/4381 [43:22<1:13:53,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  37%|███▋      | 1630/4381 [43:40<1:13:40,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  37%|███▋      | 1630/4381 [43:40<1:13:40,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  37%|███▋      | 1640/4381 [43:54<1:13:21,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  37%|███▋      | 1640/4381 [43:54<1:13:21,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  38%|███▊      | 1650/4381 [44:12<1:13:08,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  38%|███▊      | 1650/4381 [44:12<1:13:08,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  38%|███▊      | 1660/4381 [44:27<1:12:50,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  38%|███▊      | 1660/4381 [44:27<1:12:50,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  38%|███▊      | 1670/4381 [44:43<1:12:33,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  38%|███▊      | 1670/4381 [44:43<1:12:33,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  38%|███▊      | 1680/4381 [44:59<1:12:18,  1.61s/it, loss=2.74, v_num=641]Epoch 7:  38%|███▊      | 1680/4381 [44:59<1:12:18,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  39%|███▊      | 1690/4381 [45:15<1:12:01,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  39%|███▊      | 1690/4381 [45:15<1:12:01,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  39%|███▉      | 1700/4381 [45:31<1:11:44,  1.61s/it, loss=2.82, v_num=641]Epoch 7:  39%|███▉      | 1700/4381 [45:31<1:11:44,  1.61s/it, loss=2.8, v_num=641] Epoch 7:  39%|███▉      | 1710/4381 [45:46<1:11:27,  1.61s/it, loss=2.8, v_num=641]Epoch 7:  39%|███▉      | 1710/4381 [45:46<1:11:27,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  39%|███▉      | 1720/4381 [46:01<1:11:09,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  39%|███▉      | 1720/4381 [46:01<1:11:09,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  39%|███▉      | 1730/4381 [46:19<1:10:56,  1.61s/it, loss=2.75, v_num=641]Epoch 7:  39%|███▉      | 1730/4381 [46:19<1:10:56,  1.61s/it, loss=2.73, v_num=641]Epoch 7:  40%|███▉      | 1740/4381 [46:32<1:10:36,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  40%|███▉      | 1740/4381 [46:32<1:10:36,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  40%|███▉      | 1750/4381 [46:49<1:10:21,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  40%|███▉      | 1750/4381 [46:49<1:10:21,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  40%|████      | 1760/4381 [47:06<1:10:07,  1.61s/it, loss=2.78, v_num=641]Epoch 7:  40%|████      | 1760/4381 [47:06<1:10:07,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  40%|████      | 1770/4381 [47:23<1:09:51,  1.61s/it, loss=2.76, v_num=641]Epoch 7:  40%|████      | 1770/4381 [47:23<1:09:51,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  41%|████      | 1780/4381 [47:38<1:09:34,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  41%|████      | 1780/4381 [47:38<1:09:34,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  41%|████      | 1790/4381 [47:53<1:09:16,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  41%|████      | 1790/4381 [47:53<1:09:16,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  41%|████      | 1800/4381 [48:10<1:09:02,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  41%|████      | 1800/4381 [48:10<1:09:02,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  41%|████▏     | 1810/4381 [48:26<1:08:46,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  41%|████▏     | 1810/4381 [48:26<1:08:46,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  42%|████▏     | 1820/4381 [48:39<1:08:26,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  42%|████▏     | 1820/4381 [48:39<1:08:26,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1830/4381 [48:59<1:08:14,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1830/4381 [48:59<1:08:14,  1.61s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1840/4381 [49:14<1:07:57,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1840/4381 [49:14<1:07:57,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1850/4381 [49:29<1:07:40,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  42%|████▏     | 1850/4381 [49:29<1:07:40,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  42%|████▏     | 1860/4381 [49:48<1:07:28,  1.61s/it, loss=2.81, v_num=641]Epoch 7:  42%|████▏     | 1860/4381 [49:48<1:07:28,  1.61s/it, loss=2.79, v_num=641]Epoch 7:  43%|████▎     | 1870/4381 [50:02<1:07:10,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  43%|████▎     | 1870/4381 [50:02<1:07:10,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  43%|████▎     | 1880/4381 [50:17<1:06:51,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  43%|████▎     | 1880/4381 [50:17<1:06:51,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  43%|████▎     | 1890/4381 [50:34<1:06:37,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  43%|████▎     | 1890/4381 [50:34<1:06:37,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  43%|████▎     | 1900/4381 [50:49<1:06:19,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  43%|████▎     | 1900/4381 [50:49<1:06:19,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  44%|████▎     | 1910/4381 [51:03<1:06:01,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  44%|████▎     | 1910/4381 [51:03<1:06:01,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  44%|████▍     | 1920/4381 [51:20<1:05:46,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  44%|████▍     | 1920/4381 [51:20<1:05:46,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  44%|████▍     | 1930/4381 [51:33<1:05:27,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  44%|████▍     | 1930/4381 [51:33<1:05:27,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  44%|████▍     | 1940/4381 [51:48<1:05:09,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  44%|████▍     | 1940/4381 [51:48<1:05:09,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  45%|████▍     | 1950/4381 [52:07<1:04:57,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  45%|████▍     | 1950/4381 [52:07<1:04:57,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  45%|████▍     | 1960/4381 [52:21<1:04:38,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  45%|████▍     | 1960/4381 [52:21<1:04:38,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  45%|████▍     | 1970/4381 [52:34<1:04:18,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  45%|████▍     | 1970/4381 [52:34<1:04:18,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  45%|████▌     | 1980/4381 [52:52<1:04:05,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  45%|████▌     | 1980/4381 [52:52<1:04:05,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  45%|████▌     | 1990/4381 [53:10<1:03:50,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  45%|████▌     | 1990/4381 [53:10<1:03:50,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  46%|████▌     | 2000/4381 [53:26<1:03:35,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  46%|████▌     | 2000/4381 [53:26<1:03:35,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  46%|████▌     | 2010/4381 [53:42<1:03:18,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  46%|████▌     | 2010/4381 [53:42<1:03:18,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  47%|████▋     | 2040/4381 [54:28<1:02:28,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  47%|████▋     | 2040/4381 [54:28<1:02:28,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  47%|████▋     | 2050/4381 [54:46<1:02:15,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  47%|████▋     | 2050/4381 [54:46<1:02:15,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  47%|████▋     | 2060/4381 [55:01<1:01:58,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  47%|████▋     | 2060/4381 [55:01<1:01:58,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  47%|████▋     | 2070/4381 [55:15<1:01:39,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  47%|████▋     | 2070/4381 [55:15<1:01:39,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  47%|████▋     | 2080/4381 [55:33<1:01:25,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  47%|████▋     | 2080/4381 [55:33<1:01:25,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  48%|████▊     | 2090/4381 [55:48<1:01:08,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  48%|████▊     | 2090/4381 [55:48<1:01:08,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  48%|████▊     | 2100/4381 [56:01<1:00:49,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  48%|████▊     | 2100/4381 [56:01<1:00:49,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  48%|████▊     | 2110/4381 [56:18<1:00:34,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  48%|████▊     | 2110/4381 [56:18<1:00:34,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  48%|████▊     | 2120/4381 [56:34<1:00:18,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  48%|████▊     | 2120/4381 [56:34<1:00:18,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  49%|████▊     | 2130/4381 [56:50<1:00:02,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  49%|████▊     | 2130/4381 [56:50<1:00:02,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  49%|████▉     | 2140/4381 [57:09<59:49,  1.60s/it, loss=2.78, v_num=641]  Epoch 7:  49%|████▉     | 2140/4381 [57:09<59:49,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  49%|████▉     | 2150/4381 [57:22<59:30,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  49%|████▉     | 2150/4381 [57:22<59:30,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  49%|████▉     | 2160/4381 [57:38<59:14,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  49%|████▉     | 2160/4381 [57:38<59:14,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  50%|████▉     | 2170/4381 [57:57<59:01,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  50%|████▉     | 2170/4381 [57:57<59:01,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  50%|████▉     | 2180/4381 [58:13<58:45,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  50%|████▉     | 2180/4381 [58:13<58:45,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  50%|████▉     | 2190/4381 [58:28<58:28,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  50%|████▉     | 2190/4381 [58:28<58:28,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  50%|█████     | 2200/4381 [58:45<58:13,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  50%|█████     | 2200/4381 [58:45<58:13,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  50%|█████     | 2210/4381 [58:58<57:54,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  50%|█████     | 2210/4381 [58:58<57:54,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  51%|█████     | 2220/4381 [59:11<57:35,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  51%|█████     | 2220/4381 [59:11<57:35,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  51%|█████     | 2230/4381 [59:31<57:22,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  51%|█████     | 2230/4381 [59:31<57:22,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  51%|█████     | 2240/4381 [59:46<57:06,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  51%|█████     | 2240/4381 [59:46<57:06,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  51%|█████▏    | 2250/4381 [1:00:01<56:49,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  51%|█████▏    | 2250/4381 [1:00:01<56:49,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  52%|█████▏    | 2260/4381 [1:00:18<56:34,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  52%|█████▏    | 2260/4381 [1:00:18<56:34,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  52%|█████▏    | 2270/4381 [1:00:33<56:17,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  52%|█████▏    | 2270/4381 [1:00:33<56:17,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  52%|█████▏    | 2280/4381 [1:00:47<55:59,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  52%|█████▏    | 2280/4381 [1:00:47<55:59,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  52%|█████▏    | 2290/4381 [1:01:05<55:45,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  52%|█████▏    | 2290/4381 [1:01:05<55:45,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  52%|█████▏    | 2300/4381 [1:01:18<55:26,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  52%|█████▏    | 2300/4381 [1:01:18<55:26,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  53%|█████▎    | 2310/4381 [1:01:32<55:09,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  53%|█████▎    | 2310/4381 [1:01:32<55:09,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  53%|█████▎    | 2320/4381 [1:01:48<54:53,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  53%|█████▎    | 2320/4381 [1:01:48<54:53,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  53%|█████▎    | 2330/4381 [1:02:03<54:36,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  53%|█████▎    | 2330/4381 [1:02:03<54:36,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  53%|█████▎    | 2340/4381 [1:02:20<54:21,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  53%|█████▎    | 2340/4381 [1:02:20<54:21,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  54%|█████▎    | 2350/4381 [1:02:39<54:07,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  54%|█████▎    | 2350/4381 [1:02:39<54:07,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  54%|█████▍    | 2360/4381 [1:02:52<53:49,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  54%|█████▍    | 2360/4381 [1:02:52<53:49,  1.60s/it, loss=2.7, v_num=641] Epoch 7:  54%|█████▍    | 2370/4381 [1:03:06<53:31,  1.60s/it, loss=2.7, v_num=641]Epoch 7:  54%|█████▍    | 2370/4381 [1:03:06<53:31,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  54%|█████▍    | 2380/4381 [1:03:21<53:14,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  54%|█████▍    | 2380/4381 [1:03:21<53:14,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  55%|█████▍    | 2390/4381 [1:03:37<52:59,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  55%|█████▍    | 2390/4381 [1:03:37<52:59,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  55%|█████▍    | 2400/4381 [1:03:54<52:43,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  55%|█████▍    | 2400/4381 [1:03:54<52:43,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  55%|█████▌    | 2410/4381 [1:04:13<52:30,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  55%|█████▌    | 2410/4381 [1:04:13<52:30,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  55%|█████▌    | 2420/4381 [1:04:29<52:14,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  55%|█████▌    | 2420/4381 [1:04:29<52:14,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  55%|█████▌    | 2430/4381 [1:04:44<51:57,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  55%|█████▌    | 2430/4381 [1:04:44<51:57,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  56%|█████▌    | 2440/4381 [1:05:00<51:41,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  56%|█████▌    | 2440/4381 [1:05:00<51:41,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  56%|█████▌    | 2450/4381 [1:05:18<51:26,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  56%|█████▌    | 2450/4381 [1:05:18<51:26,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  56%|█████▌    | 2460/4381 [1:05:32<51:09,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  56%|█████▌    | 2460/4381 [1:05:32<51:09,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  56%|█████▋    | 2470/4381 [1:05:50<50:55,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  56%|█████▋    | 2470/4381 [1:05:50<50:55,  1.60s/it, loss=2.83, v_num=641]Epoch 7:  57%|█████▋    | 2480/4381 [1:06:01<50:35,  1.60s/it, loss=2.83, v_num=641]Epoch 7:  57%|█████▋    | 2480/4381 [1:06:01<50:35,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  57%|█████▋    | 2490/4381 [1:06:14<50:17,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  57%|█████▋    | 2490/4381 [1:06:14<50:17,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  57%|█████▋    | 2500/4381 [1:06:33<50:03,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  57%|█████▋    | 2500/4381 [1:06:33<50:03,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  57%|█████▋    | 2510/4381 [1:06:46<49:45,  1.60s/it, loss=2.76, v_num=641]Epoch 7:  57%|█████▋    | 2510/4381 [1:06:46<49:45,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  58%|█████▊    | 2520/4381 [1:07:04<49:30,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  58%|█████▊    | 2520/4381 [1:07:04<49:30,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  58%|█████▊    | 2530/4381 [1:07:20<49:14,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  58%|█████▊    | 2530/4381 [1:07:20<49:14,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  58%|█████▊    | 2540/4381 [1:07:34<48:57,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  58%|█████▊    | 2540/4381 [1:07:34<48:57,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  58%|█████▊    | 2550/4381 [1:07:49<48:41,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  58%|█████▊    | 2550/4381 [1:07:49<48:41,  1.60s/it, loss=2.7, v_num=641] Epoch 7:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.7, v_num=641]Epoch 7:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  59%|█████▊    | 2570/4381 [1:08:23<48:10,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  59%|█████▊    | 2570/4381 [1:08:23<48:10,  1.60s/it, loss=2.8, v_num=641] Epoch 7:  59%|█████▉    | 2580/4381 [1:08:37<47:53,  1.60s/it, loss=2.8, v_num=641]Epoch 7:  59%|█████▉    | 2580/4381 [1:08:37<47:53,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  59%|█████▉    | 2590/4381 [1:08:55<47:38,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  59%|█████▉    | 2590/4381 [1:08:55<47:38,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  59%|█████▉    | 2600/4381 [1:09:10<47:21,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  59%|█████▉    | 2600/4381 [1:09:10<47:21,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  60%|█████▉    | 2610/4381 [1:09:25<47:05,  1.60s/it, loss=2.73, v_num=641]Epoch 7:  60%|█████▉    | 2610/4381 [1:09:25<47:05,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  60%|█████▉    | 2620/4381 [1:09:42<46:50,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  60%|█████▉    | 2620/4381 [1:09:42<46:50,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  60%|██████    | 2630/4381 [1:09:57<46:33,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  60%|██████    | 2630/4381 [1:09:57<46:33,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  60%|██████    | 2640/4381 [1:10:12<46:16,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  60%|██████    | 2640/4381 [1:10:12<46:16,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  60%|██████    | 2650/4381 [1:10:33<46:04,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  60%|██████    | 2650/4381 [1:10:33<46:04,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  61%|██████    | 2660/4381 [1:10:47<45:47,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  61%|██████    | 2660/4381 [1:10:47<45:47,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  61%|██████    | 2670/4381 [1:11:02<45:30,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  61%|██████    | 2670/4381 [1:11:02<45:30,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  61%|██████    | 2680/4381 [1:11:18<45:14,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  61%|██████    | 2680/4381 [1:11:18<45:14,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  61%|██████▏   | 2690/4381 [1:11:34<44:58,  1.60s/it, loss=2.81, v_num=641]Epoch 7:  61%|██████▏   | 2690/4381 [1:11:34<44:58,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  62%|██████▏   | 2700/4381 [1:11:49<44:41,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  62%|██████▏   | 2700/4381 [1:11:49<44:41,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  62%|██████▏   | 2710/4381 [1:12:08<44:28,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  62%|██████▏   | 2710/4381 [1:12:08<44:28,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  62%|██████▏   | 2720/4381 [1:12:24<44:11,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  62%|██████▏   | 2720/4381 [1:12:24<44:11,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  62%|██████▏   | 2730/4381 [1:12:37<43:54,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  62%|██████▏   | 2730/4381 [1:12:37<43:54,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  63%|██████▎   | 2740/4381 [1:12:55<43:39,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  63%|██████▎   | 2740/4381 [1:12:55<43:39,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  63%|██████▎   | 2750/4381 [1:13:10<43:23,  1.60s/it, loss=2.71, v_num=641]Epoch 7:  63%|██████▎   | 2750/4381 [1:13:10<43:23,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  63%|██████▎   | 2760/4381 [1:13:25<43:06,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  63%|██████▎   | 2760/4381 [1:13:25<43:06,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  63%|██████▎   | 2770/4381 [1:13:41<42:50,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  63%|██████▎   | 2770/4381 [1:13:41<42:50,  1.60s/it, loss=2.77, v_num=641]Epoch 7:  63%|██████▎   | 2780/4381 [1:13:55<42:33,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  63%|██████▎   | 2780/4381 [1:13:55<42:33,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  64%|██████▎   | 2790/4381 [1:14:11<42:17,  1.60s/it, loss=2.79, v_num=641]Epoch 7:  64%|██████▎   | 2790/4381 [1:14:11<42:17,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  64%|██████▍   | 2800/4381 [1:14:30<42:03,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  64%|██████▍   | 2800/4381 [1:14:30<42:03,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  64%|██████▍   | 2810/4381 [1:14:47<41:47,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  64%|██████▍   | 2810/4381 [1:14:47<41:47,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  64%|██████▍   | 2820/4381 [1:15:02<41:31,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  64%|██████▍   | 2820/4381 [1:15:03<41:31,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  65%|██████▍   | 2830/4381 [1:15:19<41:15,  1.60s/it, loss=2.72, v_num=641]Epoch 7:  65%|██████▍   | 2830/4381 [1:15:19<41:15,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  65%|██████▍   | 2840/4381 [1:15:32<40:58,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  65%|██████▍   | 2840/4381 [1:15:32<40:58,  1.60s/it, loss=2.78, v_num=641]Epoch 7:  65%|██████▌   | 2850/4381 [1:15:46<40:41,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  65%|██████▌   | 2850/4381 [1:15:46<40:41,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  65%|██████▌   | 2860/4381 [1:16:03<40:26,  1.60s/it, loss=2.75, v_num=641]Epoch 7:  65%|██████▌   | 2860/4381 [1:16:03<40:26,  1.60s/it, loss=2.74, v_num=641]Epoch 7:  66%|██████▌   | 2870/4381 [1:16:18<40:09,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  66%|██████▌   | 2870/4381 [1:16:18<40:09,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  66%|██████▌   | 2880/4381 [1:16:32<39:52,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  66%|██████▌   | 2880/4381 [1:16:32<39:52,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  66%|██████▌   | 2890/4381 [1:16:48<39:36,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  66%|██████▌   | 2890/4381 [1:16:48<39:36,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  66%|██████▌   | 2900/4381 [1:17:06<39:21,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  66%|██████▌   | 2900/4381 [1:17:06<39:21,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  66%|██████▋   | 2910/4381 [1:17:20<39:04,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  66%|██████▋   | 2910/4381 [1:17:20<39:04,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  67%|██████▋   | 2920/4381 [1:17:35<38:48,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  67%|██████▋   | 2920/4381 [1:17:35<38:48,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  67%|██████▋   | 2930/4381 [1:17:51<38:32,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  67%|██████▋   | 2930/4381 [1:17:51<38:32,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  67%|██████▋   | 2940/4381 [1:18:06<38:16,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  67%|██████▋   | 2940/4381 [1:18:06<38:16,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  67%|██████▋   | 2950/4381 [1:18:23<38:00,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  67%|██████▋   | 2950/4381 [1:18:23<38:00,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2960/4381 [1:18:38<37:44,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2960/4381 [1:18:38<37:44,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2970/4381 [1:18:52<37:27,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2970/4381 [1:18:52<37:27,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  68%|██████▊   | 2980/4381 [1:19:10<37:12,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  68%|██████▊   | 2980/4381 [1:19:10<37:12,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2990/4381 [1:19:25<36:56,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  68%|██████▊   | 2990/4381 [1:19:25<36:56,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  68%|██████▊   | 3000/4381 [1:19:40<36:39,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  68%|██████▊   | 3000/4381 [1:19:40<36:39,  1.59s/it, loss=2.8, v_num=641] Epoch 7:  69%|██████▊   | 3010/4381 [1:19:57<36:24,  1.59s/it, loss=2.8, v_num=641]Epoch 7:  69%|██████▊   | 3010/4381 [1:19:57<36:24,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  69%|██████▉   | 3020/4381 [1:20:11<36:07,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  69%|██████▉   | 3020/4381 [1:20:11<36:07,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  69%|██████▉   | 3030/4381 [1:20:27<35:51,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  69%|██████▉   | 3030/4381 [1:20:27<35:51,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  69%|██████▉   | 3040/4381 [1:20:47<35:37,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  69%|██████▉   | 3040/4381 [1:20:47<35:37,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  70%|██████▉   | 3050/4381 [1:21:03<35:21,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  70%|██████▉   | 3050/4381 [1:21:03<35:21,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  70%|██████▉   | 3060/4381 [1:21:18<35:05,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  70%|██████▉   | 3060/4381 [1:21:18<35:05,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  70%|███████   | 3070/4381 [1:21:37<34:50,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  70%|███████   | 3070/4381 [1:21:37<34:50,  1.59s/it, loss=2.8, v_num=641] Epoch 7:  70%|███████   | 3080/4381 [1:21:49<34:33,  1.59s/it, loss=2.8, v_num=641]Epoch 7:  70%|███████   | 3080/4381 [1:21:49<34:33,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  71%|███████   | 3090/4381 [1:22:04<34:16,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  71%|███████   | 3090/4381 [1:22:04<34:16,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  71%|███████   | 3100/4381 [1:22:20<34:01,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  71%|███████   | 3100/4381 [1:22:20<34:01,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  71%|███████   | 3110/4381 [1:22:36<33:45,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  71%|███████   | 3110/4381 [1:22:36<33:45,  1.59s/it, loss=2.8, v_num=641] Epoch 7:  71%|███████   | 3120/4381 [1:22:49<33:27,  1.59s/it, loss=2.8, v_num=641]Epoch 7:  71%|███████   | 3120/4381 [1:22:49<33:27,  1.59s/it, loss=2.81, v_num=641]Epoch 7:  71%|███████▏  | 3130/4381 [1:23:07<33:12,  1.59s/it, loss=2.81, v_num=641]Epoch 7:  71%|███████▏  | 3130/4381 [1:23:07<33:12,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  72%|███████▏  | 3140/4381 [1:23:21<32:56,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  72%|███████▏  | 3140/4381 [1:23:21<32:56,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  72%|███████▏  | 3150/4381 [1:23:39<32:40,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  72%|███████▏  | 3150/4381 [1:23:39<32:40,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  72%|███████▏  | 3160/4381 [1:23:56<32:25,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  72%|███████▏  | 3160/4381 [1:23:56<32:25,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  72%|███████▏  | 3170/4381 [1:24:11<32:09,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  72%|███████▏  | 3170/4381 [1:24:11<32:09,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  73%|███████▎  | 3180/4381 [1:24:26<31:52,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  73%|███████▎  | 3180/4381 [1:24:26<31:52,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  73%|███████▎  | 3190/4381 [1:24:42<31:37,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  73%|███████▎  | 3190/4381 [1:24:42<31:37,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  73%|███████▎  | 3200/4381 [1:24:55<31:20,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  73%|███████▎  | 3200/4381 [1:24:55<31:20,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  73%|███████▎  | 3210/4381 [1:25:11<31:04,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  73%|███████▎  | 3210/4381 [1:25:11<31:04,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  73%|███████▎  | 3220/4381 [1:25:28<30:48,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  73%|███████▎  | 3220/4381 [1:25:28<30:48,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  74%|███████▎  | 3230/4381 [1:25:41<30:31,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  74%|███████▎  | 3230/4381 [1:25:41<30:31,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  74%|███████▍  | 3240/4381 [1:25:59<30:16,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  74%|███████▍  | 3240/4381 [1:25:59<30:16,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  74%|███████▍  | 3250/4381 [1:26:18<30:01,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  74%|███████▍  | 3250/4381 [1:26:18<30:01,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  74%|███████▍  | 3260/4381 [1:26:31<29:44,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  74%|███████▍  | 3260/4381 [1:26:31<29:44,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  75%|███████▍  | 3270/4381 [1:26:42<29:27,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  75%|███████▍  | 3270/4381 [1:26:42<29:27,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  75%|███████▍  | 3280/4381 [1:27:05<29:13,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  75%|███████▍  | 3280/4381 [1:27:05<29:13,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  75%|███████▌  | 3290/4381 [1:27:18<28:56,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  75%|███████▌  | 3290/4381 [1:27:18<28:56,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  75%|███████▌  | 3300/4381 [1:27:34<28:40,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  75%|███████▌  | 3300/4381 [1:27:34<28:40,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▌  | 3310/4381 [1:27:53<28:25,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▌  | 3310/4381 [1:27:53<28:25,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  76%|███████▌  | 3320/4381 [1:28:08<28:09,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  76%|███████▌  | 3320/4381 [1:28:08<28:09,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  76%|███████▌  | 3330/4381 [1:28:21<27:52,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  76%|███████▌  | 3330/4381 [1:28:21<27:52,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▌  | 3340/4381 [1:28:39<27:37,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▌  | 3340/4381 [1:28:39<27:37,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▋  | 3350/4381 [1:28:56<27:21,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  76%|███████▋  | 3350/4381 [1:28:56<27:21,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  77%|███████▋  | 3360/4381 [1:29:11<27:05,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  77%|███████▋  | 3360/4381 [1:29:11<27:05,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  77%|███████▋  | 3370/4381 [1:29:29<26:50,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  77%|███████▋  | 3370/4381 [1:29:29<26:50,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  77%|███████▋  | 3380/4381 [1:29:43<26:33,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  77%|███████▋  | 3380/4381 [1:29:43<26:33,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  77%|███████▋  | 3390/4381 [1:29:58<26:17,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  77%|███████▋  | 3390/4381 [1:29:58<26:17,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  78%|███████▊  | 3400/4381 [1:30:17<26:02,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  78%|███████▊  | 3400/4381 [1:30:17<26:02,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  78%|███████▊  | 3410/4381 [1:30:30<25:45,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  78%|███████▊  | 3410/4381 [1:30:30<25:45,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  78%|███████▊  | 3420/4381 [1:30:44<25:29,  1.59s/it, loss=2.71, v_num=641]Epoch 7:  78%|███████▊  | 3420/4381 [1:30:44<25:29,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  78%|███████▊  | 3430/4381 [1:31:01<25:13,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  78%|███████▊  | 3430/4381 [1:31:01<25:13,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▊  | 3440/4381 [1:31:16<24:57,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▊  | 3440/4381 [1:31:16<24:57,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  79%|███████▊  | 3450/4381 [1:31:30<24:41,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  79%|███████▊  | 3450/4381 [1:31:30<24:41,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  79%|███████▉  | 3460/4381 [1:31:47<24:25,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  79%|███████▉  | 3460/4381 [1:31:47<24:25,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▉  | 3470/4381 [1:32:01<24:09,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▉  | 3470/4381 [1:32:01<24:09,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▉  | 3480/4381 [1:32:17<23:53,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  79%|███████▉  | 3480/4381 [1:32:17<23:53,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  80%|███████▉  | 3490/4381 [1:32:36<23:38,  1.59s/it, loss=2.72, v_num=641]Epoch 7:  80%|███████▉  | 3490/4381 [1:32:36<23:38,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  80%|███████▉  | 3500/4381 [1:32:53<23:22,  1.59s/it, loss=2.73, v_num=641]Epoch 7:  80%|███████▉  | 3500/4381 [1:32:53<23:22,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  80%|████████  | 3510/4381 [1:33:05<23:05,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  80%|████████  | 3510/4381 [1:33:05<23:05,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  80%|████████  | 3520/4381 [1:33:24<22:50,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  80%|████████  | 3520/4381 [1:33:24<22:50,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  81%|████████  | 3530/4381 [1:33:39<22:34,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  81%|████████  | 3530/4381 [1:33:39<22:34,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████  | 3540/4381 [1:33:52<22:17,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████  | 3540/4381 [1:33:52<22:17,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████  | 3550/4381 [1:34:10<22:02,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████  | 3550/4381 [1:34:10<22:02,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████▏ | 3560/4381 [1:34:23<21:45,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  81%|████████▏ | 3560/4381 [1:34:23<21:45,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  82%|████████▏ | 3580/4381 [1:34:55<21:14,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  82%|████████▏ | 3580/4381 [1:34:55<21:14,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  82%|████████▏ | 3590/4381 [1:35:13<20:58,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  82%|████████▏ | 3590/4381 [1:35:13<20:58,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  82%|████████▏ | 3600/4381 [1:35:26<20:42,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  82%|████████▏ | 3600/4381 [1:35:26<20:42,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  82%|████████▏ | 3610/4381 [1:35:46<20:26,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  82%|████████▏ | 3610/4381 [1:35:46<20:26,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  83%|████████▎ | 3620/4381 [1:36:01<20:10,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  83%|████████▎ | 3620/4381 [1:36:01<20:10,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  83%|████████▎ | 3630/4381 [1:36:17<19:55,  1.59s/it, loss=2.77, v_num=641]Epoch 7:  83%|████████▎ | 3630/4381 [1:36:17<19:55,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  83%|████████▎ | 3650/4381 [1:36:48<19:22,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  83%|████████▎ | 3650/4381 [1:36:48<19:22,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  84%|████████▎ | 3660/4381 [1:37:02<19:06,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  84%|████████▎ | 3660/4381 [1:37:02<19:06,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  84%|████████▍ | 3670/4381 [1:37:20<18:51,  1.59s/it, loss=2.78, v_num=641]Epoch 7:  84%|████████▍ | 3670/4381 [1:37:20<18:51,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  84%|████████▍ | 3680/4381 [1:37:37<18:35,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  84%|████████▍ | 3680/4381 [1:37:37<18:35,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  84%|████████▍ | 3690/4381 [1:37:50<18:19,  1.59s/it, loss=2.74, v_num=641]Epoch 7:  84%|████████▍ | 3690/4381 [1:37:50<18:19,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  84%|████████▍ | 3700/4381 [1:38:09<18:03,  1.59s/it, loss=2.76, v_num=641]Epoch 7:  84%|████████▍ | 3700/4381 [1:38:09<18:03,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  85%|████████▍ | 3720/4381 [1:38:38<17:31,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  85%|████████▍ | 3720/4381 [1:38:38<17:31,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  85%|████████▌ | 3730/4381 [1:38:51<17:14,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  85%|████████▌ | 3730/4381 [1:38:51<17:14,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.75, v_num=641]Epoch 7:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.79, v_num=641]Epoch 7:  86%|████████▌ | 3750/4381 [1:38:58<16:38,  1.58s/it, loss=2.79, v_num=641]Epoch 7:  86%|████████▌ | 3750/4381 [1:38:58<16:38,  1.58s/it, loss=2.79, v_num=641]Epoch 7:  86%|████████▌ | 3760/4381 [1:38:59<16:20,  1.58s/it, loss=2.79, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.1597444089456869
valid accuracy: 0.950640082359314
validation_epoch_end
graph acc: 0.21565495207667731
valid accuracy: 0.9509161710739136
validation_epoch_end
graph acc: 0.1869009584664537
valid accuracy: 0.9547871947288513
validation_epoch_end
graph acc: 0.16773162939297126
valid accuracy: 0.9502962827682495
validation_epoch_end
graph acc: 0.1805111821086262
valid accuracy: 0.9543731212615967
validation_epoch_end
graph acc: 0.16293929712460065
valid accuracy: 0.9529499411582947
validation_epoch_end
graph acc: 0.16134185303514376
valid accuracy: 0.9539897441864014

Validating:   2%|▏         | 10/626 [00:03<03:56,  2.61it/s][AEpoch 7:  86%|████████▌ | 3770/4381 [1:39:03<16:02,  1.58s/it, loss=2.79, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:22,  4.24it/s][AEpoch 7:  86%|████████▋ | 3780/4381 [1:39:04<15:44,  1.57s/it, loss=2.79, v_num=641]
Validating:   5%|▍         | 30/626 [00:07<02:27,  4.04it/s][AEpoch 7:  87%|████████▋ | 3790/4381 [1:39:07<15:27,  1.57s/it, loss=2.79, v_num=641]
Validating:   6%|▋         | 40/626 [00:09<02:10,  4.48it/s][AEpoch 7:  87%|████████▋ | 3800/4381 [1:39:09<15:09,  1.57s/it, loss=2.79, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:48,  5.30it/s][AEpoch 7:  87%|████████▋ | 3810/4381 [1:39:10<14:51,  1.56s/it, loss=2.79, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:48,  5.22it/s][AEpoch 7:  87%|████████▋ | 3820/4381 [1:39:12<14:33,  1.56s/it, loss=2.79, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:43,  5.36it/s][AEpoch 7:  87%|████████▋ | 3830/4381 [1:39:14<14:16,  1.55s/it, loss=2.79, v_num=641]
Validating:  13%|█▎        | 80/626 [00:16<01:33,  5.83it/s][AEpoch 7:  88%|████████▊ | 3840/4381 [1:39:15<13:58,  1.55s/it, loss=2.79, v_num=641]
Validating:  14%|█▍        | 90/626 [00:18<01:43,  5.17it/s][AEpoch 7:  88%|████████▊ | 3850/4381 [1:39:17<13:41,  1.55s/it, loss=2.79, v_num=641]
Validating:  16%|█▌        | 100/626 [00:19<01:33,  5.65it/s][AEpoch 7:  88%|████████▊ | 3860/4381 [1:39:19<13:24,  1.54s/it, loss=2.79, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:26,  5.98it/s][AEpoch 7:  88%|████████▊ | 3870/4381 [1:39:20<13:06,  1.54s/it, loss=2.79, v_num=641]
Validating:  19%|█▉        | 120/626 [00:22<01:20,  6.31it/s][AEpoch 7:  89%|████████▊ | 3880/4381 [1:39:22<12:49,  1.54s/it, loss=2.79, v_num=641]
Validating:  21%|██        | 130/626 [00:24<01:23,  5.95it/s][AEpoch 7:  89%|████████▉ | 3890/4381 [1:39:24<12:32,  1.53s/it, loss=2.79, v_num=641]
Validating:  22%|██▏       | 140/626 [00:26<01:28,  5.49it/s][AEpoch 7:  89%|████████▉ | 3900/4381 [1:39:26<12:15,  1.53s/it, loss=2.79, v_num=641]
Validating:  24%|██▍       | 150/626 [00:29<01:34,  5.05it/s][AEpoch 7:  89%|████████▉ | 3910/4381 [1:39:28<11:58,  1.53s/it, loss=2.79, v_num=641]
Validating:  26%|██▌       | 160/626 [00:30<01:29,  5.20it/s][AEpoch 7:  89%|████████▉ | 3920/4381 [1:39:30<11:41,  1.52s/it, loss=2.79, v_num=641]
Validating:  27%|██▋       | 170/626 [00:33<01:34,  4.84it/s][AEpoch 7:  90%|████████▉ | 3930/4381 [1:39:32<11:25,  1.52s/it, loss=2.79, v_num=641]
Validating:  29%|██▉       | 180/626 [00:36<01:45,  4.23it/s][AEpoch 7:  90%|████████▉ | 3940/4381 [1:39:35<11:08,  1.52s/it, loss=2.79, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:33,  4.65it/s][AEpoch 7:  90%|█████████ | 3950/4381 [1:39:37<10:52,  1.51s/it, loss=2.79, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:37,  4.36it/s][AEpoch 7:  90%|█████████ | 3960/4381 [1:39:40<10:35,  1.51s/it, loss=2.79, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:24,  4.94it/s][AEpoch 7:  91%|█████████ | 3970/4381 [1:39:41<10:19,  1.51s/it, loss=2.79, v_num=641]
Validating:  35%|███▌      | 220/626 [00:44<01:27,  4.65it/s][AEpoch 7:  91%|█████████ | 3980/4381 [1:39:43<10:02,  1.50s/it, loss=2.79, v_num=641]
Validating:  37%|███▋      | 230/626 [00:46<01:18,  5.05it/s][AEpoch 7:  91%|█████████ | 3990/4381 [1:39:45<09:46,  1.50s/it, loss=2.79, v_num=641]
Validating:  38%|███▊      | 240/626 [00:48<01:21,  4.75it/s][AEpoch 7:  91%|█████████▏| 4000/4381 [1:39:47<09:30,  1.50s/it, loss=2.79, v_num=641]
Validating:  40%|███▉      | 250/626 [00:50<01:18,  4.79it/s][AEpoch 7:  92%|█████████▏| 4010/4381 [1:39:49<09:14,  1.49s/it, loss=2.79, v_num=641]
Validating:  42%|████▏     | 260/626 [00:53<01:25,  4.29it/s][AEpoch 7:  92%|█████████▏| 4020/4381 [1:39:52<08:58,  1.49s/it, loss=2.79, v_num=641]
Validating:  43%|████▎     | 270/626 [00:55<01:16,  4.65it/s][AEpoch 7:  92%|█████████▏| 4030/4381 [1:39:54<08:41,  1.49s/it, loss=2.79, v_num=641]
Validating:  45%|████▍     | 280/626 [00:57<01:20,  4.28it/s][AEpoch 7:  92%|█████████▏| 4040/4381 [1:39:57<08:26,  1.48s/it, loss=2.79, v_num=641]
Validating:  46%|████▋     | 290/626 [00:59<01:11,  4.72it/s][AEpoch 7:  92%|█████████▏| 4050/4381 [1:39:58<08:10,  1.48s/it, loss=2.79, v_num=641]
Validating:  48%|████▊     | 300/626 [01:00<01:02,  5.19it/s][AEpoch 7:  93%|█████████▎| 4060/4381 [1:40:00<07:54,  1.48s/it, loss=2.79, v_num=641]
Validating:  50%|████▉     | 310/626 [01:02<00:56,  5.63it/s][AEpoch 7:  93%|█████████▎| 4070/4381 [1:40:01<07:38,  1.47s/it, loss=2.79, v_num=641]
Validating:  51%|█████     | 320/626 [01:04<00:55,  5.51it/s][AEpoch 7:  93%|█████████▎| 4080/4381 [1:40:03<07:22,  1.47s/it, loss=2.79, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:05<00:50,  5.81it/s][AEpoch 7:  93%|█████████▎| 4090/4381 [1:40:05<07:07,  1.47s/it, loss=2.79, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:07<00:49,  5.79it/s][AEpoch 7:  94%|█████████▎| 4100/4381 [1:40:07<06:51,  1.46s/it, loss=2.79, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:10<01:00,  4.57it/s][AEpoch 7:  94%|█████████▍| 4110/4381 [1:40:10<06:36,  1.46s/it, loss=2.79, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:12<00:56,  4.69it/s][AEpoch 7:  94%|█████████▍| 4120/4381 [1:40:12<06:20,  1.46s/it, loss=2.79, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:14<00:48,  5.27it/s][AEpoch 7:  94%|█████████▍| 4130/4381 [1:40:13<06:05,  1.46s/it, loss=2.79, v_num=641]
Validating:  61%|██████    | 380/626 [01:15<00:42,  5.75it/s][AEpoch 7:  94%|█████████▍| 4140/4381 [1:40:15<05:50,  1.45s/it, loss=2.79, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:16<00:38,  6.20it/s][AEpoch 7:  95%|█████████▍| 4150/4381 [1:40:16<05:34,  1.45s/it, loss=2.79, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:18<00:33,  6.76it/s][AEpoch 7:  95%|█████████▍| 4160/4381 [1:40:17<05:19,  1.45s/it, loss=2.79, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:19<00:32,  6.62it/s][AEpoch 7:  95%|█████████▌| 4170/4381 [1:40:19<05:04,  1.44s/it, loss=2.79, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:23<00:42,  4.80it/s][AEpoch 7:  95%|█████████▌| 4180/4381 [1:40:22<04:49,  1.44s/it, loss=2.79, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:24<00:38,  5.14it/s][AEpoch 7:  96%|█████████▌| 4190/4381 [1:40:24<04:34,  1.44s/it, loss=2.79, v_num=641]
Validating:  70%|███████   | 440/626 [01:26<00:36,  5.12it/s][AEpoch 7:  96%|█████████▌| 4200/4381 [1:40:26<04:19,  1.43s/it, loss=2.79, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:28<00:35,  5.02it/s][AEpoch 7:  96%|█████████▌| 4210/4381 [1:40:28<04:04,  1.43s/it, loss=2.79, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:30<00:31,  5.33it/s][AEpoch 7:  96%|█████████▋| 4220/4381 [1:40:29<03:49,  1.43s/it, loss=2.79, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:32<00:29,  5.26it/s][AEpoch 7:  97%|█████████▋| 4230/4381 [1:40:31<03:35,  1.43s/it, loss=2.79, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:34<00:27,  5.24it/s][AEpoch 7:  97%|█████████▋| 4240/4381 [1:40:33<03:20,  1.42s/it, loss=2.79, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:37<00:30,  4.52it/s][AEpoch 7:  97%|█████████▋| 4250/4381 [1:40:36<03:06,  1.42s/it, loss=2.79, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:38<00:24,  5.16it/s][AEpoch 7:  97%|█████████▋| 4260/4381 [1:40:37<02:51,  1.42s/it, loss=2.79, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:40<00:24,  4.76it/s][AEpoch 7:  97%|█████████▋| 4270/4381 [1:40:40<02:36,  1.41s/it, loss=2.79, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:42<00:19,  5.50it/s][AEpoch 7:  98%|█████████▊| 4280/4381 [1:40:41<02:22,  1.41s/it, loss=2.79, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:44<00:18,  5.22it/s][AEpoch 7:  98%|█████████▊| 4290/4381 [1:40:43<02:08,  1.41s/it, loss=2.79, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:45<00:16,  5.36it/s][AEpoch 7:  98%|█████████▊| 4300/4381 [1:40:45<01:53,  1.41s/it, loss=2.79, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:47<00:13,  5.62it/s][AEpoch 7:  98%|█████████▊| 4310/4381 [1:40:47<01:39,  1.40s/it, loss=2.79, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:49<00:11,  5.63it/s][AEpoch 7:  99%|█████████▊| 4320/4381 [1:40:48<01:25,  1.40s/it, loss=2.79, v_num=641]
Validating:  91%|█████████ | 570/626 [01:50<00:09,  6.12it/s][AEpoch 7:  99%|█████████▉| 4330/4381 [1:40:50<01:11,  1.40s/it, loss=2.79, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:53<00:08,  5.31it/s][AEpoch 7:  99%|█████████▉| 4340/4381 [1:40:52<00:57,  1.39s/it, loss=2.79, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:54<00:06,  5.62it/s][AEpoch 7:  99%|█████████▉| 4350/4381 [1:40:54<00:43,  1.39s/it, loss=2.79, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:55<00:04,  6.21it/s][AEpoch 7: 100%|█████████▉| 4360/4381 [1:40:55<00:29,  1.39s/it, loss=2.79, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:56<00:02,  7.22it/s][AEpoch 7: 100%|█████████▉| 4370/4381 [1:40:56<00:15,  1.39s/it, loss=2.79, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:58<00:00,  7.20it/s][AEpoch 7: 100%|█████████▉| 4380/4381 [1:40:57<00:01,  1.38s/it, loss=2.79, v_num=641]
Validating: 100%|██████████| 626/626 [01:58<00:00,  7.82it/s][Avalidation_epoch_end
graph acc: 0.22364217252396165
valid accuracy: 0.9599140882492065
Epoch 7: 100%|██████████| 4381/4381 [1:40:59<00:00,  1.38s/it, loss=2.82, v_num=641]
                                                             [AEpoch 7:   0%|          | 0/4381 [00:00<00:00, 10330.80it/s, loss=2.82, v_num=641]  Epoch 8:   0%|          | 0/4381 [00:00<00:01, 2809.31it/s, loss=2.82, v_num=641] Epoch 8:   0%|          | 0/4381 [00:13<16:35:14, 13.63s/it, loss=2.82, v_num=641]Epoch 8:   0%|          | 10/4381 [00:22<2:27:39,  2.03s/it, loss=2.82, v_num=641]Epoch 8:   0%|          | 10/4381 [00:22<2:27:39,  2.03s/it, loss=2.76, v_num=641]Epoch 8:   0%|          | 20/4381 [00:39<2:17:02,  1.89s/it, loss=2.76, v_num=641]Epoch 8:   0%|          | 20/4381 [00:39<2:17:02,  1.89s/it, loss=2.68, v_num=641]Epoch 8:   1%|          | 30/4381 [00:57<2:15:18,  1.87s/it, loss=2.68, v_num=641]Epoch 8:   1%|          | 30/4381 [00:57<2:15:19,  1.87s/it, loss=2.69, v_num=641]Epoch 8:   1%|          | 40/4381 [01:11<2:06:12,  1.74s/it, loss=2.69, v_num=641]Epoch 8:   1%|          | 40/4381 [01:11<2:06:12,  1.74s/it, loss=2.68, v_num=641]Epoch 8:   1%|          | 50/4381 [01:28<2:05:17,  1.74s/it, loss=2.68, v_num=641]Epoch 8:   1%|          | 50/4381 [01:28<2:05:17,  1.74s/it, loss=2.68, v_num=641]Epoch 8:   1%|▏         | 60/4381 [01:49<2:08:55,  1.79s/it, loss=2.68, v_num=641]Epoch 8:   1%|▏         | 60/4381 [01:49<2:08:55,  1.79s/it, loss=2.67, v_num=641]Epoch 8:   2%|▏         | 70/4381 [02:02<2:04:01,  1.73s/it, loss=2.67, v_num=641]Epoch 8:   2%|▏         | 70/4381 [02:02<2:04:01,  1.73s/it, loss=2.63, v_num=641]Epoch 8:   2%|▏         | 80/4381 [02:18<2:02:58,  1.72s/it, loss=2.63, v_num=641]Epoch 8:   2%|▏         | 80/4381 [02:18<2:02:58,  1.72s/it, loss=2.67, v_num=641]Epoch 8:   2%|▏         | 90/4381 [02:35<2:02:07,  1.71s/it, loss=2.67, v_num=641]Epoch 8:   2%|▏         | 90/4381 [02:35<2:02:07,  1.71s/it, loss=2.71, v_num=641]Epoch 8:   2%|▏         | 100/4381 [02:49<1:59:39,  1.68s/it, loss=2.71, v_num=641]Epoch 8:   2%|▏         | 100/4381 [02:49<1:59:39,  1.68s/it, loss=2.69, v_num=641]Epoch 8:   3%|▎         | 110/4381 [03:05<1:58:58,  1.67s/it, loss=2.69, v_num=641]Epoch 8:   3%|▎         | 110/4381 [03:05<1:58:58,  1.67s/it, loss=2.7, v_num=641] Epoch 8:   3%|▎         | 120/4381 [03:22<1:58:38,  1.67s/it, loss=2.7, v_num=641]Epoch 8:   3%|▎         | 120/4381 [03:22<1:58:38,  1.67s/it, loss=2.7, v_num=641]Epoch 8:   3%|▎         | 130/4381 [03:39<1:58:37,  1.67s/it, loss=2.7, v_num=641]Epoch 8:   3%|▎         | 130/4381 [03:39<1:58:37,  1.67s/it, loss=2.65, v_num=641]Epoch 8:   3%|▎         | 140/4381 [03:56<1:58:22,  1.67s/it, loss=2.65, v_num=641]Epoch 8:   3%|▎         | 140/4381 [03:56<1:58:22,  1.67s/it, loss=2.67, v_num=641]Epoch 8:   3%|▎         | 150/4381 [04:11<1:57:24,  1.67s/it, loss=2.67, v_num=641]Epoch 8:   3%|▎         | 150/4381 [04:11<1:57:24,  1.67s/it, loss=2.69, v_num=641]Epoch 8:   4%|▎         | 160/4381 [04:30<1:58:03,  1.68s/it, loss=2.69, v_num=641]Epoch 8:   4%|▎         | 160/4381 [04:30<1:58:03,  1.68s/it, loss=2.7, v_num=641] Epoch 8:   4%|▍         | 170/4381 [04:46<1:57:31,  1.67s/it, loss=2.7, v_num=641]Epoch 8:   4%|▍         | 170/4381 [04:46<1:57:31,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   4%|▍         | 180/4381 [05:03<1:57:28,  1.68s/it, loss=2.71, v_num=641]Epoch 8:   4%|▍         | 180/4381 [05:03<1:57:28,  1.68s/it, loss=2.71, v_num=641]Epoch 8:   4%|▍         | 190/4381 [05:19<1:56:55,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   4%|▍         | 190/4381 [05:19<1:56:55,  1.67s/it, loss=2.7, v_num=641] Epoch 8:   5%|▍         | 200/4381 [05:33<1:55:41,  1.66s/it, loss=2.7, v_num=641]Epoch 8:   5%|▍         | 200/4381 [05:33<1:55:41,  1.66s/it, loss=2.71, v_num=641]Epoch 8:   5%|▍         | 210/4381 [05:53<1:56:26,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   5%|▍         | 210/4381 [05:53<1:56:26,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   5%|▌         | 220/4381 [06:09<1:55:59,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   5%|▌         | 220/4381 [06:09<1:55:59,  1.67s/it, loss=2.69, v_num=641]Epoch 8:   5%|▌         | 230/4381 [06:24<1:55:13,  1.67s/it, loss=2.69, v_num=641]Epoch 8:   5%|▌         | 230/4381 [06:24<1:55:13,  1.67s/it, loss=2.71, v_num=641]Epoch 8:   5%|▌         | 240/4381 [06:40<1:54:41,  1.66s/it, loss=2.71, v_num=641]Epoch 8:   5%|▌         | 240/4381 [06:40<1:54:41,  1.66s/it, loss=2.71, v_num=641]Epoch 8:   6%|▌         | 250/4381 [06:56<1:54:15,  1.66s/it, loss=2.71, v_num=641]Epoch 8:   6%|▌         | 250/4381 [06:56<1:54:15,  1.66s/it, loss=2.71, v_num=641]Epoch 8:   6%|▌         | 260/4381 [07:10<1:53:25,  1.65s/it, loss=2.71, v_num=641]Epoch 8:   6%|▌         | 260/4381 [07:10<1:53:25,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   6%|▌         | 270/4381 [07:27<1:53:09,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   6%|▌         | 270/4381 [07:27<1:53:09,  1.65s/it, loss=2.72, v_num=641]Epoch 8:   6%|▋         | 280/4381 [07:43<1:52:48,  1.65s/it, loss=2.72, v_num=641]Epoch 8:   6%|▋         | 280/4381 [07:43<1:52:48,  1.65s/it, loss=2.68, v_num=641]Epoch 8:   7%|▋         | 290/4381 [07:59<1:52:25,  1.65s/it, loss=2.68, v_num=641]Epoch 8:   7%|▋         | 290/4381 [07:59<1:52:25,  1.65s/it, loss=2.69, v_num=641]Epoch 8:   7%|▋         | 300/4381 [08:18<1:52:33,  1.65s/it, loss=2.69, v_num=641]Epoch 8:   7%|▋         | 300/4381 [08:18<1:52:33,  1.65s/it, loss=2.69, v_num=641]Epoch 8:   7%|▋         | 310/4381 [08:34<1:52:09,  1.65s/it, loss=2.69, v_num=641]Epoch 8:   7%|▋         | 310/4381 [08:34<1:52:09,  1.65s/it, loss=2.7, v_num=641] Epoch 8:   7%|▋         | 320/4381 [08:49<1:51:38,  1.65s/it, loss=2.7, v_num=641]Epoch 8:   7%|▋         | 320/4381 [08:49<1:51:38,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   8%|▊         | 330/4381 [09:06<1:51:24,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   8%|▊         | 330/4381 [09:06<1:51:24,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   8%|▊         | 340/4381 [09:23<1:51:18,  1.65s/it, loss=2.73, v_num=641]Epoch 8:   8%|▊         | 340/4381 [09:23<1:51:18,  1.65s/it, loss=2.71, v_num=641]Epoch 8:   8%|▊         | 350/4381 [09:39<1:50:58,  1.65s/it, loss=2.71, v_num=641]Epoch 8:   8%|▊         | 350/4381 [09:39<1:50:58,  1.65s/it, loss=2.74, v_num=641]Epoch 8:   8%|▊         | 360/4381 [09:54<1:50:24,  1.65s/it, loss=2.74, v_num=641]Epoch 8:   8%|▊         | 360/4381 [09:54<1:50:24,  1.65s/it, loss=2.74, v_num=641]Epoch 8:   8%|▊         | 370/4381 [10:13<1:50:27,  1.65s/it, loss=2.74, v_num=641]Epoch 8:   8%|▊         | 370/4381 [10:13<1:50:27,  1.65s/it, loss=2.71, v_num=641]Epoch 8:   9%|▊         | 380/4381 [10:25<1:49:31,  1.64s/it, loss=2.71, v_num=641]Epoch 8:   9%|▊         | 380/4381 [10:25<1:49:31,  1.64s/it, loss=2.72, v_num=641]Epoch 8:   9%|▉         | 390/4381 [10:39<1:48:51,  1.64s/it, loss=2.72, v_num=641]Epoch 8:   9%|▉         | 390/4381 [10:39<1:48:51,  1.64s/it, loss=2.71, v_num=641]Epoch 8:   9%|▉         | 400/4381 [10:56<1:48:37,  1.64s/it, loss=2.71, v_num=641]Epoch 8:   9%|▉         | 400/4381 [10:56<1:48:37,  1.64s/it, loss=2.7, v_num=641] Epoch 8:   9%|▉         | 410/4381 [11:12<1:48:20,  1.64s/it, loss=2.7, v_num=641]Epoch 8:   9%|▉         | 410/4381 [11:12<1:48:20,  1.64s/it, loss=2.71, v_num=641]Epoch 8:  10%|▉         | 420/4381 [11:29<1:48:06,  1.64s/it, loss=2.71, v_num=641]Epoch 8:  10%|▉         | 420/4381 [11:29<1:48:06,  1.64s/it, loss=2.71, v_num=641]Epoch 8:  10%|▉         | 430/4381 [11:44<1:47:41,  1.64s/it, loss=2.71, v_num=641]Epoch 8:  10%|▉         | 430/4381 [11:44<1:47:41,  1.64s/it, loss=2.72, v_num=641]Epoch 8:  10%|█         | 440/4381 [12:02<1:47:37,  1.64s/it, loss=2.72, v_num=641]Epoch 8:  10%|█         | 440/4381 [12:02<1:47:37,  1.64s/it, loss=2.74, v_num=641]Epoch 8:  10%|█         | 450/4381 [12:19<1:47:26,  1.64s/it, loss=2.74, v_num=641]Epoch 8:  10%|█         | 450/4381 [12:19<1:47:26,  1.64s/it, loss=2.7, v_num=641] Epoch 8:  10%|█         | 460/4381 [12:32<1:46:39,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  10%|█         | 460/4381 [12:32<1:46:39,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  11%|█         | 470/4381 [12:48<1:46:23,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  11%|█         | 470/4381 [12:48<1:46:23,  1.63s/it, loss=2.69, v_num=641]Epoch 8:  11%|█         | 480/4381 [13:04<1:46:05,  1.63s/it, loss=2.69, v_num=641]Epoch 8:  11%|█         | 480/4381 [13:04<1:46:05,  1.63s/it, loss=2.73, v_num=641]Epoch 8:  11%|█         | 490/4381 [13:22<1:45:59,  1.63s/it, loss=2.73, v_num=641]Epoch 8:  11%|█         | 490/4381 [13:22<1:45:59,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  11%|█▏        | 500/4381 [13:39<1:45:46,  1.64s/it, loss=2.72, v_num=641]Epoch 8:  11%|█▏        | 500/4381 [13:39<1:45:46,  1.64s/it, loss=2.71, v_num=641]Epoch 8:  12%|█▏        | 510/4381 [13:54<1:45:18,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  12%|█▏        | 510/4381 [13:54<1:45:18,  1.63s/it, loss=2.7, v_num=641] Epoch 8:  12%|█▏        | 520/4381 [14:08<1:44:49,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  12%|█▏        | 520/4381 [14:08<1:44:49,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  12%|█▏        | 530/4381 [14:26<1:44:43,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  12%|█▏        | 530/4381 [14:26<1:44:43,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  12%|█▏        | 540/4381 [14:43<1:44:29,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  12%|█▏        | 540/4381 [14:43<1:44:29,  1.63s/it, loss=2.75, v_num=641]Epoch 8:  13%|█▎        | 550/4381 [14:58<1:44:03,  1.63s/it, loss=2.75, v_num=641]Epoch 8:  13%|█▎        | 550/4381 [14:58<1:44:03,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 560/4381 [15:14<1:43:46,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 560/4381 [15:14<1:43:46,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 570/4381 [15:28<1:43:18,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 570/4381 [15:28<1:43:18,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 580/4381 [15:49<1:43:33,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 580/4381 [15:49<1:43:33,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 590/4381 [16:05<1:43:10,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  13%|█▎        | 590/4381 [16:05<1:43:10,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  14%|█▎        | 600/4381 [16:19<1:42:43,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  14%|█▎        | 600/4381 [16:19<1:42:43,  1.63s/it, loss=2.7, v_num=641] Epoch 8:  14%|█▍        | 610/4381 [16:36<1:42:28,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  14%|█▍        | 610/4381 [16:36<1:42:28,  1.63s/it, loss=2.69, v_num=641]Epoch 8:  14%|█▍        | 620/4381 [16:49<1:41:55,  1.63s/it, loss=2.69, v_num=641]Epoch 8:  14%|█▍        | 620/4381 [16:49<1:41:55,  1.63s/it, loss=2.67, v_num=641]Epoch 8:  14%|█▍        | 630/4381 [17:03<1:41:23,  1.62s/it, loss=2.67, v_num=641]Epoch 8:  14%|█▍        | 630/4381 [17:03<1:41:23,  1.62s/it, loss=2.72, v_num=641]Epoch 8:  15%|█▍        | 640/4381 [17:22<1:41:24,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  15%|█▍        | 640/4381 [17:22<1:41:24,  1.63s/it, loss=2.7, v_num=641] Epoch 8:  15%|█▍        | 650/4381 [17:38<1:41:04,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  15%|█▍        | 650/4381 [17:38<1:41:04,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  15%|█▌        | 660/4381 [17:54<1:40:47,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  15%|█▌        | 660/4381 [17:54<1:40:47,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  15%|█▌        | 670/4381 [18:12<1:40:41,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  15%|█▌        | 670/4381 [18:12<1:40:41,  1.63s/it, loss=2.67, v_num=641]Epoch 8:  16%|█▌        | 680/4381 [18:27<1:40:17,  1.63s/it, loss=2.67, v_num=641]Epoch 8:  16%|█▌        | 680/4381 [18:27<1:40:17,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  16%|█▌        | 690/4381 [18:41<1:39:49,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  16%|█▌        | 690/4381 [18:41<1:39:49,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  16%|█▌        | 700/4381 [18:59<1:39:44,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  16%|█▌        | 700/4381 [18:59<1:39:44,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  16%|█▌        | 710/4381 [19:15<1:39:26,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  16%|█▌        | 710/4381 [19:15<1:39:26,  1.63s/it, loss=2.7, v_num=641] Epoch 8:  16%|█▋        | 720/4381 [19:31<1:39:10,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  16%|█▋        | 720/4381 [19:31<1:39:10,  1.63s/it, loss=2.67, v_num=641]Epoch 8:  17%|█▋        | 730/4381 [19:48<1:38:55,  1.63s/it, loss=2.67, v_num=641]Epoch 8:  17%|█▋        | 730/4381 [19:48<1:38:55,  1.63s/it, loss=2.68, v_num=641]Epoch 8:  17%|█▋        | 740/4381 [20:02<1:38:29,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  17%|█▋        | 740/4381 [20:02<1:38:29,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  17%|█▋        | 750/4381 [20:16<1:38:04,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  17%|█▋        | 750/4381 [20:16<1:38:04,  1.62s/it, loss=2.7, v_num=641] Epoch 8:  17%|█▋        | 760/4381 [20:39<1:38:20,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  17%|█▋        | 760/4381 [20:39<1:38:20,  1.63s/it, loss=2.73, v_num=641]Epoch 8:  18%|█▊        | 770/4381 [20:54<1:37:55,  1.63s/it, loss=2.73, v_num=641]Epoch 8:  18%|█▊        | 770/4381 [20:54<1:37:55,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  18%|█▊        | 780/4381 [21:10<1:37:36,  1.63s/it, loss=2.72, v_num=641]Epoch 8:  18%|█▊        | 780/4381 [21:10<1:37:36,  1.63s/it, loss=2.7, v_num=641] Epoch 8:  18%|█▊        | 790/4381 [21:27<1:37:23,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  18%|█▊        | 790/4381 [21:27<1:37:23,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  18%|█▊        | 800/4381 [21:42<1:37:02,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  18%|█▊        | 800/4381 [21:42<1:37:02,  1.63s/it, loss=2.74, v_num=641]Epoch 8:  18%|█▊        | 810/4381 [21:56<1:36:36,  1.62s/it, loss=2.74, v_num=641]Epoch 8:  18%|█▊        | 810/4381 [21:56<1:36:36,  1.62s/it, loss=2.73, v_num=641]Epoch 8:  19%|█▊        | 820/4381 [22:15<1:36:33,  1.63s/it, loss=2.73, v_num=641]Epoch 8:  19%|█▊        | 820/4381 [22:15<1:36:33,  1.63s/it, loss=2.71, v_num=641]Epoch 8:  19%|█▉        | 830/4381 [22:29<1:36:07,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  19%|█▉        | 830/4381 [22:29<1:36:07,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  19%|█▉        | 840/4381 [22:42<1:35:38,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  19%|█▉        | 840/4381 [22:42<1:35:38,  1.62s/it, loss=2.73, v_num=641]Epoch 8:  19%|█▉        | 850/4381 [23:02<1:35:35,  1.62s/it, loss=2.73, v_num=641]Epoch 8:  19%|█▉        | 850/4381 [23:02<1:35:35,  1.62s/it, loss=2.76, v_num=641]Epoch 8:  20%|█▉        | 860/4381 [23:15<1:35:05,  1.62s/it, loss=2.76, v_num=641]Epoch 8:  20%|█▉        | 860/4381 [23:15<1:35:05,  1.62s/it, loss=2.75, v_num=641]Epoch 8:  20%|█▉        | 870/4381 [23:30<1:34:46,  1.62s/it, loss=2.75, v_num=641]Epoch 8:  20%|█▉        | 870/4381 [23:30<1:34:46,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  20%|██        | 880/4381 [23:46<1:34:28,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  20%|██        | 880/4381 [23:46<1:34:28,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  20%|██        | 890/4381 [24:04<1:34:21,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  20%|██        | 890/4381 [24:04<1:34:21,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  21%|██        | 900/4381 [24:20<1:34:04,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  21%|██        | 900/4381 [24:20<1:34:04,  1.62s/it, loss=2.7, v_num=641] Epoch 8:  21%|██        | 910/4381 [24:40<1:34:00,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  21%|██        | 910/4381 [24:40<1:34:00,  1.63s/it, loss=2.7, v_num=641]Epoch 8:  21%|██        | 920/4381 [24:54<1:33:38,  1.62s/it, loss=2.7, v_num=641]Epoch 8:  21%|██        | 920/4381 [24:54<1:33:38,  1.62s/it, loss=2.72, v_num=641]Epoch 8:  21%|██        | 930/4381 [25:08<1:33:11,  1.62s/it, loss=2.72, v_num=641]Epoch 8:  21%|██        | 930/4381 [25:08<1:33:11,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  21%|██▏       | 940/4381 [25:24<1:32:54,  1.62s/it, loss=2.71, v_num=641]Epoch 8:  21%|██▏       | 940/4381 [25:24<1:32:54,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  22%|██▏       | 950/4381 [25:38<1:32:31,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  22%|██▏       | 950/4381 [25:38<1:32:31,  1.62s/it, loss=2.72, v_num=641]Epoch 8:  22%|██▏       | 960/4381 [25:54<1:32:14,  1.62s/it, loss=2.72, v_num=641]Epoch 8:  22%|██▏       | 960/4381 [25:54<1:32:14,  1.62s/it, loss=2.74, v_num=641]Epoch 8:  22%|██▏       | 970/4381 [26:09<1:31:55,  1.62s/it, loss=2.74, v_num=641]Epoch 8:  22%|██▏       | 970/4381 [26:09<1:31:55,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  22%|██▏       | 980/4381 [26:23<1:31:30,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  22%|██▏       | 980/4381 [26:23<1:31:30,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  23%|██▎       | 990/4381 [26:39<1:31:12,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  23%|██▎       | 990/4381 [26:39<1:31:12,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  23%|██▎       | 1000/4381 [26:56<1:30:58,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  23%|██▎       | 1000/4381 [26:56<1:30:58,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  23%|██▎       | 1010/4381 [27:13<1:30:48,  1.62s/it, loss=2.69, v_num=641]Epoch 8:  23%|██▎       | 1010/4381 [27:13<1:30:48,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  23%|██▎       | 1020/4381 [27:29<1:30:31,  1.62s/it, loss=2.68, v_num=641]Epoch 8:  23%|██▎       | 1020/4381 [27:29<1:30:31,  1.62s/it, loss=2.65, v_num=641]Epoch 8:  24%|██▎       | 1030/4381 [27:48<1:30:21,  1.62s/it, loss=2.65, v_num=641]Epoch 8:  24%|██▎       | 1030/4381 [27:48<1:30:21,  1.62s/it, loss=2.7, v_num=641] Epoch 8:  24%|██▎       | 1040/4381 [28:00<1:29:55,  1.61s/it, loss=2.7, v_num=641]Epoch 8:  24%|██▎       | 1040/4381 [28:01<1:29:55,  1.61s/it, loss=2.79, v_num=641]Epoch 8:  24%|██▍       | 1050/4381 [28:15<1:29:32,  1.61s/it, loss=2.79, v_num=641]Epoch 8:  24%|██▍       | 1050/4381 [28:15<1:29:32,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  24%|██▍       | 1060/4381 [28:33<1:29:24,  1.62s/it, loss=2.73, v_num=641]Epoch 8:  24%|██▍       | 1060/4381 [28:33<1:29:24,  1.62s/it, loss=2.66, v_num=641]Epoch 8:  24%|██▍       | 1070/4381 [28:48<1:29:04,  1.61s/it, loss=2.66, v_num=641]Epoch 8:  24%|██▍       | 1070/4381 [28:48<1:29:04,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  25%|██▍       | 1080/4381 [29:04<1:28:47,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  25%|██▍       | 1080/4381 [29:04<1:28:47,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  25%|██▍       | 1090/4381 [29:19<1:28:28,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  25%|██▍       | 1090/4381 [29:19<1:28:28,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  25%|██▌       | 1100/4381 [29:36<1:28:14,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  25%|██▌       | 1100/4381 [29:36<1:28:14,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  25%|██▌       | 1110/4381 [29:50<1:27:50,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  25%|██▌       | 1110/4381 [29:50<1:27:50,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  26%|██▌       | 1120/4381 [30:07<1:27:38,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  26%|██▌       | 1120/4381 [30:07<1:27:38,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  26%|██▌       | 1130/4381 [30:23<1:27:20,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  26%|██▌       | 1130/4381 [30:23<1:27:20,  1.61s/it, loss=2.76, v_num=641]Epoch 8:  26%|██▌       | 1140/4381 [30:37<1:26:59,  1.61s/it, loss=2.76, v_num=641]Epoch 8:  26%|██▌       | 1140/4381 [30:37<1:26:59,  1.61s/it, loss=2.76, v_num=641]Epoch 8:  26%|██▌       | 1150/4381 [30:56<1:26:51,  1.61s/it, loss=2.76, v_num=641]Epoch 8:  26%|██▌       | 1150/4381 [30:56<1:26:51,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  26%|██▋       | 1160/4381 [31:12<1:26:34,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  26%|██▋       | 1160/4381 [31:12<1:26:34,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1170/4381 [31:28<1:26:17,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1170/4381 [31:28<1:26:17,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1180/4381 [31:47<1:26:09,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1180/4381 [31:47<1:26:09,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1190/4381 [32:02<1:25:51,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  27%|██▋       | 1190/4381 [32:02<1:25:51,  1.61s/it, loss=2.75, v_num=641]Epoch 8:  27%|██▋       | 1200/4381 [32:17<1:25:30,  1.61s/it, loss=2.75, v_num=641]Epoch 8:  27%|██▋       | 1200/4381 [32:17<1:25:30,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  28%|██▊       | 1210/4381 [32:32<1:25:12,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  28%|██▊       | 1210/4381 [32:32<1:25:12,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  28%|██▊       | 1220/4381 [32:46<1:24:50,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  28%|██▊       | 1220/4381 [32:46<1:24:50,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  28%|██▊       | 1230/4381 [33:03<1:24:36,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  28%|██▊       | 1230/4381 [33:03<1:24:36,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  28%|██▊       | 1240/4381 [33:17<1:24:16,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  28%|██▊       | 1240/4381 [33:17<1:24:16,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  29%|██▊       | 1250/4381 [33:38<1:24:10,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  29%|██▊       | 1250/4381 [33:38<1:24:10,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  29%|██▉       | 1260/4381 [33:52<1:23:50,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  29%|██▉       | 1260/4381 [33:52<1:23:50,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  29%|██▉       | 1270/4381 [34:08<1:23:33,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  29%|██▉       | 1270/4381 [34:08<1:23:33,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  29%|██▉       | 1280/4381 [34:24<1:23:18,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  29%|██▉       | 1280/4381 [34:24<1:23:18,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  29%|██▉       | 1290/4381 [34:41<1:23:02,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  29%|██▉       | 1290/4381 [34:41<1:23:02,  1.61s/it, loss=2.7, v_num=641] Epoch 8:  30%|██▉       | 1300/4381 [34:56<1:22:45,  1.61s/it, loss=2.7, v_num=641]Epoch 8:  30%|██▉       | 1300/4381 [34:56<1:22:45,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  30%|██▉       | 1310/4381 [35:12<1:22:29,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  30%|██▉       | 1310/4381 [35:12<1:22:29,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  30%|███       | 1320/4381 [35:27<1:22:09,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  30%|███       | 1320/4381 [35:27<1:22:09,  1.61s/it, loss=2.64, v_num=641]Epoch 8:  30%|███       | 1330/4381 [35:43<1:21:54,  1.61s/it, loss=2.64, v_num=641]Epoch 8:  30%|███       | 1330/4381 [35:43<1:21:54,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  31%|███       | 1340/4381 [36:04<1:21:47,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  31%|███       | 1340/4381 [36:04<1:21:47,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  31%|███       | 1350/4381 [36:19<1:21:29,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  31%|███       | 1350/4381 [36:19<1:21:29,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  31%|███       | 1360/4381 [36:33<1:21:07,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  31%|███       | 1360/4381 [36:33<1:21:07,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  31%|███▏      | 1370/4381 [36:50<1:20:54,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  31%|███▏      | 1370/4381 [36:50<1:20:54,  1.61s/it, loss=2.7, v_num=641] Epoch 8:  31%|███▏      | 1380/4381 [37:03<1:20:32,  1.61s/it, loss=2.7, v_num=641]Epoch 8:  31%|███▏      | 1380/4381 [37:03<1:20:32,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  32%|███▏      | 1390/4381 [37:18<1:20:13,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  32%|███▏      | 1390/4381 [37:18<1:20:13,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  32%|███▏      | 1400/4381 [37:37<1:20:04,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  32%|███▏      | 1400/4381 [37:37<1:20:04,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  32%|███▏      | 1410/4381 [37:54<1:19:48,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  32%|███▏      | 1410/4381 [37:54<1:19:48,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  32%|███▏      | 1420/4381 [38:10<1:19:33,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  32%|███▏      | 1420/4381 [38:10<1:19:33,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  33%|███▎      | 1430/4381 [38:28<1:19:21,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  33%|███▎      | 1430/4381 [38:28<1:19:21,  1.61s/it, loss=2.7, v_num=641] Epoch 8:  33%|███▎      | 1440/4381 [38:41<1:18:58,  1.61s/it, loss=2.7, v_num=641]Epoch 8:  33%|███▎      | 1440/4381 [38:41<1:18:58,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  33%|███▎      | 1450/4381 [38:56<1:18:38,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  33%|███▎      | 1450/4381 [38:56<1:18:38,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  33%|███▎      | 1460/4381 [39:13<1:18:24,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  33%|███▎      | 1460/4381 [39:13<1:18:24,  1.61s/it, loss=2.75, v_num=641]Epoch 8:  34%|███▎      | 1470/4381 [39:30<1:18:10,  1.61s/it, loss=2.75, v_num=641]Epoch 8:  34%|███▎      | 1470/4381 [39:30<1:18:10,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  34%|███▍      | 1480/4381 [39:45<1:17:52,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  34%|███▍      | 1480/4381 [39:45<1:17:52,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  34%|███▍      | 1490/4381 [40:03<1:17:39,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  34%|███▍      | 1490/4381 [40:03<1:17:39,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  34%|███▍      | 1500/4381 [40:17<1:17:20,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  34%|███▍      | 1500/4381 [40:17<1:17:20,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  34%|███▍      | 1510/4381 [40:33<1:17:04,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  34%|███▍      | 1510/4381 [40:33<1:17:04,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  35%|███▍      | 1520/4381 [40:50<1:16:49,  1.61s/it, loss=2.69, v_num=641]Epoch 8:  35%|███▍      | 1520/4381 [40:50<1:16:49,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  35%|███▍      | 1530/4381 [41:05<1:16:31,  1.61s/it, loss=2.73, v_num=641]Epoch 8:  35%|███▍      | 1530/4381 [41:05<1:16:31,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  35%|███▌      | 1540/4381 [41:19<1:16:10,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  35%|███▌      | 1540/4381 [41:19<1:16:10,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  35%|███▌      | 1550/4381 [41:35<1:15:55,  1.61s/it, loss=2.68, v_num=641]Epoch 8:  35%|███▌      | 1550/4381 [41:35<1:15:55,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▌      | 1560/4381 [41:48<1:15:33,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▌      | 1560/4381 [41:48<1:15:33,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▌      | 1570/4381 [42:04<1:15:17,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▌      | 1570/4381 [42:04<1:15:17,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  36%|███▌      | 1580/4381 [42:21<1:15:01,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  36%|███▌      | 1580/4381 [42:21<1:15:01,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▋      | 1590/4381 [42:35<1:14:43,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  36%|███▋      | 1590/4381 [42:35<1:14:43,  1.61s/it, loss=2.7, v_num=641] Epoch 8:  37%|███▋      | 1600/4381 [42:50<1:14:24,  1.61s/it, loss=2.7, v_num=641]Epoch 8:  37%|███▋      | 1600/4381 [42:50<1:14:24,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  37%|███▋      | 1610/4381 [43:07<1:14:09,  1.61s/it, loss=2.74, v_num=641]Epoch 8:  37%|███▋      | 1610/4381 [43:07<1:14:09,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  37%|███▋      | 1620/4381 [43:22<1:13:52,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  37%|███▋      | 1620/4381 [43:22<1:13:52,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  37%|███▋      | 1630/4381 [43:38<1:13:36,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  37%|███▋      | 1630/4381 [43:38<1:13:36,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  37%|███▋      | 1640/4381 [43:54<1:13:20,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  37%|███▋      | 1640/4381 [43:54<1:13:20,  1.61s/it, loss=2.71, v_num=641]Epoch 8:  38%|███▊      | 1650/4381 [44:09<1:13:01,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  38%|███▊      | 1650/4381 [44:09<1:13:01,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  38%|███▊      | 1660/4381 [44:24<1:12:44,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  38%|███▊      | 1660/4381 [44:24<1:12:44,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  38%|███▊      | 1670/4381 [44:43<1:12:33,  1.61s/it, loss=2.72, v_num=641]Epoch 8:  38%|███▊      | 1670/4381 [44:43<1:12:33,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  38%|███▊      | 1680/4381 [44:58<1:12:15,  1.61s/it, loss=2.67, v_num=641]Epoch 8:  38%|███▊      | 1680/4381 [44:58<1:12:15,  1.61s/it, loss=2.7, v_num=641] Epoch 8:  39%|███▊      | 1690/4381 [45:12<1:11:55,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  39%|███▊      | 1690/4381 [45:12<1:11:55,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  39%|███▉      | 1700/4381 [45:29<1:11:41,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  39%|███▉      | 1700/4381 [45:29<1:11:41,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  39%|███▉      | 1710/4381 [45:43<1:11:22,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  39%|███▉      | 1710/4381 [45:43<1:11:22,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  39%|███▉      | 1720/4381 [45:57<1:11:04,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  39%|███▉      | 1720/4381 [45:57<1:11:04,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  39%|███▉      | 1730/4381 [46:16<1:10:51,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  39%|███▉      | 1730/4381 [46:16<1:10:51,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  40%|███▉      | 1740/4381 [46:28<1:10:29,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  40%|███▉      | 1740/4381 [46:28<1:10:29,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  40%|███▉      | 1750/4381 [46:43<1:10:12,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  40%|███▉      | 1750/4381 [46:43<1:10:12,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  40%|████      | 1760/4381 [47:00<1:09:57,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  40%|████      | 1760/4381 [47:00<1:09:57,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  40%|████      | 1770/4381 [47:17<1:09:43,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  40%|████      | 1770/4381 [47:17<1:09:43,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  41%|████      | 1780/4381 [47:32<1:09:26,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  41%|████      | 1780/4381 [47:32<1:09:26,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  41%|████      | 1790/4381 [47:53<1:09:16,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  41%|████      | 1790/4381 [47:53<1:09:16,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  41%|████      | 1800/4381 [48:09<1:09:00,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  41%|████      | 1800/4381 [48:09<1:09:00,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  41%|████▏     | 1810/4381 [48:25<1:08:44,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  41%|████▏     | 1810/4381 [48:25<1:08:44,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  42%|████▏     | 1820/4381 [48:43<1:08:31,  1.61s/it, loss=2.66, v_num=641]Epoch 8:  42%|████▏     | 1820/4381 [48:43<1:08:31,  1.61s/it, loss=2.66, v_num=641]Epoch 8:  42%|████▏     | 1830/4381 [48:58<1:08:14,  1.61s/it, loss=2.66, v_num=641]Epoch 8:  42%|████▏     | 1830/4381 [48:58<1:08:14,  1.61s/it, loss=2.65, v_num=641]Epoch 8:  42%|████▏     | 1840/4381 [49:13<1:07:56,  1.60s/it, loss=2.65, v_num=641]Epoch 8:  42%|████▏     | 1840/4381 [49:13<1:07:56,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  42%|████▏     | 1850/4381 [49:28<1:07:39,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  42%|████▏     | 1850/4381 [49:28<1:07:39,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  42%|████▏     | 1860/4381 [49:42<1:07:20,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  42%|████▏     | 1860/4381 [49:42<1:07:20,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  43%|████▎     | 1870/4381 [49:59<1:07:05,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  43%|████▎     | 1870/4381 [49:59<1:07:05,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  43%|████▎     | 1880/4381 [50:15<1:06:49,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  43%|████▎     | 1880/4381 [50:15<1:06:49,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  43%|████▎     | 1890/4381 [50:29<1:06:30,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  43%|████▎     | 1890/4381 [50:29<1:06:30,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  43%|████▎     | 1900/4381 [50:42<1:06:11,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  43%|████▎     | 1900/4381 [50:42<1:06:11,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  44%|████▎     | 1910/4381 [51:03<1:06:00,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  44%|████▎     | 1910/4381 [51:03<1:06:00,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  44%|████▍     | 1920/4381 [51:15<1:05:40,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  44%|████▍     | 1920/4381 [51:15<1:05:40,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  44%|████▍     | 1930/4381 [51:29<1:05:21,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  44%|████▍     | 1930/4381 [51:29<1:05:21,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  44%|████▍     | 1940/4381 [51:48<1:05:09,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  44%|████▍     | 1940/4381 [51:48<1:05:09,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  45%|████▍     | 1950/4381 [52:02<1:04:51,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  45%|████▍     | 1950/4381 [52:02<1:04:51,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  45%|████▍     | 1960/4381 [52:16<1:04:32,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  45%|████▍     | 1960/4381 [52:16<1:04:32,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  45%|████▍     | 1970/4381 [52:35<1:04:19,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  45%|████▍     | 1970/4381 [52:35<1:04:19,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  45%|████▌     | 1980/4381 [52:51<1:04:03,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  45%|████▌     | 1980/4381 [52:51<1:04:03,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  45%|████▌     | 1990/4381 [53:06<1:03:46,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  45%|████▌     | 1990/4381 [53:06<1:03:46,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  46%|████▌     | 2000/4381 [53:24<1:03:32,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  46%|████▌     | 2000/4381 [53:24<1:03:32,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  46%|████▌     | 2010/4381 [53:38<1:03:15,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  46%|████▌     | 2010/4381 [53:38<1:03:15,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  46%|████▌     | 2020/4381 [53:54<1:02:58,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  46%|████▋     | 2030/4381 [54:10<1:02:42,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  46%|████▋     | 2030/4381 [54:10<1:02:42,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  47%|████▋     | 2040/4381 [54:25<1:02:25,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  47%|████▋     | 2040/4381 [54:25<1:02:25,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  47%|████▋     | 2050/4381 [54:42<1:02:10,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  47%|████▋     | 2050/4381 [54:42<1:02:10,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  47%|████▋     | 2060/4381 [55:00<1:01:56,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  47%|████▋     | 2060/4381 [55:00<1:01:56,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  47%|████▋     | 2070/4381 [55:14<1:01:38,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  47%|████▋     | 2070/4381 [55:14<1:01:38,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  47%|████▋     | 2080/4381 [55:29<1:01:21,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  47%|████▋     | 2080/4381 [55:29<1:01:21,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  48%|████▊     | 2090/4381 [55:48<1:01:09,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  48%|████▊     | 2090/4381 [55:48<1:01:09,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  48%|████▊     | 2100/4381 [56:02<1:00:50,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  48%|████▊     | 2100/4381 [56:02<1:00:50,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  48%|████▊     | 2110/4381 [56:17<1:00:33,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  48%|████▊     | 2110/4381 [56:17<1:00:33,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  48%|████▊     | 2120/4381 [56:36<1:00:20,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  48%|████▊     | 2120/4381 [56:36<1:00:20,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  49%|████▊     | 2130/4381 [56:51<1:00:04,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  49%|████▊     | 2130/4381 [56:51<1:00:04,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  49%|████▉     | 2140/4381 [57:08<59:48,  1.60s/it, loss=2.72, v_num=641]  Epoch 8:  49%|████▉     | 2140/4381 [57:08<59:48,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  49%|████▉     | 2150/4381 [57:25<59:33,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  49%|████▉     | 2150/4381 [57:25<59:33,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  49%|████▉     | 2160/4381 [57:38<59:14,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  49%|████▉     | 2160/4381 [57:38<59:14,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  50%|████▉     | 2170/4381 [57:52<58:56,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  50%|████▉     | 2170/4381 [57:52<58:56,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  50%|████▉     | 2180/4381 [58:12<58:44,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  50%|████▉     | 2180/4381 [58:12<58:44,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  50%|████▉     | 2190/4381 [58:25<58:25,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  50%|████▉     | 2190/4381 [58:25<58:25,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  50%|█████     | 2200/4381 [58:39<58:07,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  50%|█████     | 2200/4381 [58:39<58:07,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  50%|█████     | 2210/4381 [58:56<57:52,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  50%|█████     | 2210/4381 [58:56<57:52,  1.60s/it, loss=2.75, v_num=641]Epoch 8:  51%|█████     | 2220/4381 [59:10<57:34,  1.60s/it, loss=2.75, v_num=641]Epoch 8:  51%|█████     | 2220/4381 [59:10<57:34,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  51%|█████     | 2230/4381 [59:27<57:19,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  51%|█████     | 2230/4381 [59:27<57:19,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  51%|█████     | 2240/4381 [59:44<57:04,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  51%|█████     | 2240/4381 [59:44<57:04,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  51%|█████▏    | 2250/4381 [59:59<56:47,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  51%|█████▏    | 2250/4381 [59:59<56:47,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  52%|█████▏    | 2260/4381 [1:00:12<56:28,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  52%|█████▏    | 2260/4381 [1:00:12<56:28,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  52%|█████▏    | 2270/4381 [1:00:30<56:14,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  52%|█████▏    | 2270/4381 [1:00:30<56:14,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  52%|█████▏    | 2280/4381 [1:00:45<55:57,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  52%|█████▏    | 2280/4381 [1:00:45<55:57,  1.60s/it, loss=2.76, v_num=641]Epoch 8:  52%|█████▏    | 2290/4381 [1:01:00<55:40,  1.60s/it, loss=2.76, v_num=641]Epoch 8:  52%|█████▏    | 2290/4381 [1:01:00<55:40,  1.60s/it, loss=2.75, v_num=641]Epoch 8:  52%|█████▏    | 2300/4381 [1:01:18<55:27,  1.60s/it, loss=2.75, v_num=641]Epoch 8:  52%|█████▏    | 2300/4381 [1:01:18<55:27,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  53%|█████▎    | 2310/4381 [1:01:33<55:09,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  53%|█████▎    | 2310/4381 [1:01:33<55:09,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  53%|█████▎    | 2320/4381 [1:01:49<54:53,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  53%|█████▎    | 2320/4381 [1:01:49<54:53,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  53%|█████▎    | 2330/4381 [1:02:10<54:42,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  53%|█████▎    | 2330/4381 [1:02:10<54:42,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  53%|█████▎    | 2340/4381 [1:02:25<54:25,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  53%|█████▎    | 2340/4381 [1:02:25<54:25,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  54%|█████▎    | 2350/4381 [1:02:39<54:07,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  54%|█████▎    | 2350/4381 [1:02:39<54:07,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  54%|█████▍    | 2360/4381 [1:02:57<53:53,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  54%|█████▍    | 2360/4381 [1:02:57<53:53,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  54%|█████▍    | 2370/4381 [1:03:12<53:36,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  54%|█████▍    | 2370/4381 [1:03:12<53:36,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  54%|█████▍    | 2380/4381 [1:03:27<53:19,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  54%|█████▍    | 2380/4381 [1:03:27<53:19,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▍    | 2390/4381 [1:03:46<53:06,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▍    | 2390/4381 [1:03:46<53:06,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▍    | 2400/4381 [1:03:59<52:47,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▍    | 2400/4381 [1:03:59<52:47,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▌    | 2410/4381 [1:04:14<52:30,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  55%|█████▌    | 2410/4381 [1:04:14<52:30,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  55%|█████▌    | 2420/4381 [1:04:34<52:18,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  55%|█████▌    | 2420/4381 [1:04:34<52:18,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  55%|█████▌    | 2430/4381 [1:04:45<51:58,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  55%|█████▌    | 2430/4381 [1:04:45<51:58,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  56%|█████▌    | 2440/4381 [1:04:58<51:39,  1.60s/it, loss=2.73, v_num=641]Epoch 8:  56%|█████▌    | 2440/4381 [1:04:58<51:39,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  56%|█████▌    | 2450/4381 [1:05:16<51:25,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  56%|█████▌    | 2450/4381 [1:05:16<51:25,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  56%|█████▌    | 2460/4381 [1:05:31<51:08,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  56%|█████▌    | 2460/4381 [1:05:31<51:08,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  56%|█████▋    | 2470/4381 [1:05:46<50:52,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  56%|█████▋    | 2470/4381 [1:05:46<50:52,  1.60s/it, loss=2.65, v_num=641]Epoch 8:  57%|█████▋    | 2480/4381 [1:06:06<50:39,  1.60s/it, loss=2.65, v_num=641]Epoch 8:  57%|█████▋    | 2480/4381 [1:06:06<50:39,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  57%|█████▋    | 2490/4381 [1:06:22<50:23,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  57%|█████▋    | 2490/4381 [1:06:22<50:23,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  57%|█████▋    | 2500/4381 [1:06:35<50:04,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  57%|█████▋    | 2500/4381 [1:06:35<50:04,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  57%|█████▋    | 2510/4381 [1:06:54<49:51,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  57%|█████▋    | 2510/4381 [1:06:54<49:51,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  58%|█████▊    | 2520/4381 [1:07:09<49:34,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  58%|█████▊    | 2520/4381 [1:07:09<49:34,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  58%|█████▊    | 2530/4381 [1:07:23<49:17,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  58%|█████▊    | 2530/4381 [1:07:23<49:17,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  58%|█████▊    | 2540/4381 [1:07:41<49:02,  1.60s/it, loss=2.74, v_num=641]Epoch 8:  58%|█████▊    | 2540/4381 [1:07:41<49:02,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  58%|█████▊    | 2550/4381 [1:07:54<48:44,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  58%|█████▊    | 2550/4381 [1:07:54<48:44,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  58%|█████▊    | 2560/4381 [1:08:08<48:27,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  58%|█████▊    | 2560/4381 [1:08:08<48:27,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  59%|█████▊    | 2570/4381 [1:08:25<48:11,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  59%|█████▊    | 2570/4381 [1:08:25<48:11,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  59%|█████▉    | 2580/4381 [1:08:40<47:55,  1.60s/it, loss=2.67, v_num=641]Epoch 8:  59%|█████▉    | 2580/4381 [1:08:40<47:55,  1.60s/it, loss=2.7, v_num=641] Epoch 8:  59%|█████▉    | 2590/4381 [1:08:54<47:38,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  59%|█████▉    | 2590/4381 [1:08:54<47:38,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  59%|█████▉    | 2600/4381 [1:09:11<47:22,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  59%|█████▉    | 2600/4381 [1:09:11<47:22,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  60%|█████▉    | 2610/4381 [1:09:26<47:05,  1.60s/it, loss=2.7, v_num=641]Epoch 8:  60%|█████▉    | 2610/4381 [1:09:26<47:05,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  60%|█████▉    | 2620/4381 [1:09:41<46:49,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  60%|█████▉    | 2620/4381 [1:09:41<46:49,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  60%|██████    | 2630/4381 [1:09:58<46:34,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  60%|██████    | 2630/4381 [1:09:58<46:34,  1.60s/it, loss=2.71, v_num=641]Epoch 8:  60%|██████    | 2640/4381 [1:10:11<46:16,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  60%|██████    | 2640/4381 [1:10:11<46:16,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  60%|██████    | 2650/4381 [1:10:28<46:00,  1.60s/it, loss=2.72, v_num=641]Epoch 8:  60%|██████    | 2650/4381 [1:10:28<46:00,  1.60s/it, loss=2.66, v_num=641]Epoch 8:  61%|██████    | 2660/4381 [1:10:44<45:44,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  61%|██████    | 2660/4381 [1:10:44<45:44,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  61%|██████    | 2670/4381 [1:10:57<45:27,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  61%|██████    | 2670/4381 [1:10:57<45:27,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  61%|██████    | 2680/4381 [1:11:12<45:10,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  61%|██████    | 2680/4381 [1:11:12<45:10,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  61%|██████▏   | 2690/4381 [1:11:32<44:57,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  61%|██████▏   | 2690/4381 [1:11:32<44:57,  1.59s/it, loss=2.75, v_num=641]Epoch 8:  62%|██████▏   | 2700/4381 [1:11:47<44:40,  1.59s/it, loss=2.75, v_num=641]Epoch 8:  62%|██████▏   | 2700/4381 [1:11:47<44:40,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  62%|██████▏   | 2710/4381 [1:12:03<44:24,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  62%|██████▏   | 2710/4381 [1:12:03<44:24,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  62%|██████▏   | 2720/4381 [1:12:20<44:09,  1.60s/it, loss=2.68, v_num=641]Epoch 8:  62%|██████▏   | 2720/4381 [1:12:20<44:09,  1.60s/it, loss=2.69, v_num=641]Epoch 8:  62%|██████▏   | 2730/4381 [1:12:33<43:52,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  62%|██████▏   | 2730/4381 [1:12:33<43:52,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  63%|██████▎   | 2740/4381 [1:12:48<43:35,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  63%|██████▎   | 2740/4381 [1:12:48<43:35,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  63%|██████▎   | 2750/4381 [1:13:07<43:21,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  63%|██████▎   | 2750/4381 [1:13:07<43:21,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  63%|██████▎   | 2760/4381 [1:13:20<43:03,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  63%|██████▎   | 2760/4381 [1:13:20<43:03,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  63%|██████▎   | 2770/4381 [1:13:33<42:46,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  63%|██████▎   | 2770/4381 [1:13:33<42:46,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  63%|██████▎   | 2780/4381 [1:13:51<42:31,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  63%|██████▎   | 2780/4381 [1:13:51<42:31,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  64%|██████▎   | 2790/4381 [1:14:05<42:14,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  64%|██████▎   | 2790/4381 [1:14:05<42:14,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  64%|██████▍   | 2800/4381 [1:14:20<41:57,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  64%|██████▍   | 2800/4381 [1:14:20<41:57,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  64%|██████▍   | 2810/4381 [1:14:37<41:42,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  64%|██████▍   | 2810/4381 [1:14:37<41:42,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  64%|██████▍   | 2820/4381 [1:14:54<41:26,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  64%|██████▍   | 2820/4381 [1:14:54<41:26,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  65%|██████▍   | 2830/4381 [1:15:08<41:09,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  65%|██████▍   | 2830/4381 [1:15:08<41:09,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  65%|██████▍   | 2840/4381 [1:15:26<40:55,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  65%|██████▍   | 2840/4381 [1:15:26<40:55,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  65%|██████▌   | 2850/4381 [1:15:42<40:39,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  65%|██████▌   | 2850/4381 [1:15:42<40:39,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  66%|██████▌   | 2870/4381 [1:16:17<40:08,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  66%|██████▌   | 2870/4381 [1:16:17<40:08,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  66%|██████▌   | 2880/4381 [1:16:31<39:51,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  66%|██████▌   | 2880/4381 [1:16:31<39:51,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  66%|██████▌   | 2890/4381 [1:16:46<39:35,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  66%|██████▌   | 2890/4381 [1:16:46<39:35,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  66%|██████▌   | 2900/4381 [1:17:02<39:19,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  66%|██████▌   | 2900/4381 [1:17:02<39:19,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  66%|██████▋   | 2910/4381 [1:17:15<39:02,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  66%|██████▋   | 2910/4381 [1:17:15<39:02,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  67%|██████▋   | 2920/4381 [1:17:31<38:46,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  67%|██████▋   | 2920/4381 [1:17:31<38:46,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  67%|██████▋   | 2930/4381 [1:17:49<38:31,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  67%|██████▋   | 2930/4381 [1:17:49<38:31,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  67%|██████▋   | 2940/4381 [1:18:04<38:15,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  67%|██████▋   | 2940/4381 [1:18:04<38:15,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  67%|██████▋   | 2950/4381 [1:18:20<37:59,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  67%|██████▋   | 2950/4381 [1:18:20<37:59,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  68%|██████▊   | 2960/4381 [1:18:37<37:43,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  68%|██████▊   | 2960/4381 [1:18:37<37:43,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  68%|██████▊   | 2970/4381 [1:18:53<37:28,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  68%|██████▊   | 2970/4381 [1:18:53<37:28,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  68%|██████▊   | 2980/4381 [1:19:09<37:11,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  68%|██████▊   | 2980/4381 [1:19:09<37:11,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  68%|██████▊   | 2990/4381 [1:19:26<36:56,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  68%|██████▊   | 2990/4381 [1:19:26<36:56,  1.59s/it, loss=2.64, v_num=641]Epoch 8:  68%|██████▊   | 3000/4381 [1:19:42<36:40,  1.59s/it, loss=2.64, v_num=641]Epoch 8:  68%|██████▊   | 3000/4381 [1:19:42<36:40,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  69%|██████▊   | 3010/4381 [1:19:56<36:23,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  69%|██████▊   | 3010/4381 [1:19:56<36:23,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  69%|██████▉   | 3020/4381 [1:20:13<36:08,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  69%|██████▉   | 3020/4381 [1:20:13<36:08,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  69%|██████▉   | 3030/4381 [1:20:27<35:51,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  69%|██████▉   | 3030/4381 [1:20:27<35:51,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  69%|██████▉   | 3040/4381 [1:20:41<35:34,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  69%|██████▉   | 3040/4381 [1:20:41<35:34,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  70%|██████▉   | 3050/4381 [1:20:58<35:19,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  70%|██████▉   | 3050/4381 [1:20:58<35:19,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  70%|██████▉   | 3060/4381 [1:21:14<35:03,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  70%|██████▉   | 3060/4381 [1:21:14<35:03,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  70%|███████   | 3070/4381 [1:21:28<34:46,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  70%|███████   | 3070/4381 [1:21:28<34:46,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  70%|███████   | 3080/4381 [1:21:46<34:31,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  70%|███████   | 3080/4381 [1:21:46<34:31,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  71%|███████   | 3090/4381 [1:22:00<34:15,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  71%|███████   | 3090/4381 [1:22:00<34:15,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  71%|███████   | 3100/4381 [1:22:13<33:58,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  71%|███████   | 3100/4381 [1:22:13<33:58,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  71%|███████   | 3110/4381 [1:22:34<33:44,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  71%|███████   | 3110/4381 [1:22:34<33:44,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  71%|███████   | 3120/4381 [1:22:46<33:26,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  71%|███████   | 3120/4381 [1:22:46<33:26,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  71%|███████▏  | 3130/4381 [1:23:01<33:10,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  71%|███████▏  | 3130/4381 [1:23:01<33:10,  1.59s/it, loss=2.64, v_num=641]Epoch 8:  72%|███████▏  | 3140/4381 [1:23:16<32:54,  1.59s/it, loss=2.64, v_num=641]Epoch 8:  72%|███████▏  | 3140/4381 [1:23:16<32:54,  1.59s/it, loss=2.65, v_num=641]Epoch 8:  72%|███████▏  | 3150/4381 [1:23:31<32:37,  1.59s/it, loss=2.65, v_num=641]Epoch 8:  72%|███████▏  | 3150/4381 [1:23:31<32:37,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  72%|███████▏  | 3160/4381 [1:23:45<32:21,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  72%|███████▏  | 3160/4381 [1:23:45<32:21,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  72%|███████▏  | 3170/4381 [1:24:03<32:06,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  72%|███████▏  | 3170/4381 [1:24:03<32:06,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  73%|███████▎  | 3180/4381 [1:24:18<31:50,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  73%|███████▎  | 3180/4381 [1:24:18<31:50,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  73%|███████▎  | 3190/4381 [1:24:32<31:33,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  73%|███████▎  | 3190/4381 [1:24:32<31:33,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  73%|███████▎  | 3200/4381 [1:24:46<31:16,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  73%|███████▎  | 3200/4381 [1:24:46<31:16,  1.59s/it, loss=2.74, v_num=641]Epoch 8:  73%|███████▎  | 3210/4381 [1:25:02<31:00,  1.59s/it, loss=2.74, v_num=641]Epoch 8:  73%|███████▎  | 3210/4381 [1:25:02<31:00,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  73%|███████▎  | 3220/4381 [1:25:19<30:45,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  73%|███████▎  | 3220/4381 [1:25:19<30:45,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  74%|███████▎  | 3230/4381 [1:25:34<30:29,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  74%|███████▎  | 3230/4381 [1:25:34<30:29,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  74%|███████▍  | 3240/4381 [1:25:51<30:13,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  74%|███████▍  | 3240/4381 [1:25:51<30:13,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  74%|███████▍  | 3250/4381 [1:26:07<29:57,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  74%|███████▍  | 3250/4381 [1:26:07<29:57,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  74%|███████▍  | 3260/4381 [1:26:24<29:42,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  74%|███████▍  | 3260/4381 [1:26:24<29:42,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  75%|███████▍  | 3270/4381 [1:26:39<29:26,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  75%|███████▍  | 3270/4381 [1:26:39<29:26,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  75%|███████▍  | 3280/4381 [1:26:58<29:11,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  75%|███████▍  | 3280/4381 [1:26:58<29:11,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  75%|███████▌  | 3290/4381 [1:27:12<28:54,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  75%|███████▌  | 3290/4381 [1:27:12<28:54,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  75%|███████▌  | 3300/4381 [1:27:28<28:38,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  75%|███████▌  | 3300/4381 [1:27:28<28:38,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  76%|███████▌  | 3310/4381 [1:27:44<28:22,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  76%|███████▌  | 3310/4381 [1:27:44<28:22,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  76%|███████▌  | 3320/4381 [1:28:00<28:06,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  76%|███████▌  | 3320/4381 [1:28:00<28:06,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  76%|███████▌  | 3330/4381 [1:28:16<27:51,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  76%|███████▌  | 3330/4381 [1:28:16<27:51,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  76%|███████▋  | 3350/4381 [1:28:49<27:19,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  76%|███████▋  | 3350/4381 [1:28:49<27:19,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  77%|███████▋  | 3360/4381 [1:29:05<27:03,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  77%|███████▋  | 3360/4381 [1:29:05<27:03,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  77%|███████▋  | 3370/4381 [1:29:21<26:48,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  77%|███████▋  | 3370/4381 [1:29:21<26:48,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  77%|███████▋  | 3380/4381 [1:29:36<26:31,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  77%|███████▋  | 3380/4381 [1:29:36<26:31,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  77%|███████▋  | 3390/4381 [1:29:51<26:15,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  77%|███████▋  | 3390/4381 [1:29:51<26:15,  1.59s/it, loss=2.74, v_num=641]Epoch 8:  78%|███████▊  | 3400/4381 [1:30:11<26:00,  1.59s/it, loss=2.74, v_num=641]Epoch 8:  78%|███████▊  | 3400/4381 [1:30:11<26:00,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  78%|███████▊  | 3410/4381 [1:30:25<25:44,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  78%|███████▊  | 3410/4381 [1:30:25<25:44,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  78%|███████▊  | 3420/4381 [1:30:39<25:28,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  78%|███████▊  | 3420/4381 [1:30:39<25:28,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  78%|███████▊  | 3430/4381 [1:30:57<25:12,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  78%|███████▊  | 3430/4381 [1:30:57<25:12,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▊  | 3440/4381 [1:31:11<24:56,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▊  | 3440/4381 [1:31:11<24:56,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▊  | 3450/4381 [1:31:25<24:39,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▊  | 3450/4381 [1:31:25<24:39,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  79%|███████▉  | 3460/4381 [1:31:42<24:24,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  79%|███████▉  | 3460/4381 [1:31:42<24:24,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  79%|███████▉  | 3470/4381 [1:31:58<24:08,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  79%|███████▉  | 3470/4381 [1:31:58<24:08,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▉  | 3480/4381 [1:32:13<23:52,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  79%|███████▉  | 3480/4381 [1:32:13<23:52,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  80%|███████▉  | 3490/4381 [1:32:29<23:36,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  80%|███████▉  | 3490/4381 [1:32:29<23:36,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  80%|███████▉  | 3500/4381 [1:32:45<23:20,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  80%|███████▉  | 3500/4381 [1:32:45<23:20,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  80%|████████  | 3510/4381 [1:33:03<23:05,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  80%|████████  | 3510/4381 [1:33:03<23:05,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  80%|████████  | 3520/4381 [1:33:20<22:49,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  80%|████████  | 3520/4381 [1:33:20<22:49,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████  | 3530/4381 [1:33:35<22:33,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████  | 3530/4381 [1:33:35<22:33,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████  | 3540/4381 [1:33:50<22:17,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████  | 3540/4381 [1:33:50<22:17,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  81%|████████  | 3550/4381 [1:34:10<22:02,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  81%|████████  | 3550/4381 [1:34:10<22:02,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  81%|████████▏ | 3560/4381 [1:34:25<21:46,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  81%|████████▏ | 3560/4381 [1:34:25<21:46,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████▏ | 3570/4381 [1:34:39<21:29,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  81%|████████▏ | 3570/4381 [1:34:39<21:29,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  82%|████████▏ | 3580/4381 [1:34:56<21:14,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  82%|████████▏ | 3580/4381 [1:34:56<21:14,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  82%|████████▏ | 3590/4381 [1:35:12<20:58,  1.59s/it, loss=2.66, v_num=641]Epoch 8:  82%|████████▏ | 3590/4381 [1:35:12<20:58,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  82%|████████▏ | 3600/4381 [1:35:28<20:42,  1.59s/it, loss=2.67, v_num=641]Epoch 8:  82%|████████▏ | 3600/4381 [1:35:28<20:42,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  82%|████████▏ | 3610/4381 [1:35:46<20:26,  1.59s/it, loss=2.68, v_num=641]Epoch 8:  82%|████████▏ | 3610/4381 [1:35:46<20:26,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  83%|████████▎ | 3620/4381 [1:36:01<20:10,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  83%|████████▎ | 3620/4381 [1:36:01<20:10,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  83%|████████▎ | 3630/4381 [1:36:13<19:54,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  83%|████████▎ | 3630/4381 [1:36:13<19:54,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  84%|████████▎ | 3660/4381 [1:36:59<19:06,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  84%|████████▎ | 3660/4381 [1:36:59<19:06,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  84%|████████▍ | 3670/4381 [1:37:17<18:50,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  84%|████████▍ | 3670/4381 [1:37:17<18:50,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  84%|████████▍ | 3680/4381 [1:37:31<18:34,  1.59s/it, loss=2.69, v_num=641]Epoch 8:  84%|████████▍ | 3680/4381 [1:37:31<18:34,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  84%|████████▍ | 3690/4381 [1:37:46<18:18,  1.59s/it, loss=2.72, v_num=641]Epoch 8:  84%|████████▍ | 3690/4381 [1:37:46<18:18,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  84%|████████▍ | 3700/4381 [1:38:06<18:03,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  84%|████████▍ | 3700/4381 [1:38:06<18:03,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  85%|████████▍ | 3710/4381 [1:38:21<17:47,  1.59s/it, loss=2.71, v_num=641]Epoch 8:  85%|████████▍ | 3710/4381 [1:38:21<17:47,  1.59s/it, loss=2.7, v_num=641] Epoch 8:  85%|████████▍ | 3720/4381 [1:38:35<17:30,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  85%|████████▍ | 3720/4381 [1:38:35<17:30,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  85%|████████▌ | 3730/4381 [1:38:48<17:14,  1.59s/it, loss=2.7, v_num=641]Epoch 8:  85%|████████▌ | 3730/4381 [1:38:48<17:14,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  85%|████████▌ | 3740/4381 [1:38:52<16:56,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  85%|████████▌ | 3740/4381 [1:38:52<16:56,  1.59s/it, loss=2.73, v_num=641]Epoch 8:  86%|████████▌ | 3750/4381 [1:38:55<16:38,  1.58s/it, loss=2.73, v_num=641]Epoch 8:  86%|████████▌ | 3750/4381 [1:38:55<16:38,  1.58s/it, loss=2.73, v_num=641]validation_epoch_end
graph acc: 0.1900958466453674
valid accuracy: 0.9585422873497009
validation_epoch_end
graph acc: 0.22843450479233227
valid accuracy: 0.9571358561515808
validation_epoch_end
graph acc: 0.20287539936102236
valid accuracy: 0.9598226547241211
Epoch 8:  86%|████████▌ | 3760/4381 [1:38:56<16:20,  1.58s/it, loss=2.73, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.19169329073482427
valid accuracy: 0.9560553431510925
validation_epoch_end
graph acc: 0.23162939297124602
valid accuracy: 0.9559212327003479
validation_epoch_end
graph acc: 0.19488817891373802
valid accuracy: 0.9596028327941895
validation_epoch_end
graph acc: 0.22044728434504793
valid accuracy: 0.9595957398414612

Validating:   2%|▏         | 10/626 [00:04<04:26,  2.31it/s][AEpoch 8:  86%|████████▌ | 3770/4381 [1:39:01<16:02,  1.58s/it, loss=2.73, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:31,  4.00it/s][AEpoch 8:  86%|████████▋ | 3780/4381 [1:39:02<15:44,  1.57s/it, loss=2.73, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:32,  3.92it/s][AEpoch 8:  87%|████████▋ | 3790/4381 [1:39:04<15:26,  1.57s/it, loss=2.73, v_num=641]
Validating:   6%|▋         | 40/626 [00:09<02:05,  4.68it/s][AEpoch 8:  87%|████████▋ | 3800/4381 [1:39:06<15:08,  1.56s/it, loss=2.73, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:46,  5.41it/s][AEpoch 8:  87%|████████▋ | 3810/4381 [1:39:07<14:51,  1.56s/it, loss=2.73, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:44,  5.40it/s][AEpoch 8:  87%|████████▋ | 3820/4381 [1:39:09<14:33,  1.56s/it, loss=2.73, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:41,  5.48it/s][AEpoch 8:  87%|████████▋ | 3830/4381 [1:39:11<14:15,  1.55s/it, loss=2.73, v_num=641]
Validating:  13%|█▎        | 80/626 [00:15<01:26,  6.32it/s][AEpoch 8:  88%|████████▊ | 3840/4381 [1:39:12<13:58,  1.55s/it, loss=2.73, v_num=641]
Validating:  14%|█▍        | 90/626 [00:17<01:27,  6.12it/s][AEpoch 8:  88%|████████▊ | 3850/4381 [1:39:14<13:41,  1.55s/it, loss=2.73, v_num=641]
Validating:  16%|█▌        | 100/626 [00:19<01:29,  5.90it/s][AEpoch 8:  88%|████████▊ | 3860/4381 [1:39:16<13:23,  1.54s/it, loss=2.73, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:33,  5.52it/s][AEpoch 8:  88%|████████▊ | 3870/4381 [1:39:18<13:06,  1.54s/it, loss=2.73, v_num=641]
Validating:  19%|█▉        | 120/626 [00:23<01:32,  5.49it/s][AEpoch 8:  89%|████████▊ | 3880/4381 [1:39:19<12:49,  1.54s/it, loss=2.73, v_num=641]
Validating:  21%|██        | 130/626 [00:24<01:27,  5.67it/s][AEpoch 8:  89%|████████▉ | 3890/4381 [1:39:21<12:32,  1.53s/it, loss=2.73, v_num=641]
Validating:  22%|██▏       | 140/626 [00:27<01:38,  4.96it/s][AEpoch 8:  89%|████████▉ | 3900/4381 [1:39:24<12:15,  1.53s/it, loss=2.73, v_num=641]
Validating:  24%|██▍       | 150/626 [00:29<01:32,  5.14it/s][AEpoch 8:  89%|████████▉ | 3910/4381 [1:39:25<11:58,  1.53s/it, loss=2.73, v_num=641]
Validating:  26%|██▌       | 160/626 [00:30<01:25,  5.43it/s][AEpoch 8:  89%|████████▉ | 3920/4381 [1:39:27<11:41,  1.52s/it, loss=2.73, v_num=641]
Validating:  27%|██▋       | 170/626 [00:32<01:24,  5.40it/s][AEpoch 8:  90%|████████▉ | 3930/4381 [1:39:29<11:24,  1.52s/it, loss=2.73, v_num=641]
Validating:  29%|██▉       | 180/626 [00:35<01:40,  4.44it/s][AEpoch 8:  90%|████████▉ | 3940/4381 [1:39:32<11:08,  1.52s/it, loss=2.73, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:30,  4.84it/s][AEpoch 8:  90%|█████████ | 3950/4381 [1:39:34<10:51,  1.51s/it, loss=2.73, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:36,  4.43it/s][AEpoch 8:  90%|█████████ | 3960/4381 [1:39:36<10:35,  1.51s/it, loss=2.73, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:32,  4.50it/s][AEpoch 8:  91%|█████████ | 3970/4381 [1:39:39<10:18,  1.51s/it, loss=2.73, v_num=641]
Validating:  35%|███▌      | 220/626 [00:45<01:36,  4.22it/s][AEpoch 8:  91%|█████████ | 3980/4381 [1:39:41<10:02,  1.50s/it, loss=2.73, v_num=641]
Validating:  37%|███▋      | 230/626 [00:46<01:23,  4.75it/s][AEpoch 8:  91%|█████████ | 3990/4381 [1:39:43<09:46,  1.50s/it, loss=2.73, v_num=641]
Validating:  38%|███▊      | 240/626 [00:48<01:21,  4.75it/s][AEpoch 8:  91%|█████████▏| 4000/4381 [1:39:45<09:29,  1.50s/it, loss=2.73, v_num=641]
Validating:  40%|███▉      | 250/626 [00:50<01:16,  4.89it/s][AEpoch 8:  92%|█████████▏| 4010/4381 [1:39:47<09:13,  1.49s/it, loss=2.73, v_num=641]
Validating:  42%|████▏     | 260/626 [00:52<01:17,  4.74it/s][AEpoch 8:  92%|█████████▏| 4020/4381 [1:39:49<08:57,  1.49s/it, loss=2.73, v_num=641]
Validating:  43%|████▎     | 270/626 [00:54<01:06,  5.31it/s][AEpoch 8:  92%|█████████▏| 4030/4381 [1:39:50<08:41,  1.49s/it, loss=2.73, v_num=641]
Validating:  45%|████▍     | 280/626 [00:56<01:13,  4.71it/s][AEpoch 8:  92%|█████████▏| 4040/4381 [1:39:53<08:25,  1.48s/it, loss=2.73, v_num=641]
Validating:  46%|████▋     | 290/626 [00:59<01:16,  4.42it/s][AEpoch 8:  92%|█████████▏| 4050/4381 [1:39:56<08:09,  1.48s/it, loss=2.73, v_num=641]
Validating:  48%|████▊     | 300/626 [01:00<01:02,  5.21it/s][AEpoch 8:  93%|█████████▎| 4060/4381 [1:39:57<07:54,  1.48s/it, loss=2.73, v_num=641]
Validating:  50%|████▉     | 310/626 [01:01<00:53,  5.86it/s][AEpoch 8:  93%|█████████▎| 4070/4381 [1:39:58<07:38,  1.47s/it, loss=2.73, v_num=641]
Validating:  51%|█████     | 320/626 [01:03<00:54,  5.62it/s][AEpoch 8:  93%|█████████▎| 4080/4381 [1:40:00<07:22,  1.47s/it, loss=2.73, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:05<00:50,  5.82it/s][AEpoch 8:  93%|█████████▎| 4090/4381 [1:40:02<07:06,  1.47s/it, loss=2.73, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:07<00:50,  5.68it/s][AEpoch 8:  94%|█████████▎| 4100/4381 [1:40:03<06:51,  1.46s/it, loss=2.73, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:09<00:55,  4.99it/s][AEpoch 8:  94%|█████████▍| 4110/4381 [1:40:06<06:35,  1.46s/it, loss=2.73, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:11<00:53,  4.98it/s][AEpoch 8:  94%|█████████▍| 4120/4381 [1:40:08<06:20,  1.46s/it, loss=2.73, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:12<00:42,  6.06it/s][AEpoch 8:  94%|█████████▍| 4130/4381 [1:40:09<06:05,  1.45s/it, loss=2.73, v_num=641]
Validating:  61%|██████    | 380/626 [01:14<00:42,  5.79it/s][AEpoch 8:  94%|█████████▍| 4140/4381 [1:40:11<05:49,  1.45s/it, loss=2.73, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:16<00:40,  5.85it/s][AEpoch 8:  95%|█████████▍| 4150/4381 [1:40:12<05:34,  1.45s/it, loss=2.73, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:17<00:35,  6.44it/s][AEpoch 8:  95%|█████████▍| 4160/4381 [1:40:14<05:19,  1.45s/it, loss=2.73, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:18<00:32,  6.58it/s][AEpoch 8:  95%|█████████▌| 4170/4381 [1:40:15<05:04,  1.44s/it, loss=2.73, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:21<00:39,  5.18it/s][AEpoch 8:  95%|█████████▌| 4180/4381 [1:40:18<04:49,  1.44s/it, loss=2.73, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:23<00:36,  5.41it/s][AEpoch 8:  96%|█████████▌| 4190/4381 [1:40:20<04:34,  1.44s/it, loss=2.73, v_num=641]
Validating:  70%|███████   | 440/626 [01:25<00:37,  5.02it/s][AEpoch 8:  96%|█████████▌| 4200/4381 [1:40:22<04:19,  1.43s/it, loss=2.73, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:27<00:34,  5.11it/s][AEpoch 8:  96%|█████████▌| 4210/4381 [1:40:24<04:04,  1.43s/it, loss=2.73, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:29<00:32,  5.17it/s][AEpoch 8:  96%|█████████▋| 4220/4381 [1:40:26<03:49,  1.43s/it, loss=2.73, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:31<00:31,  4.92it/s][AEpoch 8:  97%|█████████▋| 4230/4381 [1:40:28<03:35,  1.42s/it, loss=2.73, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:33<00:28,  5.20it/s][AEpoch 8:  97%|█████████▋| 4240/4381 [1:40:30<03:20,  1.42s/it, loss=2.73, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:36<00:29,  4.64it/s][AEpoch 8:  97%|█████████▋| 4250/4381 [1:40:32<03:05,  1.42s/it, loss=2.73, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:37<00:24,  5.12it/s][AEpoch 8:  97%|█████████▋| 4260/4381 [1:40:34<02:51,  1.42s/it, loss=2.73, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:39<00:23,  4.93it/s][AEpoch 8:  97%|█████████▋| 4270/4381 [1:40:36<02:36,  1.41s/it, loss=2.73, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:41<00:19,  5.50it/s][AEpoch 8:  98%|█████████▊| 4280/4381 [1:40:37<02:22,  1.41s/it, loss=2.73, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:42<00:17,  5.46it/s][AEpoch 8:  98%|█████████▊| 4290/4381 [1:40:39<02:08,  1.41s/it, loss=2.73, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:44<00:15,  5.45it/s][AEpoch 8:  98%|█████████▊| 4300/4381 [1:40:41<01:53,  1.40s/it, loss=2.73, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:46<00:12,  5.97it/s][AEpoch 8:  98%|█████████▊| 4310/4381 [1:40:42<01:39,  1.40s/it, loss=2.73, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:47<00:10,  6.18it/s][AEpoch 8:  99%|█████████▊| 4320/4381 [1:40:44<01:25,  1.40s/it, loss=2.73, v_num=641]
Validating:  91%|█████████ | 570/626 [01:48<00:08,  6.38it/s][AEpoch 8:  99%|█████████▉| 4330/4381 [1:40:45<01:11,  1.40s/it, loss=2.73, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:51<00:07,  5.85it/s][AEpoch 8:  99%|█████████▉| 4340/4381 [1:40:47<00:57,  1.39s/it, loss=2.73, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:52<00:05,  6.27it/s][AEpoch 8:  99%|█████████▉| 4350/4381 [1:40:49<00:43,  1.39s/it, loss=2.73, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:53<00:03,  7.13it/s][AEpoch 8: 100%|█████████▉| 4360/4381 [1:40:50<00:29,  1.39s/it, loss=2.73, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:54<00:02,  7.40it/s][AEpoch 8: 100%|█████████▉| 4370/4381 [1:40:51<00:15,  1.38s/it, loss=2.73, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:56<00:00,  6.22it/s][AEpoch 8: 100%|█████████▉| 4380/4381 [1:40:53<00:01,  1.38s/it, loss=2.73, v_num=641]
Validating: 100%|██████████| 626/626 [01:57<00:00,  7.09it/s][Avalidation_epoch_end
graph acc: 0.24281150159744408
valid accuracy: 0.9638036489486694
Epoch 8: 100%|██████████| 4381/4381 [1:40:56<00:00,  1.38s/it, loss=2.72, v_num=641]
                                                             [AEpoch 8:   0%|          | 0/4381 [00:00<00:00, 7121.06it/s, loss=2.72, v_num=641]   Epoch 9:   0%|          | 0/4381 [00:00<00:02, 1896.16it/s, loss=2.72, v_num=641]Epoch 9:   0%|          | 0/4381 [00:16<19:44:05, 16.22s/it, loss=2.72, v_num=641]Epoch 9:   0%|          | 10/4381 [00:25<2:48:01,  2.31s/it, loss=2.72, v_num=641]Epoch 9:   0%|          | 10/4381 [00:25<2:48:01,  2.31s/it, loss=2.68, v_num=641]Epoch 9:   0%|          | 20/4381 [00:40<2:21:34,  1.95s/it, loss=2.68, v_num=641]Epoch 9:   0%|          | 20/4381 [00:40<2:21:35,  1.95s/it, loss=2.66, v_num=641]Epoch 9:   1%|          | 30/4381 [00:57<2:13:57,  1.85s/it, loss=2.66, v_num=641]Epoch 9:   1%|          | 30/4381 [00:57<2:13:57,  1.85s/it, loss=2.64, v_num=641]Epoch 9:   1%|          | 40/4381 [01:12<2:07:45,  1.77s/it, loss=2.64, v_num=641]Epoch 9:   1%|          | 40/4381 [01:12<2:07:45,  1.77s/it, loss=2.59, v_num=641]Epoch 9:   1%|          | 50/4381 [01:28<2:04:58,  1.73s/it, loss=2.59, v_num=641]Epoch 9:   1%|          | 50/4381 [01:28<2:04:58,  1.73s/it, loss=2.59, v_num=641]Epoch 9:   1%|▏         | 60/4381 [01:46<2:05:54,  1.75s/it, loss=2.59, v_num=641]Epoch 9:   1%|▏         | 60/4381 [01:46<2:05:54,  1.75s/it, loss=2.62, v_num=641]Epoch 9:   2%|▏         | 70/4381 [02:02<2:04:06,  1.73s/it, loss=2.62, v_num=641]Epoch 9:   2%|▏         | 70/4381 [02:02<2:04:06,  1.73s/it, loss=2.64, v_num=641]Epoch 9:   2%|▏         | 80/4381 [02:19<2:03:46,  1.73s/it, loss=2.64, v_num=641]Epoch 9:   2%|▏         | 80/4381 [02:19<2:03:46,  1.73s/it, loss=2.69, v_num=641]Epoch 9:   2%|▏         | 90/4381 [02:34<2:01:32,  1.70s/it, loss=2.69, v_num=641]Epoch 9:   2%|▏         | 90/4381 [02:34<2:01:32,  1.70s/it, loss=2.68, v_num=641]Epoch 9:   2%|▏         | 100/4381 [02:51<2:00:48,  1.69s/it, loss=2.68, v_num=641]Epoch 9:   2%|▏         | 100/4381 [02:51<2:00:48,  1.69s/it, loss=2.64, v_num=641]Epoch 9:   3%|▎         | 110/4381 [03:10<2:01:53,  1.71s/it, loss=2.64, v_num=641]Epoch 9:   3%|▎         | 110/4381 [03:10<2:01:53,  1.71s/it, loss=2.64, v_num=641]Epoch 9:   3%|▎         | 120/4381 [03:28<2:02:13,  1.72s/it, loss=2.64, v_num=641]Epoch 9:   3%|▎         | 120/4381 [03:28<2:02:13,  1.72s/it, loss=2.63, v_num=641]Epoch 9:   3%|▎         | 130/4381 [03:44<2:01:19,  1.71s/it, loss=2.63, v_num=641]Epoch 9:   3%|▎         | 130/4381 [03:44<2:01:19,  1.71s/it, loss=2.58, v_num=641]Epoch 9:   3%|▎         | 140/4381 [04:01<2:01:06,  1.71s/it, loss=2.58, v_num=641]Epoch 9:   3%|▎         | 140/4381 [04:01<2:01:06,  1.71s/it, loss=2.62, v_num=641]Epoch 9:   3%|▎         | 150/4381 [04:16<1:59:34,  1.70s/it, loss=2.62, v_num=641]Epoch 9:   3%|▎         | 150/4381 [04:16<1:59:34,  1.70s/it, loss=2.67, v_num=641]Epoch 9:   4%|▎         | 160/4381 [04:32<1:59:15,  1.70s/it, loss=2.67, v_num=641]Epoch 9:   4%|▎         | 160/4381 [04:32<1:59:15,  1.70s/it, loss=2.66, v_num=641]Epoch 9:   4%|▍         | 170/4381 [04:51<1:59:42,  1.71s/it, loss=2.66, v_num=641]Epoch 9:   4%|▍         | 170/4381 [04:51<1:59:42,  1.71s/it, loss=2.61, v_num=641]Epoch 9:   4%|▍         | 180/4381 [05:08<1:59:29,  1.71s/it, loss=2.61, v_num=641]Epoch 9:   4%|▍         | 180/4381 [05:08<1:59:29,  1.71s/it, loss=2.65, v_num=641]Epoch 9:   4%|▍         | 190/4381 [05:24<1:58:37,  1.70s/it, loss=2.65, v_num=641]Epoch 9:   4%|▍         | 190/4381 [05:24<1:58:37,  1.70s/it, loss=2.69, v_num=641]Epoch 9:   5%|▍         | 200/4381 [05:44<1:59:22,  1.71s/it, loss=2.69, v_num=641]Epoch 9:   5%|▍         | 200/4381 [05:44<1:59:22,  1.71s/it, loss=2.63, v_num=641]Epoch 9:   5%|▍         | 210/4381 [05:57<1:57:42,  1.69s/it, loss=2.63, v_num=641]Epoch 9:   5%|▍         | 210/4381 [05:57<1:57:42,  1.69s/it, loss=2.63, v_num=641]Epoch 9:   5%|▌         | 220/4381 [06:11<1:56:42,  1.68s/it, loss=2.63, v_num=641]Epoch 9:   5%|▌         | 220/4381 [06:11<1:56:42,  1.68s/it, loss=2.66, v_num=641]Epoch 9:   5%|▌         | 230/4381 [06:29<1:56:48,  1.69s/it, loss=2.66, v_num=641]Epoch 9:   5%|▌         | 230/4381 [06:29<1:56:48,  1.69s/it, loss=2.65, v_num=641]Epoch 9:   5%|▌         | 240/4381 [06:45<1:56:11,  1.68s/it, loss=2.65, v_num=641]Epoch 9:   5%|▌         | 240/4381 [06:45<1:56:11,  1.68s/it, loss=2.61, v_num=641]Epoch 9:   6%|▌         | 250/4381 [06:59<1:54:56,  1.67s/it, loss=2.61, v_num=641]Epoch 9:   6%|▌         | 250/4381 [06:59<1:54:56,  1.67s/it, loss=2.6, v_num=641] Epoch 9:   6%|▌         | 260/4381 [07:15<1:54:42,  1.67s/it, loss=2.6, v_num=641]Epoch 9:   6%|▌         | 260/4381 [07:15<1:54:42,  1.67s/it, loss=2.62, v_num=641]Epoch 9:   6%|▌         | 270/4381 [07:29<1:53:41,  1.66s/it, loss=2.62, v_num=641]Epoch 9:   6%|▌         | 270/4381 [07:29<1:53:41,  1.66s/it, loss=2.61, v_num=641]Epoch 9:   6%|▋         | 280/4381 [07:46<1:53:26,  1.66s/it, loss=2.61, v_num=641]Epoch 9:   6%|▋         | 280/4381 [07:46<1:53:26,  1.66s/it, loss=2.64, v_num=641]Epoch 9:   7%|▋         | 290/4381 [08:03<1:53:18,  1.66s/it, loss=2.64, v_num=641]Epoch 9:   7%|▋         | 290/4381 [08:03<1:53:18,  1.66s/it, loss=2.68, v_num=641]Epoch 9:   7%|▋         | 300/4381 [08:19<1:52:48,  1.66s/it, loss=2.68, v_num=641]Epoch 9:   7%|▋         | 300/4381 [08:19<1:52:48,  1.66s/it, loss=2.66, v_num=641]Epoch 9:   7%|▋         | 310/4381 [08:35<1:52:32,  1.66s/it, loss=2.66, v_num=641]Epoch 9:   7%|▋         | 310/4381 [08:35<1:52:32,  1.66s/it, loss=2.65, v_num=641]Epoch 9:   7%|▋         | 320/4381 [08:50<1:51:54,  1.65s/it, loss=2.65, v_num=641]Epoch 9:   7%|▋         | 320/4381 [08:50<1:51:54,  1.65s/it, loss=2.65, v_num=641]Epoch 9:   8%|▊         | 330/4381 [09:05<1:51:19,  1.65s/it, loss=2.65, v_num=641]Epoch 9:   8%|▊         | 330/4381 [09:05<1:51:19,  1.65s/it, loss=2.6, v_num=641] Epoch 9:   8%|▊         | 340/4381 [09:23<1:51:16,  1.65s/it, loss=2.6, v_num=641]Epoch 9:   8%|▊         | 340/4381 [09:23<1:51:16,  1.65s/it, loss=2.64, v_num=641]Epoch 9:   8%|▊         | 350/4381 [09:38<1:50:40,  1.65s/it, loss=2.64, v_num=641]Epoch 9:   8%|▊         | 350/4381 [09:38<1:50:40,  1.65s/it, loss=2.66, v_num=641]Epoch 9:   8%|▊         | 360/4381 [09:54<1:50:16,  1.65s/it, loss=2.66, v_num=641]Epoch 9:   8%|▊         | 360/4381 [09:54<1:50:16,  1.65s/it, loss=2.65, v_num=641]Epoch 9:   8%|▊         | 370/4381 [10:11<1:50:12,  1.65s/it, loss=2.65, v_num=641]Epoch 9:   8%|▊         | 370/4381 [10:11<1:50:12,  1.65s/it, loss=2.64, v_num=641]Epoch 9:   9%|▊         | 380/4381 [10:27<1:49:46,  1.65s/it, loss=2.64, v_num=641]Epoch 9:   9%|▊         | 380/4381 [10:27<1:49:46,  1.65s/it, loss=2.61, v_num=641]Epoch 9:   9%|▉         | 390/4381 [10:43<1:49:27,  1.65s/it, loss=2.61, v_num=641]Epoch 9:   9%|▉         | 390/4381 [10:43<1:49:27,  1.65s/it, loss=2.6, v_num=641] Epoch 9:   9%|▉         | 400/4381 [10:58<1:48:57,  1.64s/it, loss=2.6, v_num=641]Epoch 9:   9%|▉         | 400/4381 [10:58<1:48:57,  1.64s/it, loss=2.62, v_num=641]Epoch 9:   9%|▉         | 410/4381 [11:12<1:48:20,  1.64s/it, loss=2.62, v_num=641]Epoch 9:   9%|▉         | 410/4381 [11:12<1:48:20,  1.64s/it, loss=2.64, v_num=641]Epoch 9:  10%|▉         | 420/4381 [11:30<1:48:15,  1.64s/it, loss=2.64, v_num=641]Epoch 9:  10%|▉         | 420/4381 [11:30<1:48:15,  1.64s/it, loss=2.65, v_num=641]Epoch 9:  10%|▉         | 430/4381 [11:45<1:47:47,  1.64s/it, loss=2.65, v_num=641]Epoch 9:  10%|▉         | 430/4381 [11:45<1:47:47,  1.64s/it, loss=2.67, v_num=641]Epoch 9:  10%|█         | 440/4381 [12:02<1:47:39,  1.64s/it, loss=2.67, v_num=641]Epoch 9:  10%|█         | 440/4381 [12:02<1:47:39,  1.64s/it, loss=2.67, v_num=641]Epoch 9:  10%|█         | 450/4381 [12:18<1:47:16,  1.64s/it, loss=2.67, v_num=641]Epoch 9:  10%|█         | 450/4381 [12:18<1:47:16,  1.64s/it, loss=2.62, v_num=641]Epoch 9:  10%|█         | 460/4381 [12:34<1:46:56,  1.64s/it, loss=2.62, v_num=641]Epoch 9:  10%|█         | 460/4381 [12:34<1:46:56,  1.64s/it, loss=2.63, v_num=641]Epoch 9:  11%|█         | 470/4381 [12:48<1:46:19,  1.63s/it, loss=2.63, v_num=641]Epoch 9:  11%|█         | 470/4381 [12:48<1:46:19,  1.63s/it, loss=2.66, v_num=641]Epoch 9:  11%|█         | 480/4381 [13:03<1:45:52,  1.63s/it, loss=2.66, v_num=641]Epoch 9:  11%|█         | 480/4381 [13:03<1:45:52,  1.63s/it, loss=2.67, v_num=641]Epoch 9:  11%|█         | 490/4381 [13:19<1:45:38,  1.63s/it, loss=2.67, v_num=641]Epoch 9:  11%|█         | 490/4381 [13:19<1:45:38,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  11%|█▏        | 500/4381 [13:36<1:45:23,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  11%|█▏        | 500/4381 [13:36<1:45:23,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 510/4381 [13:52<1:45:06,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 510/4381 [13:52<1:45:06,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 520/4381 [14:07<1:44:41,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 520/4381 [14:07<1:44:41,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 530/4381 [14:24<1:44:28,  1.63s/it, loss=2.68, v_num=641]Epoch 9:  12%|█▏        | 530/4381 [14:24<1:44:28,  1.63s/it, loss=2.64, v_num=641]Epoch 9:  12%|█▏        | 540/4381 [14:39<1:44:04,  1.63s/it, loss=2.64, v_num=641]Epoch 9:  12%|█▏        | 540/4381 [14:39<1:44:04,  1.63s/it, loss=2.62, v_num=641]Epoch 9:  13%|█▎        | 550/4381 [14:56<1:43:56,  1.63s/it, loss=2.62, v_num=641]Epoch 9:  13%|█▎        | 550/4381 [14:56<1:43:56,  1.63s/it, loss=2.65, v_num=641]Epoch 9:  13%|█▎        | 560/4381 [15:13<1:43:40,  1.63s/it, loss=2.65, v_num=641]Epoch 9:  13%|█▎        | 560/4381 [15:13<1:43:40,  1.63s/it, loss=2.62, v_num=641]Epoch 9:  13%|█▎        | 570/4381 [15:28<1:43:17,  1.63s/it, loss=2.62, v_num=641]Epoch 9:  13%|█▎        | 570/4381 [15:28<1:43:17,  1.63s/it, loss=2.65, v_num=641]Epoch 9:  13%|█▎        | 580/4381 [15:47<1:43:17,  1.63s/it, loss=2.65, v_num=641]Epoch 9:  13%|█▎        | 580/4381 [15:47<1:43:17,  1.63s/it, loss=2.67, v_num=641]Epoch 9:  13%|█▎        | 590/4381 [16:02<1:42:56,  1.63s/it, loss=2.67, v_num=641]Epoch 9:  13%|█▎        | 590/4381 [16:02<1:42:56,  1.63s/it, loss=2.63, v_num=641]Epoch 9:  14%|█▎        | 600/4381 [16:14<1:42:10,  1.62s/it, loss=2.63, v_num=641]Epoch 9:  14%|█▎        | 600/4381 [16:14<1:42:10,  1.62s/it, loss=2.63, v_num=641]Epoch 9:  14%|█▍        | 610/4381 [16:30<1:41:55,  1.62s/it, loss=2.63, v_num=641]Epoch 9:  14%|█▍        | 610/4381 [16:30<1:41:55,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  14%|█▍        | 620/4381 [16:46<1:41:37,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  14%|█▍        | 620/4381 [16:46<1:41:37,  1.62s/it, loss=2.65, v_num=641]Epoch 9:  14%|█▍        | 630/4381 [17:01<1:41:10,  1.62s/it, loss=2.65, v_num=641]Epoch 9:  14%|█▍        | 630/4381 [17:01<1:41:10,  1.62s/it, loss=2.67, v_num=641]Epoch 9:  15%|█▍        | 640/4381 [17:19<1:41:05,  1.62s/it, loss=2.67, v_num=641]Epoch 9:  15%|█▍        | 640/4381 [17:19<1:41:05,  1.62s/it, loss=2.68, v_num=641]Epoch 9:  15%|█▍        | 650/4381 [17:34<1:40:41,  1.62s/it, loss=2.68, v_num=641]Epoch 9:  15%|█▍        | 650/4381 [17:34<1:40:41,  1.62s/it, loss=2.67, v_num=641]Epoch 9:  15%|█▌        | 660/4381 [17:51<1:40:32,  1.62s/it, loss=2.67, v_num=641]Epoch 9:  15%|█▌        | 660/4381 [17:51<1:40:32,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  15%|█▌        | 670/4381 [18:06<1:40:10,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  15%|█▌        | 670/4381 [18:06<1:40:10,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  16%|█▌        | 680/4381 [18:20<1:39:38,  1.62s/it, loss=2.64, v_num=641]Epoch 9:  16%|█▌        | 680/4381 [18:20<1:39:38,  1.62s/it, loss=2.69, v_num=641]Epoch 9:  16%|█▌        | 690/4381 [18:34<1:39:13,  1.61s/it, loss=2.69, v_num=641]Epoch 9:  16%|█▌        | 690/4381 [18:34<1:39:13,  1.61s/it, loss=2.7, v_num=641] Epoch 9:  16%|█▌        | 700/4381 [18:50<1:38:56,  1.61s/it, loss=2.7, v_num=641]Epoch 9:  16%|█▌        | 700/4381 [18:50<1:38:56,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  16%|█▌        | 710/4381 [19:04<1:38:26,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  16%|█▌        | 710/4381 [19:04<1:38:26,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  16%|█▋        | 720/4381 [19:21<1:38:18,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  16%|█▋        | 720/4381 [19:21<1:38:18,  1.61s/it, loss=2.63, v_num=641]Epoch 9:  17%|█▋        | 730/4381 [19:36<1:37:55,  1.61s/it, loss=2.63, v_num=641]Epoch 9:  17%|█▋        | 730/4381 [19:36<1:37:55,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  17%|█▋        | 740/4381 [19:54<1:37:50,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  17%|█▋        | 740/4381 [19:54<1:37:50,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  17%|█▋        | 750/4381 [20:12<1:37:42,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  17%|█▋        | 750/4381 [20:12<1:37:42,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  17%|█▋        | 760/4381 [20:28<1:37:25,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  17%|█▋        | 760/4381 [20:28<1:37:25,  1.61s/it, loss=2.61, v_num=641]Epoch 9:  18%|█▊        | 770/4381 [20:42<1:36:57,  1.61s/it, loss=2.61, v_num=641]Epoch 9:  18%|█▊        | 770/4381 [20:42<1:36:57,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  18%|█▊        | 780/4381 [21:01<1:36:55,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  18%|█▊        | 780/4381 [21:01<1:36:55,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  18%|█▊        | 790/4381 [21:17<1:36:39,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  18%|█▊        | 790/4381 [21:17<1:36:39,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  18%|█▊        | 800/4381 [21:30<1:36:08,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  18%|█▊        | 800/4381 [21:30<1:36:08,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  18%|█▊        | 810/4381 [21:49<1:36:07,  1.62s/it, loss=2.66, v_num=641]Epoch 9:  18%|█▊        | 810/4381 [21:49<1:36:07,  1.62s/it, loss=2.68, v_num=641]Epoch 9:  19%|█▊        | 820/4381 [22:05<1:35:49,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  19%|█▊        | 820/4381 [22:05<1:35:49,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  19%|█▉        | 830/4381 [22:19<1:35:23,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  19%|█▉        | 830/4381 [22:19<1:35:23,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  19%|█▉        | 840/4381 [22:37<1:35:15,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  19%|█▉        | 840/4381 [22:37<1:35:15,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  19%|█▉        | 850/4381 [22:51<1:34:52,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  19%|█▉        | 850/4381 [22:51<1:34:52,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  20%|█▉        | 860/4381 [23:06<1:34:28,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  20%|█▉        | 860/4381 [23:06<1:34:28,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  20%|█▉        | 870/4381 [23:24<1:34:19,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  20%|█▉        | 870/4381 [23:24<1:34:19,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  20%|██        | 880/4381 [23:38<1:33:56,  1.61s/it, loss=2.62, v_num=641]Epoch 9:  20%|██        | 880/4381 [23:38<1:33:56,  1.61s/it, loss=2.6, v_num=641] Epoch 9:  20%|██        | 890/4381 [23:52<1:33:31,  1.61s/it, loss=2.6, v_num=641]Epoch 9:  20%|██        | 890/4381 [23:52<1:33:31,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  21%|██        | 900/4381 [24:09<1:33:20,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  21%|██        | 900/4381 [24:09<1:33:20,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  21%|██        | 910/4381 [24:24<1:32:58,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  21%|██        | 910/4381 [24:24<1:32:58,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  21%|██        | 920/4381 [24:41<1:32:48,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  21%|██        | 920/4381 [24:41<1:32:48,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  21%|██        | 930/4381 [24:57<1:32:29,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  21%|██        | 930/4381 [24:57<1:32:29,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  21%|██▏       | 940/4381 [25:11<1:32:08,  1.61s/it, loss=2.64, v_num=641]Epoch 9:  21%|██▏       | 940/4381 [25:11<1:32:08,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  22%|██▏       | 950/4381 [25:29<1:31:56,  1.61s/it, loss=2.65, v_num=641]Epoch 9:  22%|██▏       | 950/4381 [25:29<1:31:56,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  22%|██▏       | 960/4381 [25:44<1:31:37,  1.61s/it, loss=2.67, v_num=641]Epoch 9:  22%|██▏       | 960/4381 [25:44<1:31:37,  1.61s/it, loss=2.68, v_num=641]Epoch 9:  22%|██▏       | 970/4381 [25:56<1:31:08,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  22%|██▏       | 970/4381 [25:56<1:31:08,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  22%|██▏       | 980/4381 [26:15<1:31:00,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  22%|██▏       | 980/4381 [26:15<1:31:00,  1.61s/it, loss=2.66, v_num=641]Epoch 9:  23%|██▎       | 990/4381 [26:29<1:30:39,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  23%|██▎       | 990/4381 [26:29<1:30:39,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  23%|██▎       | 1000/4381 [26:42<1:30:13,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  23%|██▎       | 1000/4381 [26:42<1:30:13,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  23%|██▎       | 1010/4381 [26:58<1:29:55,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  23%|██▎       | 1010/4381 [26:58<1:29:55,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  23%|██▎       | 1020/4381 [27:14<1:29:39,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  23%|██▎       | 1020/4381 [27:14<1:29:39,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  24%|██▎       | 1030/4381 [27:29<1:29:22,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  24%|██▎       | 1030/4381 [27:29<1:29:22,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  24%|██▎       | 1040/4381 [27:49<1:29:19,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  24%|██▎       | 1040/4381 [27:49<1:29:19,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  24%|██▍       | 1050/4381 [28:05<1:29:02,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  24%|██▍       | 1050/4381 [28:05<1:29:02,  1.60s/it, loss=2.6, v_num=641] Epoch 9:  24%|██▍       | 1060/4381 [28:20<1:28:41,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  24%|██▍       | 1060/4381 [28:20<1:28:41,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  24%|██▍       | 1070/4381 [28:37<1:28:28,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  24%|██▍       | 1070/4381 [28:37<1:28:28,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▍       | 1080/4381 [28:50<1:28:04,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▍       | 1080/4381 [28:50<1:28:04,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  25%|██▍       | 1090/4381 [29:06<1:27:48,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  25%|██▍       | 1090/4381 [29:06<1:27:48,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▌       | 1100/4381 [29:22<1:27:32,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▌       | 1100/4381 [29:22<1:27:32,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▌       | 1110/4381 [29:36<1:27:10,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  25%|██▌       | 1110/4381 [29:36<1:27:10,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  26%|██▌       | 1120/4381 [29:50<1:26:49,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  26%|██▌       | 1120/4381 [29:50<1:26:49,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  26%|██▌       | 1130/4381 [30:10<1:26:43,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  26%|██▌       | 1130/4381 [30:10<1:26:43,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  26%|██▌       | 1140/4381 [30:26<1:26:27,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  26%|██▌       | 1140/4381 [30:26<1:26:27,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  26%|██▌       | 1150/4381 [30:41<1:26:09,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  26%|██▌       | 1150/4381 [30:41<1:26:09,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  26%|██▋       | 1160/4381 [30:58<1:25:57,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  26%|██▋       | 1160/4381 [30:58<1:25:57,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  27%|██▋       | 1170/4381 [31:16<1:25:44,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  27%|██▋       | 1170/4381 [31:16<1:25:44,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  27%|██▋       | 1180/4381 [31:30<1:25:23,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  27%|██▋       | 1180/4381 [31:30<1:25:23,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  27%|██▋       | 1190/4381 [31:48<1:25:12,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  27%|██▋       | 1190/4381 [31:48<1:25:12,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  27%|██▋       | 1200/4381 [32:00<1:24:45,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  27%|██▋       | 1200/4381 [32:00<1:24:45,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  28%|██▊       | 1210/4381 [32:14<1:24:26,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  28%|██▊       | 1210/4381 [32:14<1:24:26,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  28%|██▊       | 1220/4381 [32:32<1:24:14,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  28%|██▊       | 1220/4381 [32:32<1:24:14,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  28%|██▊       | 1230/4381 [32:47<1:23:57,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  28%|██▊       | 1230/4381 [32:47<1:23:57,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  28%|██▊       | 1240/4381 [33:02<1:23:37,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  28%|██▊       | 1240/4381 [33:02<1:23:37,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▊       | 1250/4381 [33:18<1:23:22,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▊       | 1250/4381 [33:18<1:23:22,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▉       | 1260/4381 [33:33<1:23:03,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▉       | 1260/4381 [33:33<1:23:03,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▉       | 1270/4381 [33:47<1:22:41,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  29%|██▉       | 1270/4381 [33:47<1:22:41,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  29%|██▉       | 1280/4381 [34:06<1:22:33,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  29%|██▉       | 1280/4381 [34:06<1:22:33,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  29%|██▉       | 1290/4381 [34:21<1:22:15,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  29%|██▉       | 1290/4381 [34:21<1:22:15,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  30%|██▉       | 1300/4381 [34:35<1:21:54,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  30%|██▉       | 1300/4381 [34:35<1:21:54,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  30%|██▉       | 1310/4381 [34:54<1:21:46,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  30%|██▉       | 1310/4381 [34:54<1:21:46,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  30%|███       | 1320/4381 [35:09<1:21:28,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  30%|███       | 1320/4381 [35:09<1:21:28,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  30%|███       | 1330/4381 [35:26<1:21:13,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  30%|███       | 1330/4381 [35:26<1:21:13,  1.60s/it, loss=2.59, v_num=641]Epoch 9:  31%|███       | 1340/4381 [35:46<1:21:06,  1.60s/it, loss=2.59, v_num=641]Epoch 9:  31%|███       | 1340/4381 [35:46<1:21:06,  1.60s/it, loss=2.6, v_num=641] Epoch 9:  31%|███       | 1350/4381 [36:01<1:20:49,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  31%|███       | 1350/4381 [36:01<1:20:49,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  31%|███       | 1360/4381 [36:14<1:20:27,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  31%|███       | 1360/4381 [36:14<1:20:27,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  31%|███▏      | 1370/4381 [36:33<1:20:16,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  31%|███▏      | 1370/4381 [36:33<1:20:16,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  31%|███▏      | 1380/4381 [36:48<1:20:00,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  31%|███▏      | 1380/4381 [36:48<1:20:00,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  32%|███▏      | 1390/4381 [37:03<1:19:40,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  32%|███▏      | 1390/4381 [37:03<1:19:40,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  32%|███▏      | 1400/4381 [37:19<1:19:25,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  32%|███▏      | 1400/4381 [37:19<1:19:25,  1.60s/it, loss=2.71, v_num=641]Epoch 9:  32%|███▏      | 1410/4381 [37:35<1:19:09,  1.60s/it, loss=2.71, v_num=641]Epoch 9:  32%|███▏      | 1410/4381 [37:35<1:19:09,  1.60s/it, loss=2.7, v_num=641] Epoch 9:  32%|███▏      | 1420/4381 [37:51<1:18:52,  1.60s/it, loss=2.7, v_num=641]Epoch 9:  32%|███▏      | 1420/4381 [37:51<1:18:52,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  33%|███▎      | 1430/4381 [38:10<1:18:43,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  33%|███▎      | 1430/4381 [38:10<1:18:43,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  33%|███▎      | 1440/4381 [38:24<1:18:22,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  33%|███▎      | 1440/4381 [38:24<1:18:22,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  33%|███▎      | 1450/4381 [38:39<1:18:06,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  33%|███▎      | 1450/4381 [38:39<1:18:06,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  33%|███▎      | 1460/4381 [38:57<1:17:54,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  33%|███▎      | 1460/4381 [38:57<1:17:54,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▎      | 1470/4381 [39:11<1:17:33,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▎      | 1470/4381 [39:11<1:17:33,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1480/4381 [39:24<1:17:11,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1480/4381 [39:24<1:17:11,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  34%|███▍      | 1490/4381 [39:42<1:16:59,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  34%|███▍      | 1490/4381 [39:42<1:16:59,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1500/4381 [39:57<1:16:42,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1500/4381 [39:57<1:16:42,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1510/4381 [40:12<1:16:22,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  34%|███▍      | 1510/4381 [40:12<1:16:22,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  35%|███▍      | 1520/4381 [40:29<1:16:10,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  35%|███▍      | 1520/4381 [40:29<1:16:10,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  35%|███▍      | 1530/4381 [40:44<1:15:52,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  35%|███▍      | 1530/4381 [40:44<1:15:52,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  35%|███▌      | 1540/4381 [41:04<1:15:42,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  35%|███▌      | 1540/4381 [41:04<1:15:42,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  35%|███▌      | 1550/4381 [41:24<1:15:34,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  35%|███▌      | 1550/4381 [41:24<1:15:34,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  36%|███▌      | 1560/4381 [41:38<1:15:15,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  36%|███▌      | 1560/4381 [41:38<1:15:15,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  36%|███▌      | 1570/4381 [41:53<1:14:57,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  36%|███▌      | 1570/4381 [41:53<1:14:57,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  36%|███▌      | 1580/4381 [42:13<1:14:48,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  36%|███▌      | 1580/4381 [42:13<1:14:48,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  36%|███▋      | 1590/4381 [42:28<1:14:30,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  36%|███▋      | 1590/4381 [42:28<1:14:30,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  37%|███▋      | 1600/4381 [42:42<1:14:11,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  37%|███▋      | 1600/4381 [42:42<1:14:11,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  37%|███▋      | 1610/4381 [42:57<1:13:53,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  37%|███▋      | 1610/4381 [42:57<1:13:53,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  37%|███▋      | 1620/4381 [43:12<1:13:35,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  37%|███▋      | 1620/4381 [43:12<1:13:35,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  37%|███▋      | 1630/4381 [43:25<1:13:13,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  37%|███▋      | 1630/4381 [43:25<1:13:13,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  37%|███▋      | 1640/4381 [43:45<1:13:05,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  37%|███▋      | 1640/4381 [43:45<1:13:05,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  38%|███▊      | 1650/4381 [44:03<1:12:52,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  38%|███▊      | 1650/4381 [44:03<1:12:52,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  38%|███▊      | 1660/4381 [44:16<1:12:32,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  38%|███▊      | 1660/4381 [44:16<1:12:32,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  38%|███▊      | 1670/4381 [44:36<1:12:21,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  38%|███▊      | 1670/4381 [44:36<1:12:21,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  38%|███▊      | 1680/4381 [44:50<1:12:02,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  38%|███▊      | 1680/4381 [44:50<1:12:02,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  39%|███▊      | 1690/4381 [45:04<1:11:44,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  39%|███▊      | 1690/4381 [45:04<1:11:44,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  39%|███▉      | 1700/4381 [45:21<1:11:28,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  39%|███▉      | 1700/4381 [45:21<1:11:28,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  39%|███▉      | 1710/4381 [45:36<1:11:11,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  39%|███▉      | 1710/4381 [45:36<1:11:11,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  39%|███▉      | 1720/4381 [45:52<1:10:56,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  39%|███▉      | 1720/4381 [45:52<1:10:56,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  39%|███▉      | 1730/4381 [46:10<1:10:43,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  39%|███▉      | 1730/4381 [46:10<1:10:43,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  40%|███▉      | 1740/4381 [46:25<1:10:25,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  40%|███▉      | 1740/4381 [46:25<1:10:25,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  40%|███▉      | 1750/4381 [46:39<1:10:06,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  40%|███▉      | 1750/4381 [46:39<1:10:06,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  40%|████      | 1760/4381 [46:58<1:09:54,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  40%|████      | 1760/4381 [46:58<1:09:54,  1.60s/it, loss=2.6, v_num=641] Epoch 9:  40%|████      | 1770/4381 [47:12<1:09:35,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  40%|████      | 1770/4381 [47:12<1:09:35,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  41%|████      | 1780/4381 [47:29<1:09:21,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  41%|████      | 1780/4381 [47:29<1:09:21,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  41%|████      | 1790/4381 [47:45<1:09:05,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  41%|████      | 1790/4381 [47:45<1:09:05,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  41%|████      | 1800/4381 [47:59<1:08:46,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  41%|████      | 1800/4381 [47:59<1:08:46,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  41%|████▏     | 1810/4381 [48:16<1:08:31,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  41%|████▏     | 1810/4381 [48:16<1:08:31,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  42%|████▏     | 1820/4381 [48:34<1:08:18,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  42%|████▏     | 1820/4381 [48:34<1:08:18,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  42%|████▏     | 1830/4381 [48:49<1:08:01,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  42%|████▏     | 1830/4381 [48:49<1:08:01,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  42%|████▏     | 1840/4381 [49:04<1:07:43,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  42%|████▏     | 1840/4381 [49:04<1:07:43,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  42%|████▏     | 1850/4381 [49:19<1:07:27,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  42%|████▏     | 1850/4381 [49:19<1:07:27,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  42%|████▏     | 1860/4381 [49:35<1:07:11,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  42%|████▏     | 1860/4381 [49:35<1:07:11,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  43%|████▎     | 1870/4381 [49:51<1:06:54,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  43%|████▎     | 1870/4381 [49:51<1:06:54,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  43%|████▎     | 1880/4381 [50:06<1:06:37,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  43%|████▎     | 1880/4381 [50:06<1:06:37,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  43%|████▎     | 1890/4381 [50:21<1:06:19,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  43%|████▎     | 1890/4381 [50:21<1:06:19,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  43%|████▎     | 1900/4381 [50:35<1:06:01,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  43%|████▎     | 1900/4381 [50:35<1:06:01,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  44%|████▎     | 1910/4381 [50:56<1:05:52,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  44%|████▎     | 1910/4381 [50:56<1:05:52,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  44%|████▍     | 1920/4381 [51:10<1:05:33,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  44%|████▍     | 1920/4381 [51:10<1:05:33,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  44%|████▍     | 1930/4381 [51:22<1:05:12,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  44%|████▍     | 1930/4381 [51:22<1:05:12,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  44%|████▍     | 1940/4381 [51:41<1:05:01,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  44%|████▍     | 1940/4381 [51:41<1:05:01,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  45%|████▍     | 1950/4381 [51:56<1:04:43,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  45%|████▍     | 1950/4381 [51:56<1:04:43,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  45%|████▍     | 1960/4381 [52:11<1:04:26,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  45%|████▍     | 1960/4381 [52:11<1:04:26,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  45%|████▍     | 1970/4381 [52:32<1:04:16,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  45%|████▍     | 1970/4381 [52:32<1:04:16,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  45%|████▌     | 1980/4381 [52:46<1:03:58,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  45%|████▌     | 1980/4381 [52:46<1:03:58,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  45%|████▌     | 1990/4381 [53:01<1:03:40,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  45%|████▌     | 1990/4381 [53:01<1:03:40,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  46%|████▌     | 2000/4381 [53:19<1:03:27,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  46%|████▌     | 2000/4381 [53:19<1:03:27,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  46%|████▌     | 2010/4381 [53:35<1:03:11,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  46%|████▌     | 2010/4381 [53:35<1:03:11,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  46%|████▌     | 2020/4381 [53:51<1:02:54,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  46%|████▌     | 2020/4381 [53:51<1:02:54,  1.60s/it, loss=2.6, v_num=641] Epoch 9:  46%|████▋     | 2030/4381 [54:10<1:02:42,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  46%|████▋     | 2030/4381 [54:10<1:02:42,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  47%|████▋     | 2040/4381 [54:24<1:02:23,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  47%|████▋     | 2040/4381 [54:24<1:02:23,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  47%|████▋     | 2050/4381 [54:40<1:02:07,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  47%|████▋     | 2050/4381 [54:40<1:02:07,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  47%|████▋     | 2060/4381 [54:56<1:01:52,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  47%|████▋     | 2060/4381 [54:56<1:01:52,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  47%|████▋     | 2070/4381 [55:12<1:01:36,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  47%|████▋     | 2070/4381 [55:12<1:01:36,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  47%|████▋     | 2080/4381 [55:25<1:01:17,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  47%|████▋     | 2080/4381 [55:25<1:01:17,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  48%|████▊     | 2090/4381 [55:42<1:01:01,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  48%|████▊     | 2090/4381 [55:42<1:01:01,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  48%|████▊     | 2100/4381 [55:57<1:00:44,  1.60s/it, loss=2.61, v_num=641]Epoch 9:  48%|████▊     | 2100/4381 [55:57<1:00:44,  1.60s/it, loss=2.6, v_num=641] Epoch 9:  48%|████▊     | 2110/4381 [56:12<1:00:27,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  48%|████▊     | 2110/4381 [56:12<1:00:27,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  48%|████▊     | 2120/4381 [56:28<1:00:12,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  48%|████▊     | 2120/4381 [56:28<1:00:12,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  49%|████▊     | 2130/4381 [56:43<59:55,  1.60s/it, loss=2.64, v_num=641]  Epoch 9:  49%|████▊     | 2130/4381 [56:43<59:55,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  49%|████▉     | 2140/4381 [56:57<59:36,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  49%|████▉     | 2140/4381 [56:57<59:36,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  49%|████▉     | 2150/4381 [57:15<59:22,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  49%|████▉     | 2150/4381 [57:15<59:22,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  49%|████▉     | 2160/4381 [57:30<59:06,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  50%|████▉     | 2170/4381 [57:42<58:46,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  50%|████▉     | 2170/4381 [57:42<58:46,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  50%|████▉     | 2180/4381 [57:57<58:29,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  50%|████▉     | 2180/4381 [57:57<58:29,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  50%|████▉     | 2190/4381 [58:15<58:15,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  50%|████▉     | 2190/4381 [58:15<58:15,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  50%|█████     | 2200/4381 [58:29<57:57,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  50%|█████     | 2200/4381 [58:29<57:57,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  50%|█████     | 2210/4381 [58:46<57:42,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  50%|█████     | 2210/4381 [58:46<57:42,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  51%|█████     | 2220/4381 [59:03<57:27,  1.60s/it, loss=2.63, v_num=641]Epoch 9:  51%|█████     | 2220/4381 [59:03<57:27,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  51%|█████     | 2230/4381 [59:18<57:10,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  51%|█████     | 2230/4381 [59:18<57:10,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  51%|█████     | 2240/4381 [59:36<56:56,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  51%|█████     | 2240/4381 [59:36<56:56,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  51%|█████▏    | 2250/4381 [59:51<56:40,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  51%|█████▏    | 2250/4381 [59:51<56:40,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  52%|█████▏    | 2260/4381 [1:00:07<56:23,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  52%|█████▏    | 2260/4381 [1:00:07<56:23,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  52%|█████▏    | 2270/4381 [1:00:25<56:10,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  52%|█████▏    | 2270/4381 [1:00:25<56:10,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  52%|█████▏    | 2280/4381 [1:00:38<55:51,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  52%|█████▏    | 2280/4381 [1:00:38<55:51,  1.60s/it, loss=2.62, v_num=641]Epoch 9:  52%|█████▏    | 2290/4381 [1:00:51<55:32,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  52%|█████▏    | 2290/4381 [1:00:51<55:32,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  52%|█████▏    | 2300/4381 [1:01:09<55:18,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  52%|█████▏    | 2300/4381 [1:01:09<55:18,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  53%|█████▎    | 2310/4381 [1:01:26<55:03,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  53%|█████▎    | 2310/4381 [1:01:26<55:03,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  53%|█████▎    | 2320/4381 [1:01:42<54:47,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  53%|█████▎    | 2320/4381 [1:01:42<54:47,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  53%|█████▎    | 2330/4381 [1:02:00<54:33,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  53%|█████▎    | 2340/4381 [1:02:17<54:18,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  53%|█████▎    | 2340/4381 [1:02:17<54:18,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  54%|█████▎    | 2350/4381 [1:02:34<54:03,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  54%|█████▎    | 2350/4381 [1:02:34<54:03,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  54%|█████▍    | 2360/4381 [1:02:49<53:46,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  54%|█████▍    | 2360/4381 [1:02:49<53:46,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  54%|█████▍    | 2370/4381 [1:03:06<53:31,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  54%|█████▍    | 2370/4381 [1:03:06<53:31,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  54%|█████▍    | 2380/4381 [1:03:23<53:16,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  54%|█████▍    | 2380/4381 [1:03:23<53:16,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  55%|█████▍    | 2390/4381 [1:03:37<52:58,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  55%|█████▍    | 2390/4381 [1:03:37<52:58,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  55%|█████▍    | 2400/4381 [1:03:53<52:42,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  55%|█████▍    | 2400/4381 [1:03:53<52:42,  1.60s/it, loss=2.71, v_num=641]Epoch 9:  55%|█████▌    | 2410/4381 [1:04:07<52:25,  1.60s/it, loss=2.71, v_num=641]Epoch 9:  55%|█████▌    | 2410/4381 [1:04:07<52:25,  1.60s/it, loss=2.7, v_num=641] Epoch 9:  55%|█████▌    | 2420/4381 [1:04:21<52:07,  1.60s/it, loss=2.7, v_num=641]Epoch 9:  55%|█████▌    | 2420/4381 [1:04:21<52:07,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  55%|█████▌    | 2430/4381 [1:04:40<51:54,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  55%|█████▌    | 2430/4381 [1:04:40<51:54,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  56%|█████▌    | 2440/4381 [1:04:54<51:36,  1.60s/it, loss=2.68, v_num=641]Epoch 9:  56%|█████▌    | 2440/4381 [1:04:54<51:36,  1.60s/it, loss=2.69, v_num=641]Epoch 9:  56%|█████▌    | 2450/4381 [1:05:06<51:17,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  56%|█████▌    | 2450/4381 [1:05:06<51:17,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  56%|█████▌    | 2460/4381 [1:05:25<51:03,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  56%|█████▌    | 2460/4381 [1:05:25<51:03,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  56%|█████▋    | 2470/4381 [1:05:40<50:47,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  56%|█████▋    | 2470/4381 [1:05:40<50:47,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  57%|█████▋    | 2480/4381 [1:05:54<50:30,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  57%|█████▋    | 2480/4381 [1:05:54<50:30,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  57%|█████▋    | 2490/4381 [1:06:15<50:18,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  57%|█████▋    | 2490/4381 [1:06:15<50:18,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  57%|█████▋    | 2500/4381 [1:06:30<50:01,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  57%|█████▋    | 2500/4381 [1:06:30<50:01,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  57%|█████▋    | 2510/4381 [1:06:45<49:44,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  57%|█████▋    | 2510/4381 [1:06:45<49:44,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  58%|█████▊    | 2520/4381 [1:07:04<49:30,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  58%|█████▊    | 2520/4381 [1:07:04<49:30,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  58%|█████▊    | 2530/4381 [1:07:18<49:13,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  58%|█████▊    | 2530/4381 [1:07:18<49:13,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  58%|█████▊    | 2540/4381 [1:07:33<48:56,  1.60s/it, loss=2.64, v_num=641]Epoch 9:  58%|█████▊    | 2540/4381 [1:07:33<48:56,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  58%|█████▊    | 2550/4381 [1:07:50<48:41,  1.60s/it, loss=2.67, v_num=641]Epoch 9:  58%|█████▊    | 2550/4381 [1:07:50<48:41,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  58%|█████▊    | 2560/4381 [1:08:06<48:25,  1.60s/it, loss=2.66, v_num=641]Epoch 9:  59%|█████▊    | 2570/4381 [1:08:20<48:08,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  59%|█████▊    | 2570/4381 [1:08:20<48:08,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  59%|█████▉    | 2580/4381 [1:08:36<47:52,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  59%|█████▉    | 2580/4381 [1:08:36<47:52,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  59%|█████▉    | 2590/4381 [1:08:51<47:35,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  59%|█████▉    | 2590/4381 [1:08:51<47:35,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  59%|█████▉    | 2600/4381 [1:09:07<47:19,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  59%|█████▉    | 2600/4381 [1:09:07<47:19,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  60%|█████▉    | 2610/4381 [1:09:25<47:05,  1.60s/it, loss=2.6, v_num=641]Epoch 9:  60%|█████▉    | 2610/4381 [1:09:25<47:05,  1.60s/it, loss=2.65, v_num=641]Epoch 9:  60%|█████▉    | 2620/4381 [1:09:38<46:47,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  60%|█████▉    | 2620/4381 [1:09:38<46:47,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  60%|██████    | 2630/4381 [1:09:55<46:31,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  60%|██████    | 2630/4381 [1:09:55<46:31,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  60%|██████    | 2640/4381 [1:10:11<46:16,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  60%|██████    | 2640/4381 [1:10:11<46:16,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  60%|██████    | 2650/4381 [1:10:25<45:58,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  60%|██████    | 2650/4381 [1:10:25<45:58,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  61%|██████    | 2660/4381 [1:10:41<45:43,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  61%|██████    | 2660/4381 [1:10:41<45:43,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  61%|██████    | 2670/4381 [1:10:55<45:25,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  61%|██████    | 2670/4381 [1:10:55<45:25,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  61%|██████    | 2680/4381 [1:11:13<45:11,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  61%|██████    | 2680/4381 [1:11:13<45:11,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  61%|██████▏   | 2690/4381 [1:11:28<44:54,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  61%|██████▏   | 2690/4381 [1:11:28<44:54,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  62%|██████▏   | 2700/4381 [1:11:44<44:39,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  62%|██████▏   | 2700/4381 [1:11:44<44:39,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  62%|██████▏   | 2710/4381 [1:12:03<44:24,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  62%|██████▏   | 2710/4381 [1:12:03<44:24,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  62%|██████▏   | 2720/4381 [1:12:17<44:07,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  62%|██████▏   | 2720/4381 [1:12:17<44:07,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  62%|██████▏   | 2730/4381 [1:12:32<43:51,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  62%|██████▏   | 2730/4381 [1:12:32<43:51,  1.59s/it, loss=2.71, v_num=641]Epoch 9:  63%|██████▎   | 2740/4381 [1:12:50<43:36,  1.59s/it, loss=2.71, v_num=641]Epoch 9:  63%|██████▎   | 2740/4381 [1:12:50<43:36,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  63%|██████▎   | 2760/4381 [1:13:17<43:01,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  63%|██████▎   | 2760/4381 [1:13:17<43:01,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2770/4381 [1:13:33<42:45,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2770/4381 [1:13:33<42:45,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2780/4381 [1:13:51<42:31,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  63%|██████▎   | 2780/4381 [1:13:51<42:31,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▎   | 2790/4381 [1:14:06<42:14,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▎   | 2790/4381 [1:14:06<42:14,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▍   | 2800/4381 [1:14:22<41:58,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▍   | 2800/4381 [1:14:22<41:58,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  64%|██████▍   | 2810/4381 [1:14:39<41:43,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  64%|██████▍   | 2810/4381 [1:14:39<41:43,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▍   | 2820/4381 [1:14:54<41:27,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  64%|██████▍   | 2820/4381 [1:14:54<41:27,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  65%|██████▍   | 2830/4381 [1:15:11<41:11,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  65%|██████▍   | 2830/4381 [1:15:11<41:11,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  65%|██████▍   | 2840/4381 [1:15:26<40:55,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  65%|██████▍   | 2840/4381 [1:15:26<40:55,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  65%|██████▌   | 2850/4381 [1:15:41<40:38,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  65%|██████▌   | 2850/4381 [1:15:41<40:38,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  65%|██████▌   | 2860/4381 [1:15:58<40:23,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  66%|██████▌   | 2870/4381 [1:16:11<40:05,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  66%|██████▌   | 2870/4381 [1:16:11<40:05,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  66%|██████▌   | 2880/4381 [1:16:26<39:49,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  66%|██████▌   | 2880/4381 [1:16:26<39:49,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  66%|██████▌   | 2890/4381 [1:16:42<39:33,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  66%|██████▌   | 2890/4381 [1:16:42<39:33,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  66%|██████▌   | 2900/4381 [1:16:59<39:18,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  66%|██████▌   | 2900/4381 [1:16:59<39:18,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  66%|██████▋   | 2910/4381 [1:17:14<39:02,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  66%|██████▋   | 2910/4381 [1:17:14<39:02,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  67%|██████▋   | 2920/4381 [1:17:32<38:47,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  67%|██████▋   | 2920/4381 [1:17:32<38:47,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  67%|██████▋   | 2930/4381 [1:17:48<38:31,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  67%|██████▋   | 2930/4381 [1:17:48<38:31,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  67%|██████▋   | 2940/4381 [1:18:05<38:15,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  67%|██████▋   | 2940/4381 [1:18:05<38:15,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  67%|██████▋   | 2950/4381 [1:18:20<37:59,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  67%|██████▋   | 2950/4381 [1:18:20<37:59,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  68%|██████▊   | 2960/4381 [1:18:35<37:42,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  68%|██████▊   | 2960/4381 [1:18:35<37:42,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  68%|██████▊   | 2970/4381 [1:18:52<37:27,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  68%|██████▊   | 2970/4381 [1:18:52<37:27,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  68%|██████▊   | 2980/4381 [1:19:09<37:12,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  68%|██████▊   | 2980/4381 [1:19:09<37:12,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  68%|██████▊   | 2990/4381 [1:19:24<36:55,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  68%|██████▊   | 2990/4381 [1:19:24<36:55,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  68%|██████▊   | 3000/4381 [1:19:39<36:39,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  68%|██████▊   | 3000/4381 [1:19:39<36:39,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  69%|██████▊   | 3010/4381 [1:19:54<36:23,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  69%|██████▊   | 3010/4381 [1:19:54<36:23,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  69%|██████▉   | 3020/4381 [1:20:09<36:06,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  69%|██████▉   | 3020/4381 [1:20:09<36:06,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  69%|██████▉   | 3030/4381 [1:20:28<35:52,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  69%|██████▉   | 3030/4381 [1:20:28<35:52,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  69%|██████▉   | 3040/4381 [1:20:43<35:35,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  69%|██████▉   | 3040/4381 [1:20:43<35:35,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  70%|██████▉   | 3050/4381 [1:20:57<35:18,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  70%|██████▉   | 3050/4381 [1:20:57<35:18,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  70%|██████▉   | 3060/4381 [1:21:13<35:03,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  70%|██████▉   | 3060/4381 [1:21:13<35:03,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  70%|███████   | 3070/4381 [1:21:27<34:46,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  70%|███████   | 3070/4381 [1:21:27<34:46,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  70%|███████   | 3080/4381 [1:21:41<34:29,  1.59s/it, loss=2.69, v_num=641]Epoch 9:  70%|███████   | 3080/4381 [1:21:41<34:29,  1.59s/it, loss=2.7, v_num=641] Epoch 9:  71%|███████   | 3090/4381 [1:21:58<34:14,  1.59s/it, loss=2.7, v_num=641]Epoch 9:  71%|███████   | 3090/4381 [1:21:58<34:14,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  71%|███████   | 3100/4381 [1:22:11<33:57,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  71%|███████   | 3100/4381 [1:22:11<33:57,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  71%|███████   | 3110/4381 [1:22:32<33:43,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  71%|███████   | 3110/4381 [1:22:32<33:43,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  71%|███████   | 3120/4381 [1:22:46<33:26,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  71%|███████   | 3120/4381 [1:22:46<33:26,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  71%|███████▏  | 3130/4381 [1:23:05<33:11,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  71%|███████▏  | 3130/4381 [1:23:05<33:11,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  72%|███████▏  | 3140/4381 [1:23:18<32:55,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  72%|███████▏  | 3140/4381 [1:23:18<32:55,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  72%|███████▏  | 3150/4381 [1:23:35<32:39,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  72%|███████▏  | 3150/4381 [1:23:35<32:39,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  72%|███████▏  | 3160/4381 [1:23:53<32:24,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  72%|███████▏  | 3160/4381 [1:23:53<32:24,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  72%|███████▏  | 3170/4381 [1:24:05<32:07,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  72%|███████▏  | 3170/4381 [1:24:05<32:07,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  73%|███████▎  | 3180/4381 [1:24:24<31:52,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  73%|███████▎  | 3180/4381 [1:24:24<31:52,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  73%|███████▎  | 3190/4381 [1:24:39<31:35,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  73%|███████▎  | 3190/4381 [1:24:39<31:35,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3200/4381 [1:24:52<31:18,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3200/4381 [1:24:52<31:18,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3210/4381 [1:25:11<31:04,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3210/4381 [1:25:11<31:04,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3220/4381 [1:25:25<30:47,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  73%|███████▎  | 3220/4381 [1:25:25<30:47,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  74%|███████▎  | 3230/4381 [1:25:39<30:31,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  74%|███████▎  | 3230/4381 [1:25:39<30:31,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  74%|███████▍  | 3240/4381 [1:25:59<30:16,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  74%|███████▍  | 3240/4381 [1:25:59<30:16,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  74%|███████▍  | 3250/4381 [1:26:11<29:59,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  74%|███████▍  | 3250/4381 [1:26:11<29:59,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  74%|███████▍  | 3260/4381 [1:26:27<29:43,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  74%|███████▍  | 3260/4381 [1:26:27<29:43,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  75%|███████▍  | 3270/4381 [1:26:43<29:27,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  75%|███████▍  | 3270/4381 [1:26:43<29:27,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  75%|███████▍  | 3280/4381 [1:26:57<29:10,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  75%|███████▍  | 3280/4381 [1:26:57<29:10,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  75%|███████▌  | 3290/4381 [1:27:13<28:54,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  75%|███████▌  | 3290/4381 [1:27:13<28:54,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  75%|███████▌  | 3300/4381 [1:27:31<28:39,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  75%|███████▌  | 3300/4381 [1:27:31<28:39,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  76%|███████▌  | 3310/4381 [1:27:48<28:24,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  76%|███████▌  | 3310/4381 [1:27:48<28:24,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  76%|███████▌  | 3320/4381 [1:28:06<28:09,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  76%|███████▌  | 3320/4381 [1:28:06<28:09,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  76%|███████▌  | 3330/4381 [1:28:20<27:52,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  76%|███████▌  | 3330/4381 [1:28:20<27:52,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  76%|███████▌  | 3340/4381 [1:28:39<27:37,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  76%|███████▌  | 3340/4381 [1:28:39<27:37,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  76%|███████▋  | 3350/4381 [1:28:55<27:21,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  76%|███████▋  | 3350/4381 [1:28:55<27:21,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  77%|███████▋  | 3360/4381 [1:29:10<27:05,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  77%|███████▋  | 3360/4381 [1:29:10<27:05,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  77%|███████▋  | 3370/4381 [1:29:28<26:50,  1.59s/it, loss=2.61, v_num=641]Epoch 9:  77%|███████▋  | 3370/4381 [1:29:28<26:50,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  77%|███████▋  | 3380/4381 [1:29:42<26:33,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  77%|███████▋  | 3380/4381 [1:29:42<26:33,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  77%|███████▋  | 3390/4381 [1:29:55<26:16,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  77%|███████▋  | 3390/4381 [1:29:55<26:16,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  78%|███████▊  | 3400/4381 [1:30:14<26:01,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  78%|███████▊  | 3400/4381 [1:30:14<26:01,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  78%|███████▊  | 3410/4381 [1:30:28<25:45,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  78%|███████▊  | 3410/4381 [1:30:28<25:45,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  78%|███████▊  | 3420/4381 [1:30:45<25:29,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  78%|███████▊  | 3420/4381 [1:30:45<25:29,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  78%|███████▊  | 3430/4381 [1:31:00<25:13,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  78%|███████▊  | 3430/4381 [1:31:00<25:13,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▊  | 3440/4381 [1:31:17<24:57,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▊  | 3440/4381 [1:31:17<24:57,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  79%|███████▊  | 3450/4381 [1:31:32<24:41,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  79%|███████▊  | 3450/4381 [1:31:32<24:41,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▉  | 3460/4381 [1:31:48<24:25,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▉  | 3460/4381 [1:31:48<24:25,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▉  | 3470/4381 [1:32:04<24:09,  1.59s/it, loss=2.68, v_num=641]Epoch 9:  79%|███████▉  | 3470/4381 [1:32:04<24:09,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  79%|███████▉  | 3480/4381 [1:32:17<23:53,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  79%|███████▉  | 3480/4381 [1:32:17<23:53,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  80%|███████▉  | 3490/4381 [1:32:36<23:38,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  80%|███████▉  | 3490/4381 [1:32:36<23:38,  1.59s/it, loss=2.7, v_num=641] Epoch 9:  80%|███████▉  | 3500/4381 [1:32:51<23:22,  1.59s/it, loss=2.7, v_num=641]Epoch 9:  80%|███████▉  | 3500/4381 [1:32:51<23:22,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  80%|████████  | 3510/4381 [1:33:06<23:05,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  80%|████████  | 3510/4381 [1:33:06<23:05,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  80%|████████  | 3520/4381 [1:33:25<22:50,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  80%|████████  | 3520/4381 [1:33:25<22:50,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  81%|████████  | 3530/4381 [1:33:41<22:34,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  81%|████████  | 3530/4381 [1:33:41<22:34,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  81%|████████  | 3540/4381 [1:33:56<22:18,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  81%|████████  | 3540/4381 [1:33:56<22:18,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  81%|████████  | 3550/4381 [1:34:13<22:03,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  81%|████████  | 3550/4381 [1:34:13<22:03,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  81%|████████▏ | 3560/4381 [1:34:28<21:46,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  81%|████████▏ | 3560/4381 [1:34:28<21:46,  1.59s/it, loss=2.6, v_num=641] Epoch 9:  81%|████████▏ | 3570/4381 [1:34:41<21:30,  1.59s/it, loss=2.6, v_num=641]Epoch 9:  81%|████████▏ | 3570/4381 [1:34:41<21:30,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  82%|████████▏ | 3580/4381 [1:35:00<21:15,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  82%|████████▏ | 3580/4381 [1:35:00<21:15,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  82%|████████▏ | 3590/4381 [1:35:14<20:58,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  82%|████████▏ | 3590/4381 [1:35:14<20:58,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  82%|████████▏ | 3600/4381 [1:35:29<20:42,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  82%|████████▏ | 3600/4381 [1:35:29<20:42,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  82%|████████▏ | 3610/4381 [1:35:45<20:26,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  82%|████████▏ | 3610/4381 [1:35:45<20:26,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  83%|████████▎ | 3620/4381 [1:36:00<20:10,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  83%|████████▎ | 3620/4381 [1:36:00<20:10,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  83%|████████▎ | 3630/4381 [1:36:15<19:54,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  83%|████████▎ | 3630/4381 [1:36:15<19:54,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  83%|████████▎ | 3640/4381 [1:36:35<19:39,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  83%|████████▎ | 3640/4381 [1:36:35<19:39,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  83%|████████▎ | 3650/4381 [1:36:48<19:23,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  83%|████████▎ | 3650/4381 [1:36:48<19:23,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  84%|████████▎ | 3660/4381 [1:37:01<19:06,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  84%|████████▎ | 3660/4381 [1:37:01<19:06,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  84%|████████▍ | 3670/4381 [1:37:19<18:50,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  84%|████████▍ | 3670/4381 [1:37:19<18:50,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  84%|████████▍ | 3680/4381 [1:37:35<18:35,  1.59s/it, loss=2.66, v_num=641]Epoch 9:  84%|████████▍ | 3680/4381 [1:37:35<18:35,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  84%|████████▍ | 3690/4381 [1:37:49<18:18,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  84%|████████▍ | 3690/4381 [1:37:49<18:18,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  84%|████████▍ | 3700/4381 [1:38:09<18:03,  1.59s/it, loss=2.62, v_num=641]Epoch 9:  84%|████████▍ | 3700/4381 [1:38:09<18:03,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  85%|████████▍ | 3710/4381 [1:38:25<17:47,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  85%|████████▍ | 3710/4381 [1:38:25<17:47,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  85%|████████▍ | 3720/4381 [1:38:40<17:31,  1.59s/it, loss=2.67, v_num=641]Epoch 9:  85%|████████▍ | 3720/4381 [1:38:40<17:31,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  85%|████████▌ | 3730/4381 [1:38:52<17:15,  1.59s/it, loss=2.65, v_num=641]Epoch 9:  85%|████████▌ | 3730/4381 [1:38:52<17:15,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  85%|████████▌ | 3740/4381 [1:38:56<16:57,  1.59s/it, loss=2.63, v_num=641]Epoch 9:  85%|████████▌ | 3740/4381 [1:38:56<16:57,  1.59s/it, loss=2.64, v_num=641]Epoch 9:  86%|████████▌ | 3750/4381 [1:38:58<16:39,  1.58s/it, loss=2.64, v_num=641]Epoch 9:  86%|████████▌ | 3750/4381 [1:38:58<16:39,  1.58s/it, loss=2.66, v_num=641]validation_epoch_end
graph acc: 0.2220447284345048
valid accuracy: 0.961086630821228
validation_epoch_end
graph acc: 0.21725239616613418
valid accuracy: 0.9600182175636292
validation_epoch_end
graph acc: 0.22523961661341854
valid accuracy: 0.9639827013015747
validation_epoch_end
graph acc: 0.23322683706070288
valid accuracy: 0.9640851020812988
validation_epoch_end
graph acc: 0.21884984025559107
valid accuracy: 0.9624127149581909
validation_epoch_end
graph acc: 0.2539936102236422
valid accuracy: 0.959143340587616
Epoch 9:  86%|████████▌ | 3760/4381 [1:39:00<16:20,  1.58s/it, loss=2.66, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.24600638977635783
valid accuracy: 0.9619678854942322

Validating:   2%|▏         | 10/626 [00:03<04:05,  2.51it/s][AEpoch 9:  86%|████████▌ | 3770/4381 [1:39:04<16:03,  1.58s/it, loss=2.66, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:35,  3.90it/s][AEpoch 9:  86%|████████▋ | 3780/4381 [1:39:05<15:45,  1.57s/it, loss=2.66, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:55,  3.40it/s][AEpoch 9:  87%|████████▋ | 3790/4381 [1:39:09<15:27,  1.57s/it, loss=2.66, v_num=641]
Validating:   6%|▋         | 40/626 [00:10<02:25,  4.03it/s][AEpoch 9:  87%|████████▋ | 3800/4381 [1:39:10<15:09,  1.57s/it, loss=2.66, v_num=641]
Validating:   8%|▊         | 50/626 [00:11<01:57,  4.89it/s][AEpoch 9:  87%|████████▋ | 3810/4381 [1:39:12<14:51,  1.56s/it, loss=2.66, v_num=641]
Validating:  10%|▉         | 60/626 [00:14<02:07,  4.43it/s][AEpoch 9:  87%|████████▋ | 3820/4381 [1:39:14<14:34,  1.56s/it, loss=2.66, v_num=641]
Validating:  11%|█         | 70/626 [00:16<02:04,  4.46it/s][AEpoch 9:  87%|████████▋ | 3830/4381 [1:39:17<14:16,  1.55s/it, loss=2.66, v_num=641]
Validating:  13%|█▎        | 80/626 [00:18<01:43,  5.30it/s][AEpoch 9:  88%|████████▊ | 3840/4381 [1:39:18<13:59,  1.55s/it, loss=2.66, v_num=641]
Validating:  14%|█▍        | 90/626 [00:19<01:27,  6.14it/s][AEpoch 9:  88%|████████▊ | 3850/4381 [1:39:19<13:41,  1.55s/it, loss=2.66, v_num=641]
Validating:  16%|█▌        | 100/626 [00:21<01:33,  5.63it/s][AEpoch 9:  88%|████████▊ | 3860/4381 [1:39:21<13:24,  1.54s/it, loss=2.66, v_num=641]
Validating:  18%|█▊        | 110/626 [00:23<01:40,  5.13it/s][AEpoch 9:  88%|████████▊ | 3870/4381 [1:39:23<13:07,  1.54s/it, loss=2.66, v_num=641]
Validating:  19%|█▉        | 120/626 [00:25<01:33,  5.44it/s][AEpoch 9:  89%|████████▊ | 3880/4381 [1:39:25<12:50,  1.54s/it, loss=2.66, v_num=641]
Validating:  21%|██        | 130/626 [00:27<01:33,  5.30it/s][AEpoch 9:  89%|████████▉ | 3890/4381 [1:39:27<12:33,  1.53s/it, loss=2.66, v_num=641]
Validating:  22%|██▏       | 140/626 [00:29<01:46,  4.56it/s][AEpoch 9:  89%|████████▉ | 3900/4381 [1:39:30<12:16,  1.53s/it, loss=2.66, v_num=641]
Validating:  24%|██▍       | 150/626 [00:31<01:39,  4.80it/s][AEpoch 9:  89%|████████▉ | 3910/4381 [1:39:32<11:59,  1.53s/it, loss=2.66, v_num=641]
Validating:  26%|██▌       | 160/626 [00:33<01:29,  5.21it/s][AEpoch 9:  89%|████████▉ | 3920/4381 [1:39:33<11:42,  1.52s/it, loss=2.66, v_num=641]
Validating:  27%|██▋       | 170/626 [00:36<01:38,  4.65it/s][AEpoch 9:  90%|████████▉ | 3930/4381 [1:39:36<11:25,  1.52s/it, loss=2.66, v_num=641]
Validating:  29%|██▉       | 180/626 [00:37<01:28,  5.03it/s][AEpoch 9:  90%|████████▉ | 3940/4381 [1:39:37<11:08,  1.52s/it, loss=2.66, v_num=641]
Validating:  30%|███       | 190/626 [00:40<01:33,  4.65it/s][AEpoch 9:  90%|█████████ | 3950/4381 [1:39:40<10:52,  1.51s/it, loss=2.66, v_num=641]
Validating:  32%|███▏      | 200/626 [00:42<01:34,  4.52it/s][AEpoch 9:  90%|█████████ | 3960/4381 [1:39:42<10:35,  1.51s/it, loss=2.66, v_num=641]
Validating:  34%|███▎      | 210/626 [00:44<01:31,  4.57it/s][AEpoch 9:  91%|█████████ | 3970/4381 [1:39:44<10:19,  1.51s/it, loss=2.66, v_num=641]
Validating:  35%|███▌      | 220/626 [00:46<01:29,  4.56it/s][AEpoch 9:  91%|█████████ | 3980/4381 [1:39:47<10:03,  1.50s/it, loss=2.66, v_num=641]
Validating:  37%|███▋      | 230/626 [00:48<01:18,  5.03it/s][AEpoch 9:  91%|█████████ | 3990/4381 [1:39:48<09:46,  1.50s/it, loss=2.66, v_num=641]
Validating:  38%|███▊      | 240/626 [00:50<01:18,  4.94it/s][AEpoch 9:  91%|█████████▏| 4000/4381 [1:39:50<09:30,  1.50s/it, loss=2.66, v_num=641]
Validating:  40%|███▉      | 250/626 [00:52<01:10,  5.32it/s][AEpoch 9:  92%|█████████▏| 4010/4381 [1:39:52<09:14,  1.49s/it, loss=2.66, v_num=641]
Validating:  42%|████▏     | 260/626 [00:55<01:21,  4.50it/s][AEpoch 9:  92%|█████████▏| 4020/4381 [1:39:55<08:58,  1.49s/it, loss=2.66, v_num=641]
Validating:  43%|████▎     | 270/626 [00:56<01:10,  5.08it/s][AEpoch 9:  92%|█████████▏| 4030/4381 [1:39:56<08:42,  1.49s/it, loss=2.66, v_num=641]
Validating:  45%|████▍     | 280/626 [00:59<01:18,  4.42it/s][AEpoch 9:  92%|█████████▏| 4040/4381 [1:39:59<08:26,  1.48s/it, loss=2.66, v_num=641]
Validating:  46%|████▋     | 290/626 [01:01<01:11,  4.69it/s][AEpoch 9:  92%|█████████▏| 4050/4381 [1:40:01<08:10,  1.48s/it, loss=2.66, v_num=641]
Validating:  48%|████▊     | 300/626 [01:02<01:04,  5.05it/s][AEpoch 9:  93%|█████████▎| 4060/4381 [1:40:03<07:54,  1.48s/it, loss=2.66, v_num=641]
Validating:  50%|████▉     | 310/626 [01:04<00:54,  5.76it/s][AEpoch 9:  93%|█████████▎| 4070/4381 [1:40:04<07:38,  1.47s/it, loss=2.66, v_num=641]
Validating:  51%|█████     | 320/626 [01:05<00:50,  6.02it/s][AEpoch 9:  93%|█████████▎| 4080/4381 [1:40:05<07:22,  1.47s/it, loss=2.66, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:07<00:52,  5.59it/s][AEpoch 9:  93%|█████████▎| 4090/4381 [1:40:07<07:07,  1.47s/it, loss=2.66, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:09<00:50,  5.66it/s][AEpoch 9:  94%|█████████▎| 4100/4381 [1:40:09<06:51,  1.47s/it, loss=2.66, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:12<00:56,  4.88it/s][AEpoch 9:  94%|█████████▍| 4110/4381 [1:40:12<06:36,  1.46s/it, loss=2.66, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:14<00:54,  4.84it/s][AEpoch 9:  94%|█████████▍| 4120/4381 [1:40:14<06:20,  1.46s/it, loss=2.66, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:15<00:48,  5.31it/s][AEpoch 9:  94%|█████████▍| 4130/4381 [1:40:15<06:05,  1.46s/it, loss=2.66, v_num=641]
Validating:  61%|██████    | 380/626 [01:16<00:42,  5.85it/s][AEpoch 9:  94%|█████████▍| 4140/4381 [1:40:17<05:50,  1.45s/it, loss=2.66, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:18<00:42,  5.51it/s][AEpoch 9:  95%|█████████▍| 4150/4381 [1:40:19<05:34,  1.45s/it, loss=2.66, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:20<00:41,  5.50it/s][AEpoch 9:  95%|█████████▍| 4160/4381 [1:40:21<05:19,  1.45s/it, loss=2.66, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:22<00:39,  5.41it/s][AEpoch 9:  95%|█████████▌| 4170/4381 [1:40:22<05:04,  1.44s/it, loss=2.66, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:24<00:37,  5.45it/s][AEpoch 9:  95%|█████████▌| 4180/4381 [1:40:24<04:49,  1.44s/it, loss=2.66, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:25<00:33,  5.77it/s][AEpoch 9:  96%|█████████▌| 4190/4381 [1:40:26<04:34,  1.44s/it, loss=2.66, v_num=641]
Validating:  70%|███████   | 440/626 [01:28<00:37,  5.00it/s][AEpoch 9:  96%|█████████▌| 4200/4381 [1:40:28<04:19,  1.44s/it, loss=2.66, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:30<00:33,  5.32it/s][AEpoch 9:  96%|█████████▌| 4210/4381 [1:40:30<04:04,  1.43s/it, loss=2.66, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:32<00:32,  5.13it/s][AEpoch 9:  96%|█████████▋| 4220/4381 [1:40:32<03:50,  1.43s/it, loss=2.66, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:34<00:32,  4.83it/s][AEpoch 9:  97%|█████████▋| 4230/4381 [1:40:34<03:35,  1.43s/it, loss=2.66, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:36<00:29,  4.87it/s][AEpoch 9:  97%|█████████▋| 4240/4381 [1:40:36<03:20,  1.42s/it, loss=2.66, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:39<00:29,  4.55it/s][AEpoch 9:  97%|█████████▋| 4250/4381 [1:40:39<03:06,  1.42s/it, loss=2.66, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:40<00:23,  5.45it/s][AEpoch 9:  97%|█████████▋| 4260/4381 [1:40:40<02:51,  1.42s/it, loss=2.66, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:42<00:23,  4.88it/s][AEpoch 9:  97%|█████████▋| 4270/4381 [1:40:43<02:37,  1.41s/it, loss=2.66, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:44<00:19,  5.44it/s][AEpoch 9:  98%|█████████▊| 4280/4381 [1:40:44<02:22,  1.41s/it, loss=2.66, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:46<00:17,  5.38it/s][AEpoch 9:  98%|█████████▊| 4290/4381 [1:40:46<02:08,  1.41s/it, loss=2.66, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:48<00:17,  4.90it/s][AEpoch 9:  98%|█████████▊| 4300/4381 [1:40:48<01:53,  1.41s/it, loss=2.66, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:49<00:14,  5.31it/s][AEpoch 9:  98%|█████████▊| 4310/4381 [1:40:50<01:39,  1.40s/it, loss=2.66, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:51<00:12,  5.42it/s][AEpoch 9:  99%|█████████▊| 4320/4381 [1:40:51<01:25,  1.40s/it, loss=2.66, v_num=641]
Validating:  91%|█████████ | 570/626 [01:53<00:09,  5.80it/s][AEpoch 9:  99%|█████████▉| 4330/4381 [1:40:53<01:11,  1.40s/it, loss=2.66, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:55<00:09,  5.04it/s][AEpoch 9:  99%|█████████▉| 4340/4381 [1:40:56<00:57,  1.40s/it, loss=2.66, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:57<00:06,  5.40it/s][AEpoch 9:  99%|█████████▉| 4350/4381 [1:40:57<00:43,  1.39s/it, loss=2.66, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:58<00:03,  6.59it/s][AEpoch 9: 100%|█████████▉| 4360/4381 [1:40:58<00:29,  1.39s/it, loss=2.66, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:58<00:02,  7.85it/s][AEpoch 9: 100%|█████████▉| 4370/4381 [1:40:59<00:15,  1.39s/it, loss=2.66, v_num=641]
Validating:  99%|█████████▉| 620/626 [02:00<00:00,  7.12it/s][AEpoch 9: 100%|█████████▉| 4380/4381 [1:41:00<00:01,  1.38s/it, loss=2.66, v_num=641]
Validating: 100%|██████████| 626/626 [02:00<00:00,  8.35it/s][Avalidation_epoch_end
graph acc: 0.28913738019169327
valid accuracy: 0.9667367935180664
Epoch 9: 100%|██████████| 4381/4381 [1:41:01<00:00,  1.38s/it, loss=2.64, v_num=641]
                                                             [AEpoch 9:   0%|          | 0/4381 [00:00<00:00, 8097.11it/s, loss=2.64, v_num=641]   Epoch 10:   0%|          | 0/4381 [00:00<00:01, 2442.81it/s, loss=2.64, v_num=641]Epoch 10:   0%|          | 0/4381 [00:14<17:09:48, 14.10s/it, loss=2.64, v_num=641]Epoch 10:   0%|          | 10/4381 [00:21<2:23:28,  1.97s/it, loss=2.64, v_num=641]Epoch 10:   0%|          | 10/4381 [00:21<2:23:28,  1.97s/it, loss=2.68, v_num=641]Epoch 10:   0%|          | 20/4381 [00:37<2:09:19,  1.78s/it, loss=2.68, v_num=641]Epoch 10:   0%|          | 20/4381 [00:37<2:09:19,  1.78s/it, loss=2.61, v_num=641]Epoch 10:   1%|          | 30/4381 [00:54<2:07:05,  1.75s/it, loss=2.61, v_num=641]Epoch 10:   1%|          | 30/4381 [00:54<2:07:05,  1.75s/it, loss=2.58, v_num=641]Epoch 10:   1%|          | 40/4381 [01:11<2:06:42,  1.75s/it, loss=2.58, v_num=641]Epoch 10:   1%|          | 40/4381 [01:11<2:06:42,  1.75s/it, loss=2.58, v_num=641]Epoch 10:   1%|          | 50/4381 [01:27<2:03:49,  1.72s/it, loss=2.58, v_num=641]Epoch 10:   1%|          | 50/4381 [01:27<2:03:50,  1.72s/it, loss=2.56, v_num=641]Epoch 10:   1%|▏         | 60/4381 [01:42<2:01:16,  1.68s/it, loss=2.56, v_num=641]Epoch 10:   1%|▏         | 60/4381 [01:42<2:01:16,  1.68s/it, loss=2.58, v_num=641]Epoch 10:   2%|▏         | 70/4381 [01:58<2:00:05,  1.67s/it, loss=2.58, v_num=641]Epoch 10:   2%|▏         | 70/4381 [01:58<2:00:05,  1.67s/it, loss=2.57, v_num=641]Epoch 10:   2%|▏         | 80/4381 [02:14<1:59:27,  1.67s/it, loss=2.57, v_num=641]Epoch 10:   2%|▏         | 80/4381 [02:14<1:59:27,  1.67s/it, loss=2.56, v_num=641]Epoch 10:   2%|▏         | 90/4381 [02:32<1:59:36,  1.67s/it, loss=2.56, v_num=641]Epoch 10:   2%|▏         | 90/4381 [02:32<1:59:37,  1.67s/it, loss=2.57, v_num=641]Epoch 10:   2%|▏         | 100/4381 [02:54<2:03:23,  1.73s/it, loss=2.57, v_num=641]Epoch 10:   2%|▏         | 100/4381 [02:54<2:03:23,  1.73s/it, loss=2.56, v_num=641]Epoch 10:   3%|▎         | 110/4381 [03:09<2:01:15,  1.70s/it, loss=2.56, v_num=641]Epoch 10:   3%|▎         | 110/4381 [03:09<2:01:15,  1.70s/it, loss=2.62, v_num=641]Epoch 10:   3%|▎         | 120/4381 [03:24<2:00:07,  1.69s/it, loss=2.62, v_num=641]Epoch 10:   3%|▎         | 120/4381 [03:24<2:00:07,  1.69s/it, loss=2.63, v_num=641]Epoch 10:   3%|▎         | 130/4381 [03:43<2:00:52,  1.71s/it, loss=2.63, v_num=641]Epoch 10:   3%|▎         | 130/4381 [03:43<2:00:52,  1.71s/it, loss=2.57, v_num=641]Epoch 10:   3%|▎         | 140/4381 [03:59<1:59:50,  1.70s/it, loss=2.57, v_num=641]Epoch 10:   3%|▎         | 140/4381 [03:59<1:59:50,  1.70s/it, loss=2.59, v_num=641]Epoch 10:   3%|▎         | 150/4381 [04:14<1:59:04,  1.69s/it, loss=2.59, v_num=641]Epoch 10:   3%|▎         | 150/4381 [04:14<1:59:04,  1.69s/it, loss=2.6, v_num=641] Epoch 10:   4%|▎         | 160/4381 [04:33<1:59:20,  1.70s/it, loss=2.6, v_num=641]Epoch 10:   4%|▎         | 160/4381 [04:33<1:59:20,  1.70s/it, loss=2.56, v_num=641]Epoch 10:   4%|▍         | 170/4381 [04:49<1:58:41,  1.69s/it, loss=2.56, v_num=641]Epoch 10:   4%|▍         | 170/4381 [04:49<1:58:41,  1.69s/it, loss=2.57, v_num=641]Epoch 10:   4%|▍         | 180/4381 [05:05<1:58:00,  1.69s/it, loss=2.57, v_num=641]Epoch 10:   4%|▍         | 180/4381 [05:05<1:58:00,  1.69s/it, loss=2.6, v_num=641] Epoch 10:   4%|▍         | 190/4381 [05:21<1:57:24,  1.68s/it, loss=2.6, v_num=641]Epoch 10:   4%|▍         | 190/4381 [05:21<1:57:24,  1.68s/it, loss=2.59, v_num=641]Epoch 10:   5%|▍         | 200/4381 [05:39<1:57:38,  1.69s/it, loss=2.59, v_num=641]Epoch 10:   5%|▍         | 200/4381 [05:39<1:57:38,  1.69s/it, loss=2.61, v_num=641]Epoch 10:   5%|▍         | 210/4381 [05:52<1:56:12,  1.67s/it, loss=2.61, v_num=641]Epoch 10:   5%|▍         | 210/4381 [05:52<1:56:12,  1.67s/it, loss=2.61, v_num=641]Epoch 10:   5%|▌         | 220/4381 [06:08<1:55:31,  1.67s/it, loss=2.61, v_num=641]Epoch 10:   5%|▌         | 220/4381 [06:08<1:55:32,  1.67s/it, loss=2.61, v_num=641]Epoch 10:   5%|▌         | 230/4381 [06:25<1:55:30,  1.67s/it, loss=2.61, v_num=641]Epoch 10:   5%|▌         | 230/4381 [06:25<1:55:30,  1.67s/it, loss=2.66, v_num=641]Epoch 10:   5%|▌         | 240/4381 [06:40<1:54:43,  1.66s/it, loss=2.66, v_num=641]Epoch 10:   5%|▌         | 240/4381 [06:40<1:54:43,  1.66s/it, loss=2.63, v_num=641]Epoch 10:   6%|▌         | 250/4381 [06:55<1:54:05,  1.66s/it, loss=2.63, v_num=641]Epoch 10:   6%|▌         | 250/4381 [06:55<1:54:05,  1.66s/it, loss=2.59, v_num=641]Epoch 10:   6%|▌         | 260/4381 [07:11<1:53:33,  1.65s/it, loss=2.59, v_num=641]Epoch 10:   6%|▌         | 260/4381 [07:11<1:53:33,  1.65s/it, loss=2.6, v_num=641] Epoch 10:   6%|▌         | 270/4381 [07:26<1:52:55,  1.65s/it, loss=2.6, v_num=641]Epoch 10:   6%|▌         | 270/4381 [07:26<1:52:55,  1.65s/it, loss=2.61, v_num=641]Epoch 10:   6%|▋         | 280/4381 [07:40<1:51:57,  1.64s/it, loss=2.61, v_num=641]Epoch 10:   6%|▋         | 280/4381 [07:40<1:51:57,  1.64s/it, loss=2.62, v_num=641]Epoch 10:   7%|▋         | 290/4381 [08:00<1:52:30,  1.65s/it, loss=2.62, v_num=641]Epoch 10:   7%|▋         | 290/4381 [08:00<1:52:30,  1.65s/it, loss=2.65, v_num=641]Epoch 10:   7%|▋         | 300/4381 [08:16<1:52:08,  1.65s/it, loss=2.65, v_num=641]Epoch 10:   7%|▋         | 300/4381 [08:16<1:52:08,  1.65s/it, loss=2.65, v_num=641]Epoch 10:   7%|▋         | 310/4381 [08:32<1:51:52,  1.65s/it, loss=2.65, v_num=641]Epoch 10:   7%|▋         | 310/4381 [08:32<1:51:52,  1.65s/it, loss=2.62, v_num=641]Epoch 10:   7%|▋         | 320/4381 [08:50<1:51:51,  1.65s/it, loss=2.62, v_num=641]Epoch 10:   7%|▋         | 320/4381 [08:50<1:51:52,  1.65s/it, loss=2.58, v_num=641]Epoch 10:   8%|▊         | 330/4381 [09:04<1:51:08,  1.65s/it, loss=2.58, v_num=641]Epoch 10:   8%|▊         | 330/4381 [09:04<1:51:08,  1.65s/it, loss=2.57, v_num=641]Epoch 10:   8%|▊         | 340/4381 [09:20<1:50:42,  1.64s/it, loss=2.57, v_num=641]Epoch 10:   8%|▊         | 340/4381 [09:20<1:50:42,  1.64s/it, loss=2.58, v_num=641]Epoch 10:   8%|▊         | 350/4381 [09:37<1:50:33,  1.65s/it, loss=2.58, v_num=641]Epoch 10:   8%|▊         | 350/4381 [09:37<1:50:33,  1.65s/it, loss=2.6, v_num=641] Epoch 10:   8%|▊         | 360/4381 [09:53<1:50:07,  1.64s/it, loss=2.6, v_num=641]Epoch 10:   8%|▊         | 360/4381 [09:53<1:50:07,  1.64s/it, loss=2.62, v_num=641]Epoch 10:   8%|▊         | 370/4381 [10:07<1:49:28,  1.64s/it, loss=2.62, v_num=641]Epoch 10:   8%|▊         | 370/4381 [10:07<1:49:28,  1.64s/it, loss=2.58, v_num=641]Epoch 10:   9%|▊         | 380/4381 [10:22<1:48:58,  1.63s/it, loss=2.58, v_num=641]Epoch 10:   9%|▊         | 380/4381 [10:22<1:48:58,  1.63s/it, loss=2.6, v_num=641] Epoch 10:   9%|▉         | 390/4381 [10:40<1:48:56,  1.64s/it, loss=2.6, v_num=641]Epoch 10:   9%|▉         | 390/4381 [10:40<1:48:56,  1.64s/it, loss=2.63, v_num=641]Epoch 10:   9%|▉         | 400/4381 [10:54<1:48:19,  1.63s/it, loss=2.63, v_num=641]Epoch 10:   9%|▉         | 400/4381 [10:54<1:48:19,  1.63s/it, loss=2.63, v_num=641]Epoch 10:   9%|▉         | 410/4381 [11:11<1:48:09,  1.63s/it, loss=2.63, v_num=641]Epoch 10:   9%|▉         | 410/4381 [11:11<1:48:09,  1.63s/it, loss=2.6, v_num=641] Epoch 10:  10%|▉         | 420/4381 [11:28<1:48:00,  1.64s/it, loss=2.6, v_num=641]Epoch 10:  10%|▉         | 420/4381 [11:28<1:48:00,  1.64s/it, loss=2.58, v_num=641]Epoch 10:  10%|▉         | 430/4381 [11:44<1:47:37,  1.63s/it, loss=2.58, v_num=641]Epoch 10:  10%|▉         | 430/4381 [11:44<1:47:37,  1.63s/it, loss=2.6, v_num=641] Epoch 10:  10%|█         | 440/4381 [12:04<1:47:51,  1.64s/it, loss=2.6, v_num=641]Epoch 10:  10%|█         | 440/4381 [12:04<1:47:51,  1.64s/it, loss=2.57, v_num=641]Epoch 10:  10%|█         | 450/4381 [12:18<1:47:15,  1.64s/it, loss=2.57, v_num=641]Epoch 10:  10%|█         | 450/4381 [12:18<1:47:15,  1.64s/it, loss=2.59, v_num=641]Epoch 10:  10%|█         | 460/4381 [12:33<1:46:50,  1.63s/it, loss=2.59, v_num=641]Epoch 10:  10%|█         | 460/4381 [12:33<1:46:50,  1.63s/it, loss=2.62, v_num=641]Epoch 10:  11%|█         | 470/4381 [12:50<1:46:38,  1.64s/it, loss=2.62, v_num=641]Epoch 10:  11%|█         | 470/4381 [12:50<1:46:38,  1.64s/it, loss=2.6, v_num=641] Epoch 10:  11%|█         | 480/4381 [13:07<1:46:24,  1.64s/it, loss=2.6, v_num=641]Epoch 10:  11%|█         | 480/4381 [13:07<1:46:24,  1.64s/it, loss=2.6, v_num=641]Epoch 10:  11%|█         | 490/4381 [13:22<1:45:57,  1.63s/it, loss=2.6, v_num=641]Epoch 10:  11%|█         | 490/4381 [13:22<1:45:57,  1.63s/it, loss=2.61, v_num=641]Epoch 10:  11%|█▏        | 500/4381 [13:41<1:46:04,  1.64s/it, loss=2.61, v_num=641]Epoch 10:  11%|█▏        | 500/4381 [13:41<1:46:04,  1.64s/it, loss=2.63, v_num=641]Epoch 10:  12%|█▏        | 510/4381 [13:55<1:45:30,  1.64s/it, loss=2.63, v_num=641]Epoch 10:  12%|█▏        | 510/4381 [13:55<1:45:30,  1.64s/it, loss=2.64, v_num=641]Epoch 10:  12%|█▏        | 520/4381 [14:10<1:45:02,  1.63s/it, loss=2.64, v_num=641]Epoch 10:  12%|█▏        | 520/4381 [14:10<1:45:02,  1.63s/it, loss=2.64, v_num=641]Epoch 10:  12%|█▏        | 530/4381 [14:29<1:45:05,  1.64s/it, loss=2.64, v_num=641]Epoch 10:  12%|█▏        | 530/4381 [14:29<1:45:05,  1.64s/it, loss=2.62, v_num=641]Epoch 10:  12%|█▏        | 540/4381 [14:43<1:44:31,  1.63s/it, loss=2.62, v_num=641]Epoch 10:  12%|█▏        | 540/4381 [14:43<1:44:31,  1.63s/it, loss=2.58, v_num=641]Epoch 10:  13%|█▎        | 550/4381 [14:57<1:44:01,  1.63s/it, loss=2.58, v_num=641]Epoch 10:  13%|█▎        | 550/4381 [14:57<1:44:01,  1.63s/it, loss=2.6, v_num=641] Epoch 10:  13%|█▎        | 560/4381 [15:13<1:43:39,  1.63s/it, loss=2.6, v_num=641]Epoch 10:  13%|█▎        | 560/4381 [15:13<1:43:39,  1.63s/it, loss=2.61, v_num=641]Epoch 10:  13%|█▎        | 570/4381 [15:28<1:43:17,  1.63s/it, loss=2.61, v_num=641]Epoch 10:  13%|█▎        | 570/4381 [15:28<1:43:17,  1.63s/it, loss=2.6, v_num=641] Epoch 10:  13%|█▎        | 580/4381 [15:41<1:42:41,  1.62s/it, loss=2.6, v_num=641]Epoch 10:  13%|█▎        | 580/4381 [15:41<1:42:41,  1.62s/it, loss=2.6, v_num=641]Epoch 10:  13%|█▎        | 590/4381 [16:00<1:42:44,  1.63s/it, loss=2.6, v_num=641]Epoch 10:  13%|█▎        | 590/4381 [16:00<1:42:44,  1.63s/it, loss=2.58, v_num=641]Epoch 10:  14%|█▎        | 600/4381 [16:15<1:42:19,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  14%|█▎        | 600/4381 [16:15<1:42:19,  1.62s/it, loss=2.55, v_num=641]Epoch 10:  14%|█▍        | 610/4381 [16:31<1:41:57,  1.62s/it, loss=2.55, v_num=641]Epoch 10:  14%|█▍        | 610/4381 [16:31<1:41:57,  1.62s/it, loss=2.57, v_num=641]Epoch 10:  14%|█▍        | 620/4381 [16:47<1:41:44,  1.62s/it, loss=2.57, v_num=641]Epoch 10:  14%|█▍        | 620/4381 [16:47<1:41:44,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  14%|█▍        | 630/4381 [17:03<1:41:23,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  14%|█▍        | 630/4381 [17:03<1:41:23,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  15%|█▍        | 640/4381 [17:20<1:41:10,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  15%|█▍        | 640/4381 [17:20<1:41:10,  1.62s/it, loss=2.6, v_num=641] Epoch 10:  15%|█▍        | 650/4381 [17:37<1:41:02,  1.62s/it, loss=2.6, v_num=641]Epoch 10:  15%|█▍        | 650/4381 [17:37<1:41:02,  1.62s/it, loss=2.63, v_num=641]Epoch 10:  15%|█▌        | 660/4381 [17:51<1:40:34,  1.62s/it, loss=2.63, v_num=641]Epoch 10:  15%|█▌        | 660/4381 [17:51<1:40:34,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  15%|█▌        | 670/4381 [18:06<1:40:09,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  15%|█▌        | 670/4381 [18:06<1:40:09,  1.62s/it, loss=2.61, v_num=641]Epoch 10:  16%|█▌        | 680/4381 [18:24<1:40:04,  1.62s/it, loss=2.61, v_num=641]Epoch 10:  16%|█▌        | 680/4381 [18:24<1:40:04,  1.62s/it, loss=2.63, v_num=641]Epoch 10:  16%|█▌        | 690/4381 [18:39<1:39:42,  1.62s/it, loss=2.63, v_num=641]Epoch 10:  16%|█▌        | 690/4381 [18:39<1:39:42,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  16%|█▌        | 700/4381 [18:56<1:39:27,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  16%|█▌        | 700/4381 [18:56<1:39:27,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  16%|█▌        | 710/4381 [19:11<1:39:06,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  16%|█▌        | 710/4381 [19:11<1:39:06,  1.62s/it, loss=2.57, v_num=641]Epoch 10:  16%|█▋        | 720/4381 [19:26<1:38:44,  1.62s/it, loss=2.57, v_num=641]Epoch 10:  16%|█▋        | 720/4381 [19:26<1:38:44,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  17%|█▋        | 730/4381 [19:39<1:38:08,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  17%|█▋        | 730/4381 [19:39<1:38:09,  1.61s/it, loss=2.65, v_num=641]Epoch 10:  17%|█▋        | 740/4381 [19:55<1:37:52,  1.61s/it, loss=2.65, v_num=641]Epoch 10:  17%|█▋        | 740/4381 [19:55<1:37:52,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  17%|█▋        | 750/4381 [20:10<1:37:34,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  17%|█▋        | 750/4381 [20:10<1:37:34,  1.61s/it, loss=2.54, v_num=641]Epoch 10:  17%|█▋        | 760/4381 [20:28<1:37:26,  1.61s/it, loss=2.54, v_num=641]Epoch 10:  17%|█▋        | 760/4381 [20:28<1:37:26,  1.61s/it, loss=2.57, v_num=641]Epoch 10:  18%|█▊        | 770/4381 [20:46<1:37:17,  1.62s/it, loss=2.57, v_num=641]Epoch 10:  18%|█▊        | 770/4381 [20:46<1:37:17,  1.62s/it, loss=2.62, v_num=641]Epoch 10:  18%|█▊        | 780/4381 [20:59<1:36:49,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  18%|█▊        | 780/4381 [20:59<1:36:49,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  18%|█▊        | 790/4381 [21:17<1:36:37,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  18%|█▊        | 790/4381 [21:17<1:36:37,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  18%|█▊        | 800/4381 [21:33<1:36:21,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  18%|█▊        | 800/4381 [21:33<1:36:21,  1.61s/it, loss=2.6, v_num=641] Epoch 10:  18%|█▊        | 810/4381 [21:48<1:36:02,  1.61s/it, loss=2.6, v_num=641]Epoch 10:  18%|█▊        | 810/4381 [21:48<1:36:02,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  19%|█▊        | 820/4381 [22:03<1:35:41,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  19%|█▊        | 820/4381 [22:03<1:35:41,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  19%|█▉        | 830/4381 [22:20<1:35:29,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  19%|█▉        | 830/4381 [22:20<1:35:29,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  19%|█▉        | 840/4381 [22:37<1:35:14,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  19%|█▉        | 840/4381 [22:37<1:35:14,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  19%|█▉        | 850/4381 [22:51<1:34:50,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  19%|█▉        | 850/4381 [22:51<1:34:50,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  20%|█▉        | 860/4381 [23:10<1:34:48,  1.62s/it, loss=2.61, v_num=641]Epoch 10:  20%|█▉        | 860/4381 [23:10<1:34:48,  1.62s/it, loss=2.59, v_num=641]Epoch 10:  20%|█▉        | 870/4381 [23:26<1:34:28,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  20%|█▉        | 870/4381 [23:26<1:34:28,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  20%|██        | 880/4381 [23:41<1:34:07,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  20%|██        | 880/4381 [23:41<1:34:07,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  20%|██        | 890/4381 [23:59<1:33:59,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  20%|██        | 890/4381 [23:59<1:33:59,  1.62s/it, loss=2.6, v_num=641] Epoch 10:  21%|██        | 900/4381 [24:14<1:33:40,  1.61s/it, loss=2.6, v_num=641]Epoch 10:  21%|██        | 900/4381 [24:14<1:33:40,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  21%|██        | 910/4381 [24:30<1:33:23,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  21%|██        | 910/4381 [24:30<1:33:23,  1.61s/it, loss=2.56, v_num=641]Epoch 10:  21%|██        | 920/4381 [24:49<1:33:16,  1.62s/it, loss=2.56, v_num=641]Epoch 10:  21%|██        | 920/4381 [24:49<1:33:16,  1.62s/it, loss=2.6, v_num=641] Epoch 10:  21%|██        | 930/4381 [25:02<1:32:50,  1.61s/it, loss=2.6, v_num=641]Epoch 10:  21%|██        | 930/4381 [25:02<1:32:50,  1.61s/it, loss=2.67, v_num=641]Epoch 10:  21%|██▏       | 940/4381 [25:16<1:32:24,  1.61s/it, loss=2.67, v_num=641]Epoch 10:  21%|██▏       | 940/4381 [25:16<1:32:24,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  22%|██▏       | 950/4381 [25:36<1:32:21,  1.62s/it, loss=2.63, v_num=641]Epoch 10:  22%|██▏       | 950/4381 [25:36<1:32:21,  1.62s/it, loss=2.64, v_num=641]Epoch 10:  22%|██▏       | 960/4381 [25:48<1:31:52,  1.61s/it, loss=2.64, v_num=641]Epoch 10:  22%|██▏       | 960/4381 [25:48<1:31:52,  1.61s/it, loss=2.6, v_num=641] Epoch 10:  22%|██▏       | 970/4381 [26:03<1:31:33,  1.61s/it, loss=2.6, v_num=641]Epoch 10:  22%|██▏       | 970/4381 [26:03<1:31:33,  1.61s/it, loss=2.56, v_num=641]Epoch 10:  22%|██▏       | 980/4381 [26:21<1:31:22,  1.61s/it, loss=2.56, v_num=641]Epoch 10:  22%|██▏       | 980/4381 [26:21<1:31:22,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  23%|██▎       | 990/4381 [26:36<1:31:03,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  23%|██▎       | 990/4381 [26:36<1:31:03,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  23%|██▎       | 1000/4381 [26:52<1:30:45,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  23%|██▎       | 1000/4381 [26:52<1:30:45,  1.61s/it, loss=2.64, v_num=641]Epoch 10:  23%|██▎       | 1010/4381 [27:10<1:30:38,  1.61s/it, loss=2.64, v_num=641]Epoch 10:  23%|██▎       | 1010/4381 [27:10<1:30:38,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  23%|██▎       | 1020/4381 [27:25<1:30:15,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  23%|██▎       | 1020/4381 [27:25<1:30:15,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  24%|██▎       | 1030/4381 [27:41<1:30:00,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  24%|██▎       | 1030/4381 [27:41<1:30:00,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  24%|██▎       | 1040/4381 [28:00<1:29:52,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  24%|██▎       | 1040/4381 [28:00<1:29:52,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  24%|██▍       | 1050/4381 [28:15<1:29:33,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  24%|██▍       | 1050/4381 [28:15<1:29:33,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  24%|██▍       | 1060/4381 [28:28<1:29:06,  1.61s/it, loss=2.63, v_num=641]Epoch 10:  24%|██▍       | 1060/4381 [28:28<1:29:06,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  24%|██▍       | 1070/4381 [28:45<1:28:55,  1.61s/it, loss=2.62, v_num=641]Epoch 10:  24%|██▍       | 1070/4381 [28:45<1:28:55,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  25%|██▍       | 1080/4381 [29:02<1:28:42,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  25%|██▍       | 1080/4381 [29:02<1:28:42,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  25%|██▍       | 1090/4381 [29:18<1:28:24,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  25%|██▍       | 1090/4381 [29:18<1:28:24,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  25%|██▌       | 1100/4381 [29:38<1:28:19,  1.62s/it, loss=2.61, v_num=641]Epoch 10:  25%|██▌       | 1100/4381 [29:38<1:28:19,  1.62s/it, loss=2.58, v_num=641]Epoch 10:  25%|██▌       | 1110/4381 [29:51<1:27:55,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  25%|██▌       | 1110/4381 [29:51<1:27:55,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  26%|██▌       | 1120/4381 [30:06<1:27:35,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  26%|██▌       | 1120/4381 [30:06<1:27:35,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  26%|██▌       | 1130/4381 [30:22<1:27:17,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  26%|██▌       | 1130/4381 [30:22<1:27:17,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  26%|██▌       | 1140/4381 [30:38<1:27:01,  1.61s/it, loss=2.58, v_num=641]Epoch 10:  26%|██▌       | 1140/4381 [30:38<1:27:01,  1.61s/it, loss=2.57, v_num=641]Epoch 10:  26%|██▌       | 1150/4381 [30:50<1:26:35,  1.61s/it, loss=2.57, v_num=641]Epoch 10:  26%|██▌       | 1150/4381 [30:50<1:26:35,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  26%|██▋       | 1160/4381 [31:06<1:26:17,  1.61s/it, loss=2.61, v_num=641]Epoch 10:  26%|██▋       | 1160/4381 [31:06<1:26:17,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  27%|██▋       | 1170/4381 [31:20<1:25:55,  1.61s/it, loss=2.59, v_num=641]Epoch 10:  27%|██▋       | 1170/4381 [31:20<1:25:55,  1.61s/it, loss=2.56, v_num=641]Epoch 10:  27%|██▋       | 1180/4381 [31:35<1:25:37,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  27%|██▋       | 1180/4381 [31:35<1:25:37,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  27%|██▋       | 1190/4381 [31:50<1:25:18,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  27%|██▋       | 1190/4381 [31:50<1:25:18,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  27%|██▋       | 1200/4381 [32:03<1:24:53,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  27%|██▋       | 1200/4381 [32:03<1:24:53,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  28%|██▊       | 1210/4381 [32:20<1:24:40,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  28%|██▊       | 1210/4381 [32:20<1:24:40,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  28%|██▊       | 1220/4381 [32:35<1:24:22,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  28%|██▊       | 1220/4381 [32:35<1:24:22,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  28%|██▊       | 1230/4381 [32:53<1:24:11,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  28%|██▊       | 1230/4381 [32:53<1:24:11,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  28%|██▊       | 1240/4381 [33:07<1:23:50,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  28%|██▊       | 1240/4381 [33:07<1:23:50,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  29%|██▊       | 1250/4381 [33:25<1:23:40,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  29%|██▊       | 1250/4381 [33:25<1:23:40,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  29%|██▉       | 1260/4381 [33:39<1:23:18,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  29%|██▉       | 1260/4381 [33:39<1:23:18,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  29%|██▉       | 1270/4381 [33:54<1:23:00,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  29%|██▉       | 1270/4381 [33:54<1:23:00,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  29%|██▉       | 1280/4381 [34:12<1:22:49,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  29%|██▉       | 1280/4381 [34:12<1:22:49,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  29%|██▉       | 1290/4381 [34:27<1:22:29,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  29%|██▉       | 1290/4381 [34:27<1:22:29,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  30%|██▉       | 1300/4381 [34:43<1:22:14,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  30%|██▉       | 1300/4381 [34:43<1:22:14,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  30%|██▉       | 1310/4381 [34:59<1:21:57,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  30%|██▉       | 1310/4381 [34:59<1:21:57,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  30%|███       | 1320/4381 [35:16<1:21:43,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  30%|███       | 1320/4381 [35:16<1:21:43,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  30%|███       | 1330/4381 [35:31<1:21:24,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  30%|███       | 1330/4381 [35:31<1:21:24,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  31%|███       | 1340/4381 [35:46<1:21:08,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  31%|███       | 1340/4381 [35:46<1:21:08,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███       | 1350/4381 [36:00<1:20:46,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███       | 1350/4381 [36:00<1:20:46,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███       | 1360/4381 [36:14<1:20:27,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███       | 1360/4381 [36:14<1:20:27,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███▏      | 1370/4381 [36:32<1:20:14,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  31%|███▏      | 1370/4381 [36:32<1:20:14,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  31%|███▏      | 1380/4381 [36:47<1:19:56,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  31%|███▏      | 1380/4381 [36:47<1:19:56,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  32%|███▏      | 1390/4381 [37:01<1:19:35,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  32%|███▏      | 1390/4381 [37:01<1:19:35,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  32%|███▏      | 1400/4381 [37:17<1:19:20,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  32%|███▏      | 1400/4381 [37:17<1:19:20,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  32%|███▏      | 1410/4381 [37:31<1:19:00,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  32%|███▏      | 1410/4381 [37:31<1:19:00,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  32%|███▏      | 1420/4381 [37:46<1:18:43,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  32%|███▏      | 1420/4381 [37:46<1:18:43,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  33%|███▎      | 1430/4381 [38:06<1:18:35,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  33%|███▎      | 1430/4381 [38:06<1:18:35,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  33%|███▎      | 1440/4381 [38:21<1:18:17,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  33%|███▎      | 1440/4381 [38:21<1:18:17,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  33%|███▎      | 1450/4381 [38:37<1:18:00,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  33%|███▎      | 1450/4381 [38:37<1:18:00,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  33%|███▎      | 1460/4381 [38:55<1:17:50,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  33%|███▎      | 1460/4381 [38:55<1:17:50,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  34%|███▎      | 1470/4381 [39:08<1:17:27,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  34%|███▎      | 1470/4381 [39:08<1:17:27,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  34%|███▍      | 1480/4381 [39:26<1:17:16,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  34%|███▍      | 1480/4381 [39:26<1:17:16,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  34%|███▍      | 1490/4381 [39:44<1:17:04,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  34%|███▍      | 1490/4381 [39:44<1:17:04,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  34%|███▍      | 1500/4381 [39:58<1:16:43,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  34%|███▍      | 1500/4381 [39:58<1:16:43,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  34%|███▍      | 1510/4381 [40:14<1:16:26,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  34%|███▍      | 1510/4381 [40:14<1:16:26,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  35%|███▍      | 1520/4381 [40:31<1:16:13,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  35%|███▍      | 1520/4381 [40:31<1:16:13,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  35%|███▍      | 1530/4381 [40:45<1:15:54,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  35%|███▍      | 1530/4381 [40:45<1:15:54,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  35%|███▌      | 1540/4381 [41:02<1:15:39,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  35%|███▌      | 1540/4381 [41:02<1:15:39,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  35%|███▌      | 1550/4381 [41:20<1:15:26,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  35%|███▌      | 1550/4381 [41:20<1:15:26,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  36%|███▌      | 1560/4381 [41:34<1:15:08,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  36%|███▌      | 1560/4381 [41:34<1:15:08,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  36%|███▌      | 1570/4381 [41:50<1:14:52,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  36%|███▌      | 1570/4381 [41:50<1:14:52,  1.60s/it, loss=2.55, v_num=641]Epoch 10:  36%|███▌      | 1580/4381 [42:07<1:14:38,  1.60s/it, loss=2.55, v_num=641]Epoch 10:  36%|███▌      | 1580/4381 [42:07<1:14:38,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  36%|███▋      | 1590/4381 [42:20<1:14:16,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  36%|███▋      | 1590/4381 [42:20<1:14:16,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  37%|███▋      | 1600/4381 [42:34<1:13:57,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  37%|███▋      | 1600/4381 [42:34<1:13:57,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  37%|███▋      | 1610/4381 [42:54<1:13:47,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  37%|███▋      | 1610/4381 [42:54<1:13:47,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  37%|███▋      | 1620/4381 [43:08<1:13:28,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  37%|███▋      | 1620/4381 [43:08<1:13:28,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  37%|███▋      | 1630/4381 [43:25<1:13:14,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  37%|███▋      | 1630/4381 [43:25<1:13:14,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  37%|███▋      | 1640/4381 [43:41<1:12:59,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  37%|███▋      | 1640/4381 [43:41<1:12:59,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  38%|███▊      | 1650/4381 [43:55<1:12:39,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  38%|███▊      | 1650/4381 [43:55<1:12:39,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  38%|███▊      | 1660/4381 [44:10<1:12:22,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  38%|███▊      | 1660/4381 [44:10<1:12:22,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  38%|███▊      | 1670/4381 [44:29<1:12:10,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  38%|███▊      | 1670/4381 [44:29<1:12:10,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  38%|███▊      | 1680/4381 [44:42<1:11:50,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  38%|███▊      | 1680/4381 [44:42<1:11:50,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  39%|███▊      | 1690/4381 [45:02<1:11:40,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  39%|███▊      | 1690/4381 [45:02<1:11:40,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  39%|███▉      | 1700/4381 [45:18<1:11:25,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  39%|███▉      | 1700/4381 [45:18<1:11:25,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  39%|███▉      | 1710/4381 [45:36<1:11:11,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  39%|███▉      | 1710/4381 [45:36<1:11:11,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  39%|███▉      | 1720/4381 [45:49<1:10:50,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  39%|███▉      | 1720/4381 [45:49<1:10:50,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  39%|███▉      | 1730/4381 [46:04<1:10:34,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  39%|███▉      | 1730/4381 [46:04<1:10:34,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  40%|███▉      | 1740/4381 [46:19<1:10:16,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  40%|███▉      | 1740/4381 [46:19<1:10:16,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  40%|███▉      | 1750/4381 [46:36<1:10:01,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  40%|███▉      | 1750/4381 [46:36<1:10:01,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  40%|████      | 1760/4381 [46:51<1:09:45,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  40%|████      | 1760/4381 [46:51<1:09:45,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  40%|████      | 1770/4381 [47:09<1:09:32,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  40%|████      | 1770/4381 [47:09<1:09:32,  1.60s/it, loss=2.52, v_num=641]Epoch 10:  41%|████      | 1780/4381 [47:26<1:09:17,  1.60s/it, loss=2.52, v_num=641]Epoch 10:  41%|████      | 1780/4381 [47:26<1:09:17,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  41%|████      | 1790/4381 [47:44<1:09:03,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  41%|████      | 1790/4381 [47:44<1:09:03,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  41%|████      | 1800/4381 [47:58<1:08:45,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  41%|████      | 1800/4381 [47:58<1:08:45,  1.60s/it, loss=2.65, v_num=641]Epoch 10:  41%|████▏     | 1810/4381 [48:15<1:08:31,  1.60s/it, loss=2.65, v_num=641]Epoch 10:  41%|████▏     | 1810/4381 [48:15<1:08:31,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  42%|████▏     | 1820/4381 [48:33<1:08:17,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  42%|████▏     | 1820/4381 [48:33<1:08:17,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  42%|████▏     | 1830/4381 [48:48<1:07:59,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  42%|████▏     | 1830/4381 [48:48<1:07:59,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  42%|████▏     | 1840/4381 [49:03<1:07:42,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  42%|████▏     | 1840/4381 [49:03<1:07:42,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  42%|████▏     | 1850/4381 [49:20<1:07:28,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  42%|████▏     | 1850/4381 [49:20<1:07:28,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  42%|████▏     | 1860/4381 [49:32<1:07:06,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  42%|████▏     | 1860/4381 [49:32<1:07:06,  1.60s/it, loss=2.66, v_num=641]Epoch 10:  43%|████▎     | 1870/4381 [49:47<1:06:49,  1.60s/it, loss=2.66, v_num=641]Epoch 10:  43%|████▎     | 1870/4381 [49:47<1:06:49,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  43%|████▎     | 1880/4381 [50:03<1:06:34,  1.60s/it, loss=2.67, v_num=641]Epoch 10:  43%|████▎     | 1880/4381 [50:03<1:06:34,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  43%|████▎     | 1890/4381 [50:19<1:06:18,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  43%|████▎     | 1890/4381 [50:19<1:06:18,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  43%|████▎     | 1900/4381 [50:37<1:06:03,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  43%|████▎     | 1900/4381 [50:37<1:06:03,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  44%|████▎     | 1910/4381 [50:53<1:05:48,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  44%|████▎     | 1910/4381 [50:53<1:05:48,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  44%|████▍     | 1920/4381 [51:09<1:05:32,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  44%|████▍     | 1920/4381 [51:09<1:05:32,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  44%|████▍     | 1930/4381 [51:22<1:05:12,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  44%|████▍     | 1930/4381 [51:22<1:05:12,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  44%|████▍     | 1940/4381 [51:41<1:05:00,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  44%|████▍     | 1940/4381 [51:41<1:05:00,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  45%|████▍     | 1950/4381 [51:56<1:04:43,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  45%|████▍     | 1950/4381 [51:56<1:04:43,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  45%|████▍     | 1960/4381 [52:13<1:04:28,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  45%|████▍     | 1960/4381 [52:13<1:04:28,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  45%|████▍     | 1970/4381 [52:31<1:04:15,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  45%|████▍     | 1970/4381 [52:31<1:04:15,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  45%|████▌     | 1980/4381 [52:47<1:03:58,  1.60s/it, loss=2.63, v_num=641]Epoch 10:  45%|████▌     | 1980/4381 [52:47<1:03:58,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  45%|████▌     | 1990/4381 [53:01<1:03:40,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  45%|████▌     | 1990/4381 [53:01<1:03:40,  1.60s/it, loss=2.55, v_num=641]Epoch 10:  46%|████▌     | 2000/4381 [53:20<1:03:28,  1.60s/it, loss=2.55, v_num=641]Epoch 10:  46%|████▌     | 2000/4381 [53:20<1:03:28,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▌     | 2010/4381 [53:37<1:03:13,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▌     | 2010/4381 [53:37<1:03:13,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▌     | 2020/4381 [53:51<1:02:54,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▌     | 2020/4381 [53:51<1:02:54,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  46%|████▋     | 2030/4381 [54:09<1:02:41,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  47%|████▋     | 2040/4381 [54:24<1:02:24,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  47%|████▋     | 2040/4381 [54:24<1:02:24,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  47%|████▋     | 2050/4381 [54:38<1:02:05,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  47%|████▋     | 2050/4381 [54:38<1:02:05,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  47%|████▋     | 2060/4381 [54:55<1:01:50,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  47%|████▋     | 2060/4381 [54:55<1:01:50,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  47%|████▋     | 2070/4381 [55:09<1:01:32,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  47%|████▋     | 2070/4381 [55:09<1:01:32,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  47%|████▋     | 2080/4381 [55:21<1:01:12,  1.60s/it, loss=2.54, v_num=641]Epoch 10:  47%|████▋     | 2080/4381 [55:21<1:01:12,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  48%|████▊     | 2090/4381 [55:36<1:00:56,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  48%|████▊     | 2090/4381 [55:36<1:00:56,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  48%|████▊     | 2100/4381 [55:53<1:00:40,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  48%|████▊     | 2100/4381 [55:53<1:00:40,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  48%|████▊     | 2110/4381 [56:05<1:00:20,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  48%|████▊     | 2110/4381 [56:05<1:00:20,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  48%|████▊     | 2120/4381 [56:25<1:00:08,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  48%|████▊     | 2120/4381 [56:25<1:00:08,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  49%|████▊     | 2130/4381 [56:41<59:52,  1.60s/it, loss=2.58, v_num=641]  Epoch 10:  49%|████▊     | 2130/4381 [56:41<59:52,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  49%|████▉     | 2140/4381 [56:57<59:37,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  49%|████▉     | 2140/4381 [56:57<59:37,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  49%|████▉     | 2150/4381 [57:15<59:23,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  49%|████▉     | 2150/4381 [57:15<59:23,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  49%|████▉     | 2160/4381 [57:29<59:05,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  49%|████▉     | 2160/4381 [57:29<59:05,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|████▉     | 2170/4381 [57:46<58:50,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|████▉     | 2170/4381 [57:46<58:50,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  50%|████▉     | 2180/4381 [58:02<58:34,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  50%|████▉     | 2180/4381 [58:02<58:34,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  50%|████▉     | 2190/4381 [58:18<58:18,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  50%|████▉     | 2190/4381 [58:18<58:18,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|█████     | 2200/4381 [58:32<58:00,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|█████     | 2200/4381 [58:32<58:00,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|█████     | 2210/4381 [58:49<57:45,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  50%|█████     | 2210/4381 [58:49<57:45,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  51%|█████     | 2220/4381 [59:04<57:28,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  51%|█████     | 2220/4381 [59:04<57:28,  1.60s/it, loss=2.6, v_num=641] Epoch 10:  51%|█████     | 2230/4381 [59:18<57:11,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  51%|█████     | 2230/4381 [59:18<57:11,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  51%|█████     | 2240/4381 [59:37<56:57,  1.60s/it, loss=2.6, v_num=641]Epoch 10:  51%|█████     | 2240/4381 [59:37<56:57,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  51%|█████▏    | 2250/4381 [59:50<56:39,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  51%|█████▏    | 2250/4381 [59:50<56:39,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  52%|█████▏    | 2260/4381 [1:00:08<56:24,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  52%|█████▏    | 2260/4381 [1:00:08<56:24,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  52%|█████▏    | 2270/4381 [1:00:25<56:09,  1.60s/it, loss=2.56, v_num=641]Epoch 10:  52%|█████▏    | 2270/4381 [1:00:25<56:09,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  52%|█████▏    | 2280/4381 [1:00:39<55:51,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  52%|█████▏    | 2280/4381 [1:00:39<55:51,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  52%|█████▏    | 2290/4381 [1:00:52<55:33,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  52%|█████▏    | 2290/4381 [1:00:52<55:33,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  52%|█████▏    | 2300/4381 [1:01:10<55:19,  1.60s/it, loss=2.64, v_num=641]Epoch 10:  52%|█████▏    | 2300/4381 [1:01:10<55:19,  1.60s/it, loss=2.61, v_num=641]Epoch 10:  53%|█████▎    | 2310/4381 [1:01:24<55:02,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  53%|█████▎    | 2310/4381 [1:01:24<55:02,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  53%|█████▎    | 2320/4381 [1:01:38<54:43,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  53%|█████▎    | 2320/4381 [1:01:38<54:43,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  53%|█████▎    | 2330/4381 [1:01:56<54:30,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  53%|█████▎    | 2330/4381 [1:01:56<54:30,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  53%|█████▎    | 2340/4381 [1:02:12<54:13,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  53%|█████▎    | 2340/4381 [1:02:12<54:13,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  54%|█████▎    | 2350/4381 [1:02:26<53:56,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  54%|█████▎    | 2350/4381 [1:02:26<53:56,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  54%|█████▍    | 2360/4381 [1:02:45<53:43,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  54%|█████▍    | 2360/4381 [1:02:45<53:43,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  54%|█████▍    | 2370/4381 [1:02:59<53:25,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  54%|█████▍    | 2370/4381 [1:02:59<53:25,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  54%|█████▍    | 2380/4381 [1:03:13<53:07,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  54%|█████▍    | 2380/4381 [1:03:13<53:07,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▍    | 2390/4381 [1:03:30<52:52,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▍    | 2390/4381 [1:03:30<52:52,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  55%|█████▍    | 2400/4381 [1:03:46<52:37,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  55%|█████▍    | 2400/4381 [1:03:46<52:37,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  55%|█████▌    | 2410/4381 [1:04:03<52:22,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  55%|█████▌    | 2410/4381 [1:04:03<52:22,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▌    | 2420/4381 [1:04:24<52:10,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▌    | 2420/4381 [1:04:24<52:10,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▌    | 2430/4381 [1:04:40<51:53,  1.60s/it, loss=2.62, v_num=641]Epoch 10:  55%|█████▌    | 2430/4381 [1:04:40<51:53,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  56%|█████▌    | 2440/4381 [1:04:53<51:35,  1.60s/it, loss=2.58, v_num=641]Epoch 10:  56%|█████▌    | 2440/4381 [1:04:53<51:35,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  56%|█████▌    | 2450/4381 [1:05:08<51:19,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  56%|█████▌    | 2450/4381 [1:05:08<51:19,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  56%|█████▌    | 2460/4381 [1:05:23<51:02,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  56%|█████▌    | 2460/4381 [1:05:23<51:02,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  56%|█████▋    | 2470/4381 [1:05:37<50:45,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  56%|█████▋    | 2470/4381 [1:05:37<50:45,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2480/4381 [1:05:57<50:32,  1.60s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2480/4381 [1:05:57<50:32,  1.60s/it, loss=2.57, v_num=641]Epoch 10:  57%|█████▋    | 2490/4381 [1:06:11<50:14,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  57%|█████▋    | 2490/4381 [1:06:11<50:14,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2500/4381 [1:06:25<49:57,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2500/4381 [1:06:25<49:57,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2510/4381 [1:06:41<49:41,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  57%|█████▋    | 2510/4381 [1:06:41<49:41,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  58%|█████▊    | 2520/4381 [1:06:56<49:25,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  58%|█████▊    | 2520/4381 [1:06:56<49:25,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  58%|█████▊    | 2530/4381 [1:07:14<49:10,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  58%|█████▊    | 2530/4381 [1:07:14<49:10,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  58%|█████▊    | 2540/4381 [1:07:28<48:53,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  58%|█████▊    | 2540/4381 [1:07:28<48:53,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  58%|█████▊    | 2550/4381 [1:07:43<48:36,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  58%|█████▊    | 2550/4381 [1:07:43<48:36,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  58%|█████▊    | 2560/4381 [1:07:59<48:20,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  58%|█████▊    | 2560/4381 [1:07:59<48:20,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  59%|█████▊    | 2570/4381 [1:08:18<48:07,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  59%|█████▊    | 2570/4381 [1:08:18<48:07,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  59%|█████▉    | 2580/4381 [1:08:34<47:50,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  59%|█████▉    | 2580/4381 [1:08:34<47:50,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  59%|█████▉    | 2590/4381 [1:08:49<47:34,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  59%|█████▉    | 2590/4381 [1:08:49<47:34,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  59%|█████▉    | 2600/4381 [1:09:05<47:18,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  59%|█████▉    | 2600/4381 [1:09:05<47:18,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  60%|█████▉    | 2610/4381 [1:09:22<47:03,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  60%|█████▉    | 2610/4381 [1:09:22<47:03,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  60%|█████▉    | 2620/4381 [1:09:36<46:46,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  60%|█████▉    | 2620/4381 [1:09:36<46:46,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  60%|██████    | 2630/4381 [1:09:53<46:30,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  60%|██████    | 2630/4381 [1:09:53<46:30,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  60%|██████    | 2640/4381 [1:10:08<46:14,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  60%|██████    | 2640/4381 [1:10:08<46:14,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  60%|██████    | 2650/4381 [1:10:24<45:58,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  60%|██████    | 2650/4381 [1:10:24<45:58,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  61%|██████    | 2660/4381 [1:10:39<45:42,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  61%|██████    | 2660/4381 [1:10:39<45:42,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  61%|██████    | 2670/4381 [1:10:52<45:23,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  61%|██████    | 2670/4381 [1:10:52<45:23,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  61%|██████    | 2680/4381 [1:11:07<45:07,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  61%|██████    | 2680/4381 [1:11:07<45:07,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  61%|██████▏   | 2690/4381 [1:11:26<44:53,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  61%|██████▏   | 2690/4381 [1:11:26<44:53,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  62%|██████▏   | 2700/4381 [1:11:43<44:38,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  62%|██████▏   | 2700/4381 [1:11:43<44:38,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  62%|██████▏   | 2710/4381 [1:11:56<44:20,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  62%|██████▏   | 2710/4381 [1:11:56<44:20,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  62%|██████▏   | 2720/4381 [1:12:16<44:07,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  62%|██████▏   | 2720/4381 [1:12:16<44:07,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  62%|██████▏   | 2730/4381 [1:12:30<43:50,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  62%|██████▏   | 2730/4381 [1:12:30<43:50,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  63%|██████▎   | 2740/4381 [1:12:46<43:34,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  63%|██████▎   | 2740/4381 [1:12:46<43:34,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  63%|██████▎   | 2750/4381 [1:13:04<43:19,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  63%|██████▎   | 2760/4381 [1:13:19<43:02,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  63%|██████▎   | 2760/4381 [1:13:19<43:02,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  63%|██████▎   | 2770/4381 [1:13:34<42:46,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  63%|██████▎   | 2770/4381 [1:13:34<42:46,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  63%|██████▎   | 2780/4381 [1:13:50<42:30,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  63%|██████▎   | 2780/4381 [1:13:50<42:30,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  64%|██████▎   | 2790/4381 [1:14:05<42:14,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  64%|██████▎   | 2790/4381 [1:14:05<42:14,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  64%|██████▍   | 2800/4381 [1:14:19<41:57,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  64%|██████▍   | 2800/4381 [1:14:19<41:57,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  64%|██████▍   | 2810/4381 [1:14:38<41:42,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  64%|██████▍   | 2810/4381 [1:14:38<41:42,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  64%|██████▍   | 2820/4381 [1:14:55<41:27,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  64%|██████▍   | 2820/4381 [1:14:55<41:27,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  65%|██████▍   | 2830/4381 [1:15:10<41:10,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  65%|██████▍   | 2830/4381 [1:15:10<41:10,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▍   | 2840/4381 [1:15:27<40:55,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▍   | 2840/4381 [1:15:27<40:55,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▌   | 2850/4381 [1:15:43<40:39,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▌   | 2850/4381 [1:15:43<40:39,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▌   | 2860/4381 [1:15:57<40:23,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  65%|██████▌   | 2860/4381 [1:15:57<40:23,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  66%|██████▌   | 2870/4381 [1:16:13<40:06,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  66%|██████▌   | 2870/4381 [1:16:13<40:06,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  66%|██████▌   | 2880/4381 [1:16:28<39:50,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  66%|██████▌   | 2880/4381 [1:16:28<39:50,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  66%|██████▌   | 2890/4381 [1:16:44<39:34,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  66%|██████▌   | 2890/4381 [1:16:44<39:34,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  66%|██████▌   | 2900/4381 [1:17:00<39:18,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  66%|██████▌   | 2900/4381 [1:17:00<39:18,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  66%|██████▋   | 2910/4381 [1:17:14<39:02,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  66%|██████▋   | 2910/4381 [1:17:14<39:02,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  67%|██████▋   | 2920/4381 [1:17:28<38:45,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  67%|██████▋   | 2920/4381 [1:17:28<38:45,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  67%|██████▋   | 2930/4381 [1:17:46<38:29,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  67%|██████▋   | 2930/4381 [1:17:46<38:29,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  67%|██████▋   | 2940/4381 [1:18:01<38:13,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  67%|██████▋   | 2940/4381 [1:18:01<38:13,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  67%|██████▋   | 2950/4381 [1:18:17<37:57,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  67%|██████▋   | 2950/4381 [1:18:17<37:57,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  68%|██████▊   | 2960/4381 [1:18:37<37:43,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  68%|██████▊   | 2960/4381 [1:18:37<37:43,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  68%|██████▊   | 2970/4381 [1:18:50<37:26,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  68%|██████▊   | 2970/4381 [1:18:50<37:26,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  68%|██████▊   | 2980/4381 [1:19:07<37:11,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  68%|██████▊   | 2980/4381 [1:19:07<37:11,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  68%|██████▊   | 2990/4381 [1:19:26<36:56,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  68%|██████▊   | 2990/4381 [1:19:26<36:56,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  68%|██████▊   | 3000/4381 [1:19:40<36:39,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  68%|██████▊   | 3000/4381 [1:19:40<36:39,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  69%|██████▊   | 3010/4381 [1:19:55<36:23,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  69%|██████▊   | 3010/4381 [1:19:55<36:23,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  69%|██████▉   | 3020/4381 [1:20:12<36:08,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  69%|██████▉   | 3020/4381 [1:20:12<36:08,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  69%|██████▉   | 3030/4381 [1:20:26<35:51,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  69%|██████▉   | 3030/4381 [1:20:26<35:51,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  69%|██████▉   | 3040/4381 [1:20:42<35:35,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  69%|██████▉   | 3040/4381 [1:20:42<35:35,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  70%|██████▉   | 3050/4381 [1:21:01<35:20,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  70%|██████▉   | 3050/4381 [1:21:01<35:20,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  70%|██████▉   | 3060/4381 [1:21:16<35:04,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  70%|██████▉   | 3060/4381 [1:21:16<35:04,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  70%|███████   | 3070/4381 [1:21:32<34:48,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  70%|███████   | 3070/4381 [1:21:32<34:48,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  70%|███████   | 3080/4381 [1:21:50<34:33,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  70%|███████   | 3080/4381 [1:21:50<34:33,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  71%|███████   | 3090/4381 [1:22:03<34:16,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  71%|███████   | 3090/4381 [1:22:03<34:16,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  71%|███████   | 3100/4381 [1:22:18<33:59,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  71%|███████   | 3100/4381 [1:22:18<33:59,  1.59s/it, loss=2.55, v_num=641]Epoch 10:  71%|███████   | 3110/4381 [1:22:38<33:45,  1.59s/it, loss=2.55, v_num=641]Epoch 10:  71%|███████   | 3110/4381 [1:22:38<33:45,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  71%|███████   | 3120/4381 [1:22:54<33:29,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  71%|███████   | 3120/4381 [1:22:54<33:29,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  71%|███████▏  | 3130/4381 [1:23:08<33:12,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  71%|███████▏  | 3130/4381 [1:23:08<33:12,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  72%|███████▏  | 3140/4381 [1:23:26<32:57,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  72%|███████▏  | 3140/4381 [1:23:26<32:57,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  72%|███████▏  | 3150/4381 [1:23:40<32:41,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  72%|███████▏  | 3150/4381 [1:23:40<32:41,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  72%|███████▏  | 3160/4381 [1:23:57<32:25,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  72%|███████▏  | 3160/4381 [1:23:57<32:25,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  72%|███████▏  | 3170/4381 [1:24:15<32:10,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  72%|███████▏  | 3170/4381 [1:24:15<32:10,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  73%|███████▎  | 3180/4381 [1:24:28<31:53,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  73%|███████▎  | 3180/4381 [1:24:28<31:53,  1.59s/it, loss=2.67, v_num=641]Epoch 10:  73%|███████▎  | 3190/4381 [1:24:43<31:37,  1.59s/it, loss=2.67, v_num=641]Epoch 10:  73%|███████▎  | 3190/4381 [1:24:43<31:37,  1.59s/it, loss=2.69, v_num=641]Epoch 10:  73%|███████▎  | 3200/4381 [1:25:02<31:22,  1.59s/it, loss=2.69, v_num=641]Epoch 10:  73%|███████▎  | 3200/4381 [1:25:02<31:22,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  73%|███████▎  | 3210/4381 [1:25:17<31:06,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  73%|███████▎  | 3210/4381 [1:25:17<31:06,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  73%|███████▎  | 3220/4381 [1:25:33<30:50,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  73%|███████▎  | 3220/4381 [1:25:33<30:50,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  74%|███████▎  | 3230/4381 [1:25:50<30:34,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  74%|███████▎  | 3230/4381 [1:25:50<30:34,  1.59s/it, loss=2.55, v_num=641]Epoch 10:  74%|███████▍  | 3240/4381 [1:26:04<30:18,  1.59s/it, loss=2.55, v_num=641]Epoch 10:  74%|███████▍  | 3240/4381 [1:26:04<30:18,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  74%|███████▍  | 3250/4381 [1:26:18<30:01,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  74%|███████▍  | 3250/4381 [1:26:18<30:01,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  74%|███████▍  | 3260/4381 [1:26:37<29:46,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  74%|███████▍  | 3260/4381 [1:26:37<29:46,  1.59s/it, loss=2.68, v_num=641]Epoch 10:  75%|███████▍  | 3270/4381 [1:26:52<29:30,  1.59s/it, loss=2.68, v_num=641]Epoch 10:  75%|███████▍  | 3270/4381 [1:26:52<29:30,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  75%|███████▍  | 3280/4381 [1:27:06<29:13,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  75%|███████▍  | 3280/4381 [1:27:06<29:13,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  75%|███████▌  | 3290/4381 [1:27:24<28:58,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  75%|███████▌  | 3290/4381 [1:27:24<28:58,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  75%|███████▌  | 3300/4381 [1:27:41<28:43,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  75%|███████▌  | 3300/4381 [1:27:41<28:43,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  76%|███████▌  | 3310/4381 [1:27:55<28:26,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  76%|███████▌  | 3310/4381 [1:27:55<28:26,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  76%|███████▌  | 3320/4381 [1:28:13<28:11,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  76%|███████▌  | 3320/4381 [1:28:13<28:11,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  76%|███████▌  | 3330/4381 [1:28:27<27:54,  1.59s/it, loss=2.66, v_num=641]Epoch 10:  76%|███████▌  | 3330/4381 [1:28:27<27:54,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  76%|███████▌  | 3340/4381 [1:28:43<27:38,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  76%|███████▌  | 3340/4381 [1:28:43<27:38,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  76%|███████▋  | 3350/4381 [1:29:00<27:23,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  76%|███████▋  | 3350/4381 [1:29:00<27:23,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  77%|███████▋  | 3360/4381 [1:29:13<27:06,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  77%|███████▋  | 3360/4381 [1:29:13<27:06,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  77%|███████▋  | 3370/4381 [1:29:28<26:49,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  77%|███████▋  | 3370/4381 [1:29:28<26:49,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  77%|███████▋  | 3380/4381 [1:29:47<26:35,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  77%|███████▋  | 3380/4381 [1:29:47<26:35,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  77%|███████▋  | 3390/4381 [1:30:01<26:18,  1.59s/it, loss=2.56, v_num=641]Epoch 10:  77%|███████▋  | 3390/4381 [1:30:01<26:18,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  78%|███████▊  | 3400/4381 [1:30:16<26:02,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  78%|███████▊  | 3400/4381 [1:30:16<26:02,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  78%|███████▊  | 3410/4381 [1:30:32<25:46,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  78%|███████▊  | 3410/4381 [1:30:32<25:46,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  78%|███████▊  | 3420/4381 [1:30:46<25:29,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  78%|███████▊  | 3420/4381 [1:30:46<25:29,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  78%|███████▊  | 3430/4381 [1:30:59<25:13,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  78%|███████▊  | 3430/4381 [1:30:59<25:13,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▊  | 3440/4381 [1:31:15<24:57,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▊  | 3440/4381 [1:31:15<24:57,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  79%|███████▊  | 3450/4381 [1:31:29<24:40,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  79%|███████▊  | 3450/4381 [1:31:29<24:40,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  79%|███████▉  | 3460/4381 [1:31:42<24:24,  1.59s/it, loss=2.64, v_num=641]Epoch 10:  79%|███████▉  | 3460/4381 [1:31:42<24:24,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▉  | 3470/4381 [1:32:02<24:09,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▉  | 3470/4381 [1:32:02<24:09,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▉  | 3480/4381 [1:32:18<23:53,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  79%|███████▉  | 3480/4381 [1:32:18<23:53,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  80%|███████▉  | 3490/4381 [1:32:32<23:37,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  80%|███████▉  | 3490/4381 [1:32:32<23:37,  1.59s/it, loss=2.68, v_num=641]Epoch 10:  80%|███████▉  | 3500/4381 [1:32:51<23:22,  1.59s/it, loss=2.68, v_num=641]Epoch 10:  80%|███████▉  | 3500/4381 [1:32:51<23:22,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  80%|████████  | 3510/4381 [1:33:04<23:05,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  80%|████████  | 3510/4381 [1:33:04<23:05,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  80%|████████  | 3520/4381 [1:33:18<22:49,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  80%|████████  | 3520/4381 [1:33:18<22:49,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  81%|████████  | 3530/4381 [1:33:36<22:33,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  81%|████████  | 3530/4381 [1:33:36<22:33,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  81%|████████  | 3540/4381 [1:33:51<22:17,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  81%|████████  | 3540/4381 [1:33:51<22:17,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  81%|████████  | 3550/4381 [1:34:03<22:00,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  81%|████████  | 3550/4381 [1:34:03<22:00,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  81%|████████▏ | 3560/4381 [1:34:20<21:45,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  81%|████████▏ | 3560/4381 [1:34:20<21:45,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  81%|████████▏ | 3570/4381 [1:34:36<21:29,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  81%|████████▏ | 3570/4381 [1:34:36<21:29,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  82%|████████▏ | 3580/4381 [1:34:50<21:12,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  82%|████████▏ | 3580/4381 [1:34:50<21:12,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  82%|████████▏ | 3590/4381 [1:35:11<20:58,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  82%|████████▏ | 3590/4381 [1:35:11<20:58,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  82%|████████▏ | 3600/4381 [1:35:22<20:41,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  82%|████████▏ | 3600/4381 [1:35:22<20:41,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  82%|████████▏ | 3610/4381 [1:35:38<20:25,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  82%|████████▏ | 3610/4381 [1:35:38<20:25,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  83%|████████▎ | 3620/4381 [1:35:55<20:09,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  83%|████████▎ | 3620/4381 [1:35:55<20:09,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  83%|████████▎ | 3630/4381 [1:36:13<19:54,  1.59s/it, loss=2.59, v_num=641]Epoch 10:  83%|████████▎ | 3630/4381 [1:36:13<19:54,  1.59s/it, loss=2.6, v_num=641] Epoch 10:  83%|████████▎ | 3640/4381 [1:36:28<19:37,  1.59s/it, loss=2.6, v_num=641]Epoch 10:  83%|████████▎ | 3640/4381 [1:36:28<19:37,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.63, v_num=641]Epoch 10:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  84%|████████▎ | 3660/4381 [1:37:00<19:06,  1.59s/it, loss=2.65, v_num=641]Epoch 10:  84%|████████▎ | 3660/4381 [1:37:00<19:06,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  84%|████████▍ | 3670/4381 [1:37:14<18:50,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  84%|████████▍ | 3670/4381 [1:37:14<18:50,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  84%|████████▍ | 3680/4381 [1:37:30<18:34,  1.59s/it, loss=2.61, v_num=641]Epoch 10:  84%|████████▍ | 3680/4381 [1:37:30<18:34,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  84%|████████▍ | 3690/4381 [1:37:45<18:18,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  84%|████████▍ | 3690/4381 [1:37:45<18:18,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  84%|████████▍ | 3700/4381 [1:38:02<18:02,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  84%|████████▍ | 3700/4381 [1:38:02<18:02,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  85%|████████▍ | 3710/4381 [1:38:17<17:46,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  85%|████████▍ | 3710/4381 [1:38:17<17:46,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  85%|████████▍ | 3720/4381 [1:38:33<17:30,  1.59s/it, loss=2.62, v_num=641]Epoch 10:  85%|████████▍ | 3720/4381 [1:38:33<17:30,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  85%|████████▌ | 3730/4381 [1:38:44<17:13,  1.59s/it, loss=2.58, v_num=641]Epoch 10:  85%|████████▌ | 3730/4381 [1:38:44<17:13,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.57, v_num=641]Epoch 10:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.55, v_num=641]Epoch 10:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=2.55, v_num=641]Epoch 10:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=2.58, v_num=641]validation_epoch_end
graph acc: 0.24920127795527156
valid accuracy: 0.9655585289001465
validation_epoch_end
graph acc: 0.2795527156549521
valid accuracy: 0.9617640376091003
validation_epoch_end
graph acc: 0.24920127795527156
valid accuracy: 0.9635246992111206
Epoch 10:  86%|████████▌ | 3760/4381 [1:38:58<16:20,  1.58s/it, loss=2.58, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.26038338658146964
valid accuracy: 0.9672144651412964
validation_epoch_end
graph acc: 0.26517571884984026
valid accuracy: 0.9670541286468506
validation_epoch_end
graph acc: 0.26198083067092653
valid accuracy: 0.9667872190475464
validation_epoch_end
graph acc: 0.23482428115015974
valid accuracy: 0.9631045460700989

Validating:   2%|▏         | 10/626 [00:03<03:46,  2.72it/s][AEpoch 10:  86%|████████▌ | 3770/4381 [1:39:02<16:02,  1.58s/it, loss=2.58, v_num=641]
Validating:   3%|▎         | 20/626 [00:04<02:13,  4.55it/s][AEpoch 10:  86%|████████▋ | 3780/4381 [1:39:03<15:44,  1.57s/it, loss=2.58, v_num=641]
Validating:   5%|▍         | 30/626 [00:07<02:10,  4.57it/s][AEpoch 10:  87%|████████▋ | 3790/4381 [1:39:05<15:26,  1.57s/it, loss=2.58, v_num=641]
Validating:   6%|▋         | 40/626 [00:08<01:57,  5.00it/s][AEpoch 10:  87%|████████▋ | 3800/4381 [1:39:07<15:09,  1.56s/it, loss=2.58, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:44,  5.54it/s][AEpoch 10:  87%|████████▋ | 3810/4381 [1:39:08<14:51,  1.56s/it, loss=2.58, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:57,  4.82it/s][AEpoch 10:  87%|████████▋ | 3820/4381 [1:39:11<14:33,  1.56s/it, loss=2.58, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:43,  5.36it/s][AEpoch 10:  87%|████████▋ | 3830/4381 [1:39:12<14:16,  1.55s/it, loss=2.58, v_num=641]
Validating:  13%|█▎        | 80/626 [00:15<01:29,  6.12it/s][AEpoch 10:  88%|████████▊ | 3840/4381 [1:39:13<13:58,  1.55s/it, loss=2.58, v_num=641]
Validating:  14%|█▍        | 90/626 [00:17<01:31,  5.87it/s][AEpoch 10:  88%|████████▊ | 3850/4381 [1:39:15<13:41,  1.55s/it, loss=2.58, v_num=641]
Validating:  16%|█▌        | 100/626 [00:19<01:35,  5.51it/s][AEpoch 10:  88%|████████▊ | 3860/4381 [1:39:17<13:23,  1.54s/it, loss=2.58, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:42,  5.01it/s][AEpoch 10:  88%|████████▊ | 3870/4381 [1:39:20<13:06,  1.54s/it, loss=2.58, v_num=641]
Validating:  19%|█▉        | 120/626 [00:23<01:33,  5.44it/s][AEpoch 10:  89%|████████▊ | 3880/4381 [1:39:21<12:49,  1.54s/it, loss=2.58, v_num=641]
Validating:  21%|██        | 130/626 [00:25<01:39,  4.99it/s][AEpoch 10:  89%|████████▉ | 3890/4381 [1:39:24<12:32,  1.53s/it, loss=2.58, v_num=641]
Validating:  22%|██▏       | 140/626 [00:27<01:43,  4.70it/s][AEpoch 10:  89%|████████▉ | 3900/4381 [1:39:26<12:15,  1.53s/it, loss=2.58, v_num=641]
Validating:  24%|██▍       | 150/626 [00:29<01:35,  5.00it/s][AEpoch 10:  89%|████████▉ | 3910/4381 [1:39:28<11:58,  1.53s/it, loss=2.58, v_num=641]
Validating:  26%|██▌       | 160/626 [00:31<01:33,  5.00it/s][AEpoch 10:  89%|████████▉ | 3920/4381 [1:39:30<11:41,  1.52s/it, loss=2.58, v_num=641]
Validating:  27%|██▋       | 170/626 [00:33<01:33,  4.89it/s][AEpoch 10:  90%|████████▉ | 3930/4381 [1:39:32<11:25,  1.52s/it, loss=2.58, v_num=641]
Validating:  29%|██▉       | 180/626 [00:35<01:23,  5.35it/s][AEpoch 10:  90%|████████▉ | 3940/4381 [1:39:33<11:08,  1.52s/it, loss=2.58, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:32,  4.73it/s][AEpoch 10:  90%|█████████ | 3950/4381 [1:39:36<10:51,  1.51s/it, loss=2.58, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:37,  4.36it/s][AEpoch 10:  90%|█████████ | 3960/4381 [1:39:39<10:35,  1.51s/it, loss=2.58, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:31,  4.55it/s][AEpoch 10:  91%|█████████ | 3970/4381 [1:39:41<10:19,  1.51s/it, loss=2.58, v_num=641]
Validating:  35%|███▌      | 220/626 [00:44<01:31,  4.46it/s][AEpoch 10:  91%|█████████ | 3980/4381 [1:39:43<10:02,  1.50s/it, loss=2.58, v_num=641]
Validating:  37%|███▋      | 230/626 [00:47<01:28,  4.47it/s][AEpoch 10:  91%|█████████ | 3990/4381 [1:39:45<09:46,  1.50s/it, loss=2.58, v_num=641]
Validating:  38%|███▊      | 240/626 [00:49<01:26,  4.47it/s][AEpoch 10:  91%|█████████▏| 4000/4381 [1:39:48<09:30,  1.50s/it, loss=2.58, v_num=641]
Validating:  40%|███▉      | 250/626 [00:51<01:20,  4.67it/s][AEpoch 10:  92%|█████████▏| 4010/4381 [1:39:50<09:14,  1.49s/it, loss=2.58, v_num=641]
Validating:  42%|████▏     | 260/626 [00:53<01:16,  4.76it/s][AEpoch 10:  92%|█████████▏| 4020/4381 [1:39:52<08:57,  1.49s/it, loss=2.58, v_num=641]
Validating:  43%|████▎     | 270/626 [00:54<01:06,  5.38it/s][AEpoch 10:  92%|█████████▏| 4030/4381 [1:39:53<08:41,  1.49s/it, loss=2.58, v_num=641]
Validating:  45%|████▍     | 280/626 [00:57<01:15,  4.57it/s][AEpoch 10:  92%|█████████▏| 4040/4381 [1:39:56<08:25,  1.48s/it, loss=2.58, v_num=641]
Validating:  46%|████▋     | 290/626 [01:00<01:20,  4.19it/s][AEpoch 10:  92%|█████████▏| 4050/4381 [1:39:59<08:10,  1.48s/it, loss=2.58, v_num=641]
Validating:  48%|████▊     | 300/626 [01:02<01:09,  4.68it/s][AEpoch 10:  93%|█████████▎| 4060/4381 [1:40:00<07:54,  1.48s/it, loss=2.58, v_num=641]
Validating:  50%|████▉     | 310/626 [01:03<01:02,  5.06it/s][AEpoch 10:  93%|█████████▎| 4070/4381 [1:40:02<07:38,  1.47s/it, loss=2.58, v_num=641]
Validating:  51%|█████     | 320/626 [01:05<00:59,  5.17it/s][AEpoch 10:  93%|█████████▎| 4080/4381 [1:40:04<07:22,  1.47s/it, loss=2.58, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:07<00:55,  5.31it/s][AEpoch 10:  93%|█████████▎| 4090/4381 [1:40:05<07:07,  1.47s/it, loss=2.58, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:09<00:54,  5.23it/s][AEpoch 10:  94%|█████████▎| 4100/4381 [1:40:07<06:51,  1.46s/it, loss=2.58, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:12<01:00,  4.53it/s][AEpoch 10:  94%|█████████▍| 4110/4381 [1:40:10<06:36,  1.46s/it, loss=2.58, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:14<00:57,  4.63it/s][AEpoch 10:  94%|█████████▍| 4120/4381 [1:40:12<06:20,  1.46s/it, loss=2.58, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:14<00:44,  5.69it/s][AEpoch 10:  94%|█████████▍| 4130/4381 [1:40:13<06:05,  1.46s/it, loss=2.58, v_num=641]
Validating:  61%|██████    | 380/626 [01:16<00:42,  5.81it/s][AEpoch 10:  94%|█████████▍| 4140/4381 [1:40:15<05:50,  1.45s/it, loss=2.58, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:17<00:35,  6.58it/s][AEpoch 10:  95%|█████████▍| 4150/4381 [1:40:16<05:34,  1.45s/it, loss=2.58, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:19<00:38,  5.90it/s][AEpoch 10:  95%|█████████▍| 4160/4381 [1:40:18<05:19,  1.45s/it, loss=2.58, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:21<00:35,  6.12it/s][AEpoch 10:  95%|█████████▌| 4170/4381 [1:40:19<05:04,  1.44s/it, loss=2.58, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:24<00:42,  4.90it/s][AEpoch 10:  95%|█████████▌| 4180/4381 [1:40:22<04:49,  1.44s/it, loss=2.58, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:25<00:35,  5.48it/s][AEpoch 10:  96%|█████████▌| 4190/4381 [1:40:24<04:34,  1.44s/it, loss=2.58, v_num=641]
Validating:  70%|███████   | 440/626 [01:28<00:39,  4.66it/s][AEpoch 10:  96%|█████████▌| 4200/4381 [1:40:27<04:19,  1.43s/it, loss=2.58, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:30<00:36,  4.78it/s][AEpoch 10:  96%|█████████▌| 4210/4381 [1:40:29<04:04,  1.43s/it, loss=2.58, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:32<00:32,  5.05it/s][AEpoch 10:  96%|█████████▋| 4220/4381 [1:40:30<03:50,  1.43s/it, loss=2.58, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:34<00:31,  4.99it/s][AEpoch 10:  97%|█████████▋| 4230/4381 [1:40:32<03:35,  1.43s/it, loss=2.58, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:36<00:30,  4.85it/s][AEpoch 10:  97%|█████████▋| 4240/4381 [1:40:35<03:20,  1.42s/it, loss=2.58, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:38<00:30,  4.53it/s][AEpoch 10:  97%|█████████▋| 4250/4381 [1:40:37<03:06,  1.42s/it, loss=2.58, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:40<00:24,  5.17it/s][AEpoch 10:  97%|█████████▋| 4260/4381 [1:40:38<02:51,  1.42s/it, loss=2.58, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:42<00:22,  5.21it/s][AEpoch 10:  97%|█████████▋| 4270/4381 [1:40:40<02:36,  1.41s/it, loss=2.58, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:43<00:19,  5.55it/s][AEpoch 10:  98%|█████████▊| 4280/4381 [1:40:42<02:22,  1.41s/it, loss=2.58, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:45<00:16,  5.65it/s][AEpoch 10:  98%|█████████▊| 4290/4381 [1:40:44<02:08,  1.41s/it, loss=2.58, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:46<00:13,  6.19it/s][AEpoch 10:  98%|█████████▊| 4300/4381 [1:40:45<01:53,  1.41s/it, loss=2.58, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:48<00:13,  5.74it/s][AEpoch 10:  98%|█████████▊| 4310/4381 [1:40:47<01:39,  1.40s/it, loss=2.58, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:50<00:11,  5.93it/s][AEpoch 10:  99%|█████████▊| 4320/4381 [1:40:48<01:25,  1.40s/it, loss=2.58, v_num=641]
Validating:  91%|█████████ | 570/626 [01:52<00:09,  5.80it/s][AEpoch 10:  99%|█████████▉| 4330/4381 [1:40:50<01:11,  1.40s/it, loss=2.58, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:54<00:08,  5.16it/s][AEpoch 10:  99%|█████████▉| 4340/4381 [1:40:53<00:57,  1.39s/it, loss=2.58, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:55<00:06,  5.73it/s][AEpoch 10:  99%|█████████▉| 4350/4381 [1:40:54<00:43,  1.39s/it, loss=2.58, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:56<00:04,  6.49it/s][AEpoch 10: 100%|█████████▉| 4360/4381 [1:40:55<00:29,  1.39s/it, loss=2.58, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:58<00:02,  6.83it/s][AEpoch 10: 100%|█████████▉| 4370/4381 [1:40:56<00:15,  1.39s/it, loss=2.58, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:59<00:00,  7.20it/s][AEpoch 10: 100%|█████████▉| 4380/4381 [1:40:57<00:01,  1.38s/it, loss=2.58, v_num=641]
Validating: 100%|██████████| 626/626 [01:59<00:00,  8.40it/s][Avalidation_epoch_end
graph acc: 0.30670926517571884
valid accuracy: 0.9687559604644775
Epoch 10: 100%|██████████| 4381/4381 [1:40:59<00:00,  1.38s/it, loss=2.59, v_num=641]
                                                             [AEpoch 10:   0%|          | 0/4381 [00:00<00:00, 8355.19it/s, loss=2.59, v_num=641]   Epoch 11:   0%|          | 0/4381 [00:00<00:02, 2149.82it/s, loss=2.59, v_num=641]Epoch 11:   0%|          | 0/4381 [00:13<16:37:01, 13.65s/it, loss=2.59, v_num=641]Epoch 11:   0%|          | 10/4381 [00:21<2:25:37,  2.00s/it, loss=2.59, v_num=641]Epoch 11:   0%|          | 10/4381 [00:21<2:25:37,  2.00s/it, loss=2.53, v_num=641]Epoch 11:   0%|          | 20/4381 [00:36<2:05:53,  1.73s/it, loss=2.53, v_num=641]Epoch 11:   0%|          | 20/4381 [00:36<2:05:53,  1.73s/it, loss=2.52, v_num=641]Epoch 11:   1%|          | 30/4381 [00:53<2:05:18,  1.73s/it, loss=2.52, v_num=641]Epoch 11:   1%|          | 30/4381 [00:53<2:05:18,  1.73s/it, loss=2.51, v_num=641]Epoch 11:   1%|          | 40/4381 [01:08<2:01:14,  1.68s/it, loss=2.51, v_num=641]Epoch 11:   1%|          | 40/4381 [01:08<2:01:15,  1.68s/it, loss=2.5, v_num=641] Epoch 11:   1%|          | 50/4381 [01:26<2:02:51,  1.70s/it, loss=2.5, v_num=641]Epoch 11:   1%|          | 50/4381 [01:26<2:02:51,  1.70s/it, loss=2.55, v_num=641]Epoch 11:   1%|▏         | 60/4381 [01:47<2:06:26,  1.76s/it, loss=2.55, v_num=641]Epoch 11:   1%|▏         | 60/4381 [01:47<2:06:26,  1.76s/it, loss=2.58, v_num=641]Epoch 11:   2%|▏         | 70/4381 [02:01<2:03:14,  1.72s/it, loss=2.58, v_num=641]Epoch 11:   2%|▏         | 70/4381 [02:01<2:03:14,  1.72s/it, loss=2.58, v_num=641]Epoch 11:   2%|▏         | 80/4381 [02:17<2:01:50,  1.70s/it, loss=2.58, v_num=641]Epoch 11:   2%|▏         | 80/4381 [02:17<2:01:50,  1.70s/it, loss=2.55, v_num=641]Epoch 11:   2%|▏         | 90/4381 [02:38<2:04:40,  1.74s/it, loss=2.55, v_num=641]Epoch 11:   2%|▏         | 90/4381 [02:38<2:04:40,  1.74s/it, loss=2.55, v_num=641]Epoch 11:   2%|▏         | 100/4381 [02:53<2:02:42,  1.72s/it, loss=2.55, v_num=641]Epoch 11:   2%|▏         | 100/4381 [02:53<2:02:42,  1.72s/it, loss=2.57, v_num=641]Epoch 11:   3%|▎         | 110/4381 [03:08<2:00:57,  1.70s/it, loss=2.57, v_num=641]Epoch 11:   3%|▎         | 110/4381 [03:08<2:00:57,  1.70s/it, loss=2.57, v_num=641]Epoch 11:   3%|▎         | 120/4381 [03:26<2:01:11,  1.71s/it, loss=2.57, v_num=641]Epoch 11:   3%|▎         | 120/4381 [03:26<2:01:11,  1.71s/it, loss=2.56, v_num=641]Epoch 11:   3%|▎         | 130/4381 [03:42<2:00:16,  1.70s/it, loss=2.56, v_num=641]Epoch 11:   3%|▎         | 130/4381 [03:42<2:00:16,  1.70s/it, loss=2.54, v_num=641]Epoch 11:   3%|▎         | 140/4381 [03:57<1:59:18,  1.69s/it, loss=2.54, v_num=641]Epoch 11:   3%|▎         | 140/4381 [03:57<1:59:18,  1.69s/it, loss=2.55, v_num=641]Epoch 11:   3%|▎         | 150/4381 [04:12<1:57:42,  1.67s/it, loss=2.55, v_num=641]Epoch 11:   3%|▎         | 150/4381 [04:12<1:57:42,  1.67s/it, loss=2.57, v_num=641]Epoch 11:   4%|▎         | 160/4381 [04:26<1:56:32,  1.66s/it, loss=2.57, v_num=641]Epoch 11:   4%|▎         | 160/4381 [04:26<1:56:32,  1.66s/it, loss=2.59, v_num=641]Epoch 11:   4%|▍         | 170/4381 [04:46<1:57:25,  1.67s/it, loss=2.59, v_num=641]Epoch 11:   4%|▍         | 170/4381 [04:46<1:57:25,  1.67s/it, loss=2.59, v_num=641]Epoch 11:   4%|▍         | 180/4381 [05:02<1:56:51,  1.67s/it, loss=2.59, v_num=641]Epoch 11:   4%|▍         | 180/4381 [05:02<1:56:51,  1.67s/it, loss=2.58, v_num=641]Epoch 11:   4%|▍         | 190/4381 [05:14<1:54:58,  1.65s/it, loss=2.58, v_num=641]Epoch 11:   4%|▍         | 190/4381 [05:14<1:54:58,  1.65s/it, loss=2.56, v_num=641]Epoch 11:   5%|▍         | 200/4381 [05:31<1:55:03,  1.65s/it, loss=2.56, v_num=641]Epoch 11:   5%|▍         | 200/4381 [05:31<1:55:03,  1.65s/it, loss=2.51, v_num=641]Epoch 11:   5%|▍         | 210/4381 [05:47<1:54:31,  1.65s/it, loss=2.51, v_num=641]Epoch 11:   5%|▍         | 210/4381 [05:47<1:54:31,  1.65s/it, loss=2.51, v_num=641]Epoch 11:   5%|▌         | 220/4381 [06:02<1:53:39,  1.64s/it, loss=2.51, v_num=641]Epoch 11:   5%|▌         | 220/4381 [06:02<1:53:39,  1.64s/it, loss=2.54, v_num=641]Epoch 11:   5%|▌         | 230/4381 [06:22<1:54:27,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   5%|▌         | 230/4381 [06:22<1:54:27,  1.65s/it, loss=2.56, v_num=641]Epoch 11:   5%|▌         | 240/4381 [06:38<1:54:00,  1.65s/it, loss=2.56, v_num=641]Epoch 11:   5%|▌         | 240/4381 [06:38<1:54:00,  1.65s/it, loss=2.52, v_num=641]Epoch 11:   6%|▌         | 250/4381 [06:50<1:52:43,  1.64s/it, loss=2.52, v_num=641]Epoch 11:   6%|▌         | 250/4381 [06:50<1:52:43,  1.64s/it, loss=2.52, v_num=641]Epoch 11:   6%|▌         | 260/4381 [07:11<1:53:28,  1.65s/it, loss=2.52, v_num=641]Epoch 11:   6%|▌         | 260/4381 [07:11<1:53:28,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   6%|▌         | 270/4381 [07:28<1:53:19,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   6%|▌         | 270/4381 [07:28<1:53:19,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   6%|▋         | 280/4381 [07:43<1:52:50,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   6%|▋         | 280/4381 [07:43<1:52:50,  1.65s/it, loss=2.55, v_num=641]Epoch 11:   7%|▋         | 290/4381 [08:03<1:53:16,  1.66s/it, loss=2.55, v_num=641]Epoch 11:   7%|▋         | 290/4381 [08:03<1:53:16,  1.66s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 300/4381 [08:18<1:52:35,  1.66s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 300/4381 [08:18<1:52:35,  1.66s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 310/4381 [08:32<1:51:49,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 310/4381 [08:32<1:51:49,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 320/4381 [08:49<1:51:32,  1.65s/it, loss=2.54, v_num=641]Epoch 11:   7%|▋         | 320/4381 [08:49<1:51:32,  1.65s/it, loss=2.51, v_num=641]Epoch 11:   8%|▊         | 330/4381 [09:04<1:51:03,  1.65s/it, loss=2.51, v_num=641]Epoch 11:   8%|▊         | 330/4381 [09:04<1:51:03,  1.65s/it, loss=2.52, v_num=641]Epoch 11:   8%|▊         | 340/4381 [09:18<1:50:14,  1.64s/it, loss=2.52, v_num=641]Epoch 11:   8%|▊         | 340/4381 [09:18<1:50:14,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   8%|▊         | 350/4381 [09:34<1:49:57,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   8%|▊         | 350/4381 [09:34<1:49:57,  1.64s/it, loss=2.53, v_num=641]Epoch 11:   8%|▊         | 360/4381 [09:53<1:50:09,  1.64s/it, loss=2.53, v_num=641]Epoch 11:   8%|▊         | 360/4381 [09:53<1:50:09,  1.64s/it, loss=2.54, v_num=641]Epoch 11:   8%|▊         | 370/4381 [10:08<1:49:38,  1.64s/it, loss=2.54, v_num=641]Epoch 11:   8%|▊         | 370/4381 [10:08<1:49:38,  1.64s/it, loss=2.57, v_num=641]Epoch 11:   9%|▊         | 380/4381 [10:25<1:49:33,  1.64s/it, loss=2.57, v_num=641]Epoch 11:   9%|▊         | 380/4381 [10:25<1:49:33,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   9%|▉         | 390/4381 [10:40<1:48:55,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   9%|▉         | 390/4381 [10:40<1:48:55,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   9%|▉         | 400/4381 [10:56<1:48:39,  1.64s/it, loss=2.55, v_num=641]Epoch 11:   9%|▉         | 400/4381 [10:56<1:48:39,  1.64s/it, loss=2.57, v_num=641]Epoch 11:   9%|▉         | 410/4381 [11:15<1:48:44,  1.64s/it, loss=2.57, v_num=641]Epoch 11:   9%|▉         | 410/4381 [11:15<1:48:44,  1.64s/it, loss=2.54, v_num=641]Epoch 11:  10%|▉         | 420/4381 [11:30<1:48:16,  1.64s/it, loss=2.54, v_num=641]Epoch 11:  10%|▉         | 420/4381 [11:30<1:48:16,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|▉         | 430/4381 [11:45<1:47:48,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|▉         | 430/4381 [11:45<1:47:48,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|█         | 440/4381 [12:02<1:47:40,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|█         | 440/4381 [12:02<1:47:40,  1.64s/it, loss=2.54, v_num=641]Epoch 11:  10%|█         | 450/4381 [12:17<1:47:09,  1.64s/it, loss=2.54, v_num=641]Epoch 11:  10%|█         | 450/4381 [12:17<1:47:09,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|█         | 460/4381 [12:34<1:46:55,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  10%|█         | 460/4381 [12:34<1:46:55,  1.64s/it, loss=2.57, v_num=641]Epoch 11:  11%|█         | 470/4381 [12:52<1:46:57,  1.64s/it, loss=2.57, v_num=641]Epoch 11:  11%|█         | 470/4381 [12:52<1:46:57,  1.64s/it, loss=2.59, v_num=641]Epoch 11:  11%|█         | 480/4381 [13:08<1:46:31,  1.64s/it, loss=2.59, v_num=641]Epoch 11:  11%|█         | 480/4381 [13:08<1:46:31,  1.64s/it, loss=2.58, v_num=641]Epoch 11:  11%|█         | 490/4381 [13:23<1:46:08,  1.64s/it, loss=2.58, v_num=641]Epoch 11:  11%|█         | 490/4381 [13:23<1:46:08,  1.64s/it, loss=2.57, v_num=641]Epoch 11:  11%|█▏        | 500/4381 [13:42<1:46:13,  1.64s/it, loss=2.57, v_num=641]Epoch 11:  11%|█▏        | 500/4381 [13:42<1:46:13,  1.64s/it, loss=2.55, v_num=641]Epoch 11:  12%|█▏        | 510/4381 [13:55<1:45:26,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  12%|█▏        | 510/4381 [13:55<1:45:26,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  12%|█▏        | 520/4381 [14:09<1:44:53,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  12%|█▏        | 520/4381 [14:09<1:44:53,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  12%|█▏        | 530/4381 [14:27<1:44:51,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  12%|█▏        | 530/4381 [14:27<1:44:51,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  12%|█▏        | 540/4381 [14:42<1:44:22,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  12%|█▏        | 540/4381 [14:42<1:44:22,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  13%|█▎        | 550/4381 [14:57<1:43:56,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  13%|█▎        | 550/4381 [14:57<1:43:56,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  13%|█▎        | 560/4381 [15:15<1:43:58,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  13%|█▎        | 560/4381 [15:15<1:43:58,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  13%|█▎        | 570/4381 [15:30<1:43:30,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  13%|█▎        | 570/4381 [15:30<1:43:30,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  13%|█▎        | 580/4381 [15:46<1:43:09,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  13%|█▎        | 580/4381 [15:46<1:43:09,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  13%|█▎        | 590/4381 [16:05<1:43:12,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  13%|█▎        | 590/4381 [16:05<1:43:12,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  14%|█▎        | 600/4381 [16:19<1:42:44,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  14%|█▎        | 600/4381 [16:19<1:42:44,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  14%|█▍        | 610/4381 [16:34<1:42:19,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  14%|█▍        | 610/4381 [16:34<1:42:19,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  14%|█▍        | 620/4381 [16:52<1:42:13,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  14%|█▍        | 620/4381 [16:52<1:42:13,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  14%|█▍        | 630/4381 [17:08<1:41:51,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  14%|█▍        | 630/4381 [17:08<1:41:51,  1.63s/it, loss=2.59, v_num=641]Epoch 11:  15%|█▍        | 640/4381 [17:21<1:41:21,  1.63s/it, loss=2.59, v_num=641]Epoch 11:  15%|█▍        | 640/4381 [17:21<1:41:21,  1.63s/it, loss=2.6, v_num=641] Epoch 11:  15%|█▍        | 650/4381 [17:40<1:41:20,  1.63s/it, loss=2.6, v_num=641]Epoch 11:  15%|█▍        | 650/4381 [17:40<1:41:20,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  15%|█▌        | 660/4381 [17:55<1:40:53,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  15%|█▌        | 660/4381 [17:55<1:40:54,  1.63s/it, loss=2.58, v_num=641]Epoch 11:  15%|█▌        | 670/4381 [18:10<1:40:29,  1.62s/it, loss=2.58, v_num=641]Epoch 11:  15%|█▌        | 670/4381 [18:10<1:40:29,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  16%|█▌        | 680/4381 [18:28<1:40:24,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  16%|█▌        | 680/4381 [18:28<1:40:24,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  16%|█▌        | 690/4381 [18:43<1:40:02,  1.63s/it, loss=2.51, v_num=641]Epoch 11:  16%|█▌        | 690/4381 [18:43<1:40:02,  1.63s/it, loss=2.52, v_num=641]Epoch 11:  16%|█▌        | 700/4381 [19:01<1:39:51,  1.63s/it, loss=2.52, v_num=641]Epoch 11:  16%|█▌        | 700/4381 [19:01<1:39:51,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  16%|█▌        | 710/4381 [19:19<1:39:47,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  16%|█▌        | 710/4381 [19:19<1:39:47,  1.63s/it, loss=2.52, v_num=641]Epoch 11:  16%|█▋        | 720/4381 [19:36<1:39:31,  1.63s/it, loss=2.52, v_num=641]Epoch 11:  16%|█▋        | 720/4381 [19:36<1:39:31,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  17%|█▋        | 730/4381 [19:51<1:39:10,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  17%|█▋        | 730/4381 [19:51<1:39:10,  1.63s/it, loss=2.58, v_num=641]Epoch 11:  17%|█▋        | 740/4381 [20:10<1:39:06,  1.63s/it, loss=2.58, v_num=641]Epoch 11:  17%|█▋        | 740/4381 [20:10<1:39:06,  1.63s/it, loss=2.59, v_num=641]Epoch 11:  17%|█▋        | 750/4381 [20:24<1:38:38,  1.63s/it, loss=2.59, v_num=641]Epoch 11:  17%|█▋        | 750/4381 [20:24<1:38:38,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  17%|█▋        | 760/4381 [20:40<1:38:24,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  17%|█▋        | 760/4381 [20:40<1:38:24,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  18%|█▊        | 770/4381 [21:00<1:38:22,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  18%|█▊        | 770/4381 [21:00<1:38:22,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  18%|█▊        | 780/4381 [21:13<1:37:51,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  18%|█▊        | 780/4381 [21:13<1:37:51,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  18%|█▊        | 790/4381 [21:29<1:37:31,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  18%|█▊        | 790/4381 [21:29<1:37:31,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  18%|█▊        | 800/4381 [21:48<1:37:29,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  18%|█▊        | 800/4381 [21:48<1:37:29,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  18%|█▊        | 810/4381 [22:03<1:37:07,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  18%|█▊        | 810/4381 [22:03<1:37:07,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  19%|█▊        | 820/4381 [22:17<1:36:41,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  19%|█▊        | 820/4381 [22:17<1:36:41,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  19%|█▉        | 830/4381 [22:37<1:36:40,  1.63s/it, loss=2.57, v_num=641]Epoch 11:  19%|█▉        | 830/4381 [22:37<1:36:40,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  19%|█▉        | 840/4381 [22:51<1:36:16,  1.63s/it, loss=2.54, v_num=641]Epoch 11:  19%|█▉        | 840/4381 [22:51<1:36:16,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  19%|█▉        | 850/4381 [23:06<1:35:50,  1.63s/it, loss=2.53, v_num=641]Epoch 11:  19%|█▉        | 850/4381 [23:06<1:35:50,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  20%|█▉        | 860/4381 [23:22<1:35:37,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  20%|█▉        | 860/4381 [23:22<1:35:37,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  20%|█▉        | 870/4381 [23:37<1:35:15,  1.63s/it, loss=2.55, v_num=641]Epoch 11:  20%|█▉        | 870/4381 [23:37<1:35:15,  1.63s/it, loss=2.56, v_num=641]Epoch 11:  20%|██        | 880/4381 [23:50<1:34:46,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  20%|██        | 880/4381 [23:50<1:34:46,  1.62s/it, loss=2.58, v_num=641]Epoch 11:  20%|██        | 890/4381 [24:04<1:34:21,  1.62s/it, loss=2.58, v_num=641]Epoch 11:  20%|██        | 890/4381 [24:04<1:34:21,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  21%|██        | 900/4381 [24:19<1:33:59,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  21%|██        | 900/4381 [24:19<1:33:59,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██        | 910/4381 [24:32<1:33:32,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██        | 910/4381 [24:32<1:33:32,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  21%|██        | 920/4381 [24:54<1:33:35,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  21%|██        | 920/4381 [24:54<1:33:35,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██        | 930/4381 [25:09<1:33:13,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██        | 930/4381 [25:09<1:33:13,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██▏       | 940/4381 [25:22<1:32:46,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  21%|██▏       | 940/4381 [25:22<1:32:46,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  22%|██▏       | 950/4381 [25:41<1:32:43,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  22%|██▏       | 950/4381 [25:41<1:32:43,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  22%|██▏       | 960/4381 [25:56<1:32:19,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  22%|██▏       | 960/4381 [25:56<1:32:19,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  22%|██▏       | 970/4381 [26:11<1:32:02,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  22%|██▏       | 970/4381 [26:11<1:32:02,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  22%|██▏       | 980/4381 [26:29<1:31:51,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  22%|██▏       | 980/4381 [26:29<1:31:51,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  23%|██▎       | 990/4381 [26:43<1:31:27,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  23%|██▎       | 990/4381 [26:43<1:31:27,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  23%|██▎       | 1000/4381 [26:57<1:31:04,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  23%|██▎       | 1000/4381 [26:57<1:31:04,  1.62s/it, loss=2.52, v_num=641]Epoch 11:  23%|██▎       | 1010/4381 [27:17<1:30:58,  1.62s/it, loss=2.52, v_num=641]Epoch 11:  23%|██▎       | 1010/4381 [27:17<1:30:58,  1.62s/it, loss=2.52, v_num=641]Epoch 11:  23%|██▎       | 1020/4381 [27:31<1:30:35,  1.62s/it, loss=2.52, v_num=641]Epoch 11:  23%|██▎       | 1020/4381 [27:31<1:30:35,  1.62s/it, loss=2.53, v_num=641]Epoch 11:  24%|██▎       | 1030/4381 [27:46<1:30:16,  1.62s/it, loss=2.53, v_num=641]Epoch 11:  24%|██▎       | 1030/4381 [27:46<1:30:16,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  24%|██▎       | 1040/4381 [28:04<1:30:05,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  24%|██▎       | 1040/4381 [28:04<1:30:05,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  24%|██▍       | 1050/4381 [28:19<1:29:47,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  24%|██▍       | 1050/4381 [28:19<1:29:47,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  24%|██▍       | 1060/4381 [28:34<1:29:27,  1.62s/it, loss=2.55, v_num=641]Epoch 11:  24%|██▍       | 1060/4381 [28:34<1:29:27,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  24%|██▍       | 1070/4381 [28:50<1:29:11,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  24%|██▍       | 1070/4381 [28:50<1:29:11,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  25%|██▍       | 1080/4381 [29:05<1:28:50,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  25%|██▍       | 1080/4381 [29:05<1:28:50,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▍       | 1090/4381 [29:19<1:28:27,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▍       | 1090/4381 [29:19<1:28:27,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▌       | 1100/4381 [29:39<1:28:22,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▌       | 1100/4381 [29:39<1:28:22,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▌       | 1110/4381 [29:55<1:28:05,  1.62s/it, loss=2.56, v_num=641]Epoch 11:  25%|██▌       | 1110/4381 [29:55<1:28:05,  1.62s/it, loss=2.58, v_num=641]Epoch 11:  26%|██▌       | 1120/4381 [30:09<1:27:45,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  26%|██▌       | 1120/4381 [30:09<1:27:45,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  26%|██▌       | 1130/4381 [30:28<1:27:35,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  26%|██▌       | 1130/4381 [30:28<1:27:35,  1.62s/it, loss=2.53, v_num=641]Epoch 11:  26%|██▌       | 1140/4381 [30:44<1:27:19,  1.62s/it, loss=2.53, v_num=641]Epoch 11:  26%|██▌       | 1140/4381 [30:44<1:27:19,  1.62s/it, loss=2.54, v_num=641]Epoch 11:  26%|██▌       | 1150/4381 [30:58<1:26:56,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  26%|██▌       | 1150/4381 [30:58<1:26:56,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  26%|██▋       | 1160/4381 [31:16<1:26:47,  1.62s/it, loss=2.57, v_num=641]Epoch 11:  26%|██▋       | 1160/4381 [31:16<1:26:47,  1.62s/it, loss=2.6, v_num=641] Epoch 11:  27%|██▋       | 1170/4381 [31:29<1:26:22,  1.61s/it, loss=2.6, v_num=641]Epoch 11:  27%|██▋       | 1170/4381 [31:29<1:26:22,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  27%|██▋       | 1180/4381 [31:44<1:26:02,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  27%|██▋       | 1180/4381 [31:44<1:26:02,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  27%|██▋       | 1190/4381 [32:00<1:25:44,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  27%|██▋       | 1190/4381 [32:00<1:25:44,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  27%|██▋       | 1200/4381 [32:12<1:25:19,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  27%|██▋       | 1200/4381 [32:12<1:25:19,  1.61s/it, loss=2.53, v_num=641]Epoch 11:  28%|██▊       | 1210/4381 [32:29<1:25:04,  1.61s/it, loss=2.53, v_num=641]Epoch 11:  28%|██▊       | 1210/4381 [32:29<1:25:04,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  28%|██▊       | 1220/4381 [32:46<1:24:50,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  28%|██▊       | 1220/4381 [32:46<1:24:50,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  28%|██▊       | 1230/4381 [33:01<1:24:30,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  28%|██▊       | 1230/4381 [33:01<1:24:30,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  28%|██▊       | 1240/4381 [33:15<1:24:10,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  28%|██▊       | 1240/4381 [33:15<1:24:10,  1.61s/it, loss=2.59, v_num=641]Epoch 11:  29%|██▊       | 1250/4381 [33:31<1:23:55,  1.61s/it, loss=2.59, v_num=641]Epoch 11:  29%|██▊       | 1250/4381 [33:31<1:23:55,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  29%|██▉       | 1260/4381 [33:46<1:23:35,  1.61s/it, loss=2.55, v_num=641]Epoch 11:  29%|██▉       | 1260/4381 [33:46<1:23:35,  1.61s/it, loss=2.51, v_num=641]Epoch 11:  29%|██▉       | 1270/4381 [34:02<1:23:19,  1.61s/it, loss=2.51, v_num=641]Epoch 11:  29%|██▉       | 1270/4381 [34:02<1:23:19,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  29%|██▉       | 1280/4381 [34:20<1:23:06,  1.61s/it, loss=2.54, v_num=641]Epoch 11:  29%|██▉       | 1280/4381 [34:20<1:23:06,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  29%|██▉       | 1290/4381 [34:34<1:22:47,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  29%|██▉       | 1290/4381 [34:34<1:22:47,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  30%|██▉       | 1300/4381 [34:49<1:22:27,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  30%|██▉       | 1300/4381 [34:49<1:22:27,  1.61s/it, loss=2.62, v_num=641]Epoch 11:  30%|██▉       | 1310/4381 [35:07<1:22:16,  1.61s/it, loss=2.62, v_num=641]Epoch 11:  30%|██▉       | 1310/4381 [35:07<1:22:16,  1.61s/it, loss=2.65, v_num=641]Epoch 11:  30%|███       | 1320/4381 [35:23<1:21:59,  1.61s/it, loss=2.65, v_num=641]Epoch 11:  30%|███       | 1320/4381 [35:23<1:21:59,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  30%|███       | 1330/4381 [35:39<1:21:44,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  30%|███       | 1330/4381 [35:39<1:21:44,  1.61s/it, loss=2.53, v_num=641]Epoch 11:  31%|███       | 1340/4381 [35:57<1:21:32,  1.61s/it, loss=2.53, v_num=641]Epoch 11:  31%|███       | 1340/4381 [35:57<1:21:32,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  31%|███       | 1350/4381 [36:11<1:21:12,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  31%|███       | 1350/4381 [36:11<1:21:12,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  31%|███       | 1360/4381 [36:25<1:20:50,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  31%|███       | 1360/4381 [36:25<1:20:50,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  31%|███▏      | 1370/4381 [36:42<1:20:38,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  31%|███▏      | 1370/4381 [36:42<1:20:38,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  31%|███▏      | 1380/4381 [37:00<1:20:24,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  31%|███▏      | 1380/4381 [37:00<1:20:24,  1.61s/it, loss=2.59, v_num=641]Epoch 11:  32%|███▏      | 1390/4381 [37:16<1:20:09,  1.61s/it, loss=2.59, v_num=641]Epoch 11:  32%|███▏      | 1390/4381 [37:16<1:20:09,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  32%|███▏      | 1400/4381 [37:35<1:19:59,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  32%|███▏      | 1400/4381 [37:35<1:19:59,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  32%|███▏      | 1410/4381 [37:49<1:19:38,  1.61s/it, loss=2.58, v_num=641]Epoch 11:  32%|███▏      | 1410/4381 [37:49<1:19:38,  1.61s/it, loss=2.61, v_num=641]Epoch 11:  32%|███▏      | 1420/4381 [38:04<1:19:19,  1.61s/it, loss=2.61, v_num=641]Epoch 11:  32%|███▏      | 1420/4381 [38:04<1:19:19,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  33%|███▎      | 1430/4381 [38:21<1:19:05,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  33%|███▎      | 1430/4381 [38:21<1:19:05,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  33%|███▎      | 1440/4381 [38:36<1:18:48,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  33%|███▎      | 1440/4381 [38:36<1:18:48,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  33%|███▎      | 1450/4381 [38:50<1:18:27,  1.61s/it, loss=2.57, v_num=641]Epoch 11:  33%|███▎      | 1450/4381 [38:50<1:18:27,  1.61s/it, loss=2.56, v_num=641]Epoch 11:  33%|███▎      | 1460/4381 [39:04<1:18:07,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  33%|███▎      | 1460/4381 [39:04<1:18:07,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  34%|███▎      | 1470/4381 [39:18<1:17:48,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  34%|███▎      | 1470/4381 [39:18<1:17:48,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  34%|███▍      | 1480/4381 [39:34<1:17:30,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  34%|███▍      | 1480/4381 [39:34<1:17:30,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  34%|███▍      | 1490/4381 [39:50<1:17:14,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  34%|███▍      | 1490/4381 [39:50<1:17:14,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  34%|███▍      | 1500/4381 [40:04<1:16:55,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  34%|███▍      | 1500/4381 [40:04<1:16:55,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  34%|███▍      | 1510/4381 [40:19<1:16:38,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  34%|███▍      | 1510/4381 [40:19<1:16:38,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  35%|███▍      | 1520/4381 [40:35<1:16:21,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  35%|███▍      | 1520/4381 [40:35<1:16:21,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  35%|███▍      | 1530/4381 [40:53<1:16:08,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  35%|███▍      | 1530/4381 [40:53<1:16:08,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  35%|███▌      | 1540/4381 [41:09<1:15:53,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  35%|███▌      | 1540/4381 [41:09<1:15:53,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  35%|███▌      | 1550/4381 [41:27<1:15:40,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  36%|███▌      | 1560/4381 [41:42<1:15:21,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  36%|███▌      | 1560/4381 [41:42<1:15:21,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  36%|███▌      | 1570/4381 [41:55<1:15:01,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  36%|███▌      | 1570/4381 [41:55<1:15:01,  1.60s/it, loss=2.62, v_num=641]Epoch 11:  36%|███▌      | 1580/4381 [42:14<1:14:49,  1.60s/it, loss=2.62, v_num=641]Epoch 11:  36%|███▌      | 1580/4381 [42:14<1:14:49,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  36%|███▋      | 1590/4381 [42:27<1:14:29,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  36%|███▋      | 1590/4381 [42:27<1:14:29,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  37%|███▋      | 1600/4381 [42:47<1:14:20,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  37%|███▋      | 1600/4381 [42:47<1:14:20,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  37%|███▋      | 1610/4381 [43:04<1:14:04,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  37%|███▋      | 1610/4381 [43:04<1:14:04,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  37%|███▋      | 1620/4381 [43:17<1:13:43,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  37%|███▋      | 1620/4381 [43:17<1:13:43,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  37%|███▋      | 1630/4381 [43:32<1:13:26,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  37%|███▋      | 1630/4381 [43:32<1:13:26,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  37%|███▋      | 1640/4381 [43:48<1:13:10,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  37%|███▋      | 1640/4381 [43:48<1:13:10,  1.60s/it, loss=2.52, v_num=641]Epoch 11:  38%|███▊      | 1650/4381 [44:01<1:12:50,  1.60s/it, loss=2.52, v_num=641]Epoch 11:  38%|███▊      | 1650/4381 [44:01<1:12:50,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  38%|███▊      | 1660/4381 [44:14<1:12:28,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  38%|███▊      | 1660/4381 [44:14<1:12:28,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  38%|███▊      | 1670/4381 [44:32<1:12:15,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  38%|███▊      | 1670/4381 [44:32<1:12:15,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  38%|███▊      | 1680/4381 [44:46<1:11:57,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  38%|███▊      | 1680/4381 [44:46<1:11:57,  1.60s/it, loss=2.62, v_num=641]Epoch 11:  39%|███▊      | 1690/4381 [45:00<1:11:37,  1.60s/it, loss=2.62, v_num=641]Epoch 11:  39%|███▊      | 1690/4381 [45:00<1:11:37,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  39%|███▉      | 1700/4381 [45:16<1:11:21,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  39%|███▉      | 1700/4381 [45:16<1:11:21,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  39%|███▉      | 1710/4381 [45:32<1:11:05,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  39%|███▉      | 1710/4381 [45:32<1:11:05,  1.60s/it, loss=2.6, v_num=641] Epoch 11:  39%|███▉      | 1720/4381 [45:47<1:10:47,  1.60s/it, loss=2.6, v_num=641]Epoch 11:  39%|███▉      | 1720/4381 [45:47<1:10:47,  1.60s/it, loss=2.6, v_num=641]Epoch 11:  39%|███▉      | 1730/4381 [46:07<1:10:37,  1.60s/it, loss=2.6, v_num=641]Epoch 11:  39%|███▉      | 1730/4381 [46:07<1:10:37,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  40%|███▉      | 1740/4381 [46:21<1:10:18,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  40%|███▉      | 1740/4381 [46:21<1:10:18,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  40%|███▉      | 1750/4381 [46:37<1:10:02,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  40%|███▉      | 1750/4381 [46:37<1:10:02,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  40%|████      | 1760/4381 [46:55<1:09:50,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  40%|████      | 1760/4381 [46:55<1:09:50,  1.60s/it, loss=2.52, v_num=641]Epoch 11:  40%|████      | 1770/4381 [47:09<1:09:31,  1.60s/it, loss=2.52, v_num=641]Epoch 11:  40%|████      | 1770/4381 [47:09<1:09:31,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  41%|████      | 1780/4381 [47:24<1:09:13,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  41%|████      | 1780/4381 [47:24<1:09:13,  1.60s/it, loss=2.6, v_num=641] Epoch 11:  41%|████      | 1790/4381 [47:42<1:09:00,  1.60s/it, loss=2.6, v_num=641]Epoch 11:  41%|████      | 1790/4381 [47:42<1:09:00,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  41%|████      | 1800/4381 [47:56<1:08:41,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  41%|████      | 1800/4381 [47:56<1:08:41,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  41%|████▏     | 1810/4381 [48:11<1:08:25,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  41%|████▏     | 1810/4381 [48:11<1:08:25,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  42%|████▏     | 1820/4381 [48:29<1:08:12,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  42%|████▏     | 1820/4381 [48:29<1:08:12,  1.60s/it, loss=2.53, v_num=641]Epoch 11:  42%|████▏     | 1830/4381 [48:43<1:07:53,  1.60s/it, loss=2.53, v_num=641]Epoch 11:  42%|████▏     | 1830/4381 [48:43<1:07:53,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  42%|████▏     | 1840/4381 [48:58<1:07:35,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  42%|████▏     | 1840/4381 [48:58<1:07:35,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  42%|████▏     | 1850/4381 [49:18<1:07:25,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  42%|████▏     | 1850/4381 [49:18<1:07:25,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  42%|████▏     | 1860/4381 [49:33<1:07:08,  1.60s/it, loss=2.59, v_num=641]Epoch 11:  42%|████▏     | 1860/4381 [49:33<1:07:08,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  43%|████▎     | 1870/4381 [49:50<1:06:53,  1.60s/it, loss=2.61, v_num=641]Epoch 11:  43%|████▎     | 1870/4381 [49:50<1:06:53,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  43%|████▎     | 1880/4381 [50:05<1:06:36,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  43%|████▎     | 1880/4381 [50:05<1:06:36,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  43%|████▎     | 1890/4381 [50:19<1:06:17,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  43%|████▎     | 1890/4381 [50:19<1:06:17,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  43%|████▎     | 1900/4381 [50:36<1:06:02,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  43%|████▎     | 1900/4381 [50:36<1:06:02,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  44%|████▎     | 1910/4381 [50:53<1:05:48,  1.60s/it, loss=2.54, v_num=641]Epoch 11:  44%|████▎     | 1910/4381 [50:53<1:05:48,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  44%|████▍     | 1920/4381 [51:09<1:05:32,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  44%|████▍     | 1920/4381 [51:09<1:05:32,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  44%|████▍     | 1930/4381 [51:21<1:05:11,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  44%|████▍     | 1930/4381 [51:21<1:05:11,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  44%|████▍     | 1940/4381 [51:35<1:04:53,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  44%|████▍     | 1940/4381 [51:35<1:04:53,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  45%|████▍     | 1950/4381 [51:50<1:04:35,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  45%|████▍     | 1950/4381 [51:50<1:04:35,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  45%|████▍     | 1960/4381 [52:06<1:04:19,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  45%|████▍     | 1960/4381 [52:06<1:04:19,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  45%|████▍     | 1970/4381 [52:24<1:04:06,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  45%|████▍     | 1970/4381 [52:24<1:04:06,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  45%|████▌     | 1980/4381 [52:39<1:03:49,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  45%|████▌     | 1980/4381 [52:39<1:03:49,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  45%|████▌     | 1990/4381 [52:55<1:03:33,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  45%|████▌     | 1990/4381 [52:55<1:03:33,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  46%|████▌     | 2000/4381 [53:16<1:03:23,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  46%|████▌     | 2000/4381 [53:16<1:03:23,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  46%|████▌     | 2010/4381 [53:30<1:03:05,  1.60s/it, loss=2.57, v_num=641]Epoch 11:  46%|████▌     | 2010/4381 [53:30<1:03:05,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  46%|████▌     | 2020/4381 [53:46<1:02:48,  1.60s/it, loss=2.55, v_num=641]Epoch 11:  46%|████▌     | 2020/4381 [53:46<1:02:48,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  46%|████▋     | 2030/4381 [54:01<1:02:32,  1.60s/it, loss=2.56, v_num=641]Epoch 11:  46%|████▋     | 2030/4381 [54:01<1:02:32,  1.60s/it, loss=2.58, v_num=641]Epoch 11:  47%|████▋     | 2040/4381 [54:15<1:02:13,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  47%|████▋     | 2040/4381 [54:15<1:02:13,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  47%|████▋     | 2050/4381 [54:29<1:01:55,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  47%|████▋     | 2050/4381 [54:29<1:01:55,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  47%|████▋     | 2060/4381 [54:44<1:01:39,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  47%|████▋     | 2060/4381 [54:44<1:01:39,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  47%|████▋     | 2070/4381 [54:58<1:01:21,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  47%|████▋     | 2070/4381 [54:58<1:01:21,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  47%|████▋     | 2080/4381 [55:13<1:01:03,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  47%|████▋     | 2080/4381 [55:13<1:01:03,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  48%|████▊     | 2090/4381 [55:32<1:00:51,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  48%|████▊     | 2090/4381 [55:32<1:00:51,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  48%|████▊     | 2100/4381 [55:48<1:00:35,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  48%|████▊     | 2100/4381 [55:48<1:00:35,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  48%|████▊     | 2110/4381 [56:02<1:00:17,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  48%|████▊     | 2110/4381 [56:02<1:00:17,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  48%|████▊     | 2120/4381 [56:19<1:00:02,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  48%|████▊     | 2120/4381 [56:19<1:00:02,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  49%|████▊     | 2130/4381 [56:35<59:46,  1.59s/it, loss=2.53, v_num=641]  Epoch 11:  49%|████▊     | 2130/4381 [56:35<59:46,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  49%|████▉     | 2140/4381 [56:52<59:31,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  49%|████▉     | 2140/4381 [56:52<59:31,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  49%|████▉     | 2150/4381 [57:09<59:16,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  49%|████▉     | 2150/4381 [57:09<59:16,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  49%|████▉     | 2160/4381 [57:21<58:57,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  49%|████▉     | 2160/4381 [57:21<58:57,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  50%|████▉     | 2170/4381 [57:38<58:42,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  50%|████▉     | 2170/4381 [57:38<58:42,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  50%|████▉     | 2180/4381 [57:54<58:26,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  50%|████▉     | 2180/4381 [57:54<58:26,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  50%|████▉     | 2190/4381 [58:10<58:10,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  50%|████▉     | 2190/4381 [58:10<58:10,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  50%|█████     | 2200/4381 [58:23<57:51,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  50%|█████     | 2200/4381 [58:23<57:51,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  50%|█████     | 2210/4381 [58:42<57:38,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  50%|█████     | 2210/4381 [58:42<57:38,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  51%|█████     | 2220/4381 [58:57<57:22,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  51%|█████     | 2220/4381 [58:57<57:22,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  51%|█████     | 2230/4381 [59:12<57:04,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  51%|█████     | 2230/4381 [59:12<57:04,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  51%|█████     | 2240/4381 [59:27<56:48,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  51%|█████     | 2240/4381 [59:27<56:48,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  51%|█████▏    | 2250/4381 [59:42<56:31,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  51%|█████▏    | 2250/4381 [59:42<56:31,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  52%|█████▏    | 2260/4381 [59:57<56:15,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  52%|█████▏    | 2260/4381 [59:57<56:15,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  52%|█████▏    | 2270/4381 [1:00:17<56:03,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  52%|█████▏    | 2270/4381 [1:00:17<56:03,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  52%|█████▏    | 2280/4381 [1:00:35<55:48,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  52%|█████▏    | 2280/4381 [1:00:35<55:48,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  52%|█████▏    | 2290/4381 [1:00:49<55:30,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  52%|█████▏    | 2290/4381 [1:00:49<55:30,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  52%|█████▏    | 2300/4381 [1:01:09<55:18,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  52%|█████▏    | 2300/4381 [1:01:09<55:18,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  53%|█████▎    | 2310/4381 [1:01:23<55:01,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  53%|█████▎    | 2310/4381 [1:01:23<55:01,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  53%|█████▎    | 2320/4381 [1:01:37<54:43,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  53%|█████▎    | 2320/4381 [1:01:37<54:43,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  53%|█████▎    | 2330/4381 [1:01:54<54:28,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  53%|█████▎    | 2330/4381 [1:01:54<54:28,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  53%|█████▎    | 2340/4381 [1:02:10<54:12,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  53%|█████▎    | 2340/4381 [1:02:10<54:12,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  54%|█████▎    | 2350/4381 [1:02:23<53:54,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  54%|█████▎    | 2350/4381 [1:02:23<53:54,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  54%|█████▍    | 2360/4381 [1:02:43<53:41,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  54%|█████▍    | 2360/4381 [1:02:43<53:41,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  54%|█████▍    | 2370/4381 [1:02:57<53:24,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  54%|█████▍    | 2370/4381 [1:02:57<53:24,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  54%|█████▍    | 2380/4381 [1:03:11<53:06,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  54%|█████▍    | 2380/4381 [1:03:11<53:06,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  55%|█████▍    | 2390/4381 [1:03:28<52:51,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  55%|█████▍    | 2390/4381 [1:03:28<52:51,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  55%|█████▍    | 2400/4381 [1:03:43<52:34,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  55%|█████▍    | 2400/4381 [1:03:43<52:34,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  55%|█████▌    | 2410/4381 [1:03:57<52:17,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  55%|█████▌    | 2410/4381 [1:03:57<52:17,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  55%|█████▌    | 2420/4381 [1:04:15<52:03,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  55%|█████▌    | 2420/4381 [1:04:15<52:03,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  55%|█████▌    | 2430/4381 [1:04:31<51:46,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  55%|█████▌    | 2430/4381 [1:04:31<51:46,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  56%|█████▌    | 2440/4381 [1:04:47<51:31,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  56%|█████▌    | 2440/4381 [1:04:47<51:31,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  56%|█████▌    | 2450/4381 [1:05:05<51:16,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  56%|█████▌    | 2450/4381 [1:05:05<51:16,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  56%|█████▌    | 2460/4381 [1:05:20<51:00,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  56%|█████▌    | 2460/4381 [1:05:20<51:00,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  56%|█████▋    | 2470/4381 [1:05:37<50:44,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  56%|█████▋    | 2470/4381 [1:05:37<50:44,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  57%|█████▋    | 2480/4381 [1:05:53<50:29,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  57%|█████▋    | 2480/4381 [1:05:53<50:29,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  57%|█████▋    | 2490/4381 [1:06:07<50:12,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  57%|█████▋    | 2490/4381 [1:06:07<50:12,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  57%|█████▋    | 2500/4381 [1:06:24<49:56,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  57%|█████▋    | 2500/4381 [1:06:24<49:56,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  57%|█████▋    | 2510/4381 [1:06:37<49:38,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  57%|█████▋    | 2510/4381 [1:06:37<49:38,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2520/4381 [1:06:53<49:22,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2520/4381 [1:06:53<49:22,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2530/4381 [1:07:09<49:07,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2530/4381 [1:07:09<49:07,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2540/4381 [1:07:27<48:52,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  58%|█████▊    | 2540/4381 [1:07:27<48:52,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  58%|█████▊    | 2550/4381 [1:07:39<48:33,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  58%|█████▊    | 2550/4381 [1:07:39<48:33,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  58%|█████▊    | 2560/4381 [1:07:57<48:19,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  58%|█████▊    | 2560/4381 [1:07:57<48:19,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  59%|█████▊    | 2570/4381 [1:08:11<48:02,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  59%|█████▊    | 2570/4381 [1:08:11<48:02,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  59%|█████▉    | 2580/4381 [1:08:28<47:47,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  59%|█████▉    | 2580/4381 [1:08:28<47:47,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  59%|█████▉    | 2590/4381 [1:08:46<47:32,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  59%|█████▉    | 2590/4381 [1:08:46<47:32,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  59%|█████▉    | 2600/4381 [1:09:01<47:15,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  59%|█████▉    | 2600/4381 [1:09:01<47:15,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  60%|█████▉    | 2610/4381 [1:09:15<46:58,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  60%|█████▉    | 2610/4381 [1:09:15<46:58,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  60%|█████▉    | 2620/4381 [1:09:34<46:44,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  60%|█████▉    | 2620/4381 [1:09:34<46:44,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  60%|██████    | 2630/4381 [1:09:47<46:26,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  60%|██████    | 2630/4381 [1:09:47<46:26,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  60%|██████    | 2640/4381 [1:10:03<46:11,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  60%|██████    | 2640/4381 [1:10:03<46:11,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  60%|██████    | 2650/4381 [1:10:19<45:54,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  60%|██████    | 2650/4381 [1:10:19<45:54,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  61%|██████    | 2660/4381 [1:10:33<45:38,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  61%|██████    | 2660/4381 [1:10:33<45:38,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  61%|██████    | 2670/4381 [1:10:48<45:21,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  61%|██████    | 2670/4381 [1:10:48<45:21,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  61%|██████    | 2680/4381 [1:11:07<45:07,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  61%|██████    | 2680/4381 [1:11:07<45:07,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  61%|██████▏   | 2690/4381 [1:11:23<44:51,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  61%|██████▏   | 2690/4381 [1:11:23<44:51,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  62%|██████▏   | 2700/4381 [1:11:38<44:35,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  62%|██████▏   | 2700/4381 [1:11:38<44:35,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  62%|██████▏   | 2710/4381 [1:11:55<44:20,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  62%|██████▏   | 2710/4381 [1:11:55<44:20,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  62%|██████▏   | 2720/4381 [1:12:09<44:03,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  62%|██████▏   | 2720/4381 [1:12:09<44:03,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  62%|██████▏   | 2730/4381 [1:12:23<43:45,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  62%|██████▏   | 2730/4381 [1:12:23<43:45,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2740/4381 [1:12:40<43:30,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2740/4381 [1:12:40<43:30,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  63%|██████▎   | 2750/4381 [1:12:55<43:14,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  63%|██████▎   | 2750/4381 [1:12:55<43:14,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  63%|██████▎   | 2760/4381 [1:13:08<42:56,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  63%|██████▎   | 2760/4381 [1:13:08<42:56,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2770/4381 [1:13:28<42:42,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2770/4381 [1:13:28<42:42,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2780/4381 [1:13:43<42:26,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  63%|██████▎   | 2780/4381 [1:13:43<42:26,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  64%|██████▎   | 2790/4381 [1:13:57<42:09,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  64%|██████▎   | 2790/4381 [1:13:57<42:09,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  64%|██████▍   | 2800/4381 [1:14:14<41:54,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  64%|██████▍   | 2800/4381 [1:14:14<41:54,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  64%|██████▍   | 2810/4381 [1:14:29<41:37,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  64%|██████▍   | 2810/4381 [1:14:29<41:37,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  64%|██████▍   | 2820/4381 [1:14:46<41:22,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  64%|██████▍   | 2820/4381 [1:14:46<41:22,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  65%|██████▍   | 2830/4381 [1:15:05<41:08,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  65%|██████▍   | 2830/4381 [1:15:05<41:08,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  65%|██████▍   | 2840/4381 [1:15:18<40:50,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  65%|██████▍   | 2840/4381 [1:15:18<40:50,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  65%|██████▌   | 2850/4381 [1:15:31<40:33,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  65%|██████▌   | 2850/4381 [1:15:31<40:33,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  65%|██████▌   | 2860/4381 [1:15:49<40:18,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  65%|██████▌   | 2860/4381 [1:15:49<40:18,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  66%|██████▌   | 2870/4381 [1:16:05<40:02,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  66%|██████▌   | 2870/4381 [1:16:05<40:02,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  66%|██████▌   | 2880/4381 [1:16:20<39:46,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  66%|██████▌   | 2880/4381 [1:16:20<39:46,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  66%|██████▌   | 2890/4381 [1:16:39<39:32,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  66%|██████▌   | 2890/4381 [1:16:39<39:32,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  66%|██████▌   | 2900/4381 [1:16:54<39:15,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  66%|██████▌   | 2900/4381 [1:16:54<39:15,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  66%|██████▋   | 2910/4381 [1:17:10<38:59,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  66%|██████▋   | 2910/4381 [1:17:10<38:59,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  67%|██████▋   | 2920/4381 [1:17:27<38:44,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  67%|██████▋   | 2920/4381 [1:17:27<38:44,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  67%|██████▋   | 2930/4381 [1:17:40<38:27,  1.59s/it, loss=2.62, v_num=641]Epoch 11:  67%|██████▋   | 2930/4381 [1:17:40<38:27,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  67%|██████▋   | 2940/4381 [1:17:55<38:10,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  67%|██████▋   | 2940/4381 [1:17:55<38:10,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  67%|██████▋   | 2950/4381 [1:18:11<37:55,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  67%|██████▋   | 2950/4381 [1:18:11<37:55,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  68%|██████▊   | 2960/4381 [1:18:24<37:37,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  68%|██████▊   | 2960/4381 [1:18:24<37:37,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  68%|██████▊   | 2970/4381 [1:18:43<37:23,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  68%|██████▊   | 2970/4381 [1:18:43<37:23,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  68%|██████▊   | 2980/4381 [1:19:02<37:08,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  68%|██████▊   | 2980/4381 [1:19:02<37:08,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  68%|██████▊   | 2990/4381 [1:19:16<36:52,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  68%|██████▊   | 2990/4381 [1:19:16<36:52,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  68%|██████▊   | 3000/4381 [1:19:32<36:36,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  68%|██████▊   | 3000/4381 [1:19:32<36:36,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  69%|██████▊   | 3010/4381 [1:19:47<36:20,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  69%|██████▊   | 3010/4381 [1:19:47<36:20,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  69%|██████▉   | 3020/4381 [1:20:01<36:03,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  69%|██████▉   | 3020/4381 [1:20:01<36:03,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  69%|██████▉   | 3030/4381 [1:20:13<35:45,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  69%|██████▉   | 3030/4381 [1:20:13<35:45,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  69%|██████▉   | 3040/4381 [1:20:32<35:31,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  69%|██████▉   | 3040/4381 [1:20:32<35:31,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  70%|██████▉   | 3050/4381 [1:20:48<35:15,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  70%|██████▉   | 3050/4381 [1:20:48<35:15,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  70%|██████▉   | 3060/4381 [1:21:03<34:58,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  70%|██████▉   | 3060/4381 [1:21:03<34:58,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  70%|███████   | 3070/4381 [1:21:21<34:43,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  70%|███████   | 3070/4381 [1:21:21<34:43,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  70%|███████   | 3080/4381 [1:21:37<34:27,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  70%|███████   | 3080/4381 [1:21:37<34:27,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  71%|███████   | 3090/4381 [1:21:50<34:10,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  71%|███████   | 3090/4381 [1:21:50<34:10,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  71%|███████   | 3100/4381 [1:22:10<33:56,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  71%|███████   | 3100/4381 [1:22:10<33:56,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  71%|███████   | 3110/4381 [1:22:25<33:40,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  71%|███████   | 3110/4381 [1:22:25<33:40,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  71%|███████   | 3120/4381 [1:22:39<33:24,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  71%|███████   | 3120/4381 [1:22:39<33:24,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  71%|███████▏  | 3130/4381 [1:22:55<33:08,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  71%|███████▏  | 3130/4381 [1:22:55<33:08,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  72%|███████▏  | 3140/4381 [1:23:13<32:52,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  72%|███████▏  | 3140/4381 [1:23:13<32:52,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  72%|███████▏  | 3150/4381 [1:23:25<32:35,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  72%|███████▏  | 3150/4381 [1:23:25<32:35,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  72%|███████▏  | 3160/4381 [1:23:42<32:20,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  72%|███████▏  | 3160/4381 [1:23:42<32:20,  1.59s/it, loss=2.6, v_num=641] Epoch 11:  72%|███████▏  | 3170/4381 [1:23:56<32:03,  1.59s/it, loss=2.6, v_num=641]Epoch 11:  72%|███████▏  | 3170/4381 [1:23:56<32:03,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  73%|███████▎  | 3180/4381 [1:24:11<31:47,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  73%|███████▎  | 3180/4381 [1:24:11<31:47,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  73%|███████▎  | 3190/4381 [1:24:31<31:32,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  73%|███████▎  | 3190/4381 [1:24:31<31:32,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  73%|███████▎  | 3200/4381 [1:24:47<31:16,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  73%|███████▎  | 3200/4381 [1:24:47<31:16,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  73%|███████▎  | 3210/4381 [1:25:02<31:00,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  73%|███████▎  | 3210/4381 [1:25:02<31:00,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  73%|███████▎  | 3220/4381 [1:25:19<30:45,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  73%|███████▎  | 3220/4381 [1:25:19<30:45,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  74%|███████▎  | 3230/4381 [1:25:35<30:29,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  74%|███████▎  | 3230/4381 [1:25:35<30:29,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  74%|███████▍  | 3240/4381 [1:25:51<30:13,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  74%|███████▍  | 3240/4381 [1:25:51<30:13,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  74%|███████▍  | 3250/4381 [1:26:08<29:57,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  74%|███████▍  | 3250/4381 [1:26:08<29:57,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  74%|███████▍  | 3260/4381 [1:26:22<29:41,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  74%|███████▍  | 3260/4381 [1:26:22<29:41,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  75%|███████▍  | 3270/4381 [1:26:35<29:24,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  75%|███████▍  | 3270/4381 [1:26:35<29:24,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  75%|███████▍  | 3280/4381 [1:26:54<29:09,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  75%|███████▍  | 3280/4381 [1:26:54<29:09,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  75%|███████▌  | 3290/4381 [1:27:11<28:54,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  75%|███████▌  | 3290/4381 [1:27:11<28:54,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  75%|███████▌  | 3300/4381 [1:27:26<28:38,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  75%|███████▌  | 3300/4381 [1:27:26<28:38,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  76%|███████▌  | 3310/4381 [1:27:45<28:23,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  76%|███████▌  | 3310/4381 [1:27:45<28:23,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  76%|███████▌  | 3320/4381 [1:28:00<28:06,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  76%|███████▌  | 3320/4381 [1:28:00<28:06,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  76%|███████▌  | 3330/4381 [1:28:16<27:51,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  76%|███████▌  | 3330/4381 [1:28:16<27:51,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  76%|███████▌  | 3340/4381 [1:28:33<27:35,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  76%|███████▌  | 3340/4381 [1:28:33<27:35,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  76%|███████▋  | 3350/4381 [1:28:50<27:19,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  76%|███████▋  | 3350/4381 [1:28:50<27:19,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  77%|███████▋  | 3360/4381 [1:29:05<27:03,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  77%|███████▋  | 3360/4381 [1:29:05<27:03,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  77%|███████▋  | 3370/4381 [1:29:23<26:48,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  77%|███████▋  | 3370/4381 [1:29:23<26:48,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  77%|███████▋  | 3380/4381 [1:29:38<26:32,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  77%|███████▋  | 3380/4381 [1:29:38<26:32,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  77%|███████▋  | 3390/4381 [1:29:52<26:15,  1.59s/it, loss=2.56, v_num=641]Epoch 11:  77%|███████▋  | 3390/4381 [1:29:52<26:15,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  78%|███████▊  | 3400/4381 [1:30:09<26:00,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  78%|███████▊  | 3400/4381 [1:30:09<26:00,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  78%|███████▊  | 3410/4381 [1:30:22<25:43,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  78%|███████▊  | 3410/4381 [1:30:22<25:43,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  78%|███████▊  | 3420/4381 [1:30:37<25:27,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  78%|███████▊  | 3420/4381 [1:30:37<25:27,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  78%|███████▊  | 3430/4381 [1:30:55<25:12,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  78%|███████▊  | 3430/4381 [1:30:55<25:12,  1.59s/it, loss=2.63, v_num=641]Epoch 11:  79%|███████▊  | 3440/4381 [1:31:10<24:55,  1.59s/it, loss=2.63, v_num=641]Epoch 11:  79%|███████▊  | 3440/4381 [1:31:10<24:55,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  79%|███████▊  | 3450/4381 [1:31:24<24:39,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  79%|███████▊  | 3450/4381 [1:31:25<24:39,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  79%|███████▉  | 3460/4381 [1:31:41<24:23,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  79%|███████▉  | 3460/4381 [1:31:41<24:23,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  79%|███████▉  | 3470/4381 [1:31:57<24:08,  1.59s/it, loss=2.61, v_num=641]Epoch 11:  79%|███████▉  | 3470/4381 [1:31:57<24:08,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  79%|███████▉  | 3480/4381 [1:32:12<23:52,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  79%|███████▉  | 3480/4381 [1:32:12<23:52,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  80%|███████▉  | 3490/4381 [1:32:30<23:36,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  80%|███████▉  | 3490/4381 [1:32:30<23:36,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  80%|███████▉  | 3500/4381 [1:32:43<23:20,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  80%|███████▉  | 3500/4381 [1:32:43<23:20,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  80%|████████  | 3510/4381 [1:32:58<23:03,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  80%|████████  | 3510/4381 [1:32:58<23:03,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  80%|████████  | 3520/4381 [1:33:17<22:48,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  80%|████████  | 3520/4381 [1:33:17<22:48,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  81%|████████  | 3530/4381 [1:33:35<22:33,  1.59s/it, loss=2.59, v_num=641]Epoch 11:  81%|████████  | 3530/4381 [1:33:35<22:33,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  81%|████████  | 3540/4381 [1:33:49<22:17,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  81%|████████  | 3540/4381 [1:33:49<22:17,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████  | 3550/4381 [1:34:09<22:02,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████  | 3550/4381 [1:34:09<22:02,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████▏ | 3560/4381 [1:34:25<21:46,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████▏ | 3560/4381 [1:34:25<21:46,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  81%|████████▏ | 3570/4381 [1:34:38<21:29,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  82%|████████▏ | 3580/4381 [1:34:54<21:13,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  82%|████████▏ | 3580/4381 [1:34:54<21:13,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  82%|████████▏ | 3590/4381 [1:35:11<20:58,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  82%|████████▏ | 3590/4381 [1:35:11<20:58,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  82%|████████▏ | 3600/4381 [1:35:26<20:41,  1.59s/it, loss=2.54, v_num=641]Epoch 11:  82%|████████▏ | 3600/4381 [1:35:26<20:41,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  82%|████████▏ | 3610/4381 [1:35:43<20:26,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  82%|████████▏ | 3610/4381 [1:35:43<20:26,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  83%|████████▎ | 3620/4381 [1:35:58<20:10,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  83%|████████▎ | 3620/4381 [1:35:58<20:10,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  83%|████████▎ | 3630/4381 [1:36:11<19:53,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  83%|████████▎ | 3630/4381 [1:36:11<19:53,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  83%|████████▎ | 3640/4381 [1:36:32<19:38,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  83%|████████▎ | 3650/4381 [1:36:47<19:22,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  83%|████████▎ | 3650/4381 [1:36:47<19:22,  1.59s/it, loss=2.5, v_num=641] Epoch 11:  84%|████████▎ | 3660/4381 [1:37:01<19:06,  1.59s/it, loss=2.5, v_num=641]Epoch 11:  84%|████████▎ | 3660/4381 [1:37:01<19:06,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  84%|████████▍ | 3670/4381 [1:37:18<18:50,  1.59s/it, loss=2.53, v_num=641]Epoch 11:  84%|████████▍ | 3670/4381 [1:37:18<18:50,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  84%|████████▍ | 3680/4381 [1:37:34<18:34,  1.59s/it, loss=2.52, v_num=641]Epoch 11:  84%|████████▍ | 3680/4381 [1:37:34<18:34,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  84%|████████▍ | 3690/4381 [1:37:51<18:19,  1.59s/it, loss=2.51, v_num=641]Epoch 11:  84%|████████▍ | 3690/4381 [1:37:51<18:19,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  84%|████████▍ | 3700/4381 [1:38:07<18:03,  1.59s/it, loss=2.55, v_num=641]Epoch 11:  84%|████████▍ | 3700/4381 [1:38:07<18:03,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▍ | 3710/4381 [1:38:23<17:47,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▍ | 3720/4381 [1:38:39<17:31,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▍ | 3720/4381 [1:38:39<17:31,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▌ | 3730/4381 [1:38:50<17:14,  1.59s/it, loss=2.57, v_num=641]Epoch 11:  85%|████████▌ | 3730/4381 [1:38:50<17:14,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  85%|████████▌ | 3740/4381 [1:38:54<16:56,  1.59s/it, loss=2.58, v_num=641]Epoch 11:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=2.58, v_num=641]Epoch 11:  86%|████████▌ | 3750/4381 [1:38:57<16:38,  1.58s/it, loss=2.55, v_num=641]validation_epoch_end
graph acc: 0.2715654952076677
valid accuracy: 0.9676221013069153
validation_epoch_end
graph acc: 0.2747603833865815
valid accuracy: 0.9658899903297424
Epoch 11:  86%|████████▌ | 3760/4381 [1:38:58<16:20,  1.58s/it, loss=2.55, v_num=641]validation_epoch_end
graph acc: 0.2763578274760383
valid accuracy: 0.9690004587173462

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3035143769968051
valid accuracy: 0.9650935530662537
validation_epoch_end
graph acc: 0.2827476038338658
valid accuracy: 0.9693616628646851
validation_epoch_end
graph acc: 0.28753993610223644
valid accuracy: 0.9690018892288208
validation_epoch_end
graph acc: 0.2763578274760383
valid accuracy: 0.9655047655105591

Validating:   2%|▏         | 10/626 [00:04<04:16,  2.40it/s][AEpoch 11:  86%|████████▌ | 3770/4381 [1:39:03<16:02,  1.58s/it, loss=2.55, v_num=641]
Validating:   3%|▎         | 20/626 [00:05<02:28,  4.08it/s][AEpoch 11:  86%|████████▋ | 3780/4381 [1:39:04<15:44,  1.57s/it, loss=2.55, v_num=641]
Validating:   5%|▍         | 30/626 [00:08<02:48,  3.53it/s][AEpoch 11:  87%|████████▋ | 3790/4381 [1:39:07<15:27,  1.57s/it, loss=2.55, v_num=641]
Validating:   6%|▋         | 40/626 [00:10<02:13,  4.39it/s][AEpoch 11:  87%|████████▋ | 3800/4381 [1:39:09<15:09,  1.57s/it, loss=2.55, v_num=641]
Validating:   8%|▊         | 50/626 [00:11<01:52,  5.11it/s][AEpoch 11:  87%|████████▋ | 3810/4381 [1:39:10<14:51,  1.56s/it, loss=2.55, v_num=641]
Validating:  10%|▉         | 60/626 [00:13<01:48,  5.23it/s][AEpoch 11:  87%|████████▋ | 3820/4381 [1:39:12<14:33,  1.56s/it, loss=2.55, v_num=641]
Validating:  11%|█         | 70/626 [00:15<01:49,  5.08it/s][AEpoch 11:  87%|████████▋ | 3830/4381 [1:39:14<14:16,  1.55s/it, loss=2.55, v_num=641]
Validating:  13%|█▎        | 80/626 [00:16<01:37,  5.61it/s][AEpoch 11:  88%|████████▊ | 3840/4381 [1:39:15<13:58,  1.55s/it, loss=2.55, v_num=641]
Validating:  14%|█▍        | 90/626 [00:18<01:35,  5.60it/s][AEpoch 11:  88%|████████▊ | 3850/4381 [1:39:17<13:41,  1.55s/it, loss=2.55, v_num=641]
Validating:  16%|█▌        | 100/626 [00:20<01:37,  5.38it/s][AEpoch 11:  88%|████████▊ | 3860/4381 [1:39:19<13:24,  1.54s/it, loss=2.55, v_num=641]
Validating:  18%|█▊        | 110/626 [00:22<01:42,  5.02it/s][AEpoch 11:  88%|████████▊ | 3870/4381 [1:39:21<13:07,  1.54s/it, loss=2.55, v_num=641]
Validating:  19%|█▉        | 120/626 [00:24<01:38,  5.13it/s][AEpoch 11:  89%|████████▊ | 3880/4381 [1:39:23<12:49,  1.54s/it, loss=2.55, v_num=641]
Validating:  21%|██        | 130/626 [00:27<01:41,  4.87it/s][AEpoch 11:  89%|████████▉ | 3890/4381 [1:39:26<12:32,  1.53s/it, loss=2.55, v_num=641]
Validating:  22%|██▏       | 140/626 [00:29<01:38,  4.95it/s][AEpoch 11:  89%|████████▉ | 3900/4381 [1:39:28<12:15,  1.53s/it, loss=2.55, v_num=641]
Validating:  24%|██▍       | 150/626 [00:31<01:41,  4.71it/s][AEpoch 11:  89%|████████▉ | 3910/4381 [1:39:30<11:59,  1.53s/it, loss=2.55, v_num=641]
Validating:  26%|██▌       | 160/626 [00:33<01:36,  4.83it/s][AEpoch 11:  89%|████████▉ | 3920/4381 [1:39:32<11:42,  1.52s/it, loss=2.55, v_num=641]
Validating:  27%|██▋       | 170/626 [00:35<01:32,  4.92it/s][AEpoch 11:  90%|████████▉ | 3930/4381 [1:39:34<11:25,  1.52s/it, loss=2.55, v_num=641]
Validating:  29%|██▉       | 180/626 [00:37<01:31,  4.85it/s][AEpoch 11:  90%|████████▉ | 3940/4381 [1:39:36<11:08,  1.52s/it, loss=2.55, v_num=641]
Validating:  30%|███       | 190/626 [00:39<01:24,  5.14it/s][AEpoch 11:  90%|█████████ | 3950/4381 [1:39:38<10:52,  1.51s/it, loss=2.55, v_num=641]
Validating:  32%|███▏      | 200/626 [00:41<01:31,  4.65it/s][AEpoch 11:  90%|█████████ | 3960/4381 [1:39:40<10:35,  1.51s/it, loss=2.55, v_num=641]
Validating:  34%|███▎      | 210/626 [00:43<01:19,  5.22it/s][AEpoch 11:  91%|█████████ | 3970/4381 [1:39:42<10:19,  1.51s/it, loss=2.55, v_num=641]
Validating:  35%|███▌      | 220/626 [00:45<01:22,  4.93it/s][AEpoch 11:  91%|█████████ | 3980/4381 [1:39:44<10:02,  1.50s/it, loss=2.55, v_num=641]
Validating:  37%|███▋      | 230/626 [00:47<01:18,  5.02it/s][AEpoch 11:  91%|█████████ | 3990/4381 [1:39:46<09:46,  1.50s/it, loss=2.55, v_num=641]
Validating:  38%|███▊      | 240/626 [00:49<01:22,  4.66it/s][AEpoch 11:  91%|█████████▏| 4000/4381 [1:39:48<09:30,  1.50s/it, loss=2.55, v_num=641]
Validating:  40%|███▉      | 250/626 [00:51<01:10,  5.32it/s][AEpoch 11:  92%|█████████▏| 4010/4381 [1:39:50<09:14,  1.49s/it, loss=2.55, v_num=641]
Validating:  42%|████▏     | 260/626 [00:53<01:18,  4.65it/s][AEpoch 11:  92%|█████████▏| 4020/4381 [1:39:52<08:58,  1.49s/it, loss=2.55, v_num=641]
Validating:  43%|████▎     | 270/626 [00:55<01:11,  4.97it/s][AEpoch 11:  92%|█████████▏| 4030/4381 [1:39:54<08:41,  1.49s/it, loss=2.55, v_num=641]
Validating:  45%|████▍     | 280/626 [00:57<01:12,  4.75it/s][AEpoch 11:  92%|█████████▏| 4040/4381 [1:39:56<08:26,  1.48s/it, loss=2.55, v_num=641]
Validating:  46%|████▋     | 290/626 [00:59<01:08,  4.92it/s][AEpoch 11:  92%|█████████▏| 4050/4381 [1:39:58<08:10,  1.48s/it, loss=2.55, v_num=641]
Validating:  48%|████▊     | 300/626 [01:01<00:59,  5.47it/s][AEpoch 11:  93%|█████████▎| 4060/4381 [1:40:00<07:54,  1.48s/it, loss=2.55, v_num=641]
Validating:  50%|████▉     | 310/626 [01:02<00:51,  6.11it/s][AEpoch 11:  93%|█████████▎| 4070/4381 [1:40:01<07:38,  1.47s/it, loss=2.55, v_num=641]
Validating:  51%|█████     | 320/626 [01:04<00:53,  5.69it/s][AEpoch 11:  93%|█████████▎| 4080/4381 [1:40:03<07:22,  1.47s/it, loss=2.55, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:05<00:47,  6.21it/s][AEpoch 11:  93%|█████████▎| 4090/4381 [1:40:04<07:07,  1.47s/it, loss=2.55, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:07<00:52,  5.46it/s][AEpoch 11:  94%|█████████▎| 4100/4381 [1:40:06<06:51,  1.46s/it, loss=2.55, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:10<00:58,  4.74it/s][AEpoch 11:  94%|█████████▍| 4110/4381 [1:40:09<06:36,  1.46s/it, loss=2.55, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:12<00:54,  4.92it/s][AEpoch 11:  94%|█████████▍| 4120/4381 [1:40:11<06:20,  1.46s/it, loss=2.55, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:13<00:44,  5.76it/s][AEpoch 11:  94%|█████████▍| 4130/4381 [1:40:12<06:05,  1.46s/it, loss=2.55, v_num=641]
Validating:  61%|██████    | 380/626 [01:15<00:40,  6.00it/s][AEpoch 11:  94%|█████████▍| 4140/4381 [1:40:14<05:50,  1.45s/it, loss=2.55, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:16<00:36,  6.46it/s][AEpoch 11:  95%|█████████▍| 4150/4381 [1:40:15<05:34,  1.45s/it, loss=2.55, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:17<00:35,  6.33it/s][AEpoch 11:  95%|█████████▍| 4160/4381 [1:40:16<05:19,  1.45s/it, loss=2.55, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:19<00:32,  6.64it/s][AEpoch 11:  95%|█████████▌| 4170/4381 [1:40:18<05:04,  1.44s/it, loss=2.55, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:21<00:35,  5.80it/s][AEpoch 11:  95%|█████████▌| 4180/4381 [1:40:20<04:49,  1.44s/it, loss=2.55, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:22<00:31,  6.17it/s][AEpoch 11:  96%|█████████▌| 4190/4381 [1:40:21<04:34,  1.44s/it, loss=2.55, v_num=641]
Validating:  70%|███████   | 440/626 [01:25<00:33,  5.47it/s][AEpoch 11:  96%|█████████▌| 4200/4381 [1:40:24<04:19,  1.43s/it, loss=2.55, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:27<00:33,  5.28it/s][AEpoch 11:  96%|█████████▌| 4210/4381 [1:40:26<04:04,  1.43s/it, loss=2.55, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:28<00:29,  5.61it/s][AEpoch 11:  96%|█████████▋| 4220/4381 [1:40:27<03:49,  1.43s/it, loss=2.55, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:31<00:29,  5.22it/s][AEpoch 11:  97%|█████████▋| 4230/4381 [1:40:30<03:35,  1.43s/it, loss=2.55, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:32<00:28,  5.17it/s][AEpoch 11:  97%|█████████▋| 4240/4381 [1:40:31<03:20,  1.42s/it, loss=2.55, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:35<00:29,  4.55it/s][AEpoch 11:  97%|█████████▋| 4250/4381 [1:40:34<03:05,  1.42s/it, loss=2.55, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:37<00:23,  5.25it/s][AEpoch 11:  97%|█████████▋| 4260/4381 [1:40:36<02:51,  1.42s/it, loss=2.55, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:39<00:24,  4.72it/s][AEpoch 11:  97%|█████████▋| 4270/4381 [1:40:38<02:36,  1.41s/it, loss=2.55, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:41<00:22,  4.69it/s][AEpoch 11:  98%|█████████▊| 4280/4381 [1:40:40<02:22,  1.41s/it, loss=2.55, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:43<00:20,  4.79it/s][AEpoch 11:  98%|█████████▊| 4290/4381 [1:40:42<02:08,  1.41s/it, loss=2.55, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:45<00:18,  4.78it/s][AEpoch 11:  98%|█████████▊| 4300/4381 [1:40:44<01:53,  1.41s/it, loss=2.55, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:47<00:14,  5.37it/s][AEpoch 11:  98%|█████████▊| 4310/4381 [1:40:46<01:39,  1.40s/it, loss=2.55, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:49<00:12,  5.19it/s][AEpoch 11:  99%|█████████▊| 4320/4381 [1:40:48<01:25,  1.40s/it, loss=2.55, v_num=641]
Validating:  91%|█████████ | 570/626 [01:50<00:09,  5.67it/s][AEpoch 11:  99%|█████████▉| 4330/4381 [1:40:49<01:11,  1.40s/it, loss=2.55, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:52<00:08,  5.44it/s][AEpoch 11:  99%|█████████▉| 4340/4381 [1:40:51<00:57,  1.39s/it, loss=2.55, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:54<00:06,  5.72it/s][AEpoch 11:  99%|█████████▉| 4350/4381 [1:40:53<00:43,  1.39s/it, loss=2.55, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:55<00:04,  6.28it/s][AEpoch 11: 100%|█████████▉| 4360/4381 [1:40:54<00:29,  1.39s/it, loss=2.55, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:56<00:02,  6.52it/s][AEpoch 11: 100%|█████████▉| 4370/4381 [1:40:55<00:15,  1.39s/it, loss=2.55, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:58<00:00,  6.15it/s][AEpoch 11: 100%|█████████▉| 4380/4381 [1:40:57<00:01,  1.38s/it, loss=2.55, v_num=641]
Validating: 100%|██████████| 626/626 [01:59<00:00,  6.77it/s][Avalidation_epoch_end
graph acc: 0.3402555910543131
valid accuracy: 0.9712427258491516
Epoch 11: 100%|██████████| 4381/4381 [1:41:00<00:00,  1.38s/it, loss=2.53, v_num=641]
                                                             [AEpoch 11:   0%|          | 0/4381 [00:00<00:00, 10433.59it/s, loss=2.53, v_num=641]  Epoch 12:   0%|          | 0/4381 [00:00<00:01, 2914.74it/s, loss=2.53, v_num=641] Epoch 12:   0%|          | 0/4381 [00:12<15:26:59, 12.70s/it, loss=2.53, v_num=641]Epoch 12:   0%|          | 10/4381 [00:20<2:18:02,  1.89s/it, loss=2.53, v_num=641]Epoch 12:   0%|          | 10/4381 [00:20<2:18:11,  1.90s/it, loss=2.5, v_num=641] Epoch 12:   0%|          | 20/4381 [00:35<2:01:13,  1.67s/it, loss=2.5, v_num=641]Epoch 12:   0%|          | 20/4381 [00:35<2:01:13,  1.67s/it, loss=2.52, v_num=641]Epoch 12:   1%|          | 30/4381 [00:56<2:13:04,  1.84s/it, loss=2.52, v_num=641]Epoch 12:   1%|          | 30/4381 [00:56<2:13:04,  1.84s/it, loss=2.54, v_num=641]Epoch 12:   1%|          | 40/4381 [01:11<2:05:20,  1.73s/it, loss=2.54, v_num=641]Epoch 12:   1%|          | 40/4381 [01:11<2:05:20,  1.73s/it, loss=2.57, v_num=641]Epoch 12:   1%|          | 50/4381 [01:27<2:03:17,  1.71s/it, loss=2.57, v_num=641]Epoch 12:   1%|          | 50/4381 [01:27<2:03:17,  1.71s/it, loss=2.55, v_num=641]Epoch 12:   1%|▏         | 60/4381 [01:42<2:01:14,  1.68s/it, loss=2.55, v_num=641]Epoch 12:   1%|▏         | 60/4381 [01:42<2:01:14,  1.68s/it, loss=2.52, v_num=641]Epoch 12:   2%|▏         | 70/4381 [02:01<2:02:33,  1.71s/it, loss=2.52, v_num=641]Epoch 12:   2%|▏         | 70/4381 [02:01<2:02:33,  1.71s/it, loss=2.5, v_num=641] Epoch 12:   2%|▏         | 80/4381 [02:15<2:00:02,  1.67s/it, loss=2.5, v_num=641]Epoch 12:   2%|▏         | 80/4381 [02:15<2:00:02,  1.67s/it, loss=2.53, v_num=641]Epoch 12:   2%|▏         | 90/4381 [02:31<1:58:42,  1.66s/it, loss=2.53, v_num=641]Epoch 12:   2%|▏         | 90/4381 [02:31<1:58:42,  1.66s/it, loss=2.56, v_num=641]Epoch 12:   2%|▏         | 100/4381 [02:48<1:58:54,  1.67s/it, loss=2.56, v_num=641]Epoch 12:   2%|▏         | 100/4381 [02:48<1:58:54,  1.67s/it, loss=2.52, v_num=641]Epoch 12:   3%|▎         | 110/4381 [03:03<1:57:48,  1.65s/it, loss=2.52, v_num=641]Epoch 12:   3%|▎         | 110/4381 [03:03<1:57:48,  1.65s/it, loss=2.52, v_num=641]Epoch 12:   3%|▎         | 120/4381 [03:18<1:56:19,  1.64s/it, loss=2.52, v_num=641]Epoch 12:   3%|▎         | 120/4381 [03:18<1:56:19,  1.64s/it, loss=2.56, v_num=641]Epoch 12:   3%|▎         | 130/4381 [03:37<1:57:34,  1.66s/it, loss=2.56, v_num=641]Epoch 12:   3%|▎         | 130/4381 [03:37<1:57:34,  1.66s/it, loss=2.54, v_num=641]Epoch 12:   3%|▎         | 140/4381 [03:53<1:57:04,  1.66s/it, loss=2.54, v_num=641]Epoch 12:   3%|▎         | 140/4381 [03:53<1:57:04,  1.66s/it, loss=2.48, v_num=641]Epoch 12:   3%|▎         | 150/4381 [04:09<1:56:29,  1.65s/it, loss=2.48, v_num=641]Epoch 12:   3%|▎         | 150/4381 [04:09<1:56:29,  1.65s/it, loss=2.52, v_num=641]Epoch 12:   4%|▎         | 160/4381 [04:25<1:56:07,  1.65s/it, loss=2.52, v_num=641]Epoch 12:   4%|▎         | 160/4381 [04:25<1:56:07,  1.65s/it, loss=2.53, v_num=641]Epoch 12:   4%|▍         | 170/4381 [04:44<1:56:37,  1.66s/it, loss=2.53, v_num=641]Epoch 12:   4%|▍         | 170/4381 [04:44<1:56:37,  1.66s/it, loss=2.51, v_num=641]Epoch 12:   4%|▍         | 180/4381 [04:59<1:55:51,  1.65s/it, loss=2.51, v_num=641]Epoch 12:   4%|▍         | 180/4381 [04:59<1:55:51,  1.65s/it, loss=2.54, v_num=641]Epoch 12:   4%|▍         | 190/4381 [05:17<1:56:16,  1.66s/it, loss=2.54, v_num=641]Epoch 12:   4%|▍         | 190/4381 [05:17<1:56:16,  1.66s/it, loss=2.53, v_num=641]Epoch 12:   5%|▍         | 200/4381 [05:34<1:55:58,  1.66s/it, loss=2.53, v_num=641]Epoch 12:   5%|▍         | 200/4381 [05:34<1:55:58,  1.66s/it, loss=2.52, v_num=641]Epoch 12:   5%|▍         | 210/4381 [05:50<1:55:23,  1.66s/it, loss=2.52, v_num=641]Epoch 12:   5%|▍         | 210/4381 [05:50<1:55:23,  1.66s/it, loss=2.5, v_num=641] Epoch 12:   5%|▌         | 220/4381 [06:09<1:55:54,  1.67s/it, loss=2.5, v_num=641]Epoch 12:   5%|▌         | 220/4381 [06:09<1:55:54,  1.67s/it, loss=2.47, v_num=641]Epoch 12:   5%|▌         | 230/4381 [06:23<1:55:00,  1.66s/it, loss=2.47, v_num=641]Epoch 12:   5%|▌         | 230/4381 [06:23<1:55:00,  1.66s/it, loss=2.47, v_num=641]Epoch 12:   5%|▌         | 240/4381 [06:39<1:54:29,  1.66s/it, loss=2.47, v_num=641]Epoch 12:   5%|▌         | 240/4381 [06:39<1:54:29,  1.66s/it, loss=2.5, v_num=641] Epoch 12:   6%|▌         | 250/4381 [06:56<1:54:07,  1.66s/it, loss=2.5, v_num=641]Epoch 12:   6%|▌         | 250/4381 [06:56<1:54:07,  1.66s/it, loss=2.5, v_num=641]Epoch 12:   6%|▌         | 260/4381 [07:12<1:53:42,  1.66s/it, loss=2.5, v_num=641]Epoch 12:   6%|▌         | 260/4381 [07:12<1:53:42,  1.66s/it, loss=2.48, v_num=641]Epoch 12:   6%|▌         | 270/4381 [07:26<1:52:56,  1.65s/it, loss=2.48, v_num=641]Epoch 12:   6%|▌         | 270/4381 [07:26<1:52:56,  1.65s/it, loss=2.5, v_num=641] Epoch 12:   6%|▋         | 280/4381 [07:41<1:52:17,  1.64s/it, loss=2.5, v_num=641]Epoch 12:   6%|▋         | 280/4381 [07:41<1:52:17,  1.64s/it, loss=2.52, v_num=641]Epoch 12:   7%|▋         | 290/4381 [07:59<1:52:21,  1.65s/it, loss=2.52, v_num=641]Epoch 12:   7%|▋         | 290/4381 [07:59<1:52:21,  1.65s/it, loss=2.53, v_num=641]Epoch 12:   7%|▋         | 300/4381 [08:13<1:51:24,  1.64s/it, loss=2.53, v_num=641]Epoch 12:   7%|▋         | 300/4381 [08:13<1:51:25,  1.64s/it, loss=2.55, v_num=641]Epoch 12:   7%|▋         | 310/4381 [08:30<1:51:26,  1.64s/it, loss=2.55, v_num=641]Epoch 12:   7%|▋         | 310/4381 [08:30<1:51:26,  1.64s/it, loss=2.51, v_num=641]Epoch 12:   7%|▋         | 320/4381 [08:47<1:51:09,  1.64s/it, loss=2.51, v_num=641]Epoch 12:   7%|▋         | 320/4381 [08:47<1:51:09,  1.64s/it, loss=2.48, v_num=641]Epoch 12:   8%|▊         | 330/4381 [09:02<1:50:36,  1.64s/it, loss=2.48, v_num=641]Epoch 12:   8%|▊         | 330/4381 [09:02<1:50:36,  1.64s/it, loss=2.52, v_num=641]Epoch 12:   8%|▊         | 340/4381 [09:15<1:49:47,  1.63s/it, loss=2.52, v_num=641]Epoch 12:   8%|▊         | 340/4381 [09:15<1:49:47,  1.63s/it, loss=2.52, v_num=641]Epoch 12:   8%|▊         | 350/4381 [09:33<1:49:43,  1.63s/it, loss=2.52, v_num=641]Epoch 12:   8%|▊         | 350/4381 [09:33<1:49:43,  1.63s/it, loss=2.46, v_num=641]Epoch 12:   8%|▊         | 360/4381 [09:48<1:49:15,  1.63s/it, loss=2.46, v_num=641]Epoch 12:   8%|▊         | 360/4381 [09:48<1:49:15,  1.63s/it, loss=2.45, v_num=641]Epoch 12:   8%|▊         | 370/4381 [10:04<1:49:00,  1.63s/it, loss=2.45, v_num=641]Epoch 12:   8%|▊         | 370/4381 [10:04<1:49:00,  1.63s/it, loss=2.48, v_num=641]Epoch 12:   9%|▊         | 380/4381 [10:19<1:48:30,  1.63s/it, loss=2.48, v_num=641]Epoch 12:   9%|▊         | 380/4381 [10:19<1:48:30,  1.63s/it, loss=2.49, v_num=641]Epoch 12:   9%|▉         | 390/4381 [10:36<1:48:14,  1.63s/it, loss=2.49, v_num=641]Epoch 12:   9%|▉         | 390/4381 [10:36<1:48:14,  1.63s/it, loss=2.48, v_num=641]Epoch 12:   9%|▉         | 400/4381 [10:51<1:47:49,  1.63s/it, loss=2.48, v_num=641]Epoch 12:   9%|▉         | 400/4381 [10:51<1:47:49,  1.63s/it, loss=2.5, v_num=641] Epoch 12:   9%|▉         | 410/4381 [11:06<1:47:20,  1.62s/it, loss=2.5, v_num=641]Epoch 12:   9%|▉         | 410/4381 [11:06<1:47:20,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  10%|▉         | 420/4381 [11:20<1:46:46,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  10%|▉         | 420/4381 [11:20<1:46:46,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  10%|▉         | 430/4381 [11:37<1:46:30,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  10%|▉         | 430/4381 [11:37<1:46:30,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 440/4381 [11:56<1:46:39,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 440/4381 [11:56<1:46:39,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 450/4381 [12:12<1:46:27,  1.63s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 450/4381 [12:12<1:46:27,  1.63s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 460/4381 [12:26<1:45:53,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  10%|█         | 460/4381 [12:26<1:45:53,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  11%|█         | 470/4381 [12:46<1:46:06,  1.63s/it, loss=2.51, v_num=641]Epoch 12:  11%|█         | 470/4381 [12:46<1:46:06,  1.63s/it, loss=2.5, v_num=641] Epoch 12:  11%|█         | 480/4381 [13:01<1:45:35,  1.62s/it, loss=2.5, v_num=641]Epoch 12:  11%|█         | 480/4381 [13:01<1:45:35,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  11%|█         | 490/4381 [13:16<1:45:10,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  11%|█         | 490/4381 [13:16<1:45:10,  1.62s/it, loss=2.5, v_num=641] Epoch 12:  11%|█▏        | 500/4381 [13:34<1:45:13,  1.63s/it, loss=2.5, v_num=641]Epoch 12:  11%|█▏        | 500/4381 [13:34<1:45:13,  1.63s/it, loss=2.53, v_num=641]Epoch 12:  12%|█▏        | 510/4381 [13:51<1:44:56,  1.63s/it, loss=2.53, v_num=641]Epoch 12:  12%|█▏        | 510/4381 [13:51<1:44:56,  1.63s/it, loss=2.5, v_num=641] Epoch 12:  12%|█▏        | 520/4381 [14:07<1:44:40,  1.63s/it, loss=2.5, v_num=641]Epoch 12:  12%|█▏        | 520/4381 [14:07<1:44:40,  1.63s/it, loss=2.52, v_num=641]Epoch 12:  12%|█▏        | 530/4381 [14:26<1:44:41,  1.63s/it, loss=2.52, v_num=641]Epoch 12:  12%|█▏        | 530/4381 [14:26<1:44:41,  1.63s/it, loss=2.56, v_num=641]Epoch 12:  12%|█▏        | 540/4381 [14:38<1:43:59,  1.62s/it, loss=2.56, v_num=641]Epoch 12:  12%|█▏        | 540/4381 [14:38<1:43:59,  1.62s/it, loss=2.55, v_num=641]Epoch 12:  13%|█▎        | 550/4381 [14:53<1:43:33,  1.62s/it, loss=2.55, v_num=641]Epoch 12:  13%|█▎        | 550/4381 [14:53<1:43:33,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  13%|█▎        | 560/4381 [15:13<1:43:38,  1.63s/it, loss=2.51, v_num=641]Epoch 12:  13%|█▎        | 560/4381 [15:13<1:43:38,  1.63s/it, loss=2.48, v_num=641]Epoch 12:  13%|█▎        | 570/4381 [15:26<1:43:00,  1.62s/it, loss=2.48, v_num=641]Epoch 12:  13%|█▎        | 570/4381 [15:26<1:43:00,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  13%|█▎        | 580/4381 [15:38<1:42:23,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  13%|█▎        | 580/4381 [15:38<1:42:23,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  13%|█▎        | 590/4381 [15:58<1:42:29,  1.62s/it, loss=2.47, v_num=641]Epoch 12:  13%|█▎        | 590/4381 [15:58<1:42:29,  1.62s/it, loss=2.49, v_num=641]Epoch 12:  14%|█▎        | 600/4381 [16:14<1:42:10,  1.62s/it, loss=2.49, v_num=641]Epoch 12:  14%|█▎        | 600/4381 [16:14<1:42:10,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  14%|█▍        | 610/4381 [16:29<1:41:46,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  14%|█▍        | 610/4381 [16:29<1:41:46,  1.62s/it, loss=2.55, v_num=641]Epoch 12:  14%|█▍        | 620/4381 [16:50<1:41:58,  1.63s/it, loss=2.55, v_num=641]Epoch 12:  14%|█▍        | 620/4381 [16:50<1:41:58,  1.63s/it, loss=2.55, v_num=641]Epoch 12:  14%|█▍        | 630/4381 [17:04<1:41:32,  1.62s/it, loss=2.55, v_num=641]Epoch 12:  14%|█▍        | 630/4381 [17:04<1:41:32,  1.62s/it, loss=2.54, v_num=641]Epoch 12:  15%|█▍        | 640/4381 [17:21<1:41:20,  1.63s/it, loss=2.54, v_num=641]Epoch 12:  15%|█▍        | 640/4381 [17:21<1:41:20,  1.63s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▍        | 650/4381 [17:36<1:40:53,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▌        | 660/4381 [17:50<1:40:23,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▌        | 660/4381 [17:50<1:40:23,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▌        | 670/4381 [18:03<1:39:53,  1.62s/it, loss=2.53, v_num=641]Epoch 12:  15%|█▌        | 670/4381 [18:03<1:39:53,  1.62s/it, loss=2.54, v_num=641]Epoch 12:  16%|█▌        | 680/4381 [18:19<1:39:33,  1.61s/it, loss=2.54, v_num=641]Epoch 12:  16%|█▌        | 680/4381 [18:19<1:39:33,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▌        | 690/4381 [18:35<1:39:17,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▌        | 690/4381 [18:35<1:39:17,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▌        | 700/4381 [18:49<1:38:50,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▌        | 700/4381 [18:49<1:38:50,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  16%|█▌        | 710/4381 [19:08<1:38:51,  1.62s/it, loss=2.55, v_num=641]Epoch 12:  16%|█▌        | 710/4381 [19:08<1:38:51,  1.62s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▋        | 720/4381 [19:23<1:38:28,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  16%|█▋        | 720/4381 [19:23<1:38:28,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  17%|█▋        | 730/4381 [19:36<1:37:58,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  17%|█▋        | 730/4381 [19:36<1:37:58,  1.61s/it, loss=2.52, v_num=641]Epoch 12:  17%|█▋        | 740/4381 [19:53<1:37:46,  1.61s/it, loss=2.52, v_num=641]Epoch 12:  17%|█▋        | 740/4381 [19:53<1:37:46,  1.61s/it, loss=2.52, v_num=641]Epoch 12:  17%|█▋        | 750/4381 [20:08<1:37:23,  1.61s/it, loss=2.52, v_num=641]Epoch 12:  17%|█▋        | 750/4381 [20:08<1:37:23,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  17%|█▋        | 760/4381 [20:24<1:37:07,  1.61s/it, loss=2.51, v_num=641]Epoch 12:  17%|█▋        | 760/4381 [20:24<1:37:07,  1.61s/it, loss=2.53, v_num=641]Epoch 12:  18%|█▊        | 770/4381 [20:42<1:36:58,  1.61s/it, loss=2.53, v_num=641]Epoch 12:  18%|█▊        | 770/4381 [20:42<1:36:58,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 780/4381 [20:57<1:36:36,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 780/4381 [20:57<1:36:36,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 790/4381 [21:14<1:36:27,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 790/4381 [21:14<1:36:27,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 800/4381 [21:29<1:36:05,  1.61s/it, loss=2.55, v_num=641]Epoch 12:  18%|█▊        | 800/4381 [21:29<1:36:05,  1.61s/it, loss=2.53, v_num=641]Epoch 12:  18%|█▊        | 810/4381 [21:44<1:35:42,  1.61s/it, loss=2.53, v_num=641]Epoch 12:  18%|█▊        | 810/4381 [21:44<1:35:42,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  19%|█▊        | 820/4381 [22:00<1:35:28,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  19%|█▊        | 820/4381 [22:00<1:35:28,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  19%|█▉        | 830/4381 [22:14<1:35:01,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  19%|█▉        | 830/4381 [22:14<1:35:01,  1.61s/it, loss=2.54, v_num=641]Epoch 12:  19%|█▉        | 840/4381 [22:27<1:34:31,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  19%|█▉        | 840/4381 [22:27<1:34:31,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  19%|█▉        | 850/4381 [22:45<1:34:24,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  19%|█▉        | 850/4381 [22:45<1:34:24,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  20%|█▉        | 860/4381 [22:58<1:33:59,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  20%|█▉        | 860/4381 [22:58<1:33:59,  1.60s/it, loss=2.55, v_num=641]Epoch 12:  20%|█▉        | 870/4381 [23:14<1:33:39,  1.60s/it, loss=2.55, v_num=641]Epoch 12:  20%|█▉        | 870/4381 [23:14<1:33:39,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  20%|██        | 880/4381 [23:32<1:33:33,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  20%|██        | 880/4381 [23:32<1:33:33,  1.60s/it, loss=2.5, v_num=641] Epoch 12:  20%|██        | 890/4381 [23:47<1:33:14,  1.60s/it, loss=2.5, v_num=641]Epoch 12:  20%|██        | 890/4381 [23:47<1:33:14,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██        | 900/4381 [24:02<1:32:54,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██        | 900/4381 [24:02<1:32:54,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██        | 910/4381 [24:19<1:32:41,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██        | 910/4381 [24:19<1:32:41,  1.60s/it, loss=2.5, v_num=641] Epoch 12:  21%|██        | 920/4381 [24:39<1:32:41,  1.61s/it, loss=2.5, v_num=641]Epoch 12:  21%|██        | 920/4381 [24:39<1:32:41,  1.61s/it, loss=2.49, v_num=641]Epoch 12:  21%|██        | 930/4381 [24:53<1:32:16,  1.60s/it, loss=2.49, v_num=641]Epoch 12:  21%|██        | 930/4381 [24:53<1:32:16,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██▏       | 940/4381 [25:05<1:31:46,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  21%|██▏       | 940/4381 [25:05<1:31:46,  1.60s/it, loss=2.56, v_num=641]Epoch 12:  22%|██▏       | 950/4381 [25:24<1:31:39,  1.60s/it, loss=2.56, v_num=641]Epoch 12:  22%|██▏       | 950/4381 [25:24<1:31:39,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  22%|██▏       | 960/4381 [25:39<1:31:20,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  22%|██▏       | 960/4381 [25:39<1:31:20,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  22%|██▏       | 970/4381 [25:54<1:30:59,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  22%|██▏       | 970/4381 [25:54<1:30:59,  1.60s/it, loss=2.56, v_num=641]Epoch 12:  22%|██▏       | 980/4381 [26:11<1:30:47,  1.60s/it, loss=2.56, v_num=641]Epoch 12:  22%|██▏       | 980/4381 [26:11<1:30:47,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  23%|██▎       | 990/4381 [26:26<1:30:29,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  23%|██▎       | 990/4381 [26:26<1:30:29,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  23%|██▎       | 1000/4381 [26:45<1:30:24,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  23%|██▎       | 1000/4381 [26:45<1:30:24,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  23%|██▎       | 1010/4381 [27:01<1:30:05,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  23%|██▎       | 1010/4381 [27:01<1:30:05,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  23%|██▎       | 1020/4381 [27:15<1:29:43,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  23%|██▎       | 1020/4381 [27:15<1:29:43,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▎       | 1030/4381 [27:32<1:29:30,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▎       | 1030/4381 [27:32<1:29:30,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▎       | 1040/4381 [27:45<1:29:04,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▎       | 1040/4381 [27:45<1:29:04,  1.60s/it, loss=2.51, v_num=641]Epoch 12:  24%|██▍       | 1050/4381 [27:58<1:28:39,  1.60s/it, loss=2.51, v_num=641]Epoch 12:  24%|██▍       | 1050/4381 [27:58<1:28:39,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▍       | 1060/4381 [28:13<1:28:22,  1.60s/it, loss=2.52, v_num=641]Epoch 12:  24%|██▍       | 1060/4381 [28:13<1:28:22,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  24%|██▍       | 1070/4381 [28:28<1:28:00,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  24%|██▍       | 1070/4381 [28:28<1:28:00,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  25%|██▍       | 1080/4381 [28:42<1:27:41,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  25%|██▍       | 1080/4381 [28:42<1:27:41,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  25%|██▍       | 1090/4381 [29:00<1:27:29,  1.60s/it, loss=2.54, v_num=641]Epoch 12:  25%|██▍       | 1090/4381 [29:00<1:27:29,  1.60s/it, loss=2.55, v_num=641]Epoch 12:  25%|██▌       | 1100/4381 [29:13<1:27:05,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  25%|██▌       | 1100/4381 [29:13<1:27:05,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  25%|██▌       | 1110/4381 [29:31<1:26:56,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  25%|██▌       | 1110/4381 [29:31<1:26:56,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  26%|██▌       | 1120/4381 [29:46<1:26:37,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  26%|██▌       | 1120/4381 [29:46<1:26:37,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  26%|██▌       | 1130/4381 [30:02<1:26:21,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  26%|██▌       | 1130/4381 [30:02<1:26:21,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  26%|██▌       | 1140/4381 [30:18<1:26:04,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  26%|██▌       | 1140/4381 [30:18<1:26:04,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  26%|██▌       | 1150/4381 [30:32<1:25:45,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  26%|██▌       | 1150/4381 [30:32<1:25:45,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  26%|██▋       | 1160/4381 [30:49<1:25:31,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  26%|██▋       | 1160/4381 [30:49<1:25:31,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  27%|██▋       | 1170/4381 [31:07<1:25:21,  1.60s/it, loss=2.53, v_num=641]Epoch 12:  27%|██▋       | 1170/4381 [31:07<1:25:21,  1.60s/it, loss=2.55, v_num=641]Epoch 12:  27%|██▋       | 1180/4381 [31:22<1:25:02,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  27%|██▋       | 1180/4381 [31:22<1:25:02,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  27%|██▋       | 1190/4381 [31:36<1:24:40,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  27%|██▋       | 1190/4381 [31:36<1:24:40,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  27%|██▋       | 1200/4381 [31:52<1:24:24,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  27%|██▋       | 1200/4381 [31:52<1:24:24,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  28%|██▊       | 1210/4381 [32:08<1:24:09,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  28%|██▊       | 1210/4381 [32:08<1:24:09,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  28%|██▊       | 1220/4381 [32:25<1:23:57,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  28%|██▊       | 1220/4381 [32:25<1:23:57,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  28%|██▊       | 1230/4381 [32:39<1:23:35,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  28%|██▊       | 1230/4381 [32:39<1:23:35,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  28%|██▊       | 1240/4381 [32:55<1:23:20,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  28%|██▊       | 1240/4381 [32:55<1:23:20,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  29%|██▊       | 1250/4381 [33:10<1:23:02,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  29%|██▊       | 1250/4381 [33:10<1:23:02,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  29%|██▉       | 1260/4381 [33:27<1:22:49,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  29%|██▉       | 1260/4381 [33:27<1:22:49,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1270/4381 [33:42<1:22:30,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1270/4381 [33:42<1:22:30,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1280/4381 [34:00<1:22:19,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1280/4381 [34:00<1:22:19,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1290/4381 [34:17<1:22:05,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  29%|██▉       | 1290/4381 [34:17<1:22:05,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  30%|██▉       | 1300/4381 [34:33<1:21:50,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  30%|██▉       | 1300/4381 [34:33<1:21:50,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  30%|██▉       | 1310/4381 [34:45<1:21:26,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  30%|██▉       | 1310/4381 [34:45<1:21:26,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  30%|███       | 1320/4381 [35:01<1:21:09,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  30%|███       | 1320/4381 [35:01<1:21:09,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  30%|███       | 1330/4381 [35:15<1:20:48,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  30%|███       | 1330/4381 [35:15<1:20:48,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  31%|███       | 1340/4381 [35:26<1:20:23,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  31%|███       | 1340/4381 [35:26<1:20:23,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  31%|███       | 1350/4381 [35:45<1:20:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  31%|███       | 1350/4381 [35:45<1:20:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  31%|███       | 1360/4381 [36:03<1:20:01,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  31%|███       | 1360/4381 [36:03<1:20:01,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  31%|███▏      | 1370/4381 [36:17<1:19:42,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  31%|███▏      | 1370/4381 [36:17<1:19:42,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  31%|███▏      | 1380/4381 [36:34<1:19:28,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  31%|███▏      | 1380/4381 [36:34<1:19:28,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  32%|███▏      | 1390/4381 [36:49<1:19:11,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  32%|███▏      | 1390/4381 [36:49<1:19:11,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  32%|███▏      | 1400/4381 [37:06<1:18:57,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  32%|███▏      | 1400/4381 [37:06<1:18:57,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  32%|███▏      | 1410/4381 [37:18<1:18:33,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  32%|███▏      | 1410/4381 [37:18<1:18:33,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  32%|███▏      | 1420/4381 [37:35<1:18:20,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  32%|███▏      | 1420/4381 [37:35<1:18:20,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  33%|███▎      | 1430/4381 [37:54<1:18:09,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  33%|███▎      | 1430/4381 [37:54<1:18:09,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  33%|███▎      | 1440/4381 [38:11<1:17:57,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  33%|███▎      | 1440/4381 [38:11<1:17:57,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  33%|███▎      | 1450/4381 [38:27<1:17:41,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  33%|███▎      | 1450/4381 [38:27<1:17:41,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  33%|███▎      | 1460/4381 [38:41<1:17:21,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  33%|███▎      | 1460/4381 [38:41<1:17:21,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  34%|███▎      | 1470/4381 [38:58<1:17:07,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  34%|███▎      | 1470/4381 [38:58<1:17:07,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  34%|███▍      | 1480/4381 [39:13<1:16:50,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  34%|███▍      | 1480/4381 [39:13<1:16:50,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  34%|███▍      | 1490/4381 [39:27<1:16:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  34%|███▍      | 1490/4381 [39:27<1:16:31,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  34%|███▍      | 1500/4381 [39:42<1:16:12,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  34%|███▍      | 1500/4381 [39:42<1:16:12,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  34%|███▍      | 1510/4381 [40:00<1:16:00,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  34%|███▍      | 1510/4381 [40:00<1:16:00,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  35%|███▍      | 1520/4381 [40:16<1:15:45,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  35%|███▍      | 1520/4381 [40:16<1:15:45,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  35%|███▍      | 1530/4381 [40:35<1:15:36,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  35%|███▍      | 1530/4381 [40:35<1:15:36,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  35%|███▌      | 1540/4381 [40:51<1:15:19,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  35%|███▌      | 1540/4381 [40:51<1:15:19,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  35%|███▌      | 1550/4381 [41:06<1:15:01,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  35%|███▌      | 1550/4381 [41:06<1:15:01,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  36%|███▌      | 1560/4381 [41:23<1:14:47,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  36%|███▌      | 1560/4381 [41:23<1:14:47,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  36%|███▌      | 1570/4381 [41:36<1:14:26,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  36%|███▌      | 1570/4381 [41:36<1:14:26,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  36%|███▌      | 1580/4381 [41:54<1:14:14,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  36%|███▌      | 1580/4381 [41:54<1:14:14,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  36%|███▋      | 1590/4381 [42:11<1:14:00,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  36%|███▋      | 1590/4381 [42:11<1:14:00,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  37%|███▋      | 1600/4381 [42:24<1:13:40,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  37%|███▋      | 1600/4381 [42:24<1:13:40,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  37%|███▋      | 1610/4381 [42:35<1:13:16,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  37%|███▋      | 1610/4381 [42:35<1:13:16,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  37%|███▋      | 1620/4381 [42:56<1:13:08,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  37%|███▋      | 1620/4381 [42:56<1:13:08,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  37%|███▋      | 1630/4381 [43:11<1:12:50,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  37%|███▋      | 1630/4381 [43:11<1:12:50,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  37%|███▋      | 1640/4381 [43:26<1:12:33,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  37%|███▋      | 1640/4381 [43:26<1:12:33,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  38%|███▊      | 1650/4381 [43:43<1:12:20,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  38%|███▊      | 1650/4381 [43:43<1:12:20,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  38%|███▊      | 1660/4381 [44:00<1:12:05,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  38%|███▊      | 1660/4381 [44:00<1:12:05,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  38%|███▊      | 1670/4381 [44:16<1:11:49,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  38%|███▊      | 1670/4381 [44:16<1:11:49,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  38%|███▊      | 1680/4381 [44:34<1:11:36,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  38%|███▊      | 1680/4381 [44:34<1:11:36,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▊      | 1690/4381 [44:50<1:11:22,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▊      | 1690/4381 [44:50<1:11:22,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▉      | 1700/4381 [45:04<1:11:02,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▉      | 1700/4381 [45:04<1:11:02,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▉      | 1710/4381 [45:20<1:10:47,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  39%|███▉      | 1710/4381 [45:20<1:10:47,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  39%|███▉      | 1720/4381 [45:34<1:10:27,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  39%|███▉      | 1720/4381 [45:34<1:10:27,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  39%|███▉      | 1730/4381 [45:51<1:10:13,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  39%|███▉      | 1730/4381 [45:51<1:10:13,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  40%|███▉      | 1740/4381 [46:05<1:09:55,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  40%|███▉      | 1740/4381 [46:05<1:09:55,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  40%|███▉      | 1750/4381 [46:22<1:09:41,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  40%|███▉      | 1750/4381 [46:22<1:09:41,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  40%|████      | 1760/4381 [46:39<1:09:27,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  40%|████      | 1760/4381 [46:39<1:09:27,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  40%|████      | 1770/4381 [46:54<1:09:08,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  40%|████      | 1770/4381 [46:54<1:09:08,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  41%|████      | 1780/4381 [47:09<1:08:52,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  41%|████      | 1780/4381 [47:09<1:08:52,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  41%|████      | 1790/4381 [47:26<1:08:38,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  41%|████      | 1790/4381 [47:26<1:08:38,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  41%|████      | 1800/4381 [47:41<1:08:21,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  41%|████      | 1800/4381 [47:41<1:08:21,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  41%|████▏     | 1810/4381 [48:01<1:08:10,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  41%|████▏     | 1810/4381 [48:01<1:08:10,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  42%|████▏     | 1820/4381 [48:16<1:07:53,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  42%|████▏     | 1820/4381 [48:16<1:07:53,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  42%|████▏     | 1830/4381 [48:31<1:07:36,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  42%|████▏     | 1830/4381 [48:31<1:07:36,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  42%|████▏     | 1840/4381 [48:48<1:07:21,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  42%|████▏     | 1840/4381 [48:48<1:07:21,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  42%|████▏     | 1850/4381 [49:04<1:07:05,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  42%|████▏     | 1850/4381 [49:04<1:07:05,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  42%|████▏     | 1860/4381 [49:18<1:06:48,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  42%|████▏     | 1860/4381 [49:18<1:06:48,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  43%|████▎     | 1870/4381 [49:34<1:06:31,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  43%|████▎     | 1870/4381 [49:34<1:06:31,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  43%|████▎     | 1880/4381 [49:53<1:06:20,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  43%|████▎     | 1880/4381 [49:53<1:06:20,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  43%|████▎     | 1890/4381 [50:07<1:06:01,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  43%|████▎     | 1890/4381 [50:07<1:06:01,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  43%|████▎     | 1900/4381 [50:24<1:05:47,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  43%|████▎     | 1900/4381 [50:24<1:05:47,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  44%|████▎     | 1910/4381 [50:42<1:05:34,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  44%|████▎     | 1910/4381 [50:42<1:05:34,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  44%|████▍     | 1920/4381 [50:58<1:05:18,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  44%|████▍     | 1920/4381 [50:58<1:05:18,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  44%|████▍     | 1930/4381 [51:13<1:05:01,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  44%|████▍     | 1930/4381 [51:13<1:05:01,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  44%|████▍     | 1940/4381 [51:31<1:04:48,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  44%|████▍     | 1940/4381 [51:31<1:04:48,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  45%|████▍     | 1950/4381 [51:47<1:04:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  45%|████▍     | 1950/4381 [51:47<1:04:31,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  45%|████▍     | 1960/4381 [52:01<1:04:13,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  45%|████▍     | 1960/4381 [52:01<1:04:13,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  45%|████▍     | 1970/4381 [52:20<1:04:01,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  45%|████▍     | 1970/4381 [52:20<1:04:01,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  45%|████▌     | 1980/4381 [52:33<1:03:42,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  45%|████▌     | 1980/4381 [52:33<1:03:42,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  45%|████▌     | 1990/4381 [52:49<1:03:25,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  45%|████▌     | 1990/4381 [52:49<1:03:25,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  46%|████▌     | 2000/4381 [53:07<1:03:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  46%|████▌     | 2000/4381 [53:07<1:03:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  46%|████▌     | 2010/4381 [53:23<1:02:57,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  46%|████▌     | 2010/4381 [53:23<1:02:57,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  46%|████▌     | 2020/4381 [53:37<1:02:38,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  46%|████▌     | 2020/4381 [53:37<1:02:38,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  46%|████▋     | 2030/4381 [53:56<1:02:26,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  46%|████▋     | 2030/4381 [53:56<1:02:26,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  47%|████▋     | 2040/4381 [54:10<1:02:08,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  47%|████▋     | 2040/4381 [54:10<1:02:08,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  47%|████▋     | 2050/4381 [54:25<1:01:51,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  47%|████▋     | 2050/4381 [54:25<1:01:51,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  47%|████▋     | 2060/4381 [54:40<1:01:34,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  47%|████▋     | 2060/4381 [54:40<1:01:34,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  47%|████▋     | 2070/4381 [54:56<1:01:18,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  47%|████▋     | 2070/4381 [54:56<1:01:18,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  47%|████▋     | 2080/4381 [55:09<1:00:59,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  47%|████▋     | 2080/4381 [55:09<1:00:59,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  48%|████▊     | 2090/4381 [55:28<1:00:46,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  48%|████▊     | 2090/4381 [55:28<1:00:46,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  48%|████▊     | 2100/4381 [55:43<1:00:29,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  48%|████▊     | 2100/4381 [55:43<1:00:29,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  48%|████▊     | 2110/4381 [55:59<1:00:14,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  48%|████▊     | 2110/4381 [55:59<1:00:14,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  48%|████▊     | 2120/4381 [56:14<59:57,  1.59s/it, loss=2.52, v_num=641]  Epoch 12:  48%|████▊     | 2120/4381 [56:14<59:57,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  49%|████▊     | 2130/4381 [56:29<59:40,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  49%|████▊     | 2130/4381 [56:29<59:40,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  49%|████▉     | 2140/4381 [56:46<59:25,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  49%|████▉     | 2140/4381 [56:46<59:25,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  49%|████▉     | 2150/4381 [57:03<59:10,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  49%|████▉     | 2150/4381 [57:03<59:10,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  49%|████▉     | 2160/4381 [57:17<58:52,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  49%|████▉     | 2160/4381 [57:17<58:52,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  50%|████▉     | 2170/4381 [57:34<58:38,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  50%|████▉     | 2170/4381 [57:34<58:38,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  50%|████▉     | 2180/4381 [57:50<58:22,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  50%|████▉     | 2180/4381 [57:50<58:22,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  50%|████▉     | 2190/4381 [58:06<58:06,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  50%|████▉     | 2190/4381 [58:06<58:06,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  50%|█████     | 2200/4381 [58:20<57:48,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  50%|█████     | 2200/4381 [58:20<57:48,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  50%|█████     | 2210/4381 [58:34<57:31,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  50%|█████     | 2210/4381 [58:34<57:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  51%|█████     | 2220/4381 [58:47<57:12,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  51%|█████     | 2220/4381 [58:47<57:12,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  51%|█████     | 2230/4381 [59:03<56:56,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  51%|█████     | 2230/4381 [59:03<56:56,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  51%|█████     | 2240/4381 [59:23<56:44,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  51%|█████     | 2240/4381 [59:23<56:44,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  51%|█████▏    | 2250/4381 [59:38<56:27,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  51%|█████▏    | 2250/4381 [59:38<56:27,  1.59s/it, loss=2.47, v_num=641]Epoch 12:  52%|█████▏    | 2260/4381 [59:52<56:10,  1.59s/it, loss=2.47, v_num=641]Epoch 12:  52%|█████▏    | 2260/4381 [59:52<56:10,  1.59s/it, loss=2.46, v_num=641]Epoch 12:  52%|█████▏    | 2270/4381 [1:00:12<55:58,  1.59s/it, loss=2.46, v_num=641]Epoch 12:  52%|█████▏    | 2270/4381 [1:00:12<55:58,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  52%|█████▏    | 2280/4381 [1:00:26<55:40,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  52%|█████▏    | 2280/4381 [1:00:26<55:40,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  52%|█████▏    | 2290/4381 [1:00:39<55:21,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  52%|█████▏    | 2290/4381 [1:00:39<55:21,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  52%|█████▏    | 2300/4381 [1:00:55<55:06,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  52%|█████▏    | 2300/4381 [1:00:55<55:06,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  53%|█████▎    | 2310/4381 [1:01:13<54:52,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  53%|█████▎    | 2310/4381 [1:01:13<54:52,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  53%|█████▎    | 2320/4381 [1:01:26<54:33,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  53%|█████▎    | 2320/4381 [1:01:26<54:33,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  53%|█████▎    | 2330/4381 [1:01:44<54:19,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  53%|█████▎    | 2330/4381 [1:01:44<54:19,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  53%|█████▎    | 2340/4381 [1:01:59<54:02,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  53%|█████▎    | 2340/4381 [1:01:59<54:02,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  54%|█████▎    | 2350/4381 [1:02:15<53:46,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  54%|█████▎    | 2350/4381 [1:02:15<53:46,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  54%|█████▍    | 2360/4381 [1:02:34<53:33,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  54%|█████▍    | 2360/4381 [1:02:34<53:33,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  54%|█████▍    | 2370/4381 [1:02:51<53:18,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  54%|█████▍    | 2370/4381 [1:02:51<53:18,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  54%|█████▍    | 2380/4381 [1:03:06<53:02,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  54%|█████▍    | 2380/4381 [1:03:06<53:02,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  55%|█████▍    | 2390/4381 [1:03:23<52:47,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  55%|█████▍    | 2390/4381 [1:03:23<52:47,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  55%|█████▍    | 2400/4381 [1:03:40<52:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  55%|█████▍    | 2400/4381 [1:03:40<52:31,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  55%|█████▌    | 2410/4381 [1:03:55<52:15,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  55%|█████▌    | 2410/4381 [1:03:55<52:15,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  55%|█████▌    | 2420/4381 [1:04:13<52:01,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  55%|█████▌    | 2420/4381 [1:04:13<52:01,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  55%|█████▌    | 2430/4381 [1:04:28<51:44,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  55%|█████▌    | 2430/4381 [1:04:28<51:44,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  56%|█████▌    | 2440/4381 [1:04:43<51:27,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  56%|█████▌    | 2440/4381 [1:04:43<51:27,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  56%|█████▌    | 2450/4381 [1:05:00<51:12,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  56%|█████▌    | 2450/4381 [1:05:00<51:12,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  56%|█████▌    | 2460/4381 [1:05:15<50:56,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  56%|█████▌    | 2460/4381 [1:05:15<50:56,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  56%|█████▋    | 2470/4381 [1:05:31<50:40,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  56%|█████▋    | 2470/4381 [1:05:31<50:40,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  57%|█████▋    | 2480/4381 [1:05:52<50:28,  1.59s/it, loss=2.59, v_num=641]Epoch 12:  57%|█████▋    | 2480/4381 [1:05:52<50:28,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  57%|█████▋    | 2490/4381 [1:06:07<50:11,  1.59s/it, loss=2.58, v_num=641]Epoch 12:  57%|█████▋    | 2490/4381 [1:06:07<50:11,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  57%|█████▋    | 2500/4381 [1:06:22<49:54,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  57%|█████▋    | 2500/4381 [1:06:22<49:54,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  57%|█████▋    | 2510/4381 [1:06:37<49:38,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  57%|█████▋    | 2510/4381 [1:06:37<49:38,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  58%|█████▊    | 2520/4381 [1:06:51<49:21,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  58%|█████▊    | 2520/4381 [1:06:51<49:21,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  58%|█████▊    | 2530/4381 [1:07:05<49:03,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  58%|█████▊    | 2530/4381 [1:07:05<49:03,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  58%|█████▊    | 2540/4381 [1:07:22<48:48,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  58%|█████▊    | 2540/4381 [1:07:22<48:48,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  58%|█████▊    | 2550/4381 [1:07:36<48:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  58%|█████▊    | 2550/4381 [1:07:36<48:31,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  58%|█████▊    | 2560/4381 [1:07:51<48:14,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  58%|█████▊    | 2560/4381 [1:07:51<48:14,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  59%|█████▊    | 2570/4381 [1:08:08<47:59,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  59%|█████▊    | 2570/4381 [1:08:08<47:59,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  59%|█████▉    | 2580/4381 [1:08:22<47:42,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  59%|█████▉    | 2580/4381 [1:08:22<47:42,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  59%|█████▉    | 2590/4381 [1:08:38<47:27,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  59%|█████▉    | 2590/4381 [1:08:38<47:27,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  59%|█████▉    | 2600/4381 [1:08:54<47:10,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  59%|█████▉    | 2600/4381 [1:08:54<47:10,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  60%|█████▉    | 2610/4381 [1:09:07<46:53,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  60%|█████▉    | 2610/4381 [1:09:07<46:53,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  60%|█████▉    | 2620/4381 [1:09:23<46:37,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  60%|█████▉    | 2620/4381 [1:09:23<46:37,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  60%|██████    | 2630/4381 [1:09:41<46:22,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  60%|██████    | 2630/4381 [1:09:41<46:22,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  60%|██████    | 2640/4381 [1:09:56<46:06,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  60%|██████    | 2640/4381 [1:09:56<46:06,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  60%|██████    | 2650/4381 [1:10:09<45:48,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  60%|██████    | 2650/4381 [1:10:09<45:48,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████    | 2660/4381 [1:10:27<45:34,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████    | 2660/4381 [1:10:27<45:34,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  61%|██████    | 2670/4381 [1:10:44<45:18,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  61%|██████    | 2670/4381 [1:10:44<45:18,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████    | 2680/4381 [1:10:58<45:01,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████    | 2680/4381 [1:10:58<45:01,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████▏   | 2690/4381 [1:11:19<44:49,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  61%|██████▏   | 2690/4381 [1:11:19<44:49,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  62%|██████▏   | 2700/4381 [1:11:33<44:31,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  62%|██████▏   | 2700/4381 [1:11:33<44:31,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  62%|██████▏   | 2710/4381 [1:11:47<44:15,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  62%|██████▏   | 2710/4381 [1:11:47<44:15,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  62%|██████▏   | 2720/4381 [1:12:07<44:01,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  62%|██████▏   | 2720/4381 [1:12:07<44:01,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  62%|██████▏   | 2730/4381 [1:12:21<43:44,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  62%|██████▏   | 2730/4381 [1:12:21<43:44,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  63%|██████▎   | 2740/4381 [1:12:36<43:28,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  63%|██████▎   | 2740/4381 [1:12:36<43:28,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2750/4381 [1:12:55<43:13,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2750/4381 [1:12:55<43:13,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  63%|██████▎   | 2760/4381 [1:13:09<42:57,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  63%|██████▎   | 2760/4381 [1:13:09<42:57,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2780/4381 [1:13:40<42:24,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  63%|██████▎   | 2780/4381 [1:13:40<42:24,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  64%|██████▎   | 2790/4381 [1:13:53<42:07,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  64%|██████▎   | 2790/4381 [1:13:53<42:07,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  64%|██████▍   | 2800/4381 [1:14:10<41:52,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  64%|██████▍   | 2800/4381 [1:14:10<41:52,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  64%|██████▍   | 2810/4381 [1:14:28<41:37,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  64%|██████▍   | 2810/4381 [1:14:28<41:37,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  64%|██████▍   | 2820/4381 [1:14:43<41:21,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  64%|██████▍   | 2820/4381 [1:14:43<41:21,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  65%|██████▍   | 2830/4381 [1:14:58<41:04,  1.59s/it, loss=2.49, v_num=641]Epoch 12:  65%|██████▍   | 2830/4381 [1:14:58<41:04,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  65%|██████▍   | 2840/4381 [1:15:15<40:49,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  65%|██████▍   | 2840/4381 [1:15:15<40:49,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  65%|██████▌   | 2850/4381 [1:15:31<40:33,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  65%|██████▌   | 2850/4381 [1:15:31<40:33,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  65%|██████▌   | 2860/4381 [1:15:50<40:18,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  65%|██████▌   | 2860/4381 [1:15:50<40:18,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▌   | 2870/4381 [1:16:07<40:04,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▌   | 2870/4381 [1:16:07<40:04,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  66%|██████▌   | 2880/4381 [1:16:22<39:47,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  66%|██████▌   | 2880/4381 [1:16:22<39:47,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  66%|██████▌   | 2890/4381 [1:16:35<39:30,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  66%|██████▌   | 2890/4381 [1:16:35<39:30,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▌   | 2900/4381 [1:16:50<39:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▌   | 2900/4381 [1:16:50<39:13,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▋   | 2910/4381 [1:17:05<38:57,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  66%|██████▋   | 2910/4381 [1:17:05<38:57,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2920/4381 [1:17:20<38:40,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2920/4381 [1:17:20<38:40,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  67%|██████▋   | 2930/4381 [1:17:41<38:27,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  67%|██████▋   | 2930/4381 [1:17:41<38:27,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2940/4381 [1:17:56<38:11,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2940/4381 [1:17:56<38:11,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2950/4381 [1:18:12<37:55,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  67%|██████▋   | 2950/4381 [1:18:12<37:55,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  68%|██████▊   | 2960/4381 [1:18:28<37:39,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  68%|██████▊   | 2960/4381 [1:18:28<37:39,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  68%|██████▊   | 2970/4381 [1:18:45<37:24,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  68%|██████▊   | 2970/4381 [1:18:45<37:24,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  68%|██████▊   | 2980/4381 [1:19:00<37:07,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  68%|██████▊   | 2980/4381 [1:19:00<37:07,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  68%|██████▊   | 2990/4381 [1:19:17<36:52,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  68%|██████▊   | 2990/4381 [1:19:17<36:52,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  68%|██████▊   | 3000/4381 [1:19:33<36:36,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  68%|██████▊   | 3000/4381 [1:19:33<36:36,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  69%|██████▊   | 3010/4381 [1:19:48<36:20,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  69%|██████▊   | 3010/4381 [1:19:48<36:20,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  69%|██████▉   | 3020/4381 [1:20:05<36:05,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  69%|██████▉   | 3020/4381 [1:20:05<36:05,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  69%|██████▉   | 3030/4381 [1:20:19<35:48,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  69%|██████▉   | 3030/4381 [1:20:19<35:48,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  69%|██████▉   | 3040/4381 [1:20:33<35:31,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  69%|██████▉   | 3040/4381 [1:20:33<35:31,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  70%|██████▉   | 3050/4381 [1:20:52<35:16,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  70%|██████▉   | 3050/4381 [1:20:52<35:16,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  70%|██████▉   | 3060/4381 [1:21:04<34:59,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  70%|██████▉   | 3060/4381 [1:21:04<34:59,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  70%|███████   | 3070/4381 [1:21:20<34:43,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  70%|███████   | 3070/4381 [1:21:20<34:43,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  70%|███████   | 3080/4381 [1:21:37<34:28,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  70%|███████   | 3080/4381 [1:21:37<34:28,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  71%|███████   | 3090/4381 [1:21:52<34:11,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  71%|███████   | 3090/4381 [1:21:52<34:11,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  71%|███████   | 3100/4381 [1:22:06<33:54,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  71%|███████   | 3100/4381 [1:22:06<33:54,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  71%|███████   | 3110/4381 [1:22:24<33:40,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  71%|███████   | 3110/4381 [1:22:24<33:40,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  71%|███████   | 3120/4381 [1:22:39<33:23,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  71%|███████   | 3120/4381 [1:22:39<33:23,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  71%|███████▏  | 3130/4381 [1:22:55<33:07,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  71%|███████▏  | 3130/4381 [1:22:55<33:07,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  72%|███████▏  | 3140/4381 [1:23:11<32:52,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  72%|███████▏  | 3140/4381 [1:23:11<32:52,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  72%|███████▏  | 3150/4381 [1:23:26<32:35,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  72%|███████▏  | 3150/4381 [1:23:26<32:35,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  72%|███████▏  | 3160/4381 [1:23:43<32:20,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  72%|███████▏  | 3160/4381 [1:23:43<32:20,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  72%|███████▏  | 3170/4381 [1:24:04<32:06,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  72%|███████▏  | 3170/4381 [1:24:04<32:06,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  73%|███████▎  | 3180/4381 [1:24:17<31:49,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  73%|███████▎  | 3180/4381 [1:24:17<31:49,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3190/4381 [1:24:32<31:33,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3190/4381 [1:24:32<31:33,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3200/4381 [1:24:51<31:18,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3200/4381 [1:24:51<31:18,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  73%|███████▎  | 3210/4381 [1:25:08<31:03,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  73%|███████▎  | 3210/4381 [1:25:08<31:03,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3220/4381 [1:25:24<30:47,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  73%|███████▎  | 3220/4381 [1:25:24<30:47,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  74%|███████▎  | 3230/4381 [1:25:42<30:31,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  74%|███████▎  | 3230/4381 [1:25:42<30:31,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  74%|███████▍  | 3240/4381 [1:25:57<30:15,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  74%|███████▍  | 3240/4381 [1:25:57<30:15,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  74%|███████▍  | 3250/4381 [1:26:13<29:59,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  74%|███████▍  | 3250/4381 [1:26:13<29:59,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  74%|███████▍  | 3260/4381 [1:26:29<29:43,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  74%|███████▍  | 3260/4381 [1:26:29<29:43,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  75%|███████▍  | 3270/4381 [1:26:44<29:27,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  75%|███████▍  | 3270/4381 [1:26:44<29:27,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  75%|███████▍  | 3280/4381 [1:26:59<29:11,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  75%|███████▍  | 3280/4381 [1:26:59<29:11,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  75%|███████▌  | 3290/4381 [1:27:17<28:56,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  75%|███████▌  | 3290/4381 [1:27:17<28:56,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  75%|███████▌  | 3300/4381 [1:27:30<28:39,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  75%|███████▌  | 3300/4381 [1:27:30<28:39,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  76%|███████▌  | 3310/4381 [1:27:45<28:23,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  76%|███████▌  | 3310/4381 [1:27:45<28:23,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  76%|███████▌  | 3320/4381 [1:28:02<28:07,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  76%|███████▌  | 3320/4381 [1:28:02<28:07,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  76%|███████▌  | 3330/4381 [1:28:18<27:51,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  76%|███████▌  | 3330/4381 [1:28:18<27:51,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  76%|███████▌  | 3340/4381 [1:28:32<27:35,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  76%|███████▋  | 3350/4381 [1:28:51<27:20,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  76%|███████▋  | 3350/4381 [1:28:51<27:20,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3360/4381 [1:29:04<27:03,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3360/4381 [1:29:04<27:03,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3370/4381 [1:29:17<26:46,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3370/4381 [1:29:17<26:46,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  77%|███████▋  | 3380/4381 [1:29:33<26:30,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  77%|███████▋  | 3380/4381 [1:29:33<26:30,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3390/4381 [1:29:49<26:15,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  77%|███████▋  | 3390/4381 [1:29:49<26:15,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  78%|███████▊  | 3400/4381 [1:30:03<25:58,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  78%|███████▊  | 3400/4381 [1:30:03<25:58,  1.59s/it, loss=2.47, v_num=641]Epoch 12:  78%|███████▊  | 3410/4381 [1:30:19<25:42,  1.59s/it, loss=2.47, v_num=641]Epoch 12:  78%|███████▊  | 3410/4381 [1:30:19<25:42,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  78%|███████▊  | 3420/4381 [1:30:36<25:27,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  78%|███████▊  | 3420/4381 [1:30:36<25:27,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  78%|███████▊  | 3430/4381 [1:30:50<25:10,  1.59s/it, loss=2.57, v_num=641]Epoch 12:  78%|███████▊  | 3430/4381 [1:30:50<25:10,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  79%|███████▊  | 3440/4381 [1:31:09<24:55,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  79%|███████▊  | 3440/4381 [1:31:09<24:55,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  79%|███████▊  | 3450/4381 [1:31:24<24:39,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  79%|███████▊  | 3450/4381 [1:31:24<24:39,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  79%|███████▉  | 3460/4381 [1:31:38<24:23,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  79%|███████▉  | 3460/4381 [1:31:38<24:23,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  79%|███████▉  | 3470/4381 [1:31:52<24:06,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  79%|███████▉  | 3470/4381 [1:31:52<24:06,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  79%|███████▉  | 3480/4381 [1:32:06<23:50,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  79%|███████▉  | 3480/4381 [1:32:06<23:50,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  80%|███████▉  | 3490/4381 [1:32:19<23:33,  1.59s/it, loss=2.55, v_num=641]Epoch 12:  80%|███████▉  | 3490/4381 [1:32:19<23:33,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  80%|███████▉  | 3500/4381 [1:32:40<23:19,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  80%|███████▉  | 3500/4381 [1:32:40<23:19,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  80%|████████  | 3510/4381 [1:32:55<23:03,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  80%|████████  | 3510/4381 [1:32:55<23:03,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  80%|████████  | 3520/4381 [1:33:11<22:47,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  80%|████████  | 3520/4381 [1:33:11<22:47,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  81%|████████  | 3530/4381 [1:33:29<22:31,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  81%|████████  | 3530/4381 [1:33:29<22:31,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  81%|████████  | 3540/4381 [1:33:43<22:15,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  81%|████████  | 3540/4381 [1:33:43<22:15,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  81%|████████  | 3550/4381 [1:33:58<21:59,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  81%|████████  | 3550/4381 [1:33:58<21:59,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  81%|████████▏ | 3560/4381 [1:34:17<21:44,  1.59s/it, loss=2.51, v_num=641]Epoch 12:  81%|████████▏ | 3560/4381 [1:34:17<21:44,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  81%|████████▏ | 3570/4381 [1:34:32<21:28,  1.59s/it, loss=2.53, v_num=641]Epoch 12:  81%|████████▏ | 3570/4381 [1:34:32<21:28,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  82%|████████▏ | 3580/4381 [1:34:47<21:12,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  82%|████████▏ | 3580/4381 [1:34:47<21:12,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  82%|████████▏ | 3590/4381 [1:35:04<20:56,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  82%|████████▏ | 3590/4381 [1:35:04<20:56,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  82%|████████▏ | 3600/4381 [1:35:19<20:40,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  82%|████████▏ | 3600/4381 [1:35:19<20:40,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  82%|████████▏ | 3610/4381 [1:35:34<20:24,  1.59s/it, loss=2.56, v_num=641]Epoch 12:  82%|████████▏ | 3610/4381 [1:35:34<20:24,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3620/4381 [1:35:52<20:08,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3620/4381 [1:35:52<20:08,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3630/4381 [1:36:09<19:53,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3630/4381 [1:36:09<19:53,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3640/4381 [1:36:25<19:37,  1.59s/it, loss=2.54, v_num=641]Epoch 12:  83%|████████▎ | 3640/4381 [1:36:25<19:37,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  83%|████████▎ | 3650/4381 [1:36:44<19:22,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▎ | 3660/4381 [1:36:58<19:05,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▎ | 3660/4381 [1:36:58<19:05,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▍ | 3670/4381 [1:37:11<18:49,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▍ | 3670/4381 [1:37:11<18:49,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▍ | 3680/4381 [1:37:29<18:33,  1.59s/it, loss=2.52, v_num=641]Epoch 12:  84%|████████▍ | 3680/4381 [1:37:29<18:33,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  84%|████████▍ | 3690/4381 [1:37:45<18:18,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  84%|████████▍ | 3690/4381 [1:37:45<18:18,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  84%|████████▍ | 3700/4381 [1:38:01<18:02,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  84%|████████▍ | 3700/4381 [1:38:01<18:02,  1.59s/it, loss=2.5, v_num=641] Epoch 12:  85%|████████▍ | 3710/4381 [1:38:19<17:46,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  85%|████████▍ | 3710/4381 [1:38:19<17:46,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  85%|████████▍ | 3720/4381 [1:38:33<17:30,  1.59s/it, loss=2.5, v_num=641]Epoch 12:  85%|████████▍ | 3720/4381 [1:38:33<17:30,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  85%|████████▌ | 3730/4381 [1:38:44<17:13,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  85%|████████▌ | 3730/4381 [1:38:44<17:13,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  85%|████████▌ | 3740/4381 [1:38:49<16:56,  1.59s/it, loss=2.48, v_num=641]Epoch 12:  85%|████████▌ | 3740/4381 [1:38:49<16:56,  1.59s/it, loss=2.47, v_num=641]Epoch 12:  86%|████████▌ | 3750/4381 [1:38:53<16:38,  1.58s/it, loss=2.47, v_num=641]Epoch 12:  86%|████████▌ | 3750/4381 [1:38:53<16:38,  1.58s/it, loss=2.51, v_num=641]validation_epoch_end
graph acc: 0.3194888178913738
valid accuracy: 0.9705559611320496
Epoch 12:  86%|████████▌ | 3760/4381 [1:38:54<16:19,  1.58s/it, loss=2.51, v_num=641]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/626 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.2987220447284345
valid accuracy: 0.969009518623352
validation_epoch_end
graph acc: 0.30670926517571884
valid accuracy: 0.969273030757904
validation_epoch_end
graph acc: 0.305111821086262
valid accuracy: 0.9692511558532715
validation_epoch_end
graph acc: 0.2955271565495208
valid accuracy: 0.9722127318382263
validation_epoch_end
graph acc: 0.28913738019169327
valid accuracy: 0.9704462885856628
validation_epoch_end
graph acc: 0.35303514376996803
valid accuracy: 0.9673920273780823

Validating:   2%|▏         | 10/626 [00:03<03:35,  2.85it/s][AEpoch 12:  86%|████████▌ | 3770/4381 [1:38:57<16:02,  1.57s/it, loss=2.51, v_num=641]
Validating:   3%|▎         | 20/626 [00:04<02:09,  4.69it/s][AEpoch 12:  86%|████████▋ | 3780/4381 [1:38:59<15:44,  1.57s/it, loss=2.51, v_num=641]
Validating:   5%|▍         | 30/626 [00:07<02:12,  4.50it/s][AEpoch 12:  87%|████████▋ | 3790/4381 [1:39:01<15:26,  1.57s/it, loss=2.51, v_num=641]
Validating:   6%|▋         | 40/626 [00:08<02:02,  4.79it/s][AEpoch 12:  87%|████████▋ | 3800/4381 [1:39:03<15:08,  1.56s/it, loss=2.51, v_num=641]
Validating:   8%|▊         | 50/626 [00:10<01:46,  5.42it/s][AEpoch 12:  87%|████████▋ | 3810/4381 [1:39:04<14:50,  1.56s/it, loss=2.51, v_num=641]
Validating:  10%|▉         | 60/626 [00:12<01:41,  5.57it/s][AEpoch 12:  87%|████████▋ | 3820/4381 [1:39:06<14:33,  1.56s/it, loss=2.51, v_num=641]
Validating:  11%|█         | 70/626 [00:14<01:45,  5.28it/s][AEpoch 12:  87%|████████▋ | 3830/4381 [1:39:08<14:15,  1.55s/it, loss=2.51, v_num=641]
Validating:  13%|█▎        | 80/626 [00:15<01:39,  5.47it/s][AEpoch 12:  88%|████████▊ | 3840/4381 [1:39:10<13:58,  1.55s/it, loss=2.51, v_num=641]
Validating:  14%|█▍        | 90/626 [00:17<01:34,  5.67it/s][AEpoch 12:  88%|████████▊ | 3850/4381 [1:39:11<13:40,  1.55s/it, loss=2.51, v_num=641]
Validating:  16%|█▌        | 100/626 [00:19<01:41,  5.16it/s][AEpoch 12:  88%|████████▊ | 3860/4381 [1:39:14<13:23,  1.54s/it, loss=2.51, v_num=641]
Validating:  18%|█▊        | 110/626 [00:21<01:35,  5.42it/s][AEpoch 12:  88%|████████▊ | 3870/4381 [1:39:15<13:06,  1.54s/it, loss=2.51, v_num=641]
Validating:  19%|█▉        | 120/626 [00:23<01:43,  4.88it/s][AEpoch 12:  89%|████████▊ | 3880/4381 [1:39:18<12:49,  1.54s/it, loss=2.51, v_num=641]
Validating:  21%|██        | 130/626 [00:25<01:34,  5.23it/s][AEpoch 12:  89%|████████▉ | 3890/4381 [1:39:19<12:32,  1.53s/it, loss=2.51, v_num=641]
Validating:  22%|██▏       | 140/626 [00:28<01:44,  4.63it/s][AEpoch 12:  89%|████████▉ | 3900/4381 [1:39:22<12:15,  1.53s/it, loss=2.51, v_num=641]
Validating:  24%|██▍       | 150/626 [00:30<01:37,  4.86it/s][AEpoch 12:  89%|████████▉ | 3910/4381 [1:39:24<11:58,  1.53s/it, loss=2.51, v_num=641]
Validating:  26%|██▌       | 160/626 [00:31<01:32,  5.06it/s][AEpoch 12:  89%|████████▉ | 3920/4381 [1:39:26<11:41,  1.52s/it, loss=2.51, v_num=641]
Validating:  27%|██▋       | 170/626 [00:33<01:23,  5.49it/s][AEpoch 12:  90%|████████▉ | 3930/4381 [1:39:27<11:24,  1.52s/it, loss=2.51, v_num=641]
Validating:  29%|██▉       | 180/626 [00:35<01:23,  5.32it/s][AEpoch 12:  90%|████████▉ | 3940/4381 [1:39:29<11:08,  1.51s/it, loss=2.51, v_num=641]
Validating:  30%|███       | 190/626 [00:37<01:32,  4.71it/s][AEpoch 12:  90%|█████████ | 3950/4381 [1:39:32<10:51,  1.51s/it, loss=2.51, v_num=641]
Validating:  32%|███▏      | 200/626 [00:40<01:35,  4.45it/s][AEpoch 12:  90%|█████████ | 3960/4381 [1:39:34<10:35,  1.51s/it, loss=2.51, v_num=641]
Validating:  34%|███▎      | 210/626 [00:42<01:29,  4.65it/s][AEpoch 12:  91%|█████████ | 3970/4381 [1:39:36<10:18,  1.51s/it, loss=2.51, v_num=641]
Validating:  35%|███▌      | 220/626 [00:44<01:28,  4.57it/s][AEpoch 12:  91%|█████████ | 3980/4381 [1:39:39<10:02,  1.50s/it, loss=2.51, v_num=641]
Validating:  37%|███▋      | 230/626 [00:46<01:18,  5.02it/s][AEpoch 12:  91%|█████████ | 3990/4381 [1:39:40<09:45,  1.50s/it, loss=2.51, v_num=641]
Validating:  38%|███▊      | 240/626 [00:48<01:14,  5.21it/s][AEpoch 12:  91%|█████████▏| 4000/4381 [1:39:42<09:29,  1.50s/it, loss=2.51, v_num=641]
Validating:  40%|███▉      | 250/626 [00:50<01:17,  4.87it/s][AEpoch 12:  92%|█████████▏| 4010/4381 [1:39:44<09:13,  1.49s/it, loss=2.51, v_num=641]
Validating:  42%|████▏     | 260/626 [00:52<01:20,  4.56it/s][AEpoch 12:  92%|█████████▏| 4020/4381 [1:39:47<08:57,  1.49s/it, loss=2.51, v_num=641]
Validating:  43%|████▎     | 270/626 [00:54<01:09,  5.14it/s][AEpoch 12:  92%|█████████▏| 4030/4381 [1:39:48<08:41,  1.49s/it, loss=2.51, v_num=641]
Validating:  45%|████▍     | 280/626 [00:56<01:14,  4.62it/s][AEpoch 12:  92%|█████████▏| 4040/4381 [1:39:51<08:25,  1.48s/it, loss=2.51, v_num=641]
Validating:  46%|████▋     | 290/626 [00:59<01:13,  4.55it/s][AEpoch 12:  92%|█████████▏| 4050/4381 [1:39:53<08:09,  1.48s/it, loss=2.51, v_num=641]
Validating:  48%|████▊     | 300/626 [01:00<01:03,  5.15it/s][AEpoch 12:  93%|█████████▎| 4060/4381 [1:39:55<07:53,  1.48s/it, loss=2.51, v_num=641]
Validating:  50%|████▉     | 310/626 [01:01<00:52,  6.00it/s][AEpoch 12:  93%|█████████▎| 4070/4381 [1:39:56<07:38,  1.47s/it, loss=2.51, v_num=641]
Validating:  51%|█████     | 320/626 [01:03<00:54,  5.59it/s][AEpoch 12:  93%|█████████▎| 4080/4381 [1:39:58<07:22,  1.47s/it, loss=2.51, v_num=641]
Validating:  53%|█████▎    | 330/626 [01:05<00:50,  5.91it/s][AEpoch 12:  93%|█████████▎| 4090/4381 [1:39:59<07:06,  1.47s/it, loss=2.51, v_num=641]
Validating:  54%|█████▍    | 340/626 [01:07<00:50,  5.71it/s][AEpoch 12:  94%|█████████▎| 4100/4381 [1:40:01<06:51,  1.46s/it, loss=2.51, v_num=641]
Validating:  56%|█████▌    | 350/626 [01:09<00:56,  4.88it/s][AEpoch 12:  94%|█████████▍| 4110/4381 [1:40:04<06:35,  1.46s/it, loss=2.51, v_num=641]
Validating:  58%|█████▊    | 360/626 [01:11<00:54,  4.85it/s][AEpoch 12:  94%|█████████▍| 4120/4381 [1:40:06<06:20,  1.46s/it, loss=2.51, v_num=641]
Validating:  59%|█████▉    | 370/626 [01:13<00:47,  5.36it/s][AEpoch 12:  94%|█████████▍| 4130/4381 [1:40:07<06:05,  1.45s/it, loss=2.51, v_num=641]
Validating:  61%|██████    | 380/626 [01:14<00:44,  5.59it/s][AEpoch 12:  94%|█████████▍| 4140/4381 [1:40:09<05:49,  1.45s/it, loss=2.51, v_num=641]
Validating:  62%|██████▏   | 390/626 [01:16<00:41,  5.72it/s][AEpoch 12:  95%|█████████▍| 4150/4381 [1:40:11<05:34,  1.45s/it, loss=2.51, v_num=641]
Validating:  64%|██████▍   | 400/626 [01:18<00:37,  5.99it/s][AEpoch 12:  95%|█████████▍| 4160/4381 [1:40:12<05:19,  1.44s/it, loss=2.51, v_num=641]
Validating:  65%|██████▌   | 410/626 [01:19<00:33,  6.49it/s][AEpoch 12:  95%|█████████▌| 4170/4381 [1:40:13<05:04,  1.44s/it, loss=2.51, v_num=641]
Validating:  67%|██████▋   | 420/626 [01:21<00:35,  5.88it/s][AEpoch 12:  95%|█████████▌| 4180/4381 [1:40:15<04:49,  1.44s/it, loss=2.51, v_num=641]
Validating:  69%|██████▊   | 430/626 [01:23<00:33,  5.86it/s][AEpoch 12:  96%|█████████▌| 4190/4381 [1:40:17<04:34,  1.44s/it, loss=2.51, v_num=641]
Validating:  70%|███████   | 440/626 [01:25<00:34,  5.37it/s][AEpoch 12:  96%|█████████▌| 4200/4381 [1:40:19<04:19,  1.43s/it, loss=2.51, v_num=641]
Validating:  72%|███████▏  | 450/626 [01:27<00:32,  5.43it/s][AEpoch 12:  96%|█████████▌| 4210/4381 [1:40:21<04:04,  1.43s/it, loss=2.51, v_num=641]
Validating:  73%|███████▎  | 460/626 [01:28<00:30,  5.44it/s][AEpoch 12:  96%|█████████▋| 4220/4381 [1:40:23<03:49,  1.43s/it, loss=2.51, v_num=641]
Validating:  75%|███████▌  | 470/626 [01:30<00:27,  5.65it/s][AEpoch 12:  97%|█████████▋| 4230/4381 [1:40:24<03:35,  1.42s/it, loss=2.51, v_num=641]
Validating:  77%|███████▋  | 480/626 [01:32<00:26,  5.49it/s][AEpoch 12:  97%|█████████▋| 4240/4381 [1:40:26<03:20,  1.42s/it, loss=2.51, v_num=641]
Validating:  78%|███████▊  | 490/626 [01:35<00:29,  4.57it/s][AEpoch 12:  97%|█████████▋| 4250/4381 [1:40:29<03:05,  1.42s/it, loss=2.51, v_num=641]
Validating:  80%|███████▉  | 500/626 [01:36<00:23,  5.42it/s][AEpoch 12:  97%|█████████▋| 4260/4381 [1:40:31<02:51,  1.42s/it, loss=2.51, v_num=641]
Validating:  81%|████████▏ | 510/626 [01:39<00:24,  4.77it/s][AEpoch 12:  97%|█████████▋| 4270/4381 [1:40:33<02:36,  1.41s/it, loss=2.51, v_num=641]
Validating:  83%|████████▎ | 520/626 [01:40<00:18,  5.66it/s][AEpoch 12:  98%|█████████▊| 4280/4381 [1:40:34<02:22,  1.41s/it, loss=2.51, v_num=641]
Validating:  85%|████████▍ | 530/626 [01:42<00:17,  5.58it/s][AEpoch 12:  98%|█████████▊| 4290/4381 [1:40:36<02:08,  1.41s/it, loss=2.51, v_num=641]
Validating:  86%|████████▋ | 540/626 [01:43<00:14,  5.81it/s][AEpoch 12:  98%|█████████▊| 4300/4381 [1:40:38<01:53,  1.40s/it, loss=2.51, v_num=641]
Validating:  88%|████████▊ | 550/626 [01:45<00:12,  6.07it/s][AEpoch 12:  98%|█████████▊| 4310/4381 [1:40:39<01:39,  1.40s/it, loss=2.51, v_num=641]
Validating:  89%|████████▉ | 560/626 [01:46<00:10,  6.26it/s][AEpoch 12:  99%|█████████▊| 4320/4381 [1:40:41<01:25,  1.40s/it, loss=2.51, v_num=641]
Validating:  91%|█████████ | 570/626 [01:48<00:08,  6.38it/s][AEpoch 12:  99%|█████████▉| 4330/4381 [1:40:42<01:11,  1.40s/it, loss=2.51, v_num=641]
Validating:  93%|█████████▎| 580/626 [01:50<00:08,  5.58it/s][AEpoch 12:  99%|█████████▉| 4340/4381 [1:40:44<00:57,  1.39s/it, loss=2.51, v_num=641]
Validating:  94%|█████████▍| 590/626 [01:52<00:06,  5.76it/s][AEpoch 12:  99%|█████████▉| 4350/4381 [1:40:46<00:43,  1.39s/it, loss=2.51, v_num=641]
Validating:  96%|█████████▌| 600/626 [01:53<00:04,  6.04it/s][AEpoch 12: 100%|█████████▉| 4360/4381 [1:40:47<00:29,  1.39s/it, loss=2.51, v_num=641]
Validating:  97%|█████████▋| 610/626 [01:54<00:02,  6.62it/s][AEpoch 12: 100%|█████████▉| 4370/4381 [1:40:49<00:15,  1.38s/it, loss=2.51, v_num=641]
Validating:  99%|█████████▉| 620/626 [01:57<00:01,  5.48it/s][AEpoch 12: 100%|█████████▉| 4380/4381 [1:40:51<00:01,  1.38s/it, loss=2.51, v_num=641]
Validating: 100%|██████████| 626/626 [01:57<00:00,  5.89it/s][Avalidation_epoch_end
graph acc: 0.33865814696485624
valid accuracy: 0.9723479747772217
Epoch 12: 100%|██████████| 4381/4381 [1:40:56<00:00,  1.38s/it, loss=2.54, v_num=641]
                                                             [AEpoch 12:   0%|          | 0/4381 [00:00<00:00, 9776.93it/s, loss=2.54, v_num=641]   Epoch 13:   0%|          | 0/4381 [00:00<00:01, 2725.34it/s, loss=2.54, v_num=641]Epoch 13:   0%|          | 0/4381 [00:15<18:43:55, 15.39s/it, loss=2.54, v_num=641]Epoch 13:   0%|          | 10/4381 [00:22<2:26:21,  2.01s/it, loss=2.54, v_num=641]Epoch 13:   0%|          | 10/4381 [00:22<2:26:22,  2.01s/it, loss=2.52, v_num=641]Epoch 13:   0%|          | 20/4381 [00:35<2:02:58,  1.69s/it, loss=2.52, v_num=641]Epoch 13:   0%|          | 20/4381 [00:35<2:02:58,  1.69s/it, loss=2.45, v_num=641]Epoch 13:   1%|          | 30/4381 [00:52<2:03:31,  1.70s/it, loss=2.45, v_num=641]Epoch 13:   1%|          | 30/4381 [00:52<2:03:31,  1.70s/it, loss=2.47, v_num=641]Epoch 13:   1%|          | 40/4381 [01:09<2:01:58,  1.69s/it, loss=2.47, v_num=641]Epoch 13:   1%|          | 40/4381 [01:09<2:01:58,  1.69s/it, loss=2.51, v_num=641]Epoch 13:   1%|          | 50/4381 [01:25<2:00:18,  1.67s/it, loss=2.51, v_num=641]Epoch 13:   1%|          | 50/4381 [01:25<2:00:18,  1.67s/it, loss=2.48, v_num=641]Epoch 13:   1%|▏         | 60/4381 [01:42<2:01:31,  1.69s/it, loss=2.48, v_num=641]Epoch 13:   1%|▏         | 60/4381 [01:42<2:01:31,  1.69s/it, loss=2.46, v_num=641]Epoch 13:   2%|▏         | 70/4381 [01:57<1:58:56,  1.66s/it, loss=2.46, v_num=641]Epoch 13:   2%|▏         | 70/4381 [01:57<1:58:56,  1.66s/it, loss=2.43, v_num=641]Epoch 13:   2%|▏         | 80/4381 [02:14<1:59:00,  1.66s/it, loss=2.43, v_num=641]Epoch 13:   2%|▏         | 80/4381 [02:14<1:59:00,  1.66s/it, loss=2.44, v_num=641]Epoch 13:   2%|▏         | 90/4381 [02:33<2:00:44,  1.69s/it, loss=2.44, v_num=641]Epoch 13:   2%|▏         | 90/4381 [02:33<2:00:44,  1.69s/it, loss=2.49, v_num=641]Epoch 13:   2%|▏         | 100/4381 [02:52<2:01:31,  1.70s/it, loss=2.49, v_num=641]Epoch 13:   2%|▏         | 100/4381 [02:52<2:01:31,  1.70s/it, loss=2.49, v_num=641]Epoch 13:   3%|▎         | 110/4381 [03:11<2:03:00,  1.73s/it, loss=2.49, v_num=641]Epoch 13:   3%|▎         | 110/4381 [03:11<2:03:00,  1.73s/it, loss=2.5, v_num=641] Epoch 13:   3%|▎         | 120/4381 [03:27<2:01:34,  1.71s/it, loss=2.5, v_num=641]Epoch 13:   3%|▎         | 120/4381 [03:27<2:01:34,  1.71s/it, loss=2.54, v_num=641]Epoch 13:   3%|▎         | 130/4381 [03:44<2:01:20,  1.71s/it, loss=2.54, v_num=641]Epoch 13:   3%|▎         | 130/4381 [03:44<2:01:20,  1.71s/it, loss=2.5, v_num=641] Epoch 13:   3%|▎         | 140/4381 [04:03<2:02:00,  1.73s/it, loss=2.5, v_num=641]Epoch 13:   3%|▎         | 140/4381 [04:03<2:02:00,  1.73s/it, loss=2.46, v_num=641]Epoch 13:   3%|▎         | 150/4381 [04:18<2:00:47,  1.71s/it, loss=2.46, v_num=641]Epoch 13:   3%|▎         | 150/4381 [04:18<2:00:49,  1.71s/it, loss=2.46, v_num=641]Epoch 13:   4%|▎         | 160/4381 [04:33<1:59:18,  1.70s/it, loss=2.46, v_num=641]Epoch 13:   4%|▎         | 160/4381 [04:33<1:59:18,  1.70s/it, loss=2.48, v_num=641]Epoch 13:   4%|▍         | 170/4381 [04:51<1:59:38,  1.70s/it, loss=2.48, v_num=641]Epoch 13:   4%|▍         | 170/4381 [04:51<1:59:38,  1.70s/it, loss=2.5, v_num=641] Epoch 13:   4%|▍         | 180/4381 [05:06<1:58:22,  1.69s/it, loss=2.5, v_num=641]Epoch 13:   4%|▍         | 180/4381 [05:06<1:58:22,  1.69s/it, loss=2.48, v_num=641]Epoch 13:   4%|▍         | 190/4381 [05:21<1:57:28,  1.68s/it, loss=2.48, v_num=641]Epoch 13:   4%|▍         | 190/4381 [05:21<1:57:28,  1.68s/it, loss=2.48, v_num=641]Epoch 13:   5%|▍         | 200/4381 [05:39<1:57:43,  1.69s/it, loss=2.48, v_num=641]Epoch 13:   5%|▍         | 200/4381 [05:39<1:57:43,  1.69s/it, loss=2.5, v_num=641] Epoch 13:   5%|▍         | 210/4381 [05:54<1:56:46,  1.68s/it, loss=2.5, v_num=641]Epoch 13:   5%|▍         | 210/4381 [05:54<1:56:46,  1.68s/it, loss=2.49, v_num=641]Epoch 13:   5%|▌         | 220/4381 [06:08<1:55:39,  1.67s/it, loss=2.49, v_num=641]Epoch 13:   5%|▌         | 220/4381 [06:08<1:55:40,  1.67s/it, loss=2.49, v_num=641]Epoch 13:   5%|▌         | 230/4381 [06:25<1:55:32,  1.67s/it, loss=2.49, v_num=641]Epoch 13:   5%|▌         | 230/4381 [06:25<1:55:32,  1.67s/it, loss=2.46, v_num=641]Epoch 13:   5%|▌         | 240/4381 [06:40<1:54:45,  1.66s/it, loss=2.46, v_num=641]Epoch 13:   5%|▌         | 240/4381 [06:40<1:54:45,  1.66s/it, loss=2.47, v_num=641]Epoch 13:   6%|▌         | 250/4381 [06:56<1:54:10,  1.66s/it, loss=2.47, v_num=641]Epoch 13:   6%|▌         | 250/4381 [06:56<1:54:10,  1.66s/it, loss=2.51, v_num=641]Epoch 13:   6%|▌         | 260/4381 [07:15<1:54:30,  1.67s/it, loss=2.51, v_num=641]Epoch 13:   6%|▌         | 260/4381 [07:15<1:54:30,  1.67s/it, loss=2.49, v_num=641]Epoch 13:   6%|▌         | 270/4381 [07:30<1:53:59,  1.66s/it, loss=2.49, v_num=641]Epoch 13:   6%|▌         | 270/4381 [07:30<1:53:59,  1.66s/it, loss=2.47, v_num=641]Epoch 13:   6%|▋         | 280/4381 [07:44<1:52:58,  1.65s/it, loss=2.47, v_num=641]Epoch 13:   6%|▋         | 280/4381 [07:44<1:52:58,  1.65s/it, loss=2.48, v_num=641]Epoch 13:   7%|▋         | 290/4381 [08:00<1:52:31,  1.65s/it, loss=2.48, v_num=641]Epoch 13:   7%|▋         | 290/4381 [08:00<1:52:31,  1.65s/it, loss=2.49, v_num=641]Epoch 13:   7%|▋         | 300/4381 [08:19<1:52:47,  1.66s/it, loss=2.49, v_num=641]Epoch 13:   7%|▋         | 300/4381 [08:19<1:52:47,  1.66s/it, loss=2.48, v_num=641]Epoch 13:   7%|▋         | 310/4381 [08:33<1:52:03,  1.65s/it, loss=2.48, v_num=641]Epoch 13:   7%|▋         | 310/4381 [08:33<1:52:03,  1.65s/it, loss=2.47, v_num=641]Epoch 13:   7%|▋         | 320/4381 [08:50<1:51:45,  1.65s/it, loss=2.47, v_num=641]Epoch 13:   7%|▋         | 320/4381 [08:50<1:51:45,  1.65s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 330/4381 [09:08<1:51:57,  1.66s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 330/4381 [09:08<1:51:57,  1.66s/it, loss=2.46, v_num=641]Epoch 13:   8%|▊         | 340/4381 [09:23<1:51:16,  1.65s/it, loss=2.46, v_num=641]Epoch 13:   8%|▊         | 340/4381 [09:23<1:51:16,  1.65s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 350/4381 [09:38<1:50:47,  1.65s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 350/4381 [09:38<1:50:47,  1.65s/it, loss=2.52, v_num=641]Epoch 13:   8%|▊         | 360/4381 [09:57<1:50:53,  1.65s/it, loss=2.52, v_num=641]Epoch 13:   8%|▊         | 360/4381 [09:57<1:50:53,  1.65s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 370/4381 [10:09<1:49:52,  1.64s/it, loss=2.49, v_num=641]Epoch 13:   8%|▊         | 370/4381 [10:09<1:49:52,  1.64s/it, loss=2.46, v_num=641]Epoch 13:   9%|▊         | 380/4381 [10:25<1:49:25,  1.64s/it, loss=2.46, v_num=641]Epoch 13:   9%|▊         | 380/4381 [10:25<1:49:25,  1.64s/it, loss=2.49, v_num=641]Epoch 13:   9%|▉         | 390/4381 [10:41<1:49:10,  1.64s/it, loss=2.49, v_num=641]Epoch 13:   9%|▉         | 390/4381 [10:41<1:49:10,  1.64s/it, loss=2.51, v_num=641]Epoch 13:   9%|▉         | 400/4381 [10:54<1:48:14,  1.63s/it, loss=2.51, v_num=641]Epoch 13:   9%|▉         | 400/4381 [10:54<1:48:14,  1.63s/it, loss=2.47, v_num=641]Epoch 13:   9%|▉         | 410/4381 [11:10<1:47:54,  1.63s/it, loss=2.47, v_num=641]Epoch 13:   9%|▉         | 410/4381 [11:10<1:47:54,  1.63s/it, loss=2.5, v_num=641] Epoch 13:  10%|▉         | 420/4381 [11:26<1:47:38,  1.63s/it, loss=2.5, v_num=641]Epoch 13:  10%|▉         | 420/4381 [11:26<1:47:38,  1.63s/it, loss=2.52, v_num=641]Epoch 13:  10%|▉         | 430/4381 [11:43<1:47:29,  1.63s/it, loss=2.52, v_num=641]Epoch 13:  10%|▉         | 430/4381 [11:43<1:47:29,  1.63s/it, loss=2.46, v_num=641]Epoch 13:  10%|█         | 440/4381 [11:58<1:46:58,  1.63s/it, loss=2.46, v_num=641]Epoch 13:  10%|█         | 440/4381 [11:58<1:46:58,  1.63s/it, loss=2.46, v_num=641]Epoch 13:  10%|█         | 450/4381 [12:17<1:47:08,  1.64s/it, loss=2.46, v_num=641]Epoch 13:  10%|█         | 450/4381 [12:17<1:47:08,  1.64s/it, loss=2.5, v_num=641] Epoch 13:  10%|█         | 460/4381 [12:31<1:46:29,  1.63s/it, loss=2.5, v_num=641]Epoch 13:  10%|█         | 460/4381 [12:31<1:46:29,  1.63s/it, loss=2.48, v_num=641]Epoch 13:  11%|█         | 470/4381 [12:45<1:45:54,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  11%|█         | 470/4381 [12:45<1:45:54,  1.62s/it, loss=2.46, v_num=641]Epoch 13:  11%|█         | 480/4381 [13:03<1:45:54,  1.63s/it, loss=2.46, v_num=641]Epoch 13:  11%|█         | 480/4381 [13:03<1:45:54,  1.63s/it, loss=2.47, v_num=641]Epoch 13:  11%|█         | 490/4381 [13:18<1:45:29,  1.63s/it, loss=2.47, v_num=641]Epoch 13:  11%|█         | 490/4381 [13:18<1:45:29,  1.63s/it, loss=2.49, v_num=641]Epoch 13:  11%|█▏        | 500/4381 [13:34<1:45:08,  1.63s/it, loss=2.49, v_num=641]Epoch 13:  11%|█▏        | 500/4381 [13:34<1:45:08,  1.63s/it, loss=2.5, v_num=641] Epoch 13:  12%|█▏        | 510/4381 [13:52<1:45:03,  1.63s/it, loss=2.5, v_num=641]Epoch 13:  12%|█▏        | 510/4381 [13:52<1:45:03,  1.63s/it, loss=2.49, v_num=641]Epoch 13:  12%|█▏        | 520/4381 [14:06<1:44:33,  1.62s/it, loss=2.49, v_num=641]Epoch 13:  12%|█▏        | 520/4381 [14:06<1:44:33,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  12%|█▏        | 530/4381 [14:20<1:43:58,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  12%|█▏        | 530/4381 [14:20<1:43:58,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  12%|█▏        | 540/4381 [14:38<1:43:53,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  12%|█▏        | 540/4381 [14:38<1:43:53,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  13%|█▎        | 550/4381 [14:52<1:43:27,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  13%|█▎        | 550/4381 [14:52<1:43:27,  1.62s/it, loss=2.52, v_num=641]Epoch 13:  13%|█▎        | 560/4381 [15:07<1:43:03,  1.62s/it, loss=2.52, v_num=641]Epoch 13:  13%|█▎        | 560/4381 [15:07<1:43:03,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  13%|█▎        | 570/4381 [15:26<1:43:04,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  13%|█▎        | 570/4381 [15:26<1:43:04,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  13%|█▎        | 580/4381 [15:40<1:42:34,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  13%|█▎        | 580/4381 [15:40<1:42:34,  1.62s/it, loss=2.54, v_num=641]Epoch 13:  13%|█▎        | 590/4381 [15:56<1:42:17,  1.62s/it, loss=2.54, v_num=641]Epoch 13:  13%|█▎        | 590/4381 [15:56<1:42:17,  1.62s/it, loss=2.5, v_num=641] Epoch 13:  14%|█▎        | 600/4381 [16:13<1:42:05,  1.62s/it, loss=2.5, v_num=641]Epoch 13:  14%|█▎        | 600/4381 [16:13<1:42:05,  1.62s/it, loss=2.47, v_num=641]Epoch 13:  14%|█▍        | 610/4381 [16:27<1:41:32,  1.62s/it, loss=2.47, v_num=641]Epoch 13:  14%|█▍        | 610/4381 [16:27<1:41:32,  1.62s/it, loss=2.5, v_num=641] Epoch 13:  14%|█▍        | 620/4381 [16:43<1:41:17,  1.62s/it, loss=2.5, v_num=641]Epoch 13:  14%|█▍        | 620/4381 [16:43<1:41:17,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  14%|█▍        | 630/4381 [17:00<1:41:05,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  14%|█▍        | 630/4381 [17:00<1:41:05,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  15%|█▍        | 640/4381 [17:14<1:40:35,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  15%|█▍        | 640/4381 [17:14<1:40:35,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  15%|█▍        | 650/4381 [17:32<1:40:32,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  15%|█▍        | 650/4381 [17:32<1:40:32,  1.62s/it, loss=2.47, v_num=641]Epoch 13:  15%|█▌        | 660/4381 [17:46<1:40:05,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  15%|█▌        | 660/4381 [17:46<1:40:05,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  15%|█▌        | 670/4381 [18:01<1:39:40,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  15%|█▌        | 670/4381 [18:01<1:39:40,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  16%|█▌        | 680/4381 [18:20<1:39:38,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  16%|█▌        | 680/4381 [18:20<1:39:38,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  16%|█▌        | 690/4381 [18:36<1:39:25,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  16%|█▌        | 690/4381 [18:36<1:39:25,  1.62s/it, loss=2.47, v_num=641]Epoch 13:  16%|█▌        | 700/4381 [18:49<1:38:52,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  16%|█▌        | 700/4381 [18:49<1:38:52,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  16%|█▌        | 710/4381 [19:11<1:39:04,  1.62s/it, loss=2.5, v_num=641]Epoch 13:  16%|█▌        | 710/4381 [19:11<1:39:04,  1.62s/it, loss=2.49, v_num=641]Epoch 13:  16%|█▋        | 720/4381 [19:26<1:38:41,  1.62s/it, loss=2.49, v_num=641]Epoch 13:  16%|█▋        | 720/4381 [19:26<1:38:41,  1.62s/it, loss=2.49, v_num=641]Epoch 13:  17%|█▋        | 730/4381 [19:40<1:38:14,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  17%|█▋        | 730/4381 [19:40<1:38:14,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  17%|█▋        | 740/4381 [19:56<1:37:59,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  17%|█▋        | 740/4381 [19:56<1:37:59,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  17%|█▋        | 750/4381 [20:12<1:37:44,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  17%|█▋        | 750/4381 [20:12<1:37:44,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  17%|█▋        | 760/4381 [20:26<1:37:14,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  17%|█▋        | 760/4381 [20:26<1:37:14,  1.61s/it, loss=2.53, v_num=641]Epoch 13:  18%|█▊        | 770/4381 [20:42<1:36:57,  1.61s/it, loss=2.53, v_num=641]Epoch 13:  18%|█▊        | 770/4381 [20:42<1:36:57,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  18%|█▊        | 780/4381 [20:59<1:36:47,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  18%|█▊        | 780/4381 [20:59<1:36:47,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  18%|█▊        | 790/4381 [21:14<1:36:26,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  18%|█▊        | 790/4381 [21:14<1:36:26,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  18%|█▊        | 800/4381 [21:31<1:36:13,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  18%|█▊        | 800/4381 [21:31<1:36:13,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  18%|█▊        | 810/4381 [21:48<1:36:01,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  18%|█▊        | 810/4381 [21:48<1:36:01,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  19%|█▊        | 820/4381 [22:04<1:35:45,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  19%|█▊        | 820/4381 [22:04<1:35:45,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  19%|█▉        | 830/4381 [22:20<1:35:26,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  19%|█▉        | 830/4381 [22:20<1:35:26,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  19%|█▉        | 840/4381 [22:36<1:35:12,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  19%|█▉        | 840/4381 [22:36<1:35:12,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  19%|█▉        | 850/4381 [22:56<1:35:09,  1.62s/it, loss=2.49, v_num=641]Epoch 13:  19%|█▉        | 850/4381 [22:56<1:35:09,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  20%|█▉        | 860/4381 [23:10<1:34:46,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  20%|█▉        | 860/4381 [23:10<1:34:46,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  20%|█▉        | 870/4381 [23:27<1:34:35,  1.62s/it, loss=2.51, v_num=641]Epoch 13:  20%|█▉        | 870/4381 [23:27<1:34:35,  1.62s/it, loss=2.52, v_num=641]Epoch 13:  20%|██        | 880/4381 [23:42<1:34:13,  1.61s/it, loss=2.52, v_num=641]Epoch 13:  20%|██        | 880/4381 [23:42<1:34:13,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  20%|██        | 890/4381 [23:58<1:33:55,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  20%|██        | 890/4381 [23:58<1:33:55,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  21%|██        | 900/4381 [24:11<1:33:29,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  21%|██        | 900/4381 [24:11<1:33:29,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  21%|██        | 910/4381 [24:27<1:33:13,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  21%|██        | 910/4381 [24:27<1:33:13,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  21%|██        | 920/4381 [24:42<1:32:50,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  21%|██        | 920/4381 [24:42<1:32:50,  1.61s/it, loss=2.48, v_num=641]Epoch 13:  21%|██        | 930/4381 [25:03<1:32:53,  1.62s/it, loss=2.48, v_num=641]Epoch 13:  21%|██        | 930/4381 [25:03<1:32:53,  1.62s/it, loss=2.53, v_num=641]Epoch 13:  21%|██▏       | 940/4381 [25:15<1:32:20,  1.61s/it, loss=2.53, v_num=641]Epoch 13:  21%|██▏       | 940/4381 [25:15<1:32:20,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  22%|██▏       | 950/4381 [25:30<1:32:02,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  22%|██▏       | 950/4381 [25:30<1:32:02,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  22%|██▏       | 960/4381 [25:48<1:31:53,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  22%|██▏       | 960/4381 [25:48<1:31:53,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  22%|██▏       | 970/4381 [26:02<1:31:29,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  22%|██▏       | 970/4381 [26:02<1:31:29,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  22%|██▏       | 980/4381 [26:19<1:31:17,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  22%|██▏       | 980/4381 [26:19<1:31:17,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  23%|██▎       | 990/4381 [26:36<1:31:01,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  23%|██▎       | 990/4381 [26:36<1:31:01,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  23%|██▎       | 1000/4381 [26:49<1:30:37,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  23%|██▎       | 1000/4381 [26:49<1:30:37,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  23%|██▎       | 1010/4381 [27:04<1:30:18,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  23%|██▎       | 1010/4381 [27:04<1:30:18,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  23%|██▎       | 1020/4381 [27:23<1:30:11,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  23%|██▎       | 1020/4381 [27:23<1:30:11,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  24%|██▎       | 1030/4381 [27:38<1:29:49,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  24%|██▎       | 1030/4381 [27:38<1:29:49,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  24%|██▎       | 1040/4381 [27:54<1:29:33,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  24%|██▎       | 1040/4381 [27:54<1:29:33,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  24%|██▍       | 1050/4381 [28:12<1:29:23,  1.61s/it, loss=2.51, v_num=641]Epoch 13:  24%|██▍       | 1050/4381 [28:12<1:29:23,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  24%|██▍       | 1060/4381 [28:27<1:29:04,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  24%|██▍       | 1060/4381 [28:27<1:29:04,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  24%|██▍       | 1070/4381 [28:44<1:28:49,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  24%|██▍       | 1070/4381 [28:44<1:28:49,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  25%|██▍       | 1080/4381 [29:02<1:28:40,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  25%|██▍       | 1080/4381 [29:02<1:28:40,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  25%|██▍       | 1090/4381 [29:14<1:28:12,  1.61s/it, loss=2.46, v_num=641]Epoch 13:  25%|██▍       | 1090/4381 [29:14<1:28:12,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  25%|██▌       | 1100/4381 [29:29<1:27:54,  1.61s/it, loss=2.45, v_num=641]Epoch 13:  25%|██▌       | 1100/4381 [29:29<1:27:54,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  25%|██▌       | 1110/4381 [29:49<1:27:49,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  25%|██▌       | 1110/4381 [29:49<1:27:49,  1.61s/it, loss=2.54, v_num=641]Epoch 13:  26%|██▌       | 1120/4381 [30:03<1:27:27,  1.61s/it, loss=2.54, v_num=641]Epoch 13:  26%|██▌       | 1120/4381 [30:03<1:27:27,  1.61s/it, loss=2.52, v_num=641]Epoch 13:  26%|██▌       | 1130/4381 [30:20<1:27:14,  1.61s/it, loss=2.52, v_num=641]Epoch 13:  26%|██▌       | 1130/4381 [30:20<1:27:14,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  26%|██▌       | 1140/4381 [30:39<1:27:06,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  26%|██▌       | 1140/4381 [30:39<1:27:06,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  26%|██▌       | 1150/4381 [30:53<1:26:43,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  26%|██▌       | 1150/4381 [30:53<1:26:43,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  26%|██▋       | 1160/4381 [31:09<1:26:27,  1.61s/it, loss=2.47, v_num=641]Epoch 13:  26%|██▋       | 1160/4381 [31:09<1:26:27,  1.61s/it, loss=2.5, v_num=641] Epoch 13:  27%|██▋       | 1170/4381 [31:25<1:26:11,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1170/4381 [31:25<1:26:11,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1180/4381 [31:40<1:25:52,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1180/4381 [31:40<1:25:52,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1190/4381 [31:54<1:25:30,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1190/4381 [31:54<1:25:30,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1200/4381 [32:10<1:25:13,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  27%|██▋       | 1200/4381 [32:10<1:25:13,  1.61s/it, loss=2.52, v_num=641]Epoch 13:  28%|██▊       | 1210/4381 [32:25<1:24:55,  1.61s/it, loss=2.52, v_num=641]Epoch 13:  28%|██▊       | 1210/4381 [32:25<1:24:55,  1.61s/it, loss=2.53, v_num=641]Epoch 13:  28%|██▊       | 1220/4381 [32:39<1:24:33,  1.61s/it, loss=2.53, v_num=641]Epoch 13:  28%|██▊       | 1220/4381 [32:39<1:24:33,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  28%|██▊       | 1230/4381 [32:56<1:24:18,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  28%|██▊       | 1230/4381 [32:56<1:24:18,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  28%|██▊       | 1240/4381 [33:11<1:24:00,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  28%|██▊       | 1240/4381 [33:11<1:24:00,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  29%|██▊       | 1250/4381 [33:24<1:23:37,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  29%|██▊       | 1250/4381 [33:24<1:23:37,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  29%|██▉       | 1260/4381 [33:44<1:23:29,  1.61s/it, loss=2.5, v_num=641]Epoch 13:  29%|██▉       | 1260/4381 [33:44<1:23:29,  1.61s/it, loss=2.49, v_num=641]Epoch 13:  29%|██▉       | 1270/4381 [33:57<1:23:07,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  29%|██▉       | 1270/4381 [33:57<1:23:07,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  29%|██▉       | 1280/4381 [34:14<1:22:52,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  29%|██▉       | 1280/4381 [34:14<1:22:52,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  29%|██▉       | 1290/4381 [34:31<1:22:40,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  29%|██▉       | 1290/4381 [34:31<1:22:40,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  30%|██▉       | 1300/4381 [34:46<1:22:22,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  30%|██▉       | 1300/4381 [34:46<1:22:22,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  30%|██▉       | 1310/4381 [35:01<1:22:02,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  30%|██▉       | 1310/4381 [35:01<1:22:02,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  30%|███       | 1320/4381 [35:18<1:21:49,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  30%|███       | 1320/4381 [35:18<1:21:49,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  30%|███       | 1330/4381 [35:32<1:21:27,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  30%|███       | 1330/4381 [35:32<1:21:27,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███       | 1340/4381 [35:47<1:21:09,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███       | 1340/4381 [35:47<1:21:09,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  31%|███       | 1350/4381 [36:06<1:21:00,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  31%|███       | 1350/4381 [36:06<1:21:00,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███       | 1360/4381 [36:21<1:20:42,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███       | 1360/4381 [36:21<1:20:42,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  31%|███▏      | 1370/4381 [36:38<1:20:28,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  31%|███▏      | 1370/4381 [36:38<1:20:28,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███▏      | 1380/4381 [36:55<1:20:15,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  31%|███▏      | 1380/4381 [36:55<1:20:15,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  32%|███▏      | 1390/4381 [37:07<1:19:50,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  32%|███▏      | 1390/4381 [37:07<1:19:50,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  32%|███▏      | 1400/4381 [37:23<1:19:33,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  32%|███▏      | 1400/4381 [37:23<1:19:33,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  32%|███▏      | 1410/4381 [37:38<1:19:16,  1.60s/it, loss=2.45, v_num=641]Epoch 13:  32%|███▏      | 1410/4381 [37:38<1:19:16,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  32%|███▏      | 1420/4381 [37:56<1:19:03,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  32%|███▏      | 1420/4381 [37:56<1:19:03,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  33%|███▎      | 1430/4381 [38:10<1:18:44,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  33%|███▎      | 1430/4381 [38:10<1:18:44,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  33%|███▎      | 1440/4381 [38:28<1:18:31,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  33%|███▎      | 1440/4381 [38:28<1:18:31,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  33%|███▎      | 1450/4381 [38:43<1:18:14,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  33%|███▎      | 1450/4381 [38:43<1:18:14,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  33%|███▎      | 1460/4381 [38:58<1:17:55,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  33%|███▎      | 1460/4381 [38:58<1:17:55,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  34%|███▎      | 1470/4381 [39:15<1:17:41,  1.60s/it, loss=2.46, v_num=641]Epoch 13:  34%|███▎      | 1470/4381 [39:15<1:17:41,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  34%|███▍      | 1480/4381 [39:31<1:17:24,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  34%|███▍      | 1480/4381 [39:31<1:17:24,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  34%|███▍      | 1490/4381 [39:47<1:17:08,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  34%|███▍      | 1490/4381 [39:47<1:17:08,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  34%|███▍      | 1500/4381 [40:02<1:16:50,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  34%|███▍      | 1500/4381 [40:02<1:16:50,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  34%|███▍      | 1510/4381 [40:19<1:16:37,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  34%|███▍      | 1510/4381 [40:19<1:16:37,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  35%|███▍      | 1520/4381 [40:32<1:16:15,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  35%|███▍      | 1520/4381 [40:32<1:16:15,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  35%|███▍      | 1530/4381 [40:48<1:15:58,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  35%|███▍      | 1530/4381 [40:48<1:15:58,  1.60s/it, loss=2.53, v_num=641]Epoch 13:  35%|███▌      | 1540/4381 [41:02<1:15:40,  1.60s/it, loss=2.53, v_num=641]Epoch 13:  35%|███▌      | 1540/4381 [41:02<1:15:40,  1.60s/it, loss=2.54, v_num=641]Epoch 13:  35%|███▌      | 1550/4381 [41:18<1:15:23,  1.60s/it, loss=2.54, v_num=641]Epoch 13:  35%|███▌      | 1550/4381 [41:18<1:15:23,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  36%|███▌      | 1560/4381 [41:36<1:15:11,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  36%|███▌      | 1560/4381 [41:36<1:15:11,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  36%|███▌      | 1570/4381 [41:53<1:14:57,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  36%|███▌      | 1570/4381 [41:53<1:14:57,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  36%|███▌      | 1580/4381 [42:07<1:14:37,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  36%|███▌      | 1580/4381 [42:07<1:14:37,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  36%|███▋      | 1590/4381 [42:23<1:14:22,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  36%|███▋      | 1590/4381 [42:23<1:14:22,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  37%|███▋      | 1600/4381 [42:41<1:14:10,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  37%|███▋      | 1600/4381 [42:41<1:14:10,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  37%|███▋      | 1610/4381 [42:57<1:13:52,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  37%|███▋      | 1610/4381 [42:57<1:13:52,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  37%|███▋      | 1620/4381 [43:09<1:13:30,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  37%|███▋      | 1620/4381 [43:09<1:13:30,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  37%|███▋      | 1630/4381 [43:26<1:13:17,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  37%|███▋      | 1630/4381 [43:26<1:13:17,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  37%|███▋      | 1640/4381 [43:42<1:13:00,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  37%|███▋      | 1640/4381 [43:42<1:13:00,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  38%|███▊      | 1650/4381 [44:00<1:12:47,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  38%|███▊      | 1650/4381 [44:00<1:12:47,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  38%|███▊      | 1660/4381 [44:17<1:12:33,  1.60s/it, loss=2.47, v_num=641]Epoch 13:  38%|███▊      | 1660/4381 [44:17<1:12:33,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  38%|███▊      | 1670/4381 [44:32<1:12:16,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  38%|███▊      | 1670/4381 [44:32<1:12:16,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  38%|███▊      | 1680/4381 [44:47<1:11:57,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  38%|███▊      | 1680/4381 [44:47<1:11:57,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▊      | 1690/4381 [45:04<1:11:44,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▊      | 1690/4381 [45:04<1:11:44,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▉      | 1700/4381 [45:21<1:11:28,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▉      | 1700/4381 [45:21<1:11:28,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▉      | 1710/4381 [45:35<1:11:10,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  39%|███▉      | 1710/4381 [45:35<1:11:10,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  39%|███▉      | 1720/4381 [45:50<1:10:52,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  39%|███▉      | 1720/4381 [45:50<1:10:52,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  39%|███▉      | 1730/4381 [46:03<1:10:32,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  39%|███▉      | 1730/4381 [46:03<1:10:32,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  40%|███▉      | 1740/4381 [46:21<1:10:19,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  40%|███▉      | 1740/4381 [46:21<1:10:19,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  40%|███▉      | 1750/4381 [46:37<1:10:04,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  40%|███▉      | 1750/4381 [46:37<1:10:04,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  40%|████      | 1760/4381 [46:51<1:09:44,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  40%|████      | 1760/4381 [46:51<1:09:44,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  40%|████      | 1770/4381 [47:04<1:09:24,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  40%|████      | 1770/4381 [47:04<1:09:24,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  41%|████      | 1780/4381 [47:19<1:09:06,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  41%|████      | 1780/4381 [47:19<1:09:06,  1.59s/it, loss=2.45, v_num=641]Epoch 13:  41%|████      | 1790/4381 [47:34<1:08:49,  1.59s/it, loss=2.45, v_num=641]Epoch 13:  41%|████      | 1790/4381 [47:34<1:08:49,  1.59s/it, loss=2.43, v_num=641]Epoch 13:  41%|████      | 1800/4381 [47:48<1:08:31,  1.59s/it, loss=2.43, v_num=641]Epoch 13:  41%|████      | 1800/4381 [47:48<1:08:31,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  41%|████▏     | 1810/4381 [48:04<1:08:14,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  41%|████▏     | 1810/4381 [48:04<1:08:14,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  42%|████▏     | 1820/4381 [48:17<1:07:55,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  42%|████▏     | 1820/4381 [48:17<1:07:55,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  42%|████▏     | 1830/4381 [48:36<1:07:42,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  42%|████▏     | 1830/4381 [48:36<1:07:42,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  42%|████▏     | 1840/4381 [48:58<1:07:35,  1.60s/it, loss=2.48, v_num=641]Epoch 13:  42%|████▏     | 1840/4381 [48:58<1:07:35,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  42%|████▏     | 1850/4381 [49:13<1:07:19,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  42%|████▏     | 1850/4381 [49:13<1:07:19,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  42%|████▏     | 1860/4381 [49:29<1:07:02,  1.60s/it, loss=2.49, v_num=641]Epoch 13:  42%|████▏     | 1860/4381 [49:29<1:07:02,  1.60s/it, loss=2.5, v_num=641] Epoch 13:  43%|████▎     | 1870/4381 [49:44<1:06:45,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  43%|████▎     | 1870/4381 [49:44<1:06:45,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  43%|████▎     | 1880/4381 [50:02<1:06:31,  1.60s/it, loss=2.5, v_num=641]Epoch 13:  43%|████▎     | 1880/4381 [50:02<1:06:31,  1.60s/it, loss=2.52, v_num=641]Epoch 13:  43%|████▎     | 1890/4381 [50:15<1:06:12,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  43%|████▎     | 1890/4381 [50:15<1:06:12,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  43%|████▎     | 1900/4381 [50:33<1:05:59,  1.60s/it, loss=2.53, v_num=641]Epoch 13:  43%|████▎     | 1900/4381 [50:33<1:05:59,  1.60s/it, loss=2.51, v_num=641]Epoch 13:  44%|████▎     | 1910/4381 [50:45<1:05:38,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  44%|████▎     | 1910/4381 [50:45<1:05:38,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  44%|████▍     | 1920/4381 [51:01<1:05:22,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  44%|████▍     | 1920/4381 [51:01<1:05:22,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  44%|████▍     | 1930/4381 [51:19<1:05:08,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  44%|████▍     | 1930/4381 [51:19<1:05:08,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  44%|████▍     | 1940/4381 [51:33<1:04:50,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  44%|████▍     | 1940/4381 [51:33<1:04:50,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  45%|████▍     | 1950/4381 [51:49<1:04:34,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  45%|████▍     | 1950/4381 [51:49<1:04:34,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  45%|████▍     | 1960/4381 [52:03<1:04:16,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  45%|████▍     | 1960/4381 [52:03<1:04:16,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  45%|████▍     | 1970/4381 [52:19<1:04:00,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  45%|████▍     | 1970/4381 [52:19<1:04:00,  1.59s/it, loss=2.46, v_num=641]Epoch 13:  45%|████▌     | 1980/4381 [52:37<1:03:47,  1.59s/it, loss=2.46, v_num=641]Epoch 13:  45%|████▌     | 1980/4381 [52:37<1:03:47,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  45%|████▌     | 1990/4381 [52:52<1:03:30,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  45%|████▌     | 1990/4381 [52:52<1:03:30,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  46%|████▌     | 2000/4381 [53:07<1:03:12,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  46%|████▌     | 2000/4381 [53:07<1:03:12,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  46%|████▌     | 2010/4381 [53:24<1:02:58,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  46%|████▌     | 2010/4381 [53:24<1:02:58,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  46%|████▌     | 2020/4381 [53:39<1:02:40,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  46%|████▌     | 2020/4381 [53:39<1:02:40,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  46%|████▋     | 2030/4381 [53:55<1:02:25,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  46%|████▋     | 2030/4381 [53:55<1:02:25,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  47%|████▋     | 2040/4381 [54:14<1:02:12,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  47%|████▋     | 2040/4381 [54:14<1:02:12,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  47%|████▋     | 2050/4381 [54:27<1:01:54,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  47%|████▋     | 2050/4381 [54:27<1:01:54,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  47%|████▋     | 2060/4381 [54:43<1:01:37,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  47%|████▋     | 2060/4381 [54:43<1:01:37,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  47%|████▋     | 2070/4381 [55:01<1:01:23,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  47%|████▋     | 2070/4381 [55:01<1:01:23,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  47%|████▋     | 2080/4381 [55:17<1:01:07,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  47%|████▋     | 2080/4381 [55:17<1:01:07,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  48%|████▊     | 2090/4381 [55:30<1:00:48,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  48%|████▊     | 2090/4381 [55:30<1:00:48,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  48%|████▊     | 2100/4381 [55:49<1:00:36,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  48%|████▊     | 2100/4381 [55:49<1:00:36,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  48%|████▊     | 2110/4381 [56:03<1:00:18,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  48%|████▊     | 2110/4381 [56:03<1:00:18,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  48%|████▊     | 2120/4381 [56:19<1:00:02,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  48%|████▊     | 2120/4381 [56:19<1:00:02,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  49%|████▊     | 2130/4381 [56:34<59:45,  1.59s/it, loss=2.5, v_num=641]  Epoch 13:  49%|████▊     | 2130/4381 [56:34<59:45,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  49%|████▉     | 2140/4381 [56:50<59:29,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  49%|████▉     | 2140/4381 [56:50<59:29,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  49%|████▉     | 2150/4381 [57:06<59:14,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  49%|████▉     | 2150/4381 [57:06<59:14,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  49%|████▉     | 2160/4381 [57:23<58:59,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  49%|████▉     | 2160/4381 [57:23<58:59,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  50%|████▉     | 2170/4381 [57:38<58:42,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  50%|████▉     | 2170/4381 [57:38<58:42,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  50%|████▉     | 2180/4381 [57:52<58:24,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  50%|████▉     | 2180/4381 [57:52<58:24,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  50%|████▉     | 2190/4381 [58:08<58:08,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  50%|████▉     | 2190/4381 [58:08<58:08,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  50%|█████     | 2200/4381 [58:26<57:54,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  50%|█████     | 2200/4381 [58:26<57:54,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  50%|█████     | 2210/4381 [58:42<57:38,  1.59s/it, loss=2.54, v_num=641]Epoch 13:  50%|█████     | 2210/4381 [58:42<57:38,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  51%|█████     | 2220/4381 [58:54<57:18,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  51%|█████     | 2220/4381 [58:54<57:18,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  51%|█████     | 2230/4381 [59:10<57:03,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  51%|█████     | 2230/4381 [59:10<57:03,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  51%|█████     | 2240/4381 [59:26<56:47,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  51%|█████     | 2240/4381 [59:26<56:47,  1.59s/it, loss=2.58, v_num=641]Epoch 13:  51%|█████▏    | 2250/4381 [59:42<56:31,  1.59s/it, loss=2.58, v_num=641]Epoch 13:  51%|█████▏    | 2250/4381 [59:42<56:31,  1.59s/it, loss=2.58, v_num=641]Epoch 13:  52%|█████▏    | 2260/4381 [59:59<56:17,  1.59s/it, loss=2.58, v_num=641]Epoch 13:  52%|█████▏    | 2260/4381 [59:59<56:17,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  52%|█████▏    | 2270/4381 [1:00:14<56:00,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  52%|█████▏    | 2270/4381 [1:00:14<56:00,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  52%|█████▏    | 2280/4381 [1:00:29<55:43,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  52%|█████▏    | 2280/4381 [1:00:29<55:43,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  52%|█████▏    | 2290/4381 [1:00:47<55:28,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  52%|█████▏    | 2290/4381 [1:00:47<55:28,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  52%|█████▏    | 2300/4381 [1:00:59<55:10,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  52%|█████▏    | 2300/4381 [1:00:59<55:10,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2310/4381 [1:01:15<54:53,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2310/4381 [1:01:15<54:53,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2320/4381 [1:01:33<54:39,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2320/4381 [1:01:33<54:39,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2330/4381 [1:01:48<54:22,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  53%|█████▎    | 2330/4381 [1:01:48<54:22,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  53%|█████▎    | 2340/4381 [1:02:03<54:06,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  53%|█████▎    | 2340/4381 [1:02:03<54:06,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  54%|█████▎    | 2350/4381 [1:02:21<53:52,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  54%|█████▎    | 2350/4381 [1:02:21<53:52,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  54%|█████▍    | 2360/4381 [1:02:38<53:37,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  54%|█████▍    | 2360/4381 [1:02:38<53:37,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  54%|█████▍    | 2370/4381 [1:02:52<53:19,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  54%|█████▍    | 2370/4381 [1:02:52<53:19,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  54%|█████▍    | 2380/4381 [1:03:09<53:04,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  54%|█████▍    | 2380/4381 [1:03:09<53:04,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  55%|█████▍    | 2390/4381 [1:03:21<52:45,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  55%|█████▍    | 2390/4381 [1:03:21<52:45,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  55%|█████▍    | 2400/4381 [1:03:35<52:27,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  55%|█████▍    | 2400/4381 [1:03:35<52:27,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  55%|█████▌    | 2410/4381 [1:03:52<52:12,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  55%|█████▌    | 2410/4381 [1:03:52<52:12,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  55%|█████▌    | 2420/4381 [1:04:04<51:54,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  55%|█████▌    | 2420/4381 [1:04:04<51:54,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  55%|█████▌    | 2430/4381 [1:04:20<51:38,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  55%|█████▌    | 2430/4381 [1:04:20<51:38,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  56%|█████▌    | 2440/4381 [1:04:38<51:23,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  56%|█████▌    | 2440/4381 [1:04:38<51:23,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  56%|█████▌    | 2450/4381 [1:04:56<51:10,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  56%|█████▌    | 2450/4381 [1:04:56<51:10,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  56%|█████▌    | 2460/4381 [1:05:12<50:53,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  56%|█████▌    | 2460/4381 [1:05:12<50:53,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  56%|█████▋    | 2470/4381 [1:05:28<50:38,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  56%|█████▋    | 2470/4381 [1:05:28<50:38,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  57%|█████▋    | 2480/4381 [1:05:43<50:21,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  57%|█████▋    | 2480/4381 [1:05:43<50:21,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  57%|█████▋    | 2490/4381 [1:05:58<50:05,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  57%|█████▋    | 2490/4381 [1:05:58<50:05,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  57%|█████▋    | 2500/4381 [1:06:20<49:53,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  57%|█████▋    | 2500/4381 [1:06:20<49:53,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  57%|█████▋    | 2510/4381 [1:06:34<49:36,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  57%|█████▋    | 2510/4381 [1:06:34<49:36,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  58%|█████▊    | 2520/4381 [1:06:51<49:21,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  58%|█████▊    | 2520/4381 [1:06:51<49:21,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  58%|█████▊    | 2530/4381 [1:07:08<49:05,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  58%|█████▊    | 2530/4381 [1:07:08<49:05,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  58%|█████▊    | 2540/4381 [1:07:23<48:49,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  58%|█████▊    | 2540/4381 [1:07:23<48:49,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  58%|█████▊    | 2550/4381 [1:07:38<48:32,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  58%|█████▊    | 2550/4381 [1:07:38<48:32,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  58%|█████▊    | 2560/4381 [1:07:56<48:18,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  58%|█████▊    | 2560/4381 [1:07:56<48:18,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  59%|█████▊    | 2570/4381 [1:08:11<48:01,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  59%|█████▊    | 2570/4381 [1:08:11<48:01,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  59%|█████▉    | 2580/4381 [1:08:26<47:45,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  59%|█████▉    | 2580/4381 [1:08:26<47:45,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  59%|█████▉    | 2590/4381 [1:08:43<47:30,  1.59s/it, loss=2.48, v_num=641]Epoch 13:  59%|█████▉    | 2590/4381 [1:08:43<47:30,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  59%|█████▉    | 2600/4381 [1:08:57<47:13,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  59%|█████▉    | 2600/4381 [1:08:57<47:13,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|█████▉    | 2610/4381 [1:09:15<46:58,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|█████▉    | 2610/4381 [1:09:15<46:58,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  60%|█████▉    | 2620/4381 [1:09:33<46:43,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  60%|█████▉    | 2620/4381 [1:09:33<46:43,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|██████    | 2630/4381 [1:09:48<46:27,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|██████    | 2630/4381 [1:09:48<46:27,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|██████    | 2640/4381 [1:10:02<46:10,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  60%|██████    | 2640/4381 [1:10:02<46:10,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  60%|██████    | 2650/4381 [1:10:20<45:55,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  60%|██████    | 2650/4381 [1:10:20<45:55,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  61%|██████    | 2660/4381 [1:10:33<45:37,  1.59s/it, loss=2.52, v_num=641]Epoch 13:  61%|██████    | 2660/4381 [1:10:33<45:37,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  61%|██████    | 2670/4381 [1:10:49<45:21,  1.59s/it, loss=2.55, v_num=641]Epoch 13:  61%|██████    | 2670/4381 [1:10:49<45:21,  1.59s/it, loss=2.5, v_num=641] Epoch 13:  61%|██████    | 2680/4381 [1:11:04<45:05,  1.59s/it, loss=2.5, v_num=641]Epoch 13:  61%|██████    | 2680/4381 [1:11:04<45:05,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  61%|██████▏   | 2690/4381 [1:11:20<44:49,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  61%|██████▏   | 2690/4381 [1:11:20<44:49,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  62%|██████▏   | 2700/4381 [1:11:34<44:32,  1.59s/it, loss=2.51, v_num=641]Epoch 13:  62%|██████▏   | 2700/4381 [1:11:34<44:32,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  62%|██████▏   | 2710/4381 [1:11:51<44:17,  1.59s/it, loss=2.47, v_num=641]Epoch 13:  62%|██████▏   | 2710/4381 [1:11:51<44:17,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  62%|██████▏   | 2720/4381 [1:12:07<44:01,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  62%|██████▏   | 2720/4381 [1:12:07<44:01,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  62%|██████▏   | 2730/4381 [1:12:24<43:46,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  62%|██████▏   | 2730/4381 [1:12:24<43:46,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  63%|██████▎   | 2740/4381 [1:12:39<43:29,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  63%|██████▎   | 2740/4381 [1:12:39<43:29,  1.59s/it, loss=2.45, v_num=641]Epoch 13:  63%|██████▎   | 2750/4381 [1:12:56<43:14,  1.59s/it, loss=2.45, v_num=641]Epoch 13:  63%|██████▎   | 2750/4381 [1:12:56<43:14,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  63%|██████▎   | 2760/4381 [1:13:12<42:59,  1.59s/it, loss=2.49, v_num=641]Epoch 13:  63%|██████▎   | 2760/4381 [1:13:12<42:59,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.53, v_num=641]Epoch 13:  63%|██████▎   | 2770/4381 [1:13:25<42:41,  1.59s/it, loss=2.52, v_num=641]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb01166ff10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb0529ae170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa49b47fb90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa4e6132170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f42f1128910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f433b836170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2a0ebf71d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2a59806170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa044c33d90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa08f01f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f653a1a9dd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f65845e6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f05a0399f50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f05ea64c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=8, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=3, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7eff19a88c10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7eff64738170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
                                                              len(train_dataloader) 30037
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9830508232116699
len(train_dataloader) 30037
Training:   0%|          | 0/4381 [00:00<00:00, 11586.48it/s]validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 30037
Epoch 14:   0%|          | 0/4381 [00:00<00:01, 4156.89it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9908256530761719
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9621212482452393
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9836956858634949
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9261363744735718
len(train_dataloader) 30037
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 30037
Epoch 14:   0%|          | 10/4381 [00:20<2:16:00,  1.87s/it]Epoch 14:   0%|          | 10/4381 [00:20<2:16:01,  1.87s/it, loss=2.42, v_num=642]Epoch 14:   0%|          | 20/4381 [00:36<2:05:58,  1.73s/it, loss=2.42, v_num=642]Epoch 14:   0%|          | 20/4381 [00:36<2:05:58,  1.73s/it, loss=2.46, v_num=642]Epoch 14:   1%|          | 30/4381 [00:51<2:00:38,  1.66s/it, loss=2.46, v_num=642]Epoch 14:   1%|          | 30/4381 [00:51<2:00:39,  1.66s/it, loss=2.45, v_num=642]Epoch 14:   1%|          | 40/4381 [01:08<2:00:49,  1.67s/it, loss=2.45, v_num=642]Epoch 14:   1%|          | 40/4381 [01:08<2:00:49,  1.67s/it, loss=2.43, v_num=642]Epoch 14:   1%|          | 50/4381 [01:23<1:57:37,  1.63s/it, loss=2.43, v_num=642]Epoch 14:   1%|          | 50/4381 [01:23<1:57:37,  1.63s/it, loss=2.44, v_num=642]Epoch 14:   1%|▏         | 60/4381 [01:36<1:54:12,  1.59s/it, loss=2.44, v_num=642]Epoch 14:   1%|▏         | 60/4381 [01:36<1:54:12,  1.59s/it, loss=2.46, v_num=642]Epoch 14:   2%|▏         | 70/4381 [01:57<1:58:33,  1.65s/it, loss=2.46, v_num=642]Epoch 14:   2%|▏         | 70/4381 [01:57<1:58:33,  1.65s/it, loss=2.47, v_num=642]Epoch 14:   2%|▏         | 80/4381 [02:10<1:55:34,  1.61s/it, loss=2.47, v_num=642]Epoch 14:   2%|▏         | 80/4381 [02:10<1:55:34,  1.61s/it, loss=2.44, v_num=642]Epoch 14:   2%|▏         | 90/4381 [02:23<1:52:26,  1.57s/it, loss=2.44, v_num=642]Epoch 14:   2%|▏         | 90/4381 [02:23<1:52:26,  1.57s/it, loss=2.41, v_num=642]Epoch 14:   2%|▏         | 100/4381 [02:38<1:51:50,  1.57s/it, loss=2.41, v_num=642]Epoch 14:   2%|▏         | 100/4381 [02:38<1:51:50,  1.57s/it, loss=2.43, v_num=642]Epoch 14:   3%|▎         | 110/4381 [02:52<1:50:36,  1.55s/it, loss=2.43, v_num=642]Epoch 14:   3%|▎         | 110/4381 [02:52<1:50:36,  1.55s/it, loss=2.43, v_num=642]Epoch 14:   3%|▎         | 120/4381 [03:06<1:49:13,  1.54s/it, loss=2.43, v_num=642]Epoch 14:   3%|▎         | 120/4381 [03:06<1:49:13,  1.54s/it, loss=2.42, v_num=642]Epoch 14:   3%|▎         | 130/4381 [03:21<1:49:01,  1.54s/it, loss=2.42, v_num=642]Epoch 14:   3%|▎         | 130/4381 [03:21<1:49:01,  1.54s/it, loss=2.46, v_num=642]Epoch 14:   3%|▎         | 140/4381 [03:34<1:47:34,  1.52s/it, loss=2.46, v_num=642]Epoch 14:   3%|▎         | 140/4381 [03:34<1:47:34,  1.52s/it, loss=2.48, v_num=642]Epoch 14:   3%|▎         | 150/4381 [03:47<1:46:12,  1.51s/it, loss=2.48, v_num=642]Epoch 14:   3%|▎         | 150/4381 [03:47<1:46:12,  1.51s/it, loss=2.48, v_num=642]Epoch 14:   4%|▎         | 160/4381 [04:03<1:46:33,  1.51s/it, loss=2.48, v_num=642]Epoch 14:   4%|▎         | 160/4381 [04:03<1:46:33,  1.51s/it, loss=2.44, v_num=642]Epoch 14:   4%|▍         | 170/4381 [04:18<1:46:09,  1.51s/it, loss=2.44, v_num=642]Epoch 14:   4%|▍         | 170/4381 [04:18<1:46:09,  1.51s/it, loss=2.42, v_num=642]Epoch 14:   4%|▍         | 180/4381 [04:32<1:45:16,  1.50s/it, loss=2.42, v_num=642]Epoch 14:   4%|▍         | 180/4381 [04:32<1:45:16,  1.50s/it, loss=2.41, v_num=642]Epoch 14:   4%|▍         | 190/4381 [04:50<1:46:09,  1.52s/it, loss=2.41, v_num=642]Epoch 14:   4%|▍         | 190/4381 [04:50<1:46:09,  1.52s/it, loss=2.39, v_num=642]Epoch 14:   5%|▍         | 200/4381 [05:02<1:44:55,  1.51s/it, loss=2.39, v_num=642]Epoch 14:   5%|▍         | 200/4381 [05:02<1:44:55,  1.51s/it, loss=2.4, v_num=642] Epoch 14:   5%|▍         | 210/4381 [05:19<1:45:19,  1.52s/it, loss=2.4, v_num=642]Epoch 14:   5%|▍         | 210/4381 [05:19<1:45:19,  1.52s/it, loss=2.42, v_num=642]Epoch 14:   5%|▌         | 220/4381 [05:34<1:44:53,  1.51s/it, loss=2.42, v_num=642]Epoch 14:   5%|▌         | 220/4381 [05:34<1:44:53,  1.51s/it, loss=2.42, v_num=642]Epoch 14:   5%|▌         | 230/4381 [05:47<1:44:12,  1.51s/it, loss=2.42, v_num=642]Epoch 14:   5%|▌         | 230/4381 [05:47<1:44:12,  1.51s/it, loss=2.4, v_num=642] Epoch 14:   5%|▌         | 240/4381 [06:00<1:43:09,  1.49s/it, loss=2.4, v_num=642]Epoch 14:   5%|▌         | 240/4381 [06:00<1:43:09,  1.49s/it, loss=2.4, v_num=642]Epoch 14:   6%|▌         | 250/4381 [06:18<1:43:55,  1.51s/it, loss=2.4, v_num=642]Epoch 14:   6%|▌         | 250/4381 [06:18<1:43:55,  1.51s/it, loss=2.42, v_num=642]Epoch 14:   6%|▌         | 260/4381 [06:30<1:42:53,  1.50s/it, loss=2.42, v_num=642]Epoch 14:   6%|▌         | 260/4381 [06:30<1:42:53,  1.50s/it, loss=2.44, v_num=642]Epoch 14:   6%|▌         | 270/4381 [06:42<1:41:39,  1.48s/it, loss=2.44, v_num=642]Epoch 14:   6%|▌         | 270/4381 [06:42<1:41:39,  1.48s/it, loss=2.41, v_num=642]Epoch 14:   6%|▋         | 280/4381 [07:00<1:42:13,  1.50s/it, loss=2.41, v_num=642]Epoch 14:   6%|▋         | 280/4381 [07:00<1:42:13,  1.50s/it, loss=2.36, v_num=642]Epoch 14:   7%|▋         | 290/4381 [07:11<1:41:10,  1.48s/it, loss=2.36, v_num=642]Epoch 14:   7%|▋         | 290/4381 [07:11<1:41:10,  1.48s/it, loss=2.35, v_num=642]Epoch 14:   7%|▋         | 300/4381 [07:26<1:40:58,  1.48s/it, loss=2.35, v_num=642]Epoch 14:   7%|▋         | 300/4381 [07:26<1:40:58,  1.48s/it, loss=2.35, v_num=642]Epoch 14:   7%|▋         | 310/4381 [07:43<1:41:10,  1.49s/it, loss=2.35, v_num=642]Epoch 14:   7%|▋         | 310/4381 [07:43<1:41:10,  1.49s/it, loss=2.4, v_num=642] Epoch 14:   7%|▋         | 320/4381 [07:56<1:40:34,  1.49s/it, loss=2.4, v_num=642]Epoch 14:   7%|▋         | 320/4381 [07:56<1:40:34,  1.49s/it, loss=2.38, v_num=642]Epoch 14:   8%|▊         | 330/4381 [08:12<1:40:29,  1.49s/it, loss=2.38, v_num=642]Epoch 14:   8%|▊         | 330/4381 [08:12<1:40:29,  1.49s/it, loss=2.35, v_num=642]Epoch 14:   8%|▊         | 340/4381 [08:27<1:40:18,  1.49s/it, loss=2.35, v_num=642]Epoch 14:   8%|▊         | 340/4381 [08:27<1:40:18,  1.49s/it, loss=2.41, v_num=642]Epoch 14:   8%|▊         | 350/4381 [08:43<1:40:07,  1.49s/it, loss=2.41, v_num=642]Epoch 14:   8%|▊         | 350/4381 [08:43<1:40:07,  1.49s/it, loss=2.43, v_num=642]Epoch 14:   8%|▊         | 360/4381 [08:56<1:39:41,  1.49s/it, loss=2.43, v_num=642]Epoch 14:   8%|▊         | 360/4381 [08:56<1:39:41,  1.49s/it, loss=2.41, v_num=642]Epoch 14:   8%|▊         | 370/4381 [09:11<1:39:22,  1.49s/it, loss=2.41, v_num=642]Epoch 14:   8%|▊         | 370/4381 [09:11<1:39:22,  1.49s/it, loss=2.43, v_num=642]Epoch 14:   9%|▊         | 380/4381 [09:25<1:38:55,  1.48s/it, loss=2.43, v_num=642]Epoch 14:   9%|▊         | 380/4381 [09:25<1:38:55,  1.48s/it, loss=2.44, v_num=642]Epoch 14:   9%|▉         | 390/4381 [09:40<1:38:42,  1.48s/it, loss=2.44, v_num=642]Epoch 14:   9%|▉         | 390/4381 [09:40<1:38:42,  1.48s/it, loss=2.43, v_num=642]Epoch 14:   9%|▉         | 400/4381 [09:56<1:38:44,  1.49s/it, loss=2.43, v_num=642]Epoch 14:   9%|▉         | 400/4381 [09:56<1:38:44,  1.49s/it, loss=2.43, v_num=642]Epoch 14:   9%|▉         | 410/4381 [10:09<1:38:09,  1.48s/it, loss=2.43, v_num=642]Epoch 14:   9%|▉         | 410/4381 [10:09<1:38:09,  1.48s/it, loss=2.44, v_num=642]Epoch 14:  10%|▉         | 420/4381 [10:24<1:37:56,  1.48s/it, loss=2.44, v_num=642]Epoch 14:  10%|▉         | 420/4381 [10:24<1:37:56,  1.48s/it, loss=2.41, v_num=642]Epoch 14:  10%|▉         | 430/4381 [10:37<1:37:27,  1.48s/it, loss=2.41, v_num=642]Epoch 14:  10%|▉         | 430/4381 [10:37<1:37:27,  1.48s/it, loss=2.39, v_num=642]Epoch 14:  10%|█         | 440/4381 [10:52<1:37:10,  1.48s/it, loss=2.39, v_num=642]Epoch 14:  10%|█         | 440/4381 [10:52<1:37:10,  1.48s/it, loss=2.42, v_num=642]Epoch 14:  10%|█         | 450/4381 [11:09<1:37:12,  1.48s/it, loss=2.42, v_num=642]Epoch 14:  10%|█         | 450/4381 [11:09<1:37:12,  1.48s/it, loss=2.43, v_num=642]Epoch 14:  10%|█         | 460/4381 [11:21<1:36:33,  1.48s/it, loss=2.43, v_num=642]Epoch 14:  10%|█         | 460/4381 [11:21<1:36:33,  1.48s/it, loss=2.36, v_num=642]Epoch 14:  11%|█         | 470/4381 [11:33<1:35:58,  1.47s/it, loss=2.36, v_num=642]Epoch 14:  11%|█         | 470/4381 [11:33<1:35:58,  1.47s/it, loss=2.34, v_num=642]Epoch 14:  11%|█         | 480/4381 [11:50<1:36:03,  1.48s/it, loss=2.34, v_num=642]Epoch 14:  11%|█         | 480/4381 [11:50<1:36:03,  1.48s/it, loss=2.38, v_num=642]Epoch 14:  11%|█         | 490/4381 [12:02<1:35:25,  1.47s/it, loss=2.38, v_num=642]Epoch 14:  11%|█         | 490/4381 [12:02<1:35:25,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  11%|█▏        | 500/4381 [12:18<1:35:19,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  11%|█▏        | 500/4381 [12:18<1:35:19,  1.47s/it, loss=2.42, v_num=642]Epoch 14:  12%|█▏        | 510/4381 [12:32<1:35:03,  1.47s/it, loss=2.42, v_num=642]Epoch 14:  12%|█▏        | 510/4381 [12:32<1:35:03,  1.47s/it, loss=2.39, v_num=642]Epoch 14:  12%|█▏        | 520/4381 [12:47<1:34:47,  1.47s/it, loss=2.39, v_num=642]Epoch 14:  12%|█▏        | 520/4381 [12:47<1:34:47,  1.47s/it, loss=2.38, v_num=642]Epoch 14:  12%|█▏        | 530/4381 [12:59<1:34:14,  1.47s/it, loss=2.38, v_num=642]Epoch 14:  12%|█▏        | 530/4381 [12:59<1:34:14,  1.47s/it, loss=2.33, v_num=642]Epoch 14:  12%|█▏        | 540/4381 [13:14<1:34:01,  1.47s/it, loss=2.33, v_num=642]Epoch 14:  12%|█▏        | 540/4381 [13:14<1:34:01,  1.47s/it, loss=2.37, v_num=642]Epoch 14:  13%|█▎        | 550/4381 [13:27<1:33:33,  1.47s/it, loss=2.37, v_num=642]Epoch 14:  13%|█▎        | 550/4381 [13:27<1:33:33,  1.47s/it, loss=2.38, v_num=642]Epoch 14:  13%|█▎        | 560/4381 [13:41<1:33:15,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  13%|█▎        | 560/4381 [13:41<1:33:15,  1.46s/it, loss=2.34, v_num=642]Epoch 14:  13%|█▎        | 570/4381 [13:58<1:33:16,  1.47s/it, loss=2.34, v_num=642]Epoch 14:  13%|█▎        | 570/4381 [13:58<1:33:16,  1.47s/it, loss=2.39, v_num=642]Epoch 14:  13%|█▎        | 580/4381 [14:12<1:32:56,  1.47s/it, loss=2.39, v_num=642]Epoch 14:  13%|█▎        | 580/4381 [14:12<1:32:56,  1.47s/it, loss=2.41, v_num=642]Epoch 14:  13%|█▎        | 590/4381 [14:27<1:32:45,  1.47s/it, loss=2.41, v_num=642]Epoch 14:  13%|█▎        | 590/4381 [14:27<1:32:45,  1.47s/it, loss=2.42, v_num=642]Epoch 14:  14%|█▎        | 600/4381 [14:44<1:32:44,  1.47s/it, loss=2.42, v_num=642]Epoch 14:  14%|█▎        | 600/4381 [14:44<1:32:44,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  14%|█▍        | 610/4381 [14:57<1:32:16,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  14%|█▍        | 610/4381 [14:57<1:32:16,  1.47s/it, loss=2.42, v_num=642]Epoch 14:  14%|█▍        | 620/4381 [15:08<1:31:42,  1.46s/it, loss=2.42, v_num=642]Epoch 14:  14%|█▍        | 620/4381 [15:08<1:31:42,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  14%|█▍        | 630/4381 [15:24<1:31:38,  1.47s/it, loss=2.44, v_num=642]Epoch 14:  14%|█▍        | 630/4381 [15:24<1:31:38,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  15%|█▍        | 640/4381 [15:40<1:31:28,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  15%|█▍        | 640/4381 [15:40<1:31:28,  1.47s/it, loss=2.38, v_num=642]Epoch 14:  15%|█▍        | 650/4381 [15:52<1:30:56,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  15%|█▍        | 650/4381 [15:52<1:30:56,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  15%|█▌        | 660/4381 [16:07<1:30:47,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  15%|█▌        | 660/4381 [16:07<1:30:47,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  15%|█▌        | 670/4381 [16:22<1:30:31,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  15%|█▌        | 670/4381 [16:22<1:30:31,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  16%|█▌        | 680/4381 [16:37<1:30:21,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  16%|█▌        | 680/4381 [16:37<1:30:21,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  16%|█▌        | 690/4381 [16:54<1:30:18,  1.47s/it, loss=2.41, v_num=642]Epoch 14:  16%|█▌        | 690/4381 [16:54<1:30:18,  1.47s/it, loss=2.45, v_num=642]Epoch 14:  16%|█▌        | 700/4381 [17:07<1:29:54,  1.47s/it, loss=2.45, v_num=642]Epoch 14:  16%|█▌        | 700/4381 [17:07<1:29:54,  1.47s/it, loss=2.46, v_num=642]Epoch 14:  16%|█▌        | 710/4381 [17:21<1:29:36,  1.46s/it, loss=2.46, v_num=642]Epoch 14:  16%|█▌        | 710/4381 [17:21<1:29:36,  1.46s/it, loss=2.43, v_num=642]Epoch 14:  16%|█▋        | 720/4381 [17:36<1:29:26,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  16%|█▋        | 720/4381 [17:36<1:29:26,  1.47s/it, loss=2.43, v_num=642]Epoch 14:  17%|█▋        | 730/4381 [17:49<1:29:03,  1.46s/it, loss=2.43, v_num=642]Epoch 14:  17%|█▋        | 730/4381 [17:49<1:29:03,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  17%|█▋        | 740/4381 [18:03<1:28:43,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  17%|█▋        | 740/4381 [18:03<1:28:43,  1.46s/it, loss=2.4, v_num=642] Epoch 14:  17%|█▋        | 750/4381 [18:18<1:28:33,  1.46s/it, loss=2.4, v_num=642]Epoch 14:  17%|█▋        | 750/4381 [18:18<1:28:33,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  17%|█▋        | 760/4381 [18:31<1:28:10,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  17%|█▋        | 760/4381 [18:31<1:28:10,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  18%|█▊        | 770/4381 [18:44<1:27:45,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  18%|█▊        | 770/4381 [18:44<1:27:45,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  18%|█▊        | 780/4381 [19:00<1:27:37,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  18%|█▊        | 780/4381 [19:00<1:27:37,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  18%|█▊        | 790/4381 [19:12<1:27:12,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  18%|█▊        | 790/4381 [19:12<1:27:12,  1.46s/it, loss=2.42, v_num=642]Epoch 14:  18%|█▊        | 800/4381 [19:26<1:26:56,  1.46s/it, loss=2.42, v_num=642]Epoch 14:  18%|█▊        | 800/4381 [19:26<1:26:56,  1.46s/it, loss=2.4, v_num=642] Epoch 14:  18%|█▊        | 810/4381 [19:43<1:26:50,  1.46s/it, loss=2.4, v_num=642]Epoch 14:  18%|█▊        | 810/4381 [19:43<1:26:50,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  19%|█▊        | 820/4381 [19:57<1:26:32,  1.46s/it, loss=2.38, v_num=642]Epoch 14:  19%|█▊        | 820/4381 [19:57<1:26:32,  1.46s/it, loss=2.42, v_num=642]Epoch 14:  19%|█▉        | 830/4381 [20:10<1:26:11,  1.46s/it, loss=2.42, v_num=642]Epoch 14:  19%|█▉        | 830/4381 [20:10<1:26:11,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  19%|█▉        | 840/4381 [20:26<1:26:03,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  19%|█▉        | 840/4381 [20:26<1:26:03,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  19%|█▉        | 850/4381 [20:41<1:25:49,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  19%|█▉        | 850/4381 [20:41<1:25:49,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  20%|█▉        | 860/4381 [20:54<1:25:30,  1.46s/it, loss=2.37, v_num=642]Epoch 14:  20%|█▉        | 860/4381 [20:54<1:25:30,  1.46s/it, loss=2.36, v_num=642]Epoch 14:  20%|█▉        | 870/4381 [21:08<1:25:12,  1.46s/it, loss=2.36, v_num=642]Epoch 14:  20%|█▉        | 870/4381 [21:08<1:25:12,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  20%|██        | 880/4381 [21:23<1:25:01,  1.46s/it, loss=2.39, v_num=642]Epoch 14:  20%|██        | 880/4381 [21:23<1:25:01,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  20%|██        | 890/4381 [21:37<1:24:42,  1.46s/it, loss=2.41, v_num=642]Epoch 14:  20%|██        | 890/4381 [21:37<1:24:42,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  21%|██        | 900/4381 [21:53<1:24:33,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  21%|██        | 900/4381 [21:53<1:24:33,  1.46s/it, loss=2.44, v_num=642]Epoch 14:  21%|██        | 910/4381 [22:04<1:24:05,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  21%|██        | 910/4381 [22:04<1:24:05,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  21%|██        | 920/4381 [22:17<1:23:47,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  21%|██        | 920/4381 [22:17<1:23:47,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  21%|██        | 930/4381 [22:31<1:23:31,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  21%|██        | 930/4381 [22:31<1:23:31,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  21%|██▏       | 940/4381 [22:46<1:23:17,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  21%|██▏       | 940/4381 [22:46<1:23:17,  1.45s/it, loss=2.37, v_num=642]Epoch 14:  22%|██▏       | 950/4381 [23:01<1:23:04,  1.45s/it, loss=2.37, v_num=642]Epoch 14:  22%|██▏       | 950/4381 [23:01<1:23:04,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  22%|██▏       | 960/4381 [23:17<1:22:55,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  22%|██▏       | 960/4381 [23:17<1:22:55,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  22%|██▏       | 970/4381 [23:31<1:22:37,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  22%|██▏       | 970/4381 [23:31<1:22:37,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  22%|██▏       | 980/4381 [23:45<1:22:21,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  22%|██▏       | 980/4381 [23:45<1:22:21,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  23%|██▎       | 990/4381 [24:00<1:22:07,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  23%|██▎       | 990/4381 [24:00<1:22:07,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  23%|██▎       | 1000/4381 [24:14<1:21:54,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  23%|██▎       | 1000/4381 [24:14<1:21:54,  1.45s/it, loss=2.47, v_num=642]Epoch 14:  23%|██▎       | 1010/4381 [24:26<1:21:29,  1.45s/it, loss=2.47, v_num=642]Epoch 14:  23%|██▎       | 1010/4381 [24:26<1:21:29,  1.45s/it, loss=2.47, v_num=642]Epoch 14:  23%|██▎       | 1020/4381 [24:39<1:21:11,  1.45s/it, loss=2.47, v_num=642]Epoch 14:  23%|██▎       | 1020/4381 [24:39<1:21:11,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  24%|██▎       | 1030/4381 [24:53<1:20:54,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  24%|██▎       | 1030/4381 [24:53<1:20:54,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  24%|██▎       | 1040/4381 [25:06<1:20:36,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  24%|██▎       | 1040/4381 [25:06<1:20:36,  1.45s/it, loss=2.37, v_num=642]Epoch 14:  24%|██▍       | 1050/4381 [25:23<1:20:28,  1.45s/it, loss=2.37, v_num=642]Epoch 14:  24%|██▍       | 1050/4381 [25:23<1:20:28,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  24%|██▍       | 1060/4381 [25:36<1:20:08,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  24%|██▍       | 1060/4381 [25:36<1:20:08,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  24%|██▍       | 1070/4381 [25:50<1:19:52,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  24%|██▍       | 1070/4381 [25:50<1:19:52,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  25%|██▍       | 1080/4381 [26:04<1:19:36,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  25%|██▍       | 1080/4381 [26:04<1:19:36,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  25%|██▍       | 1090/4381 [26:19<1:19:23,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  25%|██▍       | 1090/4381 [26:19<1:19:23,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  25%|██▌       | 1100/4381 [26:34<1:19:13,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  25%|██▌       | 1100/4381 [26:34<1:19:13,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  25%|██▌       | 1110/4381 [26:49<1:18:57,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  25%|██▌       | 1110/4381 [26:49<1:18:57,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  26%|██▌       | 1120/4381 [27:05<1:18:47,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  26%|██▌       | 1120/4381 [27:05<1:18:47,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  26%|██▌       | 1130/4381 [27:17<1:18:28,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  26%|██▌       | 1130/4381 [27:17<1:18:28,  1.45s/it, loss=2.35, v_num=642]Epoch 14:  26%|██▌       | 1140/4381 [27:33<1:18:15,  1.45s/it, loss=2.35, v_num=642]Epoch 14:  26%|██▌       | 1140/4381 [27:33<1:18:15,  1.45s/it, loss=2.35, v_num=642]Epoch 14:  26%|██▌       | 1150/4381 [27:48<1:18:04,  1.45s/it, loss=2.35, v_num=642]Epoch 14:  26%|██▌       | 1150/4381 [27:48<1:18:04,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  26%|██▋       | 1160/4381 [28:00<1:17:41,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  26%|██▋       | 1160/4381 [28:00<1:17:41,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  27%|██▋       | 1170/4381 [28:14<1:17:27,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  27%|██▋       | 1170/4381 [28:14<1:17:27,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  27%|██▋       | 1180/4381 [28:31<1:17:19,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  27%|██▋       | 1180/4381 [28:31<1:17:19,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  27%|██▋       | 1190/4381 [28:45<1:17:02,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  27%|██▋       | 1190/4381 [28:45<1:17:02,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  27%|██▋       | 1200/4381 [29:00<1:16:50,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  27%|██▋       | 1200/4381 [29:00<1:16:50,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  28%|██▊       | 1210/4381 [29:16<1:16:39,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  28%|██▊       | 1210/4381 [29:16<1:16:39,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  28%|██▊       | 1220/4381 [29:30<1:16:23,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  28%|██▊       | 1220/4381 [29:30<1:16:23,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  28%|██▊       | 1230/4381 [29:44<1:16:08,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  28%|██▊       | 1230/4381 [29:44<1:16:08,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  28%|██▊       | 1240/4381 [30:00<1:15:56,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  28%|██▊       | 1240/4381 [30:00<1:15:56,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  29%|██▊       | 1250/4381 [30:12<1:15:36,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  29%|██▊       | 1250/4381 [30:12<1:15:36,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  29%|██▉       | 1260/4381 [30:24<1:15:15,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  29%|██▉       | 1260/4381 [30:24<1:15:15,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  29%|██▉       | 1270/4381 [30:42<1:15:08,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  29%|██▉       | 1270/4381 [30:42<1:15:08,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  29%|██▉       | 1280/4381 [30:56<1:14:53,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  29%|██▉       | 1280/4381 [30:56<1:14:53,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  29%|██▉       | 1290/4381 [31:07<1:14:30,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  29%|██▉       | 1290/4381 [31:07<1:14:30,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  30%|██▉       | 1300/4381 [31:22<1:14:19,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  30%|██▉       | 1300/4381 [31:22<1:14:19,  1.45s/it, loss=2.45, v_num=642]Epoch 14:  30%|██▉       | 1310/4381 [31:33<1:13:56,  1.44s/it, loss=2.45, v_num=642]Epoch 14:  30%|██▉       | 1310/4381 [31:33<1:13:56,  1.44s/it, loss=2.37, v_num=642]Epoch 14:  30%|███       | 1320/4381 [31:48<1:13:41,  1.44s/it, loss=2.37, v_num=642]Epoch 14:  30%|███       | 1320/4381 [31:48<1:13:41,  1.44s/it, loss=2.4, v_num=642] Epoch 14:  30%|███       | 1330/4381 [32:06<1:13:35,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  30%|███       | 1330/4381 [32:06<1:13:35,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  31%|███       | 1340/4381 [32:19<1:13:17,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  31%|███       | 1340/4381 [32:19<1:13:17,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  31%|███       | 1350/4381 [32:33<1:13:02,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  31%|███       | 1350/4381 [32:33<1:13:02,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  31%|███       | 1360/4381 [32:50<1:12:54,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  31%|███       | 1360/4381 [32:50<1:12:54,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  31%|███▏      | 1370/4381 [33:05<1:12:41,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  31%|███▏      | 1370/4381 [33:05<1:12:41,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  31%|███▏      | 1380/4381 [33:16<1:12:17,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  31%|███▏      | 1380/4381 [33:16<1:12:17,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  32%|███▏      | 1390/4381 [33:34<1:12:12,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  32%|███▏      | 1390/4381 [33:34<1:12:12,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  32%|███▏      | 1400/4381 [33:48<1:11:57,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  32%|███▏      | 1400/4381 [33:48<1:11:57,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  32%|███▏      | 1410/4381 [34:01<1:11:39,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  32%|███▏      | 1410/4381 [34:01<1:11:39,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  32%|███▏      | 1420/4381 [34:18<1:11:30,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  32%|███▏      | 1420/4381 [34:18<1:11:30,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  33%|███▎      | 1430/4381 [34:33<1:11:16,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  33%|███▎      | 1430/4381 [34:33<1:11:16,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  33%|███▎      | 1440/4381 [34:45<1:10:57,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  33%|███▎      | 1440/4381 [34:45<1:10:57,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  33%|███▎      | 1450/4381 [35:01<1:10:44,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  33%|███▎      | 1450/4381 [35:01<1:10:44,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  33%|███▎      | 1460/4381 [35:15<1:10:28,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  33%|███▎      | 1460/4381 [35:15<1:10:28,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  34%|███▎      | 1470/4381 [35:28<1:10:11,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  34%|███▎      | 1470/4381 [35:28<1:10:11,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  34%|███▍      | 1480/4381 [35:43<1:09:59,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  34%|███▍      | 1480/4381 [35:43<1:09:59,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  34%|███▍      | 1490/4381 [35:57<1:09:43,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  34%|███▍      | 1490/4381 [35:57<1:09:43,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  34%|███▍      | 1500/4381 [36:11<1:09:28,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  34%|███▍      | 1500/4381 [36:11<1:09:28,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  34%|███▍      | 1510/4381 [36:25<1:09:12,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  34%|███▍      | 1510/4381 [36:25<1:09:12,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  35%|███▍      | 1520/4381 [36:39<1:08:57,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  35%|███▍      | 1520/4381 [36:39<1:08:57,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  35%|███▍      | 1530/4381 [36:52<1:08:40,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  35%|███▍      | 1530/4381 [36:52<1:08:40,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  35%|███▌      | 1540/4381 [37:08<1:08:27,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  35%|███▌      | 1540/4381 [37:08<1:08:27,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  35%|███▌      | 1550/4381 [37:22<1:08:12,  1.45s/it, loss=2.44, v_num=642]Epoch 14:  35%|███▌      | 1550/4381 [37:22<1:08:12,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  36%|███▌      | 1560/4381 [37:35<1:07:56,  1.45s/it, loss=2.4, v_num=642]Epoch 14:  36%|███▌      | 1560/4381 [37:35<1:07:56,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  36%|███▌      | 1570/4381 [37:52<1:07:46,  1.45s/it, loss=2.38, v_num=642]Epoch 14:  36%|███▌      | 1570/4381 [37:52<1:07:46,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  36%|███▌      | 1580/4381 [38:05<1:07:29,  1.45s/it, loss=2.39, v_num=642]Epoch 14:  36%|███▌      | 1580/4381 [38:05<1:07:29,  1.45s/it, loss=2.4, v_num=642] Epoch 14:  36%|███▋      | 1590/4381 [38:18<1:07:12,  1.44s/it, loss=2.4, v_num=642]Epoch 14:  36%|███▋      | 1590/4381 [38:18<1:07:12,  1.44s/it, loss=2.42, v_num=642]Epoch 14:  37%|███▋      | 1600/4381 [38:35<1:07:02,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  37%|███▋      | 1600/4381 [38:35<1:07:02,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  37%|███▋      | 1610/4381 [38:47<1:06:42,  1.44s/it, loss=2.43, v_num=642]Epoch 14:  37%|███▋      | 1610/4381 [38:47<1:06:42,  1.44s/it, loss=2.41, v_num=642]Epoch 14:  37%|███▋      | 1620/4381 [38:58<1:06:23,  1.44s/it, loss=2.41, v_num=642]Epoch 14:  37%|███▋      | 1620/4381 [38:58<1:06:23,  1.44s/it, loss=2.42, v_num=642]Epoch 14:  37%|███▋      | 1630/4381 [39:17<1:06:16,  1.45s/it, loss=2.42, v_num=642]Epoch 14:  37%|███▋      | 1630/4381 [39:17<1:06:16,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  37%|███▋      | 1640/4381 [39:33<1:06:03,  1.45s/it, loss=2.43, v_num=642]Epoch 14:  37%|███▋      | 1640/4381 [39:33<1:06:03,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  38%|███▊      | 1650/4381 [39:46<1:05:47,  1.45s/it, loss=2.41, v_num=642]Epoch 14:  38%|███▊      | 1650/4381 [39:46<1:05:47,  1.45s/it, loss=2.45, v_num=642]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f92d5f80250>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f932025b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fbe5c025c10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fbea6a50170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0f02a58990>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0f4d522170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6d167adf50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6d6127c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7faf950e0890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fafd9f98170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f51f04a9e90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f5235385170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
                                                              len(train_dataloader) 30037
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9620253443717957
len(train_dataloader) 30037
Training:   0%|          | 0/5842 [00:00<00:00, 24244.53it/s]Epoch 15:   0%|          | 0/5842 [00:00<00:00, 5991.86it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9895833730697632
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9144737124443054
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9928571581840515
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.985714316368103
len(train_dataloader) 30037
Epoch 15:   0%|          | 10/5842 [00:22<3:17:59,  2.04s/it]Epoch 15:   0%|          | 10/5842 [00:22<3:17:59,  2.04s/it, loss=1.82, v_num=643]Epoch 15:   0%|          | 20/5842 [00:32<2:29:09,  1.54s/it, loss=1.82, v_num=643]Epoch 15:   0%|          | 20/5842 [00:32<2:29:09,  1.54s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 30/5842 [00:43<2:14:51,  1.39s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 30/5842 [00:43<2:14:51,  1.39s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 40/5842 [00:55<2:11:35,  1.36s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 40/5842 [00:55<2:11:43,  1.36s/it, loss=1.82, v_num=643]Epoch 15:   1%|          | 50/5842 [01:08<2:09:35,  1.34s/it, loss=1.82, v_num=643]Epoch 15:   1%|          | 50/5842 [01:08<2:09:35,  1.34s/it, loss=1.78, v_num=643]Epoch 15:   1%|          | 60/5842 [01:19<2:06:04,  1.31s/it, loss=1.78, v_num=643]Epoch 15:   1%|          | 60/5842 [01:19<2:06:04,  1.31s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 70/5842 [01:29<2:01:07,  1.26s/it, loss=1.79, v_num=643]Epoch 15:   1%|          | 70/5842 [01:29<2:01:07,  1.26s/it, loss=1.81, v_num=643]Epoch 15:   1%|▏         | 80/5842 [01:41<1:59:57,  1.25s/it, loss=1.81, v_num=643]Epoch 15:   1%|▏         | 80/5842 [01:41<1:59:57,  1.25s/it, loss=1.78, v_num=643]Epoch 15:   2%|▏         | 90/5842 [01:51<1:57:57,  1.23s/it, loss=1.78, v_num=643]Epoch 15:   2%|▏         | 90/5842 [01:51<1:57:57,  1.23s/it, loss=1.76, v_num=643]Epoch 15:   2%|▏         | 100/5842 [02:03<1:56:37,  1.22s/it, loss=1.76, v_num=643]Epoch 15:   2%|▏         | 100/5842 [02:03<1:56:37,  1.22s/it, loss=1.76, v_num=643]Epoch 15:   2%|▏         | 110/5842 [02:15<1:56:26,  1.22s/it, loss=1.76, v_num=643]Epoch 15:   2%|▏         | 110/5842 [02:15<1:56:26,  1.22s/it, loss=1.79, v_num=643]Epoch 15:   2%|▏         | 120/5842 [02:27<1:55:59,  1.22s/it, loss=1.79, v_num=643]Epoch 15:   2%|▏         | 120/5842 [02:27<1:55:59,  1.22s/it, loss=1.81, v_num=643]Epoch 15:   2%|▏         | 130/5842 [02:37<1:54:30,  1.20s/it, loss=1.81, v_num=643]Epoch 15:   2%|▏         | 130/5842 [02:37<1:54:30,  1.20s/it, loss=1.82, v_num=643]Epoch 15:   2%|▏         | 140/5842 [02:49<1:54:05,  1.20s/it, loss=1.82, v_num=643]Epoch 15:   2%|▏         | 140/5842 [02:49<1:54:05,  1.20s/it, loss=1.82, v_num=643]Epoch 15:   3%|▎         | 150/5842 [02:59<1:52:45,  1.19s/it, loss=1.82, v_num=643]Epoch 15:   3%|▎         | 150/5842 [02:59<1:52:45,  1.19s/it, loss=1.79, v_num=643]Epoch 15:   3%|▎         | 160/5842 [03:11<1:52:30,  1.19s/it, loss=1.79, v_num=643]Epoch 15:   3%|▎         | 160/5842 [03:11<1:52:30,  1.19s/it, loss=1.78, v_num=643]Epoch 15:   3%|▎         | 170/5842 [03:21<1:51:12,  1.18s/it, loss=1.78, v_num=643]Epoch 15:   3%|▎         | 170/5842 [03:21<1:51:12,  1.18s/it, loss=1.77, v_num=643]Epoch 15:   3%|▎         | 180/5842 [03:33<1:51:06,  1.18s/it, loss=1.77, v_num=643]Epoch 15:   3%|▎         | 180/5842 [03:33<1:51:06,  1.18s/it, loss=1.79, v_num=643]Epoch 15:   3%|▎         | 190/5842 [03:43<1:50:10,  1.17s/it, loss=1.79, v_num=643]Epoch 15:   3%|▎         | 190/5842 [03:43<1:50:10,  1.17s/it, loss=1.78, v_num=643]Epoch 15:   3%|▎         | 200/5842 [03:53<1:49:21,  1.16s/it, loss=1.78, v_num=643]Epoch 15:   3%|▎         | 200/5842 [03:53<1:49:21,  1.16s/it, loss=1.81, v_num=643]Epoch 15:   4%|▎         | 210/5842 [04:03<1:48:29,  1.16s/it, loss=1.81, v_num=643]Epoch 15:   4%|▎         | 210/5842 [04:03<1:48:29,  1.16s/it, loss=1.83, v_num=643]Epoch 15:   4%|▍         | 220/5842 [04:16<1:48:34,  1.16s/it, loss=1.83, v_num=643]Epoch 15:   4%|▍         | 220/5842 [04:16<1:48:34,  1.16s/it, loss=1.81, v_num=643]Epoch 15:   4%|▍         | 230/5842 [04:25<1:47:19,  1.15s/it, loss=1.81, v_num=643]Epoch 15:   4%|▍         | 230/5842 [04:25<1:47:19,  1.15s/it, loss=1.81, v_num=643]Epoch 15:   4%|▍         | 240/5842 [04:35<1:46:41,  1.14s/it, loss=1.81, v_num=643]Epoch 15:   4%|▍         | 240/5842 [04:35<1:46:41,  1.14s/it, loss=1.84, v_num=643]Epoch 15:   4%|▍         | 250/5842 [04:48<1:47:02,  1.15s/it, loss=1.84, v_num=643]Epoch 15:   4%|▍         | 250/5842 [04:48<1:47:02,  1.15s/it, loss=1.82, v_num=643]Epoch 15:   4%|▍         | 260/5842 [04:57<1:45:53,  1.14s/it, loss=1.82, v_num=643]Epoch 15:   4%|▍         | 260/5842 [04:57<1:45:53,  1.14s/it, loss=1.77, v_num=643]Epoch 15:   5%|▍         | 270/5842 [05:09<1:45:55,  1.14s/it, loss=1.77, v_num=643]Epoch 15:   5%|▍         | 270/5842 [05:09<1:45:56,  1.14s/it, loss=1.79, v_num=643]Epoch 15:   5%|▍         | 280/5842 [05:18<1:45:01,  1.13s/it, loss=1.79, v_num=643]Epoch 15:   5%|▍         | 280/5842 [05:18<1:45:01,  1.13s/it, loss=1.84, v_num=643]Epoch 15:   5%|▍         | 290/5842 [05:27<1:44:15,  1.13s/it, loss=1.84, v_num=643]Epoch 15:   5%|▍         | 290/5842 [05:27<1:44:15,  1.13s/it, loss=1.84, v_num=643]o_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f64fc0f2e50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f653ea2b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Epoch 15:   5%|▌         | 300/5842 [05:36<1:43:18,  1.12s/it, loss=1.84, v_num=643]Epoch 15:   5%|▌         | 300/5842 [05:36<1:43:19,  1.12s/it, loss=1.83, v_num=643]Epoch 15:   5%|▌         | 310/5842 [05:46<1:42:44,  1.11s/it, loss=1.83, v_num=643]Epoch 15:   5%|▌         | 310/5842 [05:46<1:42:44,  1.11s/it, loss=1.83, v_num=643]o_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f00c97f96d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0114459170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Epoch 15:   5%|▌         | 320/5842 [05:56<1:42:12,  1.11s/it, loss=1.83, v_num=643]Epoch 15:   5%|▌         | 320/5842 [05:56<1:42:12,  1.11s/it, loss=1.8, v_num=643] Epoch 15:   6%|▌         | 330/5842 [06:06<1:41:47,  1.11s/it, loss=1.8, v_num=643]Epoch 15:   6%|▌         | 330/5842 [06:06<1:41:48,  1.11s/it, loss=1.79, v_num=643]to_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fe686372850>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe6d0fac170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Epoch 15:   6%|▌         | 340/5842 [06:16<1:41:17,  1.10s/it, loss=1.79, v_num=643]Epoch 15:   6%|▌         | 340/5842 [06:16<1:41:17,  1.10s/it, loss=1.83, v_num=643]Epoch 15:   6%|▌         | 340/5842 [06:26<1:44:00,  1.13s/it, loss=1.83, v_num=643]Epoch 15:   6%|▌         | 350/5842 [06:27<1:41:10,  1.11s/it, loss=1.83, v_num=643]Epoch 15:   6%|▌         | 350/5842 [06:27<1:41:10,  1.11s/it, loss=1.8, v_num=643] Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f071cc0b590>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0761c5f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Epoch 15:   6%|▌         | 360/5842 [06:38<1:40:54,  1.10s/it, loss=1.8, v_num=643]Epoch 15:   6%|▌         | 360/5842 [06:38<1:40:54,  1.10s/it, loss=1.77, v_num=643]Epoch 15:   6%|▋         | 370/5842 [06:48<1:40:22,  1.10s/it, loss=1.77, v_num=643]Epoch 15:   6%|▋         | 370/5842 [06:48<1:40:22,  1.10s/it, loss=1.77, v_num=643]Epoch 15:   7%|▋         | 380/5842 [07:00<1:40:26,  1.10s/it, loss=1.77, v_num=643]Epoch 15:   7%|▋         | 380/5842 [07:00<1:40:27,  1.10s/it, loss=1.79, v_num=643]Epoch 15:   7%|▋         | 390/5842 [07:11<1:40:10,  1.10s/it, loss=1.79, v_num=643]Epoch 15:   7%|▋         | 390/5842 [07:11<1:40:10,  1.10s/it, loss=1.8, v_num=643] Epoch 15:   7%|▋         | 400/5842 [07:21<1:39:54,  1.10s/it, loss=1.8, v_num=643]Epoch 15:   7%|▋         | 400/5842 [07:21<1:39:54,  1.10s/it, loss=1.78, v_num=643]Epoch 15:   7%|▋         | 410/5842 [07:32<1:39:42,  1.10s/it, loss=1.78, v_num=643]Epoch 15:   7%|▋         | 410/5842 [07:32<1:39:42,  1.10s/it, loss=1.78, v_num=643]o_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa25f770390>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa2a0a4b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Epoch 15:   7%|▋         | 420/5842 [07:41<1:38:57,  1.10s/it, loss=1.78, v_num=643]Epoch 15:   7%|▋         | 420/5842 [07:41<1:38:57,  1.10s/it, loss=1.81, v_num=643]Epoch 15:   7%|▋         | 430/5842 [07:51<1:38:41,  1.09s/it, loss=1.81, v_num=643]Epoch 15:   7%|▋         | 430/5842 [07:51<1:38:42,  1.09s/it, loss=1.85, v_num=643]Epoch 15:   8%|▊         | 440/5842 [08:01<1:38:22,  1.09s/it, loss=1.85, v_num=643]Epoch 15:   8%|▊         | 440/5842 [08:01<1:38:22,  1.09s/it, loss=1.86, v_num=643]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcd277a3e90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcd6a0ca170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc0e8be2e50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc132fa0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f24c85fde50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2513241170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=6, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc5a84e90d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc5ed543170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7facef46b090>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fad39558170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f97e3628f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f982e269170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f8b8f7152d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f8bd09db170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6772359850>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f67bcf99170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f38dd19d3d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f3927555170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fefdf31d610>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ff0205f7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f89ad778cd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f89f7b2b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7efb8980c4d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7efbd446a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb2553d57d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb2a0010170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f7d51c4ff10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7d9c6c2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2fa59afc90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2fe6c7a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
