Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fed7dd3d450>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fedc7f46170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f78a0705810>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f78eb304170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f44c059af50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f450a9dc170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f59b3c92050>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f59fe8e5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
l_dataloader) 5005
len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]pout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f058d20cf90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f05d7e4e170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f17f6253950>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f183ce03170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f35eed67f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f36399ab170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f87bf38f790>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f880068b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9532709717750549
                                                              len(train_dataloader) 30037
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 30037
Training:   0%|          | 0/2062 [00:00<00:00, 18558.87it/s]validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9904762506484985
len(train_dataloader) 30037
Epoch 15:   0%|          | 0/2062 [00:00<00:00, 4922.89it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9190751314163208
len(train_dataloader) 30037
ation_epoch_end
graph acc: 0.5
valid accuracy: 0.9887640476226807
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9694656729698181
len(train_dataloader) 30037
Epoch 15:   0%|          | 10/2062 [00:52<2:43:19,  4.78s/it]Epoch 15:   0%|          | 10/2062 [00:52<2:43:20,  4.78s/it, loss=5.07, v_num=645]Epoch 15:   1%|          | 20/2062 [01:38<2:39:23,  4.68s/it, loss=5.07, v_num=645]Epoch 15:   1%|          | 20/2062 [01:38<2:39:23,  4.68s/it, loss=5.07, v_num=645]Epoch 15:   1%|▏         | 30/2062 [02:19<2:32:35,  4.51s/it, loss=5.07, v_num=645]Epoch 15:   1%|▏         | 30/2062 [02:19<2:32:35,  4.51s/it, loss=5.03, v_num=645]Epoch 15:   2%|▏         | 40/2062 [03:02<2:29:53,  4.45s/it, loss=5.03, v_num=645]Epoch 15:   2%|▏         | 40/2062 [03:02<2:29:53,  4.45s/it, loss=5.01, v_num=645]Epoch 15:   2%|▏         | 50/2062 [03:41<2:25:33,  4.34s/it, loss=5.01, v_num=645]Epoch 15:   2%|▏         | 50/2062 [03:41<2:25:33,  4.34s/it, loss=5.04, v_num=645]Epoch 15:   3%|▎         | 60/2062 [04:26<2:25:31,  4.36s/it, loss=5.04, v_num=645]Epoch 15:   3%|▎         | 60/2062 [04:26<2:25:31,  4.36s/it, loss=5, v_num=645]   Epoch 15:   3%|▎         | 70/2062 [05:09<2:24:39,  4.36s/it, loss=5, v_num=645]Epoch 15:   3%|▎         | 70/2062 [05:09<2:24:39,  4.36s/it, loss=5, v_num=645]Epoch 15:   4%|▍         | 80/2062 [05:50<2:22:52,  4.33s/it, loss=5, v_num=645]Epoch 15:   4%|▍         | 80/2062 [05:50<2:22:52,  4.33s/it, loss=5.05, v_num=645]Epoch 15:   4%|▍         | 90/2062 [06:33<2:22:14,  4.33s/it, loss=5.05, v_num=645]Epoch 15:   4%|▍         | 90/2062 [06:33<2:22:14,  4.33s/it, loss=5.02, v_num=645]Epoch 15:   5%|▍         | 100/2062 [07:16<2:21:16,  4.32s/it, loss=5.02, v_num=645]Epoch 15:   5%|▍         | 100/2062 [07:16<2:21:16,  4.32s/it, loss=5.03, v_num=645]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc8930affd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc8dda86170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ff1d444c390>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ff21ee16170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7faf5cc1f690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fafa7687170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ef871ee6410>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ef8bc0f0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd27a599990>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd2c500d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd516b0a0d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd560cf3170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb03cf0af10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb087b4f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9695121645927429
len(train_dataloader) 30037
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9887640476226807
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9929077625274658
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9694656729698181
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9932432770729065
len(train_dataloader) 30037
 0/2062 [00:00<00:00, 4332.96it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9932885766029358
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9909502863883972
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9718309640884399
len(train_dataloader) 30037
Epoch 15:   0%|          | 10/2062 [00:31<1:38:25,  2.88s/it]Epoch 15:   0%|          | 10/2062 [00:31<1:38:25,  2.88s/it, loss=5.08, v_num=646]Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb8baf65f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb90515f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0dd36e0610>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0e15e66170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=17, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f66ac2814d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f66d9fcd170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=8, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1205e16510>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f12507f5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=8, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6702de7f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f674d065170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=8, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdd90d53f50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fdddb7d5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=2, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=8, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd34fd7d690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd39a9cf170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9948453307151794
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9894737005233765
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9787233471870422
len(train_dataloader) 30037
00:00, 5761.41it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9526627063751221
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9800000190734863
len(train_dataloader) 30037
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9489361643791199
len(train_dataloader) 30037
Epoch 15:   0%|          | 10/2191 [00:16<53:23,  1.47s/it] Epoch 15:   0%|          | 10/2191 [00:16<53:23,  1.47s/it, loss=2.39, v_num=647]Epoch 15:   1%|          | 20/2191 [00:24<42:41,  1.18s/it, loss=2.39, v_num=647]Epoch 15:   1%|          | 20/2191 [00:24<42:42,  1.18s/it, loss=2.39, v_num=647]Epoch 15:   1%|▏         | 30/2191 [00:34<39:51,  1.11s/it, loss=2.39, v_num=647]Epoch 15:   1%|▏         | 30/2191 [00:34<39:51,  1.11s/it, loss=2.37, v_num=647]Epoch 15:   2%|▏         | 40/2191 [00:42<36:54,  1.03s/it, loss=2.37, v_num=647]Epoch 15:   2%|▏         | 40/2191 [00:42<36:54,  1.03s/it, loss=2.36, v_num=647]Epoch 15:   2%|▏         | 50/2191 [00:49<34:53,  1.02it/s, loss=2.36, v_num=647]Epoch 15:   2%|▏         | 50/2191 [00:49<34:53,  1.02it/s, loss=2.39, v_num=647]Epoch 15:   3%|▎         | 60/2191 [00:57<33:45,  1.05it/s, loss=2.39, v_num=647]Epoch 15:   3%|▎         | 60/2191 [00:57<33:45,  1.05it/s, loss=2.4, v_num=647] Epoch 15:   3%|▎         | 70/2191 [01:06<32:57,  1.07it/s, loss=2.4, v_num=647]Epoch 15:   3%|▎         | 70/2191 [01:06<32:58,  1.07it/s, loss=2.36, v_num=647]Epoch 15:   4%|▎         | 80/2191 [01:14<32:21,  1.09it/s, loss=2.36, v_num=647]Epoch 15:   4%|▎         | 80/2191 [01:14<32:21,  1.09it/s, loss=2.39, v_num=647]Epoch 15:   4%|▍         | 90/2191 [01:22<31:39,  1.11it/s, loss=2.39, v_num=647]Epoch 15:   4%|▍         | 90/2191 [01:22<31:39,  1.11it/s, loss=2.4, v_num=647] Epoch 15:   5%|▍         | 100/2191 [01:30<31:04,  1.12it/s, loss=2.4, v_num=647]Epoch 15:   5%|▍         | 100/2191 [01:30<31:04,  1.12it/s, loss=2.38, v_num=647]Epoch 15:   5%|▌         | 110/2191 [01:38<30:39,  1.13it/s, loss=2.38, v_num=647]Epoch 15:   5%|▌         | 110/2191 [01:38<30:39,  1.13it/s, loss=2.38, v_num=647]Epoch 15:   5%|▌         | 120/2191 [01:46<30:28,  1.13it/s, loss=2.38, v_num=647]Epoch 15:   5%|▌         | 120/2191 [01:46<30:28,  1.13it/s, loss=2.37, v_num=647]Epoch 15:   6%|▌         | 130/2191 [01:55<30:15,  1.14it/s, loss=2.37, v_num=647]Epoch 15:   6%|▌         | 130/2191 [01:55<30:15,  1.14it/s, loss=2.36, v_num=647]Epoch 15:   6%|▋         | 140/2191 [02:03<30:02,  1.14it/s, loss=2.36, v_num=647]Epoch 15:   6%|▋         | 140/2191 [02:03<30:02,  1.14it/s, loss=2.32, v_num=647]Epoch 15:   7%|▋         | 150/2191 [02:11<29:35,  1.15it/s, loss=2.32, v_num=647]Epoch 15:   7%|▋         | 150/2191 [02:11<29:35,  1.15it/s, loss=2.31, v_num=647]Epoch 15:   7%|▋         | 160/2191 [02:18<29:10,  1.16it/s, loss=2.31, v_num=647]Epoch 15:   7%|▋         | 160/2191 [02:18<29:10,  1.16it/s, loss=2.39, v_num=647]Epoch 15:   8%|▊         | 170/2191 [02:26<28:52,  1.17it/s, loss=2.39, v_num=647]Epoch 15:   8%|▊         | 170/2191 [02:26<28:52,  1.17it/s, loss=2.4, v_num=647] Epoch 15:   8%|▊         | 180/2191 [02:34<28:35,  1.17it/s, loss=2.4, v_num=647]Epoch 15:   8%|▊         | 180/2191 [02:34<28:35,  1.17it/s, loss=2.37, v_num=647]Epoch 15:   9%|▊         | 190/2191 [02:42<28:26,  1.17it/s, loss=2.37, v_num=647]Epoch 15:   9%|▊         | 190/2191 [02:42<28:26,  1.17it/s, loss=2.38, v_num=647]Epoch 15:   9%|▉         | 200/2191 [02:50<28:10,  1.18it/s, loss=2.38, v_num=647]Epoch 15:   9%|▉         | 200/2191 [02:50<28:10,  1.18it/s, loss=2.4, v_num=647] Epoch 15:  10%|▉         | 210/2191 [02:57<27:50,  1.19it/s, loss=2.4, v_num=647]Epoch 15:  10%|▉         | 210/2191 [02:57<27:50,  1.19it/s, loss=2.4, v_num=647]Epoch 15:  10%|█         | 220/2191 [03:05<27:37,  1.19it/s, loss=2.4, v_num=647]Epoch 15:  10%|█         | 220/2191 [03:05<27:37,  1.19it/s, loss=2.41, v_num=647]Epoch 15:  10%|█         | 230/2191 [03:12<27:18,  1.20it/s, loss=2.41, v_num=647]Epoch 15:  10%|█         | 230/2191 [03:12<27:18,  1.20it/s, loss=2.4, v_num=647] Epoch 15:  11%|█         | 240/2191 [03:20<27:06,  1.20it/s, loss=2.4, v_num=647]Epoch 15:  11%|█         | 240/2191 [03:20<27:06,  1.20it/s, loss=2.39, v_num=647]Epoch 15:  11%|█▏        | 250/2191 [03:29<26:57,  1.20it/s, loss=2.39, v_num=647]Epoch 15:  11%|█▏        | 250/2191 [03:29<26:57,  1.20it/s, loss=2.39, v_num=647]Epoch 15:  12%|█▏        | 260/2191 [03:36<26:44,  1.20it/s, loss=2.39, v_num=647]Epoch 15:  12%|█▏        | 260/2191 [03:36<26:44,  1.20it/s, loss=2.36, v_num=647]Epoch 15:  12%|█▏        | 270/2191 [03:44<26:32,  1.21it/s, loss=2.36, v_num=647]Epoch 15:  12%|█▏        | 270/2191 [03:44<26:32,  1.21it/s, loss=2.36, v_num=647]Epoch 15:  13%|█▎        | 280/2191 [03:54<26:35,  1.20it/s, loss=2.36, v_num=647]Epoch 15:  13%|█▎        | 280/2191 [03:54<26:35,  1.20it/s, loss=2.38, v_num=647]Epoch 15:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.38, v_num=647]Epoch 15:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.38, v_num=647]Epoch 15:  14%|█▎        | 300/2191 [04:10<26:14,  1.20it/s, loss=2.38, v_num=647]Epoch 15:  14%|█▎        | 300/2191 [04:10<26:14,  1.20it/s, loss=2.37, v_num=647]Epoch 15:  14%|█▍        | 310/2191 [04:17<25:59,  1.21it/s, loss=2.37, v_num=647]Epoch 15:  14%|█▍        | 310/2191 [04:17<25:59,  1.21it/s, loss=2.37, v_num=647]Epoch 15:  15%|█▍        | 320/2191 [04:26<25:51,  1.21it/s, loss=2.37, v_num=647]Epoch 15:  15%|█▍        | 320/2191 [04:26<25:51,  1.21it/s, loss=2.38, v_num=647]Epoch 15:  15%|█▌        | 330/2191 [04:32<25:34,  1.21it/s, loss=2.38, v_num=647]Epoch 15:  15%|█▌        | 330/2191 [04:32<25:34,  1.21it/s, loss=2.37, v_num=647]Epoch 15:  16%|█▌        | 340/2191 [04:40<25:20,  1.22it/s, loss=2.37, v_num=647]Epoch 15:  16%|█▌        | 340/2191 [04:40<25:20,  1.22it/s, loss=2.36, v_num=647]Epoch 15:  16%|█▌        | 350/2191 [04:47<25:08,  1.22it/s, loss=2.36, v_num=647]Epoch 15:  16%|█▌        | 350/2191 [04:47<25:08,  1.22it/s, loss=2.36, v_num=647]Epoch 15:  16%|█▋        | 360/2191 [04:54<24:54,  1.23it/s, loss=2.36, v_num=647]Epoch 15:  16%|█▋        | 360/2191 [04:54<24:54,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  17%|█▋        | 370/2191 [05:02<24:44,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  17%|█▋        | 370/2191 [05:02<24:44,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  17%|█▋        | 380/2191 [05:10<24:36,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  17%|█▋        | 380/2191 [05:10<24:36,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  18%|█▊        | 390/2191 [05:23<24:49,  1.21it/s, loss=2.43, v_num=647]Epoch 15:  18%|█▊        | 390/2191 [05:23<24:49,  1.21it/s, loss=2.39, v_num=647]Epoch 15:  18%|█▊        | 400/2191 [05:30<24:35,  1.21it/s, loss=2.39, v_num=647]Epoch 15:  18%|█▊        | 400/2191 [05:30<24:35,  1.21it/s, loss=2.36, v_num=647]Epoch 15:  19%|█▊        | 410/2191 [05:38<24:25,  1.22it/s, loss=2.36, v_num=647]Epoch 15:  19%|█▊        | 410/2191 [05:38<24:25,  1.22it/s, loss=2.37, v_num=647]Epoch 15:  19%|█▉        | 420/2191 [05:45<24:14,  1.22it/s, loss=2.37, v_num=647]Epoch 15:  19%|█▉        | 420/2191 [05:45<24:14,  1.22it/s, loss=2.38, v_num=647]Epoch 15:  20%|█▉        | 430/2191 [05:53<24:05,  1.22it/s, loss=2.38, v_num=647]Epoch 15:  20%|█▉        | 430/2191 [05:53<24:05,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  20%|██        | 440/2191 [06:01<23:54,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  20%|██        | 440/2191 [06:01<23:54,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  21%|██        | 450/2191 [06:08<23:41,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  21%|██        | 450/2191 [06:08<23:41,  1.22it/s, loss=2.37, v_num=647]Epoch 15:  21%|██        | 460/2191 [06:17<23:37,  1.22it/s, loss=2.37, v_num=647]Epoch 15:  21%|██        | 460/2191 [06:17<23:37,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  21%|██▏       | 470/2191 [06:25<23:29,  1.22it/s, loss=2.39, v_num=647]Epoch 15:  21%|██▏       | 470/2191 [06:25<23:29,  1.22it/s, loss=2.42, v_num=647]Epoch 15:  22%|██▏       | 480/2191 [06:33<23:18,  1.22it/s, loss=2.42, v_num=647]Epoch 15:  22%|██▏       | 480/2191 [06:33<23:18,  1.22it/s, loss=2.43, v_num=647]Epoch 15:  22%|██▏       | 490/2191 [06:42<23:14,  1.22it/s, loss=2.43, v_num=647]Epoch 15:  22%|██▏       | 490/2191 [06:42<23:14,  1.22it/s, loss=2.43, v_num=647]Epoch 15:  23%|██▎       | 500/2191 [06:49<23:03,  1.22it/s, loss=2.43, v_num=647]Epoch 15:  23%|██▎       | 500/2191 [06:49<23:03,  1.22it/s, loss=2.42, v_num=647]Epoch 15:  23%|██▎       | 510/2191 [06:56<22:50,  1.23it/s, loss=2.42, v_num=647]Epoch 15:  23%|██▎       | 510/2191 [06:56<22:50,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  24%|██▎       | 520/2191 [07:03<22:38,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  24%|██▎       | 520/2191 [07:03<22:38,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  24%|██▍       | 530/2191 [07:11<22:28,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  24%|██▍       | 530/2191 [07:11<22:28,  1.23it/s, loss=2.37, v_num=647]Epoch 15:  25%|██▍       | 540/2191 [07:19<22:22,  1.23it/s, loss=2.37, v_num=647]Epoch 15:  25%|██▍       | 540/2191 [07:19<22:22,  1.23it/s, loss=2.4, v_num=647] Epoch 15:  25%|██▌       | 550/2191 [07:28<22:16,  1.23it/s, loss=2.4, v_num=647]Epoch 15:  25%|██▌       | 550/2191 [07:28<22:16,  1.23it/s, loss=2.46, v_num=647]Epoch 15:  26%|██▌       | 560/2191 [07:35<22:04,  1.23it/s, loss=2.46, v_num=647]Epoch 15:  26%|██▌       | 560/2191 [07:35<22:05,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  26%|██▌       | 570/2191 [07:42<21:52,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  26%|██▌       | 570/2191 [07:42<21:52,  1.23it/s, loss=2.4, v_num=647] Epoch 15:  26%|██▋       | 580/2191 [07:50<21:43,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  26%|██▋       | 580/2191 [07:50<21:43,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  27%|██▋       | 590/2191 [07:57<21:34,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  27%|██▋       | 590/2191 [07:57<21:34,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  27%|██▋       | 600/2191 [08:05<21:24,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  27%|██▋       | 600/2191 [08:05<21:24,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  28%|██▊       | 610/2191 [08:14<21:20,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  28%|██▊       | 610/2191 [08:14<21:20,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  28%|██▊       | 620/2191 [08:23<21:13,  1.23it/s, loss=2.42, v_num=647]Epoch 15:  28%|██▊       | 620/2191 [08:23<21:13,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  29%|██▉       | 630/2191 [08:30<21:02,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  29%|██▉       | 630/2191 [08:30<21:02,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  29%|██▉       | 640/2191 [08:38<20:54,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  29%|██▉       | 640/2191 [08:38<20:54,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  30%|██▉       | 650/2191 [08:46<20:46,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  30%|██▉       | 650/2191 [08:46<20:46,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  30%|███       | 660/2191 [08:55<20:39,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  30%|███       | 660/2191 [08:55<20:39,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  31%|███       | 670/2191 [09:02<20:29,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  31%|███       | 670/2191 [09:02<20:29,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  31%|███       | 680/2191 [09:09<20:19,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  31%|███       | 680/2191 [09:09<20:19,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  31%|███▏      | 690/2191 [09:17<20:10,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  31%|███▏      | 690/2191 [09:17<20:10,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  32%|███▏      | 700/2191 [09:24<20:01,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  32%|███▏      | 700/2191 [09:24<20:01,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  32%|███▏      | 710/2191 [09:34<19:56,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  32%|███▏      | 710/2191 [09:34<19:56,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  33%|███▎      | 720/2191 [09:42<19:48,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  33%|███▎      | 720/2191 [09:42<19:48,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  33%|███▎      | 730/2191 [09:50<19:40,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  33%|███▎      | 730/2191 [09:50<19:40,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  34%|███▍      | 740/2191 [09:58<19:32,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  34%|███▍      | 740/2191 [09:58<19:32,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  34%|███▍      | 750/2191 [10:07<19:24,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  34%|███▍      | 750/2191 [10:07<19:24,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  35%|███▍      | 760/2191 [10:13<19:14,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  35%|███▍      | 760/2191 [10:13<19:14,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  35%|███▌      | 770/2191 [10:21<19:05,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  35%|███▌      | 770/2191 [10:21<19:05,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  36%|███▌      | 780/2191 [10:29<18:57,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  36%|███▌      | 780/2191 [10:29<18:57,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  36%|███▌      | 790/2191 [10:38<18:50,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  36%|███▌      | 790/2191 [10:38<18:50,  1.24it/s, loss=2.46, v_num=647]Epoch 15:  37%|███▋      | 800/2191 [10:46<18:43,  1.24it/s, loss=2.46, v_num=647]Epoch 15:  37%|███▋      | 800/2191 [10:46<18:43,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  37%|███▋      | 810/2191 [10:55<18:35,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  37%|███▋      | 810/2191 [10:55<18:35,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  37%|███▋      | 820/2191 [11:04<18:29,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  37%|███▋      | 820/2191 [11:04<18:29,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  38%|███▊      | 830/2191 [11:11<18:20,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  38%|███▊      | 830/2191 [11:11<18:20,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  38%|███▊      | 840/2191 [11:18<18:10,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  38%|███▊      | 840/2191 [11:18<18:10,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  39%|███▉      | 850/2191 [11:28<18:04,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  39%|███▉      | 850/2191 [11:28<18:04,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  39%|███▉      | 860/2191 [11:36<17:56,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  39%|███▉      | 860/2191 [11:36<17:56,  1.24it/s, loss=2.35, v_num=647]Epoch 15:  40%|███▉      | 870/2191 [11:45<17:50,  1.23it/s, loss=2.35, v_num=647]Epoch 15:  40%|███▉      | 870/2191 [11:45<17:50,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  40%|████      | 880/2191 [11:55<17:44,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  40%|████      | 880/2191 [11:55<17:44,  1.23it/s, loss=2.4, v_num=647] Epoch 15:  41%|████      | 890/2191 [12:03<17:36,  1.23it/s, loss=2.4, v_num=647]Epoch 15:  41%|████      | 890/2191 [12:03<17:36,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  41%|████      | 900/2191 [12:11<17:27,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  41%|████      | 900/2191 [12:11<17:27,  1.23it/s, loss=2.4, v_num=647] Epoch 15:  42%|████▏     | 910/2191 [12:18<17:18,  1.23it/s, loss=2.4, v_num=647]Epoch 15:  42%|████▏     | 910/2191 [12:18<17:18,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  42%|████▏     | 920/2191 [12:26<17:10,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  42%|████▏     | 920/2191 [12:26<17:10,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  42%|████▏     | 930/2191 [12:34<17:01,  1.23it/s, loss=2.38, v_num=647]Epoch 15:  42%|████▏     | 930/2191 [12:34<17:01,  1.23it/s, loss=2.43, v_num=647]Epoch 15:  43%|████▎     | 940/2191 [12:41<16:52,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  43%|████▎     | 940/2191 [12:41<16:52,  1.24it/s, loss=2.44, v_num=647]Epoch 15:  43%|████▎     | 950/2191 [12:48<16:43,  1.24it/s, loss=2.44, v_num=647]Epoch 15:  43%|████▎     | 950/2191 [12:48<16:43,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  44%|████▍     | 960/2191 [12:56<16:34,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  44%|████▍     | 960/2191 [12:56<16:34,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  44%|████▍     | 970/2191 [13:06<16:28,  1.24it/s, loss=2.41, v_num=647]Epoch 15:  44%|████▍     | 970/2191 [13:06<16:28,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  45%|████▍     | 980/2191 [13:14<16:20,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  45%|████▍     | 980/2191 [13:14<16:20,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  45%|████▌     | 990/2191 [13:22<16:12,  1.24it/s, loss=2.42, v_num=647]Epoch 15:  45%|████▌     | 990/2191 [13:22<16:12,  1.24it/s, loss=2.39, v_num=647]Epoch 15:  46%|████▌     | 1000/2191 [13:30<16:04,  1.23it/s, loss=2.39, v_num=647]Epoch 15:  46%|████▌     | 1000/2191 [13:30<16:04,  1.23it/s, loss=2.35, v_num=647]Epoch 15:  46%|████▌     | 1010/2191 [13:37<15:54,  1.24it/s, loss=2.35, v_num=647]Epoch 15:  46%|████▌     | 1010/2191 [13:37<15:54,  1.24it/s, loss=2.4, v_num=647] Epoch 15:  47%|████▋     | 1020/2191 [13:44<15:45,  1.24it/s, loss=2.4, v_num=647]Epoch 15:  47%|████▋     | 1020/2191 [13:44<15:45,  1.24it/s, loss=2.45, v_num=647]Epoch 15:  47%|████▋     | 1030/2191 [13:53<15:38,  1.24it/s, loss=2.45, v_num=647]Epoch 15:  47%|████▋     | 1030/2191 [13:53<15:38,  1.24it/s, loss=2.44, v_num=647]Epoch 15:  47%|████▋     | 1040/2191 [14:00<15:28,  1.24it/s, loss=2.44, v_num=647]Epoch 15:  47%|████▋     | 1040/2191 [14:00<15:28,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  48%|████▊     | 1050/2191 [14:08<15:21,  1.24it/s, loss=2.36, v_num=647]Epoch 15:  48%|████▊     | 1050/2191 [14:08<15:21,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  48%|████▊     | 1060/2191 [14:15<15:11,  1.24it/s, loss=2.37, v_num=647]Epoch 15:  48%|████▊     | 1060/2191 [14:15<15:11,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  49%|████▉     | 1070/2191 [14:22<15:02,  1.24it/s, loss=2.38, v_num=647]Epoch 15:  49%|████▉     | 1070/2191 [14:22<15:02,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  49%|████▉     | 1080/2191 [14:29<14:53,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  49%|████▉     | 1080/2191 [14:29<14:53,  1.24it/s, loss=2.43, v_num=647]Epoch 15:  50%|████▉     | 1090/2191 [14:35<14:43,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  50%|████▉     | 1090/2191 [14:35<14:43,  1.25it/s, loss=2.4, v_num=647] Epoch 15:  50%|█████     | 1100/2191 [14:43<14:35,  1.25it/s, loss=2.4, v_num=647]Epoch 15:  50%|█████     | 1100/2191 [14:43<14:35,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  51%|█████     | 1110/2191 [14:50<14:26,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  51%|█████     | 1110/2191 [14:50<14:26,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  51%|█████     | 1120/2191 [14:59<14:19,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  51%|█████     | 1120/2191 [14:59<14:19,  1.25it/s, loss=2.4, v_num=647] Epoch 15:  52%|█████▏    | 1130/2191 [15:06<14:10,  1.25it/s, loss=2.4, v_num=647]Epoch 15:  52%|█████▏    | 1130/2191 [15:06<14:10,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  52%|█████▏    | 1140/2191 [15:14<14:02,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  52%|█████▏    | 1140/2191 [15:14<14:02,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  52%|█████▏    | 1150/2191 [15:22<13:54,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  52%|█████▏    | 1150/2191 [15:22<13:54,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  53%|█████▎    | 1160/2191 [15:30<13:46,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  53%|█████▎    | 1160/2191 [15:30<13:46,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  53%|█████▎    | 1170/2191 [15:37<13:37,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  53%|█████▎    | 1170/2191 [15:37<13:37,  1.25it/s, loss=2.36, v_num=647]Epoch 15:  54%|█████▍    | 1180/2191 [15:45<13:29,  1.25it/s, loss=2.36, v_num=647]Epoch 15:  54%|█████▍    | 1180/2191 [15:45<13:29,  1.25it/s, loss=2.39, v_num=647]Epoch 15:  54%|█████▍    | 1190/2191 [15:55<13:23,  1.25it/s, loss=2.39, v_num=647]Epoch 15:  54%|█████▍    | 1190/2191 [15:55<13:23,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  55%|█████▍    | 1200/2191 [16:03<13:14,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  55%|█████▍    | 1200/2191 [16:03<13:14,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  55%|█████▌    | 1210/2191 [16:10<13:06,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  55%|█████▌    | 1210/2191 [16:10<13:06,  1.25it/s, loss=2.44, v_num=647]Epoch 15:  56%|█████▌    | 1220/2191 [16:18<12:58,  1.25it/s, loss=2.44, v_num=647]Epoch 15:  56%|█████▌    | 1220/2191 [16:18<12:58,  1.25it/s, loss=2.45, v_num=647]Epoch 15:  56%|█████▌    | 1230/2191 [16:26<12:49,  1.25it/s, loss=2.45, v_num=647]Epoch 15:  56%|█████▌    | 1230/2191 [16:26<12:49,  1.25it/s, loss=2.44, v_num=647]Epoch 15:  57%|█████▋    | 1240/2191 [16:33<12:41,  1.25it/s, loss=2.44, v_num=647]Epoch 15:  57%|█████▋    | 1240/2191 [16:33<12:41,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  57%|█████▋    | 1250/2191 [16:40<12:32,  1.25it/s, loss=2.43, v_num=647]Epoch 15:  57%|█████▋    | 1250/2191 [16:40<12:32,  1.25it/s, loss=2.42, v_num=647]Epoch 15:  58%|█████▊    | 1260/2191 [16:49<12:25,  1.25it/s, loss=2.42, v_num=647]Epoch 15:  58%|█████▊    | 1260/2191 [16:49<12:25,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  58%|█████▊    | 1270/2191 [16:56<12:16,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  58%|█████▊    | 1270/2191 [16:56<12:16,  1.25it/s, loss=2.4, v_num=647] Epoch 15:  58%|█████▊    | 1280/2191 [17:04<12:08,  1.25it/s, loss=2.4, v_num=647]Epoch 15:  58%|█████▊    | 1280/2191 [17:04<12:08,  1.25it/s, loss=2.42, v_num=647]Epoch 15:  59%|█████▉    | 1290/2191 [17:12<12:00,  1.25it/s, loss=2.42, v_num=647]Epoch 15:  59%|█████▉    | 1290/2191 [17:12<12:00,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  59%|█████▉    | 1300/2191 [17:19<11:51,  1.25it/s, loss=2.41, v_num=647]Epoch 15:  59%|█████▉    | 1300/2191 [17:19<11:51,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  60%|█████▉    | 1310/2191 [17:26<11:43,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  60%|█████▉    | 1310/2191 [17:26<11:43,  1.25it/s, loss=2.37, v_num=647]Epoch 15:  60%|██████    | 1320/2191 [17:33<11:34,  1.25it/s, loss=2.37, v_num=647]Epoch 15:  60%|██████    | 1320/2191 [17:33<11:34,  1.25it/s, loss=2.4, v_num=647] Epoch 15:  61%|██████    | 1330/2191 [17:40<11:26,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  61%|██████    | 1330/2191 [17:40<11:26,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  61%|██████    | 1340/2191 [17:47<11:17,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  61%|██████    | 1340/2191 [17:47<11:17,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  62%|██████▏   | 1350/2191 [17:56<11:10,  1.25it/s, loss=2.39, v_num=647]Epoch 15:  62%|██████▏   | 1350/2191 [17:56<11:10,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  62%|██████▏   | 1360/2191 [18:03<11:01,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  62%|██████▏   | 1360/2191 [18:03<11:01,  1.26it/s, loss=2.37, v_num=647]Epoch 15:  63%|██████▎   | 1370/2191 [18:12<10:54,  1.25it/s, loss=2.37, v_num=647]Epoch 15:  63%|██████▎   | 1370/2191 [18:12<10:54,  1.25it/s, loss=2.38, v_num=647]Epoch 15:  63%|██████▎   | 1380/2191 [18:19<10:45,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  63%|██████▎   | 1380/2191 [18:19<10:45,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  63%|██████▎   | 1390/2191 [18:26<10:37,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  63%|██████▎   | 1390/2191 [18:26<10:37,  1.26it/s, loss=2.4, v_num=647] Epoch 15:  64%|██████▍   | 1400/2191 [18:33<10:28,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  64%|██████▍   | 1400/2191 [18:33<10:28,  1.26it/s, loss=2.43, v_num=647]Epoch 15:  64%|██████▍   | 1410/2191 [18:40<10:20,  1.26it/s, loss=2.43, v_num=647]Epoch 15:  64%|██████▍   | 1410/2191 [18:40<10:20,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  65%|██████▍   | 1420/2191 [18:50<10:13,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  65%|██████▍   | 1420/2191 [18:50<10:13,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  65%|██████▌   | 1430/2191 [18:57<10:05,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  65%|██████▌   | 1430/2191 [18:57<10:05,  1.26it/s, loss=2.44, v_num=647]Epoch 15:  66%|██████▌   | 1440/2191 [19:05<09:56,  1.26it/s, loss=2.44, v_num=647]Epoch 15:  66%|██████▌   | 1440/2191 [19:05<09:56,  1.26it/s, loss=2.4, v_num=647] Epoch 15:  66%|██████▌   | 1450/2191 [19:11<09:48,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  66%|██████▌   | 1450/2191 [19:11<09:48,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  67%|██████▋   | 1460/2191 [19:19<09:40,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  67%|██████▋   | 1460/2191 [19:19<09:40,  1.26it/s, loss=2.46, v_num=647]Epoch 15:  67%|██████▋   | 1470/2191 [19:27<09:32,  1.26it/s, loss=2.46, v_num=647]Epoch 15:  67%|██████▋   | 1470/2191 [19:27<09:32,  1.26it/s, loss=2.4, v_num=647] Epoch 15:  68%|██████▊   | 1480/2191 [19:36<09:24,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  68%|██████▊   | 1480/2191 [19:36<09:24,  1.26it/s, loss=2.42, v_num=647]Epoch 15:  68%|██████▊   | 1490/2191 [19:43<09:16,  1.26it/s, loss=2.42, v_num=647]Epoch 15:  68%|██████▊   | 1490/2191 [19:43<09:16,  1.26it/s, loss=2.47, v_num=647]Epoch 15:  68%|██████▊   | 1500/2191 [19:51<09:08,  1.26it/s, loss=2.47, v_num=647]Epoch 15:  68%|██████▊   | 1500/2191 [19:51<09:08,  1.26it/s, loss=2.43, v_num=647]Epoch 15:  69%|██████▉   | 1510/2191 [19:58<09:00,  1.26it/s, loss=2.43, v_num=647]Epoch 15:  69%|██████▉   | 1510/2191 [19:58<09:00,  1.26it/s, loss=2.4, v_num=647] Epoch 15:  69%|██████▉   | 1520/2191 [20:06<08:52,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  69%|██████▉   | 1520/2191 [20:06<08:52,  1.26it/s, loss=2.42, v_num=647]Epoch 15:  70%|██████▉   | 1530/2191 [20:14<08:44,  1.26it/s, loss=2.42, v_num=647]Epoch 15:  70%|██████▉   | 1530/2191 [20:14<08:44,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  70%|███████   | 1540/2191 [20:21<08:36,  1.26it/s, loss=2.41, v_num=647]Epoch 15:  70%|███████   | 1540/2191 [20:21<08:36,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  71%|███████   | 1550/2191 [20:28<08:27,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  71%|███████   | 1550/2191 [20:28<08:27,  1.26it/s, loss=2.35, v_num=647]Epoch 15:  71%|███████   | 1560/2191 [20:35<08:19,  1.26it/s, loss=2.35, v_num=647]Epoch 15:  71%|███████   | 1560/2191 [20:35<08:19,  1.26it/s, loss=2.35, v_num=647]Epoch 15:  72%|███████▏  | 1570/2191 [20:44<08:11,  1.26it/s, loss=2.35, v_num=647]Epoch 15:  72%|███████▏  | 1570/2191 [20:44<08:11,  1.26it/s, loss=2.37, v_num=647]Epoch 15:  72%|███████▏  | 1580/2191 [20:50<08:03,  1.26it/s, loss=2.37, v_num=647]Epoch 15:  72%|███████▏  | 1580/2191 [20:50<08:03,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  73%|███████▎  | 1590/2191 [20:57<07:54,  1.27it/s, loss=2.38, v_num=647]Epoch 15:  73%|███████▎  | 1590/2191 [20:57<07:54,  1.27it/s, loss=2.4, v_num=647] Epoch 15:  73%|███████▎  | 1600/2191 [21:05<07:47,  1.26it/s, loss=2.4, v_num=647]Epoch 15:  73%|███████▎  | 1600/2191 [21:05<07:47,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  73%|███████▎  | 1610/2191 [21:13<07:39,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  73%|███████▎  | 1610/2191 [21:13<07:39,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  74%|███████▍  | 1620/2191 [21:22<07:31,  1.26it/s, loss=2.39, v_num=647]Epoch 15:  74%|███████▍  | 1620/2191 [21:22<07:31,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  74%|███████▍  | 1630/2191 [21:30<07:23,  1.26it/s, loss=2.38, v_num=647]Epoch 15:  74%|███████▍  | 1630/2191 [21:30<07:23,  1.26it/s, loss=2.42, v_num=647]Epoch 15:  75%|███████▍  | 1640/2191 [21:37<07:15,  1.27it/s, loss=2.42, v_num=647]Epoch 15:  75%|███████▍  | 1640/2191 [21:37<07:15,  1.27it/s, loss=2.44, v_num=647]Epoch 15:  75%|███████▌  | 1650/2191 [21:45<07:07,  1.26it/s, loss=2.44, v_num=647]Epoch 15:  75%|███████▌  | 1650/2191 [21:45<07:07,  1.26it/s, loss=2.4, v_num=647] Epoch 15:  76%|███████▌  | 1660/2191 [21:52<06:59,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  76%|███████▌  | 1660/2191 [21:52<06:59,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  76%|███████▌  | 1670/2191 [21:59<06:51,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  76%|███████▌  | 1670/2191 [21:59<06:51,  1.27it/s, loss=2.4, v_num=647] Epoch 15:  77%|███████▋  | 1680/2191 [22:06<06:43,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  77%|███████▋  | 1680/2191 [22:06<06:43,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  77%|███████▋  | 1690/2191 [22:14<06:35,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  77%|███████▋  | 1690/2191 [22:14<06:35,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  78%|███████▊  | 1700/2191 [22:22<06:27,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  78%|███████▊  | 1700/2191 [22:22<06:27,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  78%|███████▊  | 1710/2191 [22:30<06:19,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  78%|███████▊  | 1710/2191 [22:30<06:19,  1.27it/s, loss=2.41, v_num=647]Epoch 15:  79%|███████▊  | 1720/2191 [22:37<06:11,  1.27it/s, loss=2.41, v_num=647]Epoch 15:  79%|███████▊  | 1720/2191 [22:37<06:11,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  79%|███████▉  | 1730/2191 [22:44<06:03,  1.27it/s, loss=2.39, v_num=647]Epoch 15:  79%|███████▉  | 1730/2191 [22:44<06:03,  1.27it/s, loss=2.38, v_num=647]Epoch 15:  79%|███████▉  | 1740/2191 [22:51<05:55,  1.27it/s, loss=2.38, v_num=647]Epoch 15:  79%|███████▉  | 1740/2191 [22:51<05:55,  1.27it/s, loss=2.42, v_num=647]Epoch 15:  80%|███████▉  | 1750/2191 [22:58<05:47,  1.27it/s, loss=2.42, v_num=647]Epoch 15:  80%|███████▉  | 1750/2191 [22:58<05:47,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  80%|████████  | 1760/2191 [23:07<05:39,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  80%|████████  | 1760/2191 [23:07<05:39,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  81%|████████  | 1770/2191 [23:14<05:31,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  81%|████████  | 1770/2191 [23:14<05:31,  1.27it/s, loss=2.41, v_num=647]Epoch 15:  81%|████████  | 1780/2191 [23:21<05:23,  1.27it/s, loss=2.41, v_num=647]Epoch 15:  81%|████████  | 1780/2191 [23:21<05:23,  1.27it/s, loss=2.38, v_num=647]Epoch 15:  82%|████████▏ | 1790/2191 [23:29<05:15,  1.27it/s, loss=2.38, v_num=647]Epoch 15:  82%|████████▏ | 1790/2191 [23:29<05:15,  1.27it/s, loss=2.4, v_num=647] Epoch 15:  82%|████████▏ | 1800/2191 [23:36<05:07,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  82%|████████▏ | 1800/2191 [23:36<05:07,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  83%|████████▎ | 1810/2191 [23:43<04:59,  1.27it/s, loss=2.43, v_num=647]Epoch 15:  83%|████████▎ | 1810/2191 [23:43<04:59,  1.27it/s, loss=2.42, v_num=647]Epoch 15:  83%|████████▎ | 1820/2191 [23:51<04:51,  1.27it/s, loss=2.42, v_num=647]Epoch 15:  83%|████████▎ | 1820/2191 [23:51<04:51,  1.27it/s, loss=2.4, v_num=647] Epoch 15:  84%|████████▎ | 1830/2191 [23:58<04:43,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  84%|████████▎ | 1830/2191 [23:58<04:43,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  84%|████████▍ | 1840/2191 [24:04<04:35,  1.27it/s, loss=2.4, v_num=647]Epoch 15:  84%|████████▍ | 1840/2191 [24:04<04:35,  1.27it/s, loss=2.41, v_num=647]Epoch 15:  84%|████████▍ | 1850/2191 [24:10<04:27,  1.28it/s, loss=2.41, v_num=647]Epoch 15:  84%|████████▍ | 1850/2191 [24:10<04:27,  1.28it/s, loss=2.41, v_num=647]Epoch 15:  85%|████████▍ | 1860/2191 [24:15<04:18,  1.28it/s, loss=2.41, v_num=647]Epoch 15:  85%|████████▍ | 1860/2191 [24:15<04:18,  1.28it/s, loss=2.41, v_num=647]Epoch 15:  85%|████████▌ | 1870/2191 [24:19<04:10,  1.28it/s, loss=2.41, v_num=647]Epoch 15:  85%|████████▌ | 1870/2191 [24:19<04:10,  1.28it/s, loss=2.47, v_num=647]Epoch 15:  86%|████████▌ | 1880/2191 [24:21<04:01,  1.29it/s, loss=2.47, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/313 [00:01<00:53,  5.65it/s][AEpoch 15:  86%|████████▋ | 1890/2191 [24:23<03:52,  1.29it/s, loss=2.47, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:56,  5.19it/s][AEpoch 15:  87%|████████▋ | 1900/2191 [24:25<03:44,  1.30it/s, loss=2.47, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:36,  7.66it/s][AEpoch 15:  87%|████████▋ | 1910/2191 [24:25<03:35,  1.30it/s, loss=2.47, v_num=647]
Validating:  13%|█▎        | 40/313 [00:05<00:29,  9.39it/s][AEpoch 15:  88%|████████▊ | 1920/2191 [24:26<03:26,  1.31it/s, loss=2.47, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:24, 10.72it/s][AEpoch 15:  88%|████████▊ | 1930/2191 [24:27<03:18,  1.32it/s, loss=2.47, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.85it/s][AEpoch 15:  89%|████████▊ | 1940/2191 [24:28<03:09,  1.32it/s, loss=2.47, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:22, 10.65it/s][AEpoch 15:  89%|████████▉ | 1950/2191 [24:29<03:01,  1.33it/s, loss=2.47, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:23,  9.99it/s][AEpoch 15:  89%|████████▉ | 1960/2191 [24:30<02:53,  1.33it/s, loss=2.47, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:22,  9.90it/s][AEpoch 15:  90%|████████▉ | 1970/2191 [24:31<02:44,  1.34it/s, loss=2.47, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:21, 10.12it/s][AEpoch 15:  90%|█████████ | 1980/2191 [24:32<02:36,  1.35it/s, loss=2.47, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.82it/s][AEpoch 15:  91%|█████████ | 1990/2191 [24:33<02:28,  1.35it/s, loss=2.47, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.77it/s][AEpoch 15:  91%|█████████▏| 2000/2191 [24:34<02:20,  1.36it/s, loss=2.47, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:21,  8.54it/s][AEpoch 15:  92%|█████████▏| 2010/2191 [24:35<02:12,  1.36it/s, loss=2.47, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  8.21it/s][AEpoch 15:  92%|█████████▏| 2020/2191 [24:37<02:04,  1.37it/s, loss=2.47, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.10it/s][AEpoch 15:  93%|█████████▎| 2030/2191 [24:37<01:57,  1.37it/s, loss=2.47, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:16,  9.38it/s][AEpoch 15:  93%|█████████▎| 2040/2191 [24:38<01:49,  1.38it/s, loss=2.47, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:13, 10.59it/s][AEpoch 15:  94%|█████████▎| 2050/2191 [24:39<01:41,  1.39it/s, loss=2.47, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:19<00:12, 10.34it/s][AEpoch 15:  94%|█████████▍| 2060/2191 [24:40<01:34,  1.39it/s, loss=2.47, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 12.13it/s][AEpoch 15:  94%|█████████▍| 2070/2191 [24:41<01:26,  1.40it/s, loss=2.47, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.67it/s][AEpoch 15:  95%|█████████▍| 2080/2191 [24:42<01:19,  1.40it/s, loss=2.47, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 10.69it/s][AEpoch 15:  95%|█████████▌| 2090/2191 [24:43<01:11,  1.41it/s, loss=2.47, v_num=647]
Validating:  70%|███████   | 220/313 [00:23<00:09,  9.51it/s][AEpoch 15:  96%|█████████▌| 2100/2191 [24:44<01:04,  1.42it/s, loss=2.47, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.87it/s][AEpoch 15:  96%|█████████▋| 2110/2191 [24:45<00:56,  1.42it/s, loss=2.47, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:06, 10.80it/s][AEpoch 15:  97%|█████████▋| 2120/2191 [24:46<00:49,  1.43it/s, loss=2.47, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06,  9.99it/s][AEpoch 15:  97%|█████████▋| 2130/2191 [24:47<00:42,  1.43it/s, loss=2.47, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.93it/s][AEpoch 15:  98%|█████████▊| 2140/2191 [24:48<00:35,  1.44it/s, loss=2.47, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.30it/s][AEpoch 15:  98%|█████████▊| 2150/2191 [24:48<00:28,  1.44it/s, loss=2.47, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:28<00:02, 12.00it/s][AEpoch 15:  99%|█████████▊| 2160/2191 [24:49<00:21,  1.45it/s, loss=2.47, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:29<00:02, 11.38it/s][AEpoch 15:  99%|█████████▉| 2170/2191 [24:50<00:14,  1.46it/s, loss=2.47, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.38it/s][AEpoch 15:  99%|█████████▉| 2180/2191 [24:50<00:07,  1.46it/s, loss=2.47, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 13.55it/s][AEpoch 15: 100%|█████████▉| 2190/2191 [24:51<00:00,  1.47it/s, loss=2.47, v_num=647]validation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9769350290298462
Epoch 15: 100%|██████████| 2191/2191 [24:52<00:00,  1.47it/s, loss=2.41, v_num=647]
                                                             [AEpoch 15:   0%|          | 0/2191 [00:00<00:00, 9078.58it/s, loss=2.41, v_num=647] Epoch 16:   0%|          | 0/2191 [00:00<00:00, 2261.08it/s, loss=2.41, v_num=647]Epoch 16:   0%|          | 10/2191 [00:14<48:14,  1.33s/it, loss=2.41, v_num=647] Epoch 16:   0%|          | 10/2191 [00:14<48:14,  1.33s/it, loss=2.39, v_num=647]Epoch 16:   1%|          | 20/2191 [00:25<43:39,  1.21s/it, loss=2.39, v_num=647]Epoch 16:   1%|          | 20/2191 [00:25<43:39,  1.21s/it, loss=2.42, v_num=647]Epoch 16:   1%|▏         | 30/2191 [00:35<41:43,  1.16s/it, loss=2.42, v_num=647]Epoch 16:   1%|▏         | 30/2191 [00:35<41:43,  1.16s/it, loss=2.36, v_num=647]Epoch 16:   2%|▏         | 40/2191 [00:46<40:19,  1.12s/it, loss=2.36, v_num=647]Epoch 16:   2%|▏         | 40/2191 [00:46<40:19,  1.12s/it, loss=2.37, v_num=647]Epoch 16:   2%|▏         | 50/2191 [00:56<39:27,  1.11s/it, loss=2.37, v_num=647]Epoch 16:   2%|▏         | 50/2191 [00:56<39:27,  1.11s/it, loss=2.39, v_num=647]Epoch 16:   3%|▎         | 60/2191 [01:06<38:42,  1.09s/it, loss=2.39, v_num=647]Epoch 16:   3%|▎         | 60/2191 [01:06<38:42,  1.09s/it, loss=2.39, v_num=647]Epoch 16:   3%|▎         | 70/2191 [01:16<38:09,  1.08s/it, loss=2.39, v_num=647]Epoch 16:   3%|▎         | 70/2191 [01:16<38:09,  1.08s/it, loss=2.41, v_num=647]Epoch 16:   4%|▎         | 80/2191 [01:26<37:30,  1.07s/it, loss=2.41, v_num=647]Epoch 16:   4%|▎         | 80/2191 [01:26<37:30,  1.07s/it, loss=2.38, v_num=647]Epoch 16:   4%|▍         | 90/2191 [01:35<36:48,  1.05s/it, loss=2.38, v_num=647]Epoch 16:   4%|▍         | 90/2191 [01:35<36:48,  1.05s/it, loss=2.35, v_num=647]Epoch 16:   5%|▍         | 100/2191 [01:45<36:15,  1.04s/it, loss=2.35, v_num=647]Epoch 16:   5%|▍         | 100/2191 [01:45<36:15,  1.04s/it, loss=2.35, v_num=647]Epoch 16:   5%|▌         | 110/2191 [01:53<35:30,  1.02s/it, loss=2.35, v_num=647]Epoch 16:   5%|▌         | 110/2191 [01:53<35:30,  1.02s/it, loss=2.37, v_num=647]Epoch 16:   5%|▌         | 120/2191 [02:01<34:40,  1.00s/it, loss=2.37, v_num=647]Epoch 16:   5%|▌         | 120/2191 [02:01<34:40,  1.00s/it, loss=2.37, v_num=647]Epoch 16:   6%|▌         | 130/2191 [02:10<34:08,  1.01it/s, loss=2.37, v_num=647]Epoch 16:   6%|▌         | 130/2191 [02:10<34:08,  1.01it/s, loss=2.38, v_num=647]Epoch 16:   6%|▋         | 140/2191 [02:17<33:17,  1.03it/s, loss=2.38, v_num=647]Epoch 16:   6%|▋         | 140/2191 [02:17<33:18,  1.03it/s, loss=2.41, v_num=647]Epoch 16:   7%|▋         | 150/2191 [02:24<32:38,  1.04it/s, loss=2.41, v_num=647]Epoch 16:   7%|▋         | 150/2191 [02:24<32:38,  1.04it/s, loss=2.42, v_num=647]Epoch 16:   7%|▋         | 160/2191 [02:31<31:55,  1.06it/s, loss=2.42, v_num=647]Epoch 16:   7%|▋         | 160/2191 [02:31<31:55,  1.06it/s, loss=2.4, v_num=647] Epoch 16:   8%|▊         | 170/2191 [02:39<31:23,  1.07it/s, loss=2.4, v_num=647]Epoch 16:   8%|▊         | 170/2191 [02:39<31:23,  1.07it/s, loss=2.4, v_num=647]Epoch 16:   8%|▊         | 180/2191 [02:46<30:49,  1.09it/s, loss=2.4, v_num=647]Epoch 16:   8%|▊         | 180/2191 [02:46<30:49,  1.09it/s, loss=2.39, v_num=647]Epoch 16:   9%|▊         | 190/2191 [02:53<30:17,  1.10it/s, loss=2.39, v_num=647]Epoch 16:   9%|▊         | 190/2191 [02:53<30:17,  1.10it/s, loss=2.39, v_num=647]Epoch 16:   9%|▉         | 200/2191 [03:01<29:55,  1.11it/s, loss=2.39, v_num=647]Epoch 16:   9%|▉         | 200/2191 [03:01<29:55,  1.11it/s, loss=2.38, v_num=647]Epoch 16:  10%|▉         | 210/2191 [03:08<29:30,  1.12it/s, loss=2.38, v_num=647]Epoch 16:  10%|▉         | 210/2191 [03:08<29:30,  1.12it/s, loss=2.42, v_num=647]Epoch 16:  10%|█         | 220/2191 [03:15<29:04,  1.13it/s, loss=2.42, v_num=647]Epoch 16:  10%|█         | 220/2191 [03:15<29:04,  1.13it/s, loss=2.39, v_num=647]Epoch 16:  10%|█         | 230/2191 [03:22<28:40,  1.14it/s, loss=2.39, v_num=647]Epoch 16:  10%|█         | 230/2191 [03:22<28:40,  1.14it/s, loss=2.34, v_num=647]Epoch 16:  11%|█         | 240/2191 [03:30<28:24,  1.14it/s, loss=2.34, v_num=647]Epoch 16:  11%|█         | 240/2191 [03:30<28:24,  1.14it/s, loss=2.34, v_num=647]Epoch 16:  11%|█▏        | 250/2191 [03:37<28:05,  1.15it/s, loss=2.34, v_num=647]Epoch 16:  11%|█▏        | 250/2191 [03:37<28:05,  1.15it/s, loss=2.34, v_num=647]Epoch 16:  12%|█▏        | 260/2191 [03:46<27:52,  1.15it/s, loss=2.34, v_num=647]Epoch 16:  12%|█▏        | 260/2191 [03:46<27:52,  1.15it/s, loss=2.36, v_num=647]Epoch 16:  12%|█▏        | 270/2191 [03:52<27:30,  1.16it/s, loss=2.36, v_num=647]Epoch 16:  12%|█▏        | 270/2191 [03:52<27:30,  1.16it/s, loss=2.38, v_num=647]Epoch 16:  13%|█▎        | 280/2191 [03:59<27:11,  1.17it/s, loss=2.38, v_num=647]Epoch 16:  13%|█▎        | 280/2191 [03:59<27:11,  1.17it/s, loss=2.41, v_num=647]Epoch 16:  13%|█▎        | 290/2191 [04:07<26:55,  1.18it/s, loss=2.41, v_num=647]Epoch 16:  13%|█▎        | 290/2191 [04:07<26:55,  1.18it/s, loss=2.42, v_num=647]Epoch 16:  14%|█▎        | 300/2191 [04:14<26:36,  1.18it/s, loss=2.42, v_num=647]Epoch 16:  14%|█▎        | 300/2191 [04:14<26:36,  1.18it/s, loss=2.42, v_num=647]Epoch 16:  14%|█▍        | 310/2191 [04:21<26:19,  1.19it/s, loss=2.42, v_num=647]Epoch 16:  14%|█▍        | 310/2191 [04:21<26:19,  1.19it/s, loss=2.38, v_num=647]Epoch 16:  15%|█▍        | 320/2191 [04:28<26:05,  1.20it/s, loss=2.38, v_num=647]Epoch 16:  15%|█▍        | 320/2191 [04:28<26:05,  1.20it/s, loss=2.39, v_num=647]Epoch 16:  15%|█▌        | 330/2191 [04:35<25:49,  1.20it/s, loss=2.39, v_num=647]Epoch 16:  15%|█▌        | 330/2191 [04:35<25:49,  1.20it/s, loss=2.4, v_num=647] Epoch 16:  16%|█▌        | 340/2191 [04:42<25:31,  1.21it/s, loss=2.4, v_num=647]Epoch 16:  16%|█▌        | 340/2191 [04:42<25:31,  1.21it/s, loss=2.4, v_num=647]Epoch 16:  16%|█▌        | 350/2191 [04:49<25:16,  1.21it/s, loss=2.4, v_num=647]Epoch 16:  16%|█▌        | 350/2191 [04:49<25:16,  1.21it/s, loss=2.4, v_num=647]Epoch 16:  16%|█▋        | 360/2191 [04:56<25:02,  1.22it/s, loss=2.4, v_num=647]Epoch 16:  16%|█▋        | 360/2191 [04:56<25:02,  1.22it/s, loss=2.39, v_num=647]Epoch 16:  17%|█▋        | 370/2191 [05:03<24:50,  1.22it/s, loss=2.39, v_num=647]Epoch 16:  17%|█▋        | 370/2191 [05:03<24:50,  1.22it/s, loss=2.4, v_num=647] Epoch 16:  17%|█▋        | 380/2191 [05:11<24:38,  1.22it/s, loss=2.4, v_num=647]Epoch 16:  17%|█▋        | 380/2191 [05:11<24:38,  1.22it/s, loss=2.39, v_num=647]Epoch 16:  18%|█▊        | 390/2191 [05:18<24:28,  1.23it/s, loss=2.39, v_num=647]Epoch 16:  18%|█▊        | 390/2191 [05:18<24:28,  1.23it/s, loss=2.38, v_num=647]Epoch 16:  18%|█▊        | 400/2191 [05:26<24:16,  1.23it/s, loss=2.38, v_num=647]Epoch 16:  18%|█▊        | 400/2191 [05:26<24:16,  1.23it/s, loss=2.36, v_num=647]Epoch 16:  19%|█▊        | 410/2191 [05:33<24:03,  1.23it/s, loss=2.36, v_num=647]Epoch 16:  19%|█▊        | 410/2191 [05:33<24:03,  1.23it/s, loss=2.37, v_num=647]Epoch 16:  19%|█▉        | 420/2191 [05:40<23:52,  1.24it/s, loss=2.37, v_num=647]Epoch 16:  19%|█▉        | 420/2191 [05:40<23:52,  1.24it/s, loss=2.38, v_num=647]Epoch 16:  20%|█▉        | 430/2191 [05:47<23:40,  1.24it/s, loss=2.38, v_num=647]Epoch 16:  20%|█▉        | 430/2191 [05:47<23:40,  1.24it/s, loss=2.35, v_num=647]Epoch 16:  20%|██        | 440/2191 [05:56<23:33,  1.24it/s, loss=2.35, v_num=647]Epoch 16:  20%|██        | 440/2191 [05:56<23:33,  1.24it/s, loss=2.35, v_num=647]Epoch 16:  21%|██        | 450/2191 [06:04<23:26,  1.24it/s, loss=2.35, v_num=647]Epoch 16:  21%|██        | 450/2191 [06:04<23:26,  1.24it/s, loss=2.33, v_num=647]Epoch 16:  21%|██        | 460/2191 [06:11<23:16,  1.24it/s, loss=2.33, v_num=647]Epoch 16:  21%|██        | 460/2191 [06:11<23:16,  1.24it/s, loss=2.33, v_num=647]Epoch 16:  21%|██▏       | 470/2191 [06:19<23:05,  1.24it/s, loss=2.33, v_num=647]Epoch 16:  21%|██▏       | 470/2191 [06:19<23:05,  1.24it/s, loss=2.34, v_num=647]Epoch 16:  22%|██▏       | 480/2191 [06:26<22:56,  1.24it/s, loss=2.34, v_num=647]Epoch 16:  22%|██▏       | 480/2191 [06:26<22:56,  1.24it/s, loss=2.38, v_num=647]Epoch 16:  22%|██▏       | 490/2191 [06:34<22:47,  1.24it/s, loss=2.38, v_num=647]Epoch 16:  22%|██▏       | 490/2191 [06:34<22:47,  1.24it/s, loss=2.4, v_num=647] Epoch 16:  23%|██▎       | 500/2191 [06:42<22:37,  1.25it/s, loss=2.4, v_num=647]Epoch 16:  23%|██▎       | 500/2191 [06:42<22:37,  1.25it/s, loss=2.4, v_num=647]Epoch 16:  23%|██▎       | 510/2191 [06:49<22:27,  1.25it/s, loss=2.4, v_num=647]Epoch 16:  23%|██▎       | 510/2191 [06:49<22:27,  1.25it/s, loss=2.35, v_num=647]Epoch 16:  24%|██▎       | 520/2191 [06:56<22:16,  1.25it/s, loss=2.35, v_num=647]Epoch 16:  24%|██▎       | 520/2191 [06:56<22:16,  1.25it/s, loss=2.37, v_num=647]Epoch 16:  24%|██▍       | 530/2191 [07:03<22:05,  1.25it/s, loss=2.37, v_num=647]Epoch 16:  24%|██▍       | 530/2191 [07:03<22:05,  1.25it/s, loss=2.4, v_num=647] Epoch 16:  25%|██▍       | 540/2191 [07:10<21:53,  1.26it/s, loss=2.4, v_num=647]Epoch 16:  25%|██▍       | 540/2191 [07:10<21:53,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  25%|██▌       | 550/2191 [07:17<21:43,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  25%|██▌       | 550/2191 [07:17<21:43,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  26%|██▌       | 560/2191 [07:25<21:33,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  26%|██▌       | 560/2191 [07:25<21:33,  1.26it/s, loss=2.36, v_num=647]Epoch 16:  26%|██▌       | 570/2191 [07:32<21:23,  1.26it/s, loss=2.36, v_num=647]Epoch 16:  26%|██▌       | 570/2191 [07:32<21:23,  1.26it/s, loss=2.37, v_num=647]Epoch 16:  26%|██▋       | 580/2191 [07:38<21:12,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  26%|██▋       | 580/2191 [07:38<21:12,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  27%|██▋       | 590/2191 [07:46<21:02,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  27%|██▋       | 590/2191 [07:46<21:02,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  27%|██▋       | 600/2191 [07:53<20:54,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  27%|██▋       | 600/2191 [07:53<20:54,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  28%|██▊       | 610/2191 [08:00<20:43,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  28%|██▊       | 610/2191 [08:00<20:43,  1.27it/s, loss=2.45, v_num=647]Epoch 16:  28%|██▊       | 620/2191 [08:08<20:36,  1.27it/s, loss=2.45, v_num=647]Epoch 16:  28%|██▊       | 620/2191 [08:08<20:36,  1.27it/s, loss=2.43, v_num=647]Epoch 16:  29%|██▉       | 630/2191 [08:16<20:28,  1.27it/s, loss=2.43, v_num=647]Epoch 16:  29%|██▉       | 630/2191 [08:16<20:28,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  29%|██▉       | 640/2191 [08:23<20:19,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  29%|██▉       | 640/2191 [08:23<20:19,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  30%|██▉       | 650/2191 [08:31<20:11,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  30%|██▉       | 650/2191 [08:31<20:11,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  30%|███       | 660/2191 [08:39<20:02,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  30%|███       | 660/2191 [08:39<20:02,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  31%|███       | 670/2191 [08:46<19:54,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  31%|███       | 670/2191 [08:46<19:54,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  31%|███       | 680/2191 [08:54<19:45,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  31%|███       | 680/2191 [08:54<19:45,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  31%|███▏      | 690/2191 [09:01<19:36,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  31%|███▏      | 690/2191 [09:01<19:36,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  32%|███▏      | 700/2191 [09:08<19:26,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  32%|███▏      | 700/2191 [09:08<19:26,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  32%|███▏      | 710/2191 [09:16<19:19,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  32%|███▏      | 710/2191 [09:16<19:19,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  33%|███▎      | 720/2191 [09:24<19:10,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  33%|███▎      | 720/2191 [09:24<19:10,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  33%|███▎      | 730/2191 [09:32<19:03,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  33%|███▎      | 730/2191 [09:32<19:03,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  34%|███▍      | 740/2191 [09:39<18:55,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  34%|███▍      | 740/2191 [09:39<18:55,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  34%|███▍      | 750/2191 [09:47<18:46,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  34%|███▍      | 750/2191 [09:47<18:46,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  35%|███▍      | 760/2191 [09:54<18:37,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  35%|███▍      | 760/2191 [09:54<18:37,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  35%|███▌      | 770/2191 [10:02<18:30,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  35%|███▌      | 770/2191 [10:02<18:30,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  36%|███▌      | 780/2191 [10:10<18:22,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  36%|███▌      | 780/2191 [10:10<18:22,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  36%|███▌      | 790/2191 [10:17<18:14,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  36%|███▌      | 790/2191 [10:17<18:14,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  37%|███▋      | 800/2191 [10:25<18:06,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  37%|███▋      | 800/2191 [10:25<18:06,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  37%|███▋      | 810/2191 [10:33<17:58,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  37%|███▋      | 810/2191 [10:33<17:58,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  37%|███▋      | 820/2191 [10:42<17:52,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  37%|███▋      | 820/2191 [10:42<17:52,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  37%|███▋      | 820/2191 [10:54<18:13,  1.25it/s, loss=2.41, v_num=647]Epoch 16:  38%|███▊      | 830/2191 [10:56<17:54,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  38%|███▊      | 830/2191 [10:56<17:54,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  38%|███▊      | 840/2191 [11:04<17:48,  1.26it/s, loss=2.4, v_num=647]Epoch 16:  38%|███▊      | 840/2191 [11:04<17:48,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  39%|███▉      | 850/2191 [11:13<17:40,  1.26it/s, loss=2.38, v_num=647]Epoch 16:  39%|███▉      | 850/2191 [11:13<17:40,  1.26it/s, loss=2.37, v_num=647]Epoch 16:  39%|███▉      | 860/2191 [11:20<17:31,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  39%|███▉      | 860/2191 [11:20<17:31,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  40%|███▉      | 870/2191 [11:28<17:24,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  40%|███▉      | 870/2191 [11:28<17:24,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  40%|████      | 880/2191 [11:35<17:15,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  40%|████      | 880/2191 [11:35<17:15,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  41%|████      | 890/2191 [11:42<17:06,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  41%|████      | 890/2191 [11:42<17:06,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  41%|████      | 900/2191 [11:50<16:57,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  41%|████      | 900/2191 [11:50<16:57,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  42%|████▏     | 910/2191 [11:58<16:50,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  42%|████▏     | 910/2191 [11:58<16:50,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  42%|████▏     | 920/2191 [12:06<16:42,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  42%|████▏     | 920/2191 [12:06<16:42,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  42%|████▏     | 930/2191 [12:15<16:35,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  42%|████▏     | 930/2191 [12:15<16:35,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  43%|████▎     | 940/2191 [12:22<16:27,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  43%|████▎     | 940/2191 [12:22<16:27,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  43%|████▎     | 950/2191 [12:30<16:19,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  43%|████▎     | 950/2191 [12:30<16:19,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  44%|████▍     | 960/2191 [12:38<16:11,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  44%|████▍     | 960/2191 [12:38<16:11,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  44%|████▍     | 970/2191 [12:45<16:02,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  44%|████▍     | 970/2191 [12:45<16:02,  1.27it/s, loss=2.43, v_num=647]Epoch 16:  45%|████▍     | 980/2191 [12:52<15:54,  1.27it/s, loss=2.43, v_num=647]Epoch 16:  45%|████▍     | 980/2191 [12:52<15:54,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  45%|████▌     | 990/2191 [13:00<15:46,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  45%|████▌     | 990/2191 [13:00<15:46,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  46%|████▌     | 1000/2191 [13:09<15:39,  1.27it/s, loss=2.33, v_num=647]Epoch 16:  46%|████▌     | 1000/2191 [13:09<15:39,  1.27it/s, loss=2.34, v_num=647]Epoch 16:  46%|████▌     | 1010/2191 [13:16<15:30,  1.27it/s, loss=2.34, v_num=647]Epoch 16:  46%|████▌     | 1010/2191 [13:16<15:30,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  47%|████▋     | 1020/2191 [13:25<15:23,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  47%|████▋     | 1020/2191 [13:25<15:23,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  47%|████▋     | 1030/2191 [13:32<15:15,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  47%|████▋     | 1030/2191 [13:32<15:15,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  47%|████▋     | 1040/2191 [13:41<15:07,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  47%|████▋     | 1040/2191 [13:41<15:07,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  48%|████▊     | 1050/2191 [13:49<15:00,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  48%|████▊     | 1050/2191 [13:49<15:00,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  48%|████▊     | 1060/2191 [13:56<14:51,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  48%|████▊     | 1060/2191 [13:56<14:51,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  49%|████▉     | 1070/2191 [14:04<14:43,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  49%|████▉     | 1070/2191 [14:04<14:43,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  49%|████▉     | 1080/2191 [14:11<14:35,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  49%|████▉     | 1080/2191 [14:11<14:35,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  50%|████▉     | 1090/2191 [14:18<14:26,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  50%|████▉     | 1090/2191 [14:18<14:26,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  50%|█████     | 1100/2191 [14:26<14:18,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  50%|█████     | 1100/2191 [14:26<14:18,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  51%|█████     | 1110/2191 [14:35<14:11,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  51%|█████     | 1110/2191 [14:35<14:11,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  51%|█████     | 1120/2191 [14:43<14:04,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  51%|█████     | 1120/2191 [14:43<14:04,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  52%|█████▏    | 1130/2191 [14:52<13:56,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  52%|█████▏    | 1130/2191 [14:52<13:56,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  52%|█████▏    | 1140/2191 [14:59<13:48,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  52%|█████▏    | 1140/2191 [14:59<13:48,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  52%|█████▏    | 1150/2191 [15:07<13:40,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  52%|█████▏    | 1150/2191 [15:07<13:40,  1.27it/s, loss=2.44, v_num=647]Epoch 16:  53%|█████▎    | 1160/2191 [15:14<13:32,  1.27it/s, loss=2.44, v_num=647]Epoch 16:  53%|█████▎    | 1160/2191 [15:14<13:32,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  53%|█████▎    | 1170/2191 [15:22<13:24,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  53%|█████▎    | 1170/2191 [15:22<13:24,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  54%|█████▍    | 1180/2191 [15:29<13:15,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  54%|█████▍    | 1180/2191 [15:29<13:15,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  54%|█████▍    | 1190/2191 [15:37<13:08,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  54%|█████▍    | 1190/2191 [15:37<13:08,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  55%|█████▍    | 1200/2191 [15:44<12:59,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  55%|█████▍    | 1200/2191 [15:44<12:59,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  55%|█████▌    | 1210/2191 [15:52<12:51,  1.27it/s, loss=2.41, v_num=647]Epoch 16:  55%|█████▌    | 1210/2191 [15:52<12:51,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  56%|█████▌    | 1220/2191 [15:59<12:42,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  56%|█████▌    | 1220/2191 [15:59<12:42,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  56%|█████▌    | 1230/2191 [16:08<12:35,  1.27it/s, loss=2.37, v_num=647]Epoch 16:  56%|█████▌    | 1230/2191 [16:08<12:35,  1.27it/s, loss=2.36, v_num=647]Epoch 16:  57%|█████▋    | 1240/2191 [16:14<12:27,  1.27it/s, loss=2.36, v_num=647]Epoch 16:  57%|█████▋    | 1240/2191 [16:14<12:27,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  57%|█████▋    | 1250/2191 [16:22<12:18,  1.27it/s, loss=2.39, v_num=647]Epoch 16:  57%|█████▋    | 1250/2191 [16:22<12:18,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  58%|█████▊    | 1260/2191 [16:30<12:11,  1.27it/s, loss=2.42, v_num=647]Epoch 16:  58%|█████▊    | 1260/2191 [16:30<12:11,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  58%|█████▊    | 1270/2191 [16:37<12:03,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  58%|█████▊    | 1270/2191 [16:37<12:03,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  58%|█████▊    | 1280/2191 [16:45<11:54,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  58%|█████▊    | 1280/2191 [16:45<11:54,  1.27it/s, loss=2.4, v_num=647] Epoch 16:  59%|█████▉    | 1290/2191 [16:52<11:46,  1.27it/s, loss=2.4, v_num=647]Epoch 16:  59%|█████▉    | 1290/2191 [16:52<11:46,  1.27it/s, loss=2.38, v_num=647]Epoch 16:  59%|█████▉    | 1300/2191 [17:00<11:38,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  59%|█████▉    | 1300/2191 [17:00<11:38,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  60%|█████▉    | 1310/2191 [17:07<11:30,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  60%|█████▉    | 1310/2191 [17:07<11:30,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  60%|██████    | 1320/2191 [17:15<11:22,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  60%|██████    | 1320/2191 [17:15<11:22,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  61%|██████    | 1330/2191 [17:22<11:14,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  61%|██████    | 1330/2191 [17:22<11:14,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  61%|██████    | 1340/2191 [17:29<11:06,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  61%|██████    | 1340/2191 [17:29<11:06,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  62%|██████▏   | 1350/2191 [17:37<10:57,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  62%|██████▏   | 1350/2191 [17:37<10:57,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  62%|██████▏   | 1360/2191 [17:44<10:49,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  62%|██████▏   | 1360/2191 [17:44<10:49,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  63%|██████▎   | 1370/2191 [17:51<10:41,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  63%|██████▎   | 1370/2191 [17:51<10:41,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  63%|██████▎   | 1380/2191 [17:58<10:33,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  63%|██████▎   | 1380/2191 [17:58<10:33,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  63%|██████▎   | 1390/2191 [18:05<10:24,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  63%|██████▎   | 1390/2191 [18:05<10:24,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  64%|██████▍   | 1400/2191 [18:12<10:16,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  64%|██████▍   | 1400/2191 [18:12<10:16,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  64%|██████▍   | 1410/2191 [18:19<10:08,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  64%|██████▍   | 1410/2191 [18:19<10:08,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  65%|██████▍   | 1420/2191 [18:27<10:00,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  65%|██████▍   | 1420/2191 [18:27<10:00,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  65%|██████▌   | 1430/2191 [18:36<09:53,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  65%|██████▌   | 1430/2191 [18:36<09:53,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  66%|██████▌   | 1440/2191 [18:44<09:46,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  66%|██████▌   | 1440/2191 [18:44<09:46,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  66%|██████▌   | 1450/2191 [18:51<09:37,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  66%|██████▌   | 1450/2191 [18:51<09:37,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  67%|██████▋   | 1460/2191 [18:58<09:29,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  67%|██████▋   | 1460/2191 [18:58<09:29,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  67%|██████▋   | 1470/2191 [19:05<09:21,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  67%|██████▋   | 1470/2191 [19:05<09:21,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  68%|██████▊   | 1480/2191 [19:13<09:13,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  68%|██████▊   | 1480/2191 [19:13<09:13,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  68%|██████▊   | 1490/2191 [19:20<09:05,  1.29it/s, loss=2.41, v_num=647]Epoch 16:  68%|██████▊   | 1490/2191 [19:20<09:05,  1.29it/s, loss=2.38, v_num=647]Epoch 16:  68%|██████▊   | 1500/2191 [19:28<08:57,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  68%|██████▊   | 1500/2191 [19:28<08:57,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  69%|██████▉   | 1510/2191 [19:35<08:49,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  69%|██████▉   | 1510/2191 [19:35<08:49,  1.29it/s, loss=2.36, v_num=647]Epoch 16:  69%|██████▉   | 1520/2191 [19:44<08:42,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  69%|██████▉   | 1520/2191 [19:44<08:42,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  70%|██████▉   | 1530/2191 [19:52<08:34,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  70%|██████▉   | 1530/2191 [19:52<08:34,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  70%|███████   | 1540/2191 [20:00<08:26,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  70%|███████   | 1540/2191 [20:00<08:26,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  71%|███████   | 1550/2191 [20:08<08:19,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  71%|███████   | 1550/2191 [20:08<08:19,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  71%|███████   | 1560/2191 [20:16<08:11,  1.28it/s, loss=2.42, v_num=647]Epoch 16:  71%|███████   | 1560/2191 [20:16<08:11,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  72%|███████▏  | 1570/2191 [20:24<08:03,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  72%|███████▏  | 1570/2191 [20:24<08:03,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  72%|███████▏  | 1580/2191 [20:32<07:56,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  72%|███████▏  | 1580/2191 [20:32<07:56,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  73%|███████▎  | 1590/2191 [20:40<07:48,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  73%|███████▎  | 1590/2191 [20:40<07:48,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  73%|███████▎  | 1600/2191 [20:47<07:40,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  73%|███████▎  | 1600/2191 [20:47<07:40,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  73%|███████▎  | 1610/2191 [20:55<07:32,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  73%|███████▎  | 1610/2191 [20:55<07:32,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  74%|███████▍  | 1620/2191 [21:03<07:25,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  74%|███████▍  | 1620/2191 [21:03<07:25,  1.28it/s, loss=2.43, v_num=647]Epoch 16:  74%|███████▍  | 1630/2191 [21:13<07:18,  1.28it/s, loss=2.43, v_num=647]Epoch 16:  74%|███████▍  | 1630/2191 [21:13<07:18,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  75%|███████▍  | 1640/2191 [21:21<07:10,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  75%|███████▍  | 1640/2191 [21:21<07:10,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  75%|███████▌  | 1650/2191 [21:29<07:02,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  75%|███████▌  | 1650/2191 [21:29<07:02,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  76%|███████▌  | 1660/2191 [21:37<06:54,  1.28it/s, loss=2.38, v_num=647]Epoch 16:  76%|███████▌  | 1660/2191 [21:37<06:54,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  76%|███████▌  | 1670/2191 [21:44<06:46,  1.28it/s, loss=2.36, v_num=647]Epoch 16:  76%|███████▌  | 1670/2191 [21:44<06:46,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  77%|███████▋  | 1680/2191 [21:51<06:38,  1.28it/s, loss=2.35, v_num=647]Epoch 16:  77%|███████▋  | 1680/2191 [21:51<06:38,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  77%|███████▋  | 1690/2191 [22:01<06:31,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  77%|███████▋  | 1690/2191 [22:01<06:31,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  78%|███████▊  | 1700/2191 [22:07<06:23,  1.28it/s, loss=2.39, v_num=647]Epoch 16:  78%|███████▊  | 1700/2191 [22:07<06:23,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  78%|███████▊  | 1710/2191 [22:14<06:15,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  78%|███████▊  | 1710/2191 [22:14<06:15,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  79%|███████▊  | 1720/2191 [22:22<06:07,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  79%|███████▊  | 1720/2191 [22:22<06:07,  1.28it/s, loss=2.4, v_num=647] Epoch 16:  79%|███████▉  | 1730/2191 [22:29<05:59,  1.28it/s, loss=2.4, v_num=647]Epoch 16:  79%|███████▉  | 1730/2191 [22:29<05:59,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  79%|███████▉  | 1740/2191 [22:36<05:51,  1.28it/s, loss=2.37, v_num=647]Epoch 16:  79%|███████▉  | 1740/2191 [22:36<05:51,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  80%|███████▉  | 1750/2191 [22:43<05:43,  1.28it/s, loss=2.41, v_num=647]Epoch 16:  80%|███████▉  | 1750/2191 [22:43<05:43,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  80%|████████  | 1760/2191 [22:50<05:35,  1.29it/s, loss=2.44, v_num=647]Epoch 16:  80%|████████  | 1760/2191 [22:50<05:35,  1.29it/s, loss=2.41, v_num=647]Epoch 16:  81%|████████  | 1770/2191 [22:57<05:27,  1.29it/s, loss=2.41, v_num=647]Epoch 16:  81%|████████  | 1770/2191 [22:57<05:27,  1.29it/s, loss=2.37, v_num=647]Epoch 16:  81%|████████  | 1780/2191 [23:05<05:19,  1.29it/s, loss=2.37, v_num=647]Epoch 16:  81%|████████  | 1780/2191 [23:05<05:19,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  82%|████████▏ | 1790/2191 [23:13<05:12,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  82%|████████▏ | 1790/2191 [23:13<05:12,  1.29it/s, loss=2.44, v_num=647]Epoch 16:  82%|████████▏ | 1800/2191 [23:21<05:04,  1.28it/s, loss=2.44, v_num=647]Epoch 16:  82%|████████▏ | 1800/2191 [23:21<05:04,  1.28it/s, loss=2.48, v_num=647]Epoch 16:  83%|████████▎ | 1810/2191 [23:29<04:56,  1.29it/s, loss=2.48, v_num=647]Epoch 16:  83%|████████▎ | 1810/2191 [23:29<04:56,  1.29it/s, loss=2.42, v_num=647]Epoch 16:  83%|████████▎ | 1820/2191 [23:36<04:48,  1.29it/s, loss=2.42, v_num=647]Epoch 16:  83%|████████▎ | 1820/2191 [23:36<04:48,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  84%|████████▎ | 1830/2191 [23:42<04:40,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  84%|████████▎ | 1830/2191 [23:42<04:40,  1.29it/s, loss=2.4, v_num=647] Epoch 16:  84%|████████▍ | 1840/2191 [23:46<04:31,  1.29it/s, loss=2.4, v_num=647]Epoch 16:  84%|████████▍ | 1840/2191 [23:46<04:31,  1.29it/s, loss=2.36, v_num=647]Epoch 16:  84%|████████▍ | 1850/2191 [23:50<04:23,  1.29it/s, loss=2.36, v_num=647]Epoch 16:  84%|████████▍ | 1850/2191 [23:50<04:23,  1.29it/s, loss=2.39, v_num=647]Epoch 16:  85%|████████▍ | 1860/2191 [23:53<04:15,  1.30it/s, loss=2.39, v_num=647]Epoch 16:  85%|████████▍ | 1860/2191 [23:53<04:15,  1.30it/s, loss=2.4, v_num=647] validation_epoch_end
graph acc: 0.3514376996805112
valid accuracy: 0.9749715924263
98
validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9746177792549133
validation_epoch_end
graph acc: 0.329073482428115
valid accuracy: 0.9752295613288879
alid accuracy: 0.975863516330719
Epoch 16:  86%|████████▌ | 1880/2191 [23:58<03:57,  1.31it/s, loss=2.39, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.34824281150159747
valid accuracy: 0.9737935662269592
validation_epoch_end
graph acc: 0.38338658146964855
valid accuracy: 0.9763031601905823

Validating:   3%|▎         | 10/313 [00:01<00:46,  6.55it/s][AEpoch 16:  86%|████████▋ | 1890/2191 [23:59<03:49,  1.31it/s, loss=2.39, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:49,  5.88it/s][AEpoch 16:  87%|████████▋ | 1900/2191 [24:01<03:40,  1.32it/s, loss=2.39, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:37,  7.52it/s][AEpoch 16:  87%|████████▋ | 1910/2191 [24:02<03:32,  1.32it/s, loss=2.39, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:29,  9.25it/s][AEpoch 16:  88%|████████▊ | 1920/2191 [24:03<03:23,  1.33it/s, loss=2.39, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26,  9.80it/s][AEpoch 16:  88%|████████▊ | 1930/2191 [24:03<03:15,  1.34it/s, loss=2.39, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.57it/s][AEpoch 16:  89%|████████▊ | 1940/2191 [24:04<03:06,  1.34it/s, loss=2.39, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:25,  9.37it/s][AEpoch 16:  89%|████████▉ | 1950/2191 [24:06<02:58,  1.35it/s, loss=2.39, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.37it/s][AEpoch 16:  89%|████████▉ | 1960/2191 [24:06<02:50,  1.36it/s, loss=2.39, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:20, 10.83it/s][AEpoch 16:  90%|████████▉ | 1970/2191 [24:07<02:42,  1.36it/s, loss=2.39, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:21, 10.13it/s][AEpoch 16:  90%|█████████ | 1980/2191 [24:08<02:34,  1.37it/s, loss=2.39, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.86it/s][AEpoch 16:  91%|█████████ | 1990/2191 [24:09<02:26,  1.37it/s, loss=2.39, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.71it/s][AEpoch 16:  91%|█████████▏| 2000/2191 [24:10<02:18,  1.38it/s, loss=2.39, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.30it/s][AEpoch 16:  92%|█████████▏| 2010/2191 [24:12<02:10,  1.38it/s, loss=2.39, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.63it/s][AEpoch 16:  92%|█████████▏| 2020/2191 [24:13<02:02,  1.39it/s, loss=2.39, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:16,  9.78it/s][AEpoch 16:  93%|█████████▎| 2030/2191 [24:14<01:55,  1.40it/s, loss=2.39, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:14, 10.42it/s][AEpoch 16:  93%|█████████▎| 2040/2191 [24:14<01:47,  1.40it/s, loss=2.39, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:12, 11.37it/s][AEpoch 16:  94%|█████████▎| 2050/2191 [24:15<01:40,  1.41it/s, loss=2.39, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 11.03it/s][AEpoch 16:  94%|█████████▍| 2060/2191 [24:16<01:32,  1.41it/s, loss=2.39, v_num=647]
Validating:  61%|██████    | 190/313 [00:18<00:08, 13.79it/s][AEpoch 16:  94%|█████████▍| 2070/2191 [24:16<01:25,  1.42it/s, loss=2.39, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.60it/s][AEpoch 16:  95%|█████████▍| 2080/2191 [24:18<01:17,  1.43it/s, loss=2.39, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:20<00:09, 11.28it/s][AEpoch 16:  95%|█████████▌| 2090/2191 [24:19<01:10,  1.43it/s, loss=2.39, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09, 10.28it/s][AEpoch 16:  96%|█████████▌| 2100/2191 [24:20<01:03,  1.44it/s, loss=2.39, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:22<00:07, 11.18it/s][AEpoch 16:  96%|█████████▋| 2110/2191 [24:20<00:56,  1.44it/s, loss=2.39, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:23<00:06, 10.63it/s][AEpoch 16:  97%|█████████▋| 2120/2191 [24:21<00:48,  1.45it/s, loss=2.39, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:24<00:06, 10.32it/s][AEpoch 16:  97%|█████████▋| 2130/2191 [24:23<00:41,  1.46it/s, loss=2.39, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:25<00:05, 10.14it/s][AEpoch 16:  98%|█████████▊| 2140/2191 [24:24<00:34,  1.46it/s, loss=2.39, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 11.52it/s][AEpoch 16:  98%|█████████▊| 2150/2191 [24:24<00:27,  1.47it/s, loss=2.39, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.42it/s][AEpoch 16:  99%|█████████▊| 2160/2191 [24:25<00:21,  1.47it/s, loss=2.39, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.39it/s][AEpoch 16:  99%|█████████▉| 2170/2191 [24:26<00:14,  1.48it/s, loss=2.39, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:28<00:00, 13.40it/s][AEpoch 16:  99%|█████████▉| 2180/2191 [24:26<00:07,  1.49it/s, loss=2.39, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.46it/s][AEpoch 16: 100%|█████████▉| 2190/2191 [24:27<00:00,  1.49it/s, loss=2.39, v_num=647]validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9764299392700195
Epoch 16: 100%|██████████| 2191/2191 [24:29<00:00,  1.49it/s, loss=2.42, v_num=647]
                                                             [AEpoch 16:   0%|          | 0/2191 [00:00<00:00, 16320.25it/s, loss=2.42, v_num=647]Epoch 17:   0%|          | 0/2191 [00:00<00:00, 3923.58it/s, loss=2.42, v_num=647] Epoch 17:   0%|          | 10/2191 [00:13<45:54,  1.26s/it, loss=2.42, v_num=647] Epoch 17:   0%|          | 10/2191 [00:13<45:54,  1.26s/it, loss=2.43, v_num=647]Epoch 17:   1%|          | 20/2191 [00:23<40:36,  1.12s/it, loss=2.43, v_num=647]Epoch 17:   1%|          | 20/2191 [00:23<40:36,  1.12s/it, loss=2.37, v_num=647]Epoch 17:   1%|▏         | 30/2191 [00:32<38:09,  1.06s/it, loss=2.37, v_num=647]Epoch 17:   1%|▏         | 30/2191 [00:32<38:09,  1.06s/it, loss=2.37, v_num=647]Epoch 17:   2%|▏         | 40/2191 [00:43<37:37,  1.05s/it, loss=2.37, v_num=647]Epoch 17:   2%|▏         | 40/2191 [00:43<37:37,  1.05s/it, loss=2.4, v_num=647] Epoch 17:   2%|▏         | 50/2191 [00:53<37:08,  1.04s/it, loss=2.4, v_num=647]Epoch 17:   2%|▏         | 50/2191 [00:53<37:08,  1.04s/it, loss=2.42, v_num=647]Epoch 17:   3%|▎         | 60/2191 [01:03<36:46,  1.04s/it, loss=2.42, v_num=647]Epoch 17:   3%|▎         | 60/2191 [01:03<36:46,  1.04s/it, loss=2.41, v_num=647]Epoch 17:   3%|▎         | 70/2191 [01:12<36:03,  1.02s/it, loss=2.41, v_num=647]Epoch 17:   3%|▎         | 70/2191 [01:12<36:03,  1.02s/it, loss=2.39, v_num=647]Epoch 17:   4%|▎         | 80/2191 [01:22<35:52,  1.02s/it, loss=2.39, v_num=647]Epoch 17:   4%|▎         | 80/2191 [01:22<35:52,  1.02s/it, loss=2.4, v_num=647] Epoch 17:   4%|▍         | 90/2191 [01:31<35:05,  1.00s/it, loss=2.4, v_num=647]Epoch 17:   4%|▍         | 90/2191 [01:31<35:05,  1.00s/it, loss=2.39, v_num=647]Epoch 17:   5%|▍         | 100/2191 [01:40<34:39,  1.01it/s, loss=2.39, v_num=647]Epoch 17:   5%|▍         | 100/2191 [01:40<34:39,  1.01it/s, loss=2.33, v_num=647]Epoch 17:   5%|▌         | 110/2191 [01:49<34:17,  1.01it/s, loss=2.33, v_num=647]Epoch 17:   5%|▌         | 110/2191 [01:49<34:17,  1.01it/s, loss=2.32, v_num=647]Epoch 17:   5%|▌         | 120/2191 [01:58<33:43,  1.02it/s, loss=2.32, v_num=647]Epoch 17:   5%|▌         | 120/2191 [01:58<33:43,  1.02it/s, loss=2.34, v_num=647]Epoch 17:   6%|▌         | 130/2191 [02:06<33:09,  1.04it/s, loss=2.34, v_num=647]Epoch 17:   6%|▌         | 130/2191 [02:06<33:09,  1.04it/s, loss=2.33, v_num=647]Epoch 17:   6%|▋         | 140/2191 [02:14<32:34,  1.05it/s, loss=2.33, v_num=647]Epoch 17:   6%|▋         | 140/2191 [02:14<32:34,  1.05it/s, loss=2.35, v_num=647]Epoch 17:   7%|▋         | 150/2191 [02:22<32:01,  1.06it/s, loss=2.35, v_num=647]Epoch 17:   7%|▋         | 150/2191 [02:22<32:01,  1.06it/s, loss=2.37, v_num=647]Epoch 17:   7%|▋         | 160/2191 [02:29<31:21,  1.08it/s, loss=2.37, v_num=647]Epoch 17:   7%|▋         | 160/2191 [02:29<31:21,  1.08it/s, loss=2.38, v_num=647]Epoch 17:   8%|▊         | 170/2191 [02:36<30:45,  1.10it/s, loss=2.38, v_num=647]Epoch 17:   8%|▊         | 170/2191 [02:36<30:45,  1.10it/s, loss=2.38, v_num=647]Epoch 17:   8%|▊         | 180/2191 [02:43<30:14,  1.11it/s, loss=2.38, v_num=647]Epoch 17:   8%|▊         | 180/2191 [02:43<30:14,  1.11it/s, loss=2.35, v_num=647]Epoch 17:   9%|▊         | 190/2191 [02:50<29:44,  1.12it/s, loss=2.35, v_num=647]Epoch 17:   9%|▊         | 190/2191 [02:50<29:44,  1.12it/s, loss=2.34, v_num=647]Epoch 17:   9%|▉         | 200/2191 [02:57<29:14,  1.13it/s, loss=2.34, v_num=647]Epoch 17:   9%|▉         | 200/2191 [02:57<29:14,  1.13it/s, loss=2.35, v_num=647]Epoch 17:  10%|▉         | 210/2191 [03:04<28:50,  1.14it/s, loss=2.35, v_num=647]Epoch 17:  10%|▉         | 210/2191 [03:04<28:50,  1.14it/s, loss=2.38, v_num=647]Epoch 17:  10%|█         | 220/2191 [03:11<28:27,  1.15it/s, loss=2.38, v_num=647]Epoch 17:  10%|█         | 220/2191 [03:11<28:27,  1.15it/s, loss=2.39, v_num=647]Epoch 17:  10%|█         | 230/2191 [03:17<28:00,  1.17it/s, loss=2.39, v_num=647]Epoch 17:  10%|█         | 230/2191 [03:17<28:00,  1.17it/s, loss=2.36, v_num=647]Epoch 17:  11%|█         | 240/2191 [03:25<27:41,  1.17it/s, loss=2.36, v_num=647]Epoch 17:  11%|█         | 240/2191 [03:25<27:41,  1.17it/s, loss=2.33, v_num=647]Epoch 17:  11%|█▏        | 250/2191 [03:34<27:36,  1.17it/s, loss=2.33, v_num=647]Epoch 17:  11%|█▏        | 250/2191 [03:34<27:36,  1.17it/s, loss=2.35, v_num=647]Epoch 17:  12%|█▏        | 260/2191 [03:41<27:16,  1.18it/s, loss=2.35, v_num=647]Epoch 17:  12%|█▏        | 260/2191 [03:41<27:16,  1.18it/s, loss=2.38, v_num=647]Epoch 17:  12%|█▏        | 270/2191 [03:48<26:58,  1.19it/s, loss=2.38, v_num=647]Epoch 17:  12%|█▏        | 270/2191 [03:48<26:58,  1.19it/s, loss=2.37, v_num=647]Epoch 17:  13%|█▎        | 280/2191 [03:55<26:41,  1.19it/s, loss=2.37, v_num=647]Epoch 17:  13%|█▎        | 280/2191 [03:55<26:41,  1.19it/s, loss=2.36, v_num=647]Epoch 17:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.36, v_num=647]Epoch 17:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.4, v_num=647] Epoch 17:  14%|█▎        | 300/2191 [04:09<26:06,  1.21it/s, loss=2.4, v_num=647]Epoch 17:  14%|█▎        | 300/2191 [04:09<26:06,  1.21it/s, loss=2.39, v_num=647]Epoch 17:  14%|█▍        | 310/2191 [04:16<25:50,  1.21it/s, loss=2.39, v_num=647]Epoch 17:  14%|█▍        | 310/2191 [04:16<25:50,  1.21it/s, loss=2.36, v_num=647]Epoch 17:  15%|█▍        | 320/2191 [04:23<25:35,  1.22it/s, loss=2.36, v_num=647]Epoch 17:  15%|█▍        | 320/2191 [04:23<25:35,  1.22it/s, loss=2.38, v_num=647]Epoch 17:  15%|█▌        | 330/2191 [04:30<25:19,  1.22it/s, loss=2.38, v_num=647]Epoch 17:  15%|█▌        | 330/2191 [04:30<25:19,  1.22it/s, loss=2.4, v_num=647] Epoch 17:  16%|█▌        | 340/2191 [04:37<25:04,  1.23it/s, loss=2.4, v_num=647]Epoch 17:  16%|█▌        | 340/2191 [04:37<25:04,  1.23it/s, loss=2.41, v_num=647]Epoch 17:  16%|█▌        | 350/2191 [04:44<24:52,  1.23it/s, loss=2.41, v_num=647]Epoch 17:  16%|█▌        | 350/2191 [04:44<24:52,  1.23it/s, loss=2.42, v_num=647]Epoch 17:  16%|█▋        | 360/2191 [04:51<24:37,  1.24it/s, loss=2.42, v_num=647]Epoch 17:  16%|█▋        | 360/2191 [04:51<24:37,  1.24it/s, loss=2.4, v_num=647] Epoch 17:  17%|█▋        | 370/2191 [04:58<24:25,  1.24it/s, loss=2.4, v_num=647]Epoch 17:  17%|█▋        | 370/2191 [04:58<24:25,  1.24it/s, loss=2.38, v_num=647]Epoch 17:  17%|█▋        | 380/2191 [05:05<24:14,  1.25it/s, loss=2.38, v_num=647]Epoch 17:  17%|█▋        | 380/2191 [05:05<24:14,  1.25it/s, loss=2.38, v_num=647]Epoch 17:  18%|█▊        | 390/2191 [05:12<24:01,  1.25it/s, loss=2.38, v_num=647]Epoch 17:  18%|█▊        | 390/2191 [05:12<24:01,  1.25it/s, loss=2.34, v_num=647]Epoch 17:  18%|█▊        | 400/2191 [05:19<23:47,  1.25it/s, loss=2.34, v_num=647]Epoch 17:  18%|█▊        | 400/2191 [05:19<23:47,  1.25it/s, loss=2.31, v_num=647]Epoch 17:  19%|█▊        | 410/2191 [05:27<23:38,  1.26it/s, loss=2.31, v_num=647]Epoch 17:  19%|█▊        | 410/2191 [05:27<23:38,  1.26it/s, loss=2.35, v_num=647]Epoch 17:  19%|█▉        | 420/2191 [05:34<23:28,  1.26it/s, loss=2.35, v_num=647]Epoch 17:  19%|█▉        | 420/2191 [05:34<23:28,  1.26it/s, loss=2.35, v_num=647]Epoch 17:  20%|█▉        | 430/2191 [05:43<23:24,  1.25it/s, loss=2.35, v_num=647]Epoch 17:  20%|█▉        | 430/2191 [05:43<23:24,  1.25it/s, loss=2.35, v_num=647]Epoch 17:  20%|██        | 440/2191 [05:50<23:12,  1.26it/s, loss=2.35, v_num=647]Epoch 17:  20%|██        | 440/2191 [05:50<23:12,  1.26it/s, loss=2.37, v_num=647]Epoch 17:  21%|██        | 450/2191 [05:58<23:02,  1.26it/s, loss=2.37, v_num=647]Epoch 17:  21%|██        | 450/2191 [05:58<23:02,  1.26it/s, loss=2.4, v_num=647] Epoch 17:  21%|██        | 460/2191 [06:05<22:53,  1.26it/s, loss=2.4, v_num=647]Epoch 17:  21%|██        | 460/2191 [06:05<22:53,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  21%|██▏       | 470/2191 [06:14<22:48,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  21%|██▏       | 470/2191 [06:14<22:48,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  22%|██▏       | 480/2191 [06:22<22:41,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  22%|██▏       | 480/2191 [06:22<22:41,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  22%|██▏       | 490/2191 [06:29<22:31,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  22%|██▏       | 490/2191 [06:30<22:31,  1.26it/s, loss=2.4, v_num=647] Epoch 17:  23%|██▎       | 500/2191 [06:36<22:19,  1.26it/s, loss=2.4, v_num=647]Epoch 17:  23%|██▎       | 500/2191 [06:36<22:19,  1.26it/s, loss=2.39, v_num=647]Epoch 17:  23%|██▎       | 510/2191 [06:44<22:09,  1.26it/s, loss=2.39, v_num=647]Epoch 17:  23%|██▎       | 510/2191 [06:44<22:09,  1.26it/s, loss=2.4, v_num=647] Epoch 17:  24%|██▎       | 520/2191 [06:50<21:58,  1.27it/s, loss=2.4, v_num=647]Epoch 17:  24%|██▎       | 520/2191 [06:50<21:58,  1.27it/s, loss=2.41, v_num=647]Epoch 17:  24%|██▍       | 530/2191 [06:59<21:53,  1.26it/s, loss=2.41, v_num=647]Epoch 17:  24%|██▍       | 530/2191 [06:59<21:53,  1.26it/s, loss=2.43, v_num=647]Epoch 17:  25%|██▍       | 540/2191 [07:07<21:43,  1.27it/s, loss=2.43, v_num=647]Epoch 17:  25%|██▍       | 540/2191 [07:07<21:43,  1.27it/s, loss=2.44, v_num=647]Epoch 17:  25%|██▌       | 550/2191 [07:14<21:34,  1.27it/s, loss=2.44, v_num=647]Epoch 17:  25%|██▌       | 550/2191 [07:14<21:34,  1.27it/s, loss=2.39, v_num=647]Epoch 17:  26%|██▌       | 560/2191 [07:22<21:27,  1.27it/s, loss=2.39, v_num=647]Epoch 17:  26%|██▌       | 560/2191 [07:22<21:27,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  26%|██▌       | 570/2191 [07:30<21:18,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  26%|██▌       | 570/2191 [07:30<21:18,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  26%|██▋       | 580/2191 [07:37<21:08,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  26%|██▋       | 580/2191 [07:37<21:08,  1.27it/s, loss=2.37, v_num=647]Epoch 17:  27%|██▋       | 590/2191 [07:47<21:05,  1.26it/s, loss=2.37, v_num=647]Epoch 17:  27%|██▋       | 590/2191 [07:47<21:05,  1.26it/s, loss=2.41, v_num=647]Epoch 17:  27%|██▋       | 600/2191 [07:55<20:58,  1.26it/s, loss=2.41, v_num=647]Epoch 17:  27%|██▋       | 600/2191 [07:55<20:58,  1.26it/s, loss=2.41, v_num=647]Epoch 17:  28%|██▊       | 610/2191 [08:03<20:50,  1.26it/s, loss=2.41, v_num=647]Epoch 17:  28%|██▊       | 610/2191 [08:03<20:50,  1.26it/s, loss=2.37, v_num=647]Epoch 17:  28%|██▊       | 620/2191 [08:10<20:41,  1.26it/s, loss=2.37, v_num=647]Epoch 17:  28%|██▊       | 620/2191 [08:10<20:41,  1.26it/s, loss=2.38, v_num=647]Epoch 17:  29%|██▉       | 630/2191 [08:17<20:29,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  29%|██▉       | 630/2191 [08:17<20:29,  1.27it/s, loss=2.41, v_num=647]Epoch 17:  29%|██▉       | 640/2191 [08:24<20:20,  1.27it/s, loss=2.41, v_num=647]Epoch 17:  29%|██▉       | 640/2191 [08:24<20:20,  1.27it/s, loss=2.4, v_num=647] Epoch 17:  30%|██▉       | 650/2191 [08:31<20:10,  1.27it/s, loss=2.4, v_num=647]Epoch 17:  30%|██▉       | 650/2191 [08:31<20:10,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  30%|███       | 660/2191 [08:38<20:01,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  30%|███       | 660/2191 [08:38<20:01,  1.27it/s, loss=2.41, v_num=647]Epoch 17:  31%|███       | 670/2191 [08:46<19:53,  1.27it/s, loss=2.41, v_num=647]Epoch 17:  31%|███       | 670/2191 [08:46<19:53,  1.27it/s, loss=2.39, v_num=647]Epoch 17:  31%|███       | 680/2191 [08:54<19:45,  1.27it/s, loss=2.39, v_num=647]Epoch 17:  31%|███       | 680/2191 [08:54<19:45,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  31%|███▏      | 690/2191 [09:01<19:36,  1.28it/s, loss=2.38, v_num=647]Epoch 17:  31%|███▏      | 690/2191 [09:01<19:36,  1.28it/s, loss=2.4, v_num=647] Epoch 17:  32%|███▏      | 700/2191 [09:09<19:28,  1.28it/s, loss=2.4, v_num=647]Epoch 17:  32%|███▏      | 700/2191 [09:09<19:28,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  32%|███▏      | 710/2191 [09:16<19:20,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  32%|███▏      | 710/2191 [09:16<19:20,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  33%|███▎      | 720/2191 [09:24<19:12,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  33%|███▎      | 720/2191 [09:24<19:12,  1.28it/s, loss=2.38, v_num=647]Epoch 17:  33%|███▎      | 730/2191 [09:31<19:02,  1.28it/s, loss=2.38, v_num=647]Epoch 17:  33%|███▎      | 730/2191 [09:31<19:02,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  34%|███▍      | 740/2191 [09:38<18:53,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  34%|███▍      | 740/2191 [09:38<18:53,  1.28it/s, loss=2.38, v_num=647]Epoch 17:  34%|███▍      | 750/2191 [09:49<18:51,  1.27it/s, loss=2.38, v_num=647]Epoch 17:  34%|███▍      | 750/2191 [09:49<18:51,  1.27it/s, loss=2.37, v_num=647]Epoch 17:  35%|███▍      | 760/2191 [09:57<18:43,  1.27it/s, loss=2.37, v_num=647]Epoch 17:  35%|███▍      | 760/2191 [09:57<18:43,  1.27it/s, loss=2.42, v_num=647]Epoch 17:  35%|███▌      | 770/2191 [10:04<18:34,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  35%|███▌      | 770/2191 [10:04<18:34,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  36%|███▌      | 780/2191 [10:11<18:24,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  36%|███▌      | 780/2191 [10:11<18:24,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  36%|███▌      | 790/2191 [10:18<18:14,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  36%|███▌      | 790/2191 [10:18<18:14,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  37%|███▋      | 800/2191 [10:25<18:06,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  37%|███▋      | 800/2191 [10:25<18:06,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  37%|███▋      | 810/2191 [10:31<17:56,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  37%|███▋      | 810/2191 [10:31<17:56,  1.28it/s, loss=2.4, v_num=647] Epoch 17:  37%|███▋      | 820/2191 [10:39<17:48,  1.28it/s, loss=2.4, v_num=647]Epoch 17:  37%|███▋      | 820/2191 [10:39<17:48,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  38%|███▊      | 830/2191 [10:47<17:40,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  38%|███▊      | 830/2191 [10:47<17:40,  1.28it/s, loss=2.39, v_num=647]Epoch 17:  38%|███▊      | 840/2191 [10:54<17:30,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  38%|███▊      | 840/2191 [10:54<17:30,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  39%|███▉      | 850/2191 [11:01<17:22,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  39%|███▉      | 850/2191 [11:01<17:22,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  39%|███▉      | 860/2191 [11:09<17:15,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  39%|███▉      | 860/2191 [11:09<17:15,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  40%|███▉      | 870/2191 [11:16<17:06,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  40%|███▉      | 870/2191 [11:16<17:06,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  40%|████      | 880/2191 [11:25<17:00,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  40%|████      | 880/2191 [11:25<17:00,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  41%|████      | 890/2191 [11:33<16:52,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  41%|████      | 890/2191 [11:33<16:52,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  41%|████      | 900/2191 [11:40<16:43,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  41%|████      | 900/2191 [11:40<16:43,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  42%|████▏     | 910/2191 [11:49<16:37,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  42%|████▏     | 910/2191 [11:49<16:37,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  42%|████▏     | 920/2191 [11:57<16:29,  1.28it/s, loss=2.42, v_num=647]Epoch 17:  42%|████▏     | 920/2191 [11:57<16:29,  1.28it/s, loss=2.43, v_num=647]Epoch 17:  42%|████▏     | 930/2191 [12:04<16:21,  1.28it/s, loss=2.43, v_num=647]Epoch 17:  42%|████▏     | 930/2191 [12:04<16:21,  1.28it/s, loss=2.41, v_num=647]Epoch 17:  43%|████▎     | 940/2191 [12:11<16:12,  1.29it/s, loss=2.41, v_num=647]Epoch 17:  43%|████▎     | 940/2191 [12:11<16:12,  1.29it/s, loss=2.42, v_num=647]Epoch 17:  43%|████▎     | 950/2191 [12:19<16:04,  1.29it/s, loss=2.42, v_num=647]Epoch 17:  43%|████▎     | 950/2191 [12:19<16:04,  1.29it/s, loss=2.4, v_num=647] Epoch 17:  44%|████▍     | 960/2191 [12:27<15:57,  1.29it/s, loss=2.4, v_num=647]Epoch 17:  44%|████▍     | 960/2191 [12:27<15:57,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  44%|████▍     | 970/2191 [12:34<15:49,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  44%|████▍     | 970/2191 [12:34<15:49,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  45%|████▌     | 990/2191 [12:51<15:34,  1.28it/s, loss=2.37, v_num=647]Epoch 17:  45%|████▌     | 990/2191 [12:51<15:34,  1.28it/s, loss=2.38, v_num=647]Epoch 17:  46%|████▌     | 1000/2191 [12:58<15:26,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  46%|████▌     | 1000/2191 [12:58<15:26,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  46%|████▌     | 1010/2191 [13:05<15:17,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  46%|████▌     | 1010/2191 [13:05<15:17,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  47%|████▋     | 1020/2191 [13:12<15:08,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  47%|████▋     | 1020/2191 [13:12<15:08,  1.29it/s, loss=2.44, v_num=647]Epoch 17:  47%|████▋     | 1030/2191 [13:19<14:59,  1.29it/s, loss=2.44, v_num=647]Epoch 17:  47%|████▋     | 1030/2191 [13:19<14:59,  1.29it/s, loss=2.45, v_num=647]Epoch 17:  47%|████▋     | 1040/2191 [13:26<14:52,  1.29it/s, loss=2.45, v_num=647]Epoch 17:  47%|████▋     | 1040/2191 [13:26<14:52,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  48%|████▊     | 1050/2191 [13:34<14:43,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  48%|████▊     | 1050/2191 [13:34<14:43,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  48%|████▊     | 1060/2191 [13:41<14:35,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  48%|████▊     | 1060/2191 [13:41<14:35,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  49%|████▉     | 1070/2191 [13:49<14:28,  1.29it/s, loss=2.33, v_num=647]Epoch 17:  49%|████▉     | 1070/2191 [13:49<14:28,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  49%|████▉     | 1080/2191 [13:57<14:21,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  49%|████▉     | 1080/2191 [13:57<14:21,  1.29it/s, loss=2.4, v_num=647] Epoch 17:  50%|████▉     | 1090/2191 [14:04<14:12,  1.29it/s, loss=2.4, v_num=647]Epoch 17:  50%|████▉     | 1090/2191 [14:04<14:12,  1.29it/s, loss=2.4, v_num=647]Epoch 17:  50%|█████     | 1100/2191 [14:12<14:04,  1.29it/s, loss=2.4, v_num=647]Epoch 17:  50%|█████     | 1100/2191 [14:12<14:04,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  51%|█████     | 1110/2191 [14:20<13:57,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  51%|█████     | 1110/2191 [14:20<13:57,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  51%|█████     | 1120/2191 [14:27<13:49,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  51%|█████     | 1120/2191 [14:27<13:49,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  52%|█████▏    | 1130/2191 [14:34<13:40,  1.29it/s, loss=2.39, v_num=647]Epoch 17:  52%|█████▏    | 1130/2191 [14:34<13:40,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  52%|█████▏    | 1140/2191 [14:42<13:32,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  52%|█████▏    | 1140/2191 [14:42<13:32,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  52%|█████▏    | 1150/2191 [14:49<13:24,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  52%|█████▏    | 1150/2191 [14:49<13:24,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  53%|█████▎    | 1160/2191 [14:57<13:16,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  53%|█████▎    | 1160/2191 [14:57<13:16,  1.29it/s, loss=2.37, v_num=647]Epoch 17:  53%|█████▎    | 1170/2191 [15:03<13:07,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  53%|█████▎    | 1170/2191 [15:03<13:07,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  54%|█████▍    | 1180/2191 [15:10<12:59,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  54%|█████▍    | 1180/2191 [15:10<12:59,  1.30it/s, loss=2.4, v_num=647] Epoch 17:  54%|█████▍    | 1190/2191 [15:22<12:55,  1.29it/s, loss=2.4, v_num=647]Epoch 17:  54%|█████▍    | 1190/2191 [15:22<12:55,  1.29it/s, loss=2.44, v_num=647]Epoch 17:  55%|█████▍    | 1200/2191 [15:28<12:46,  1.29it/s, loss=2.44, v_num=647]Epoch 17:  55%|█████▍    | 1200/2191 [15:28<12:46,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  55%|█████▌    | 1210/2191 [15:35<12:37,  1.29it/s, loss=2.38, v_num=647]Epoch 17:  55%|█████▌    | 1210/2191 [15:35<12:37,  1.29it/s, loss=2.34, v_num=647]Epoch 17:  56%|█████▌    | 1220/2191 [15:42<12:29,  1.30it/s, loss=2.34, v_num=647]Epoch 17:  56%|█████▌    | 1220/2191 [15:42<12:29,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  56%|█████▌    | 1230/2191 [15:48<12:20,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  56%|█████▌    | 1230/2191 [15:48<12:20,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  57%|█████▋    | 1240/2191 [15:56<12:13,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  57%|█████▋    | 1240/2191 [15:56<12:13,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  57%|█████▋    | 1250/2191 [16:05<12:06,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  57%|█████▋    | 1250/2191 [16:05<12:06,  1.30it/s, loss=2.42, v_num=647]Epoch 17:  58%|█████▊    | 1260/2191 [16:12<11:58,  1.30it/s, loss=2.42, v_num=647]Epoch 17:  58%|█████▊    | 1260/2191 [16:12<11:58,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  58%|█████▊    | 1270/2191 [16:19<11:49,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  58%|█████▊    | 1270/2191 [16:19<11:49,  1.30it/s, loss=2.4, v_num=647] Epoch 17:  58%|█████▊    | 1280/2191 [16:26<11:41,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  58%|█████▊    | 1280/2191 [16:26<11:41,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  59%|█████▉    | 1290/2191 [16:33<11:33,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  59%|█████▉    | 1290/2191 [16:33<11:33,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  59%|█████▉    | 1300/2191 [16:40<11:25,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  59%|█████▉    | 1300/2191 [16:40<11:25,  1.30it/s, loss=2.35, v_num=647]Epoch 17:  60%|█████▉    | 1310/2191 [16:47<11:16,  1.30it/s, loss=2.35, v_num=647]Epoch 17:  60%|█████▉    | 1310/2191 [16:47<11:16,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  60%|██████    | 1320/2191 [16:54<11:08,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  60%|██████    | 1320/2191 [16:54<11:08,  1.30it/s, loss=2.4, v_num=647] Epoch 17:  61%|██████    | 1330/2191 [17:01<11:00,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  61%|██████    | 1330/2191 [17:01<11:00,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  61%|██████    | 1340/2191 [17:08<10:52,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  61%|██████    | 1340/2191 [17:08<10:52,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  62%|██████▏   | 1350/2191 [17:15<10:44,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  62%|██████▏   | 1350/2191 [17:15<10:44,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  62%|██████▏   | 1360/2191 [17:22<10:36,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  62%|██████▏   | 1360/2191 [17:22<10:36,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  63%|██████▎   | 1370/2191 [17:29<10:28,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  63%|██████▎   | 1370/2191 [17:29<10:28,  1.31it/s, loss=2.39, v_num=647]Epoch 17:  63%|██████▎   | 1380/2191 [17:36<10:20,  1.31it/s, loss=2.39, v_num=647]Epoch 17:  63%|██████▎   | 1380/2191 [17:36<10:20,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  63%|██████▎   | 1390/2191 [17:44<10:12,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  63%|██████▎   | 1390/2191 [17:44<10:12,  1.31it/s, loss=2.4, v_num=647] Epoch 17:  64%|██████▍   | 1400/2191 [17:51<10:04,  1.31it/s, loss=2.4, v_num=647]Epoch 17:  64%|██████▍   | 1400/2191 [17:51<10:04,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  64%|██████▍   | 1410/2191 [17:59<09:57,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  64%|██████▍   | 1410/2191 [17:59<09:57,  1.31it/s, loss=2.39, v_num=647]Epoch 17:  65%|██████▍   | 1420/2191 [18:06<09:49,  1.31it/s, loss=2.39, v_num=647]Epoch 17:  65%|██████▍   | 1420/2191 [18:06<09:49,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  65%|██████▌   | 1430/2191 [18:13<09:41,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  65%|██████▌   | 1430/2191 [18:13<09:41,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  66%|██████▌   | 1440/2191 [18:20<09:33,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  66%|██████▌   | 1440/2191 [18:20<09:33,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  66%|██████▌   | 1450/2191 [18:28<09:26,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  66%|██████▌   | 1450/2191 [18:28<09:26,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  67%|██████▋   | 1460/2191 [18:36<09:18,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  67%|██████▋   | 1460/2191 [18:36<09:18,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  67%|██████▋   | 1470/2191 [18:43<09:10,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  67%|██████▋   | 1470/2191 [18:43<09:10,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  68%|██████▊   | 1480/2191 [18:50<09:02,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  68%|██████▊   | 1480/2191 [18:50<09:02,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  68%|██████▊   | 1490/2191 [18:59<08:55,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  68%|██████▊   | 1490/2191 [18:59<08:55,  1.31it/s, loss=2.4, v_num=647] Epoch 17:  68%|██████▊   | 1500/2191 [19:08<08:48,  1.31it/s, loss=2.4, v_num=647]Epoch 17:  68%|██████▊   | 1500/2191 [19:08<08:48,  1.31it/s, loss=2.4, v_num=647]Epoch 17:  69%|██████▉   | 1510/2191 [19:16<08:41,  1.31it/s, loss=2.4, v_num=647]Epoch 17:  69%|██████▉   | 1510/2191 [19:16<08:41,  1.31it/s, loss=2.36, v_num=647]Epoch 17:  69%|██████▉   | 1520/2191 [19:24<08:33,  1.31it/s, loss=2.36, v_num=647]Epoch 17:  69%|██████▉   | 1520/2191 [19:24<08:33,  1.31it/s, loss=2.33, v_num=647]Epoch 17:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.33, v_num=647]Epoch 17:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  70%|███████   | 1540/2191 [19:40<08:18,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  70%|███████   | 1540/2191 [19:40<08:18,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  71%|███████   | 1560/2191 [19:55<08:03,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  71%|███████   | 1560/2191 [19:55<08:03,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  72%|███████▏  | 1570/2191 [20:03<07:55,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  72%|███████▏  | 1570/2191 [20:03<07:55,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  72%|███████▏  | 1580/2191 [20:09<07:47,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  72%|███████▏  | 1580/2191 [20:09<07:47,  1.31it/s, loss=2.42, v_num=647]Epoch 17:  73%|███████▎  | 1590/2191 [20:17<07:39,  1.31it/s, loss=2.42, v_num=647]Epoch 17:  73%|███████▎  | 1590/2191 [20:17<07:39,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  73%|███████▎  | 1600/2191 [20:24<07:31,  1.31it/s, loss=2.41, v_num=647]Epoch 17:  73%|███████▎  | 1600/2191 [20:24<07:31,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  74%|███████▍  | 1620/2191 [20:40<07:16,  1.31it/s, loss=2.35, v_num=647]Epoch 17:  74%|███████▍  | 1620/2191 [20:40<07:16,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  74%|███████▍  | 1630/2191 [20:49<07:09,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  74%|███████▍  | 1630/2191 [20:49<07:09,  1.31it/s, loss=2.37, v_num=647]Epoch 17:  75%|███████▍  | 1640/2191 [20:58<07:02,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  75%|███████▍  | 1640/2191 [20:58<07:02,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  75%|███████▌  | 1650/2191 [21:07<06:55,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  75%|███████▌  | 1650/2191 [21:07<06:55,  1.30it/s, loss=2.44, v_num=647]Epoch 17:  76%|███████▌  | 1660/2191 [21:14<06:47,  1.30it/s, loss=2.44, v_num=647]Epoch 17:  76%|███████▌  | 1660/2191 [21:14<06:47,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  76%|███████▌  | 1670/2191 [21:24<06:40,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  76%|███████▌  | 1670/2191 [21:24<06:40,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  77%|███████▋  | 1680/2191 [21:31<06:32,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  77%|███████▋  | 1680/2191 [21:31<06:32,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  77%|███████▋  | 1690/2191 [21:39<06:25,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  77%|███████▋  | 1690/2191 [21:39<06:25,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  78%|███████▊  | 1700/2191 [21:46<06:17,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  78%|███████▊  | 1700/2191 [21:46<06:17,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  78%|███████▊  | 1710/2191 [21:54<06:09,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  78%|███████▊  | 1710/2191 [21:54<06:09,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  79%|███████▊  | 1720/2191 [22:03<06:02,  1.30it/s, loss=2.37, v_num=647]Epoch 17:  79%|███████▊  | 1720/2191 [22:03<06:02,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  79%|███████▉  | 1730/2191 [22:11<05:54,  1.30it/s, loss=2.39, v_num=647]Epoch 17:  79%|███████▉  | 1730/2191 [22:11<05:54,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  79%|███████▉  | 1740/2191 [22:18<05:46,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  79%|███████▉  | 1740/2191 [22:18<05:46,  1.30it/s, loss=2.43, v_num=647]Epoch 17:  80%|███████▉  | 1750/2191 [22:26<05:39,  1.30it/s, loss=2.43, v_num=647]Epoch 17:  80%|███████▉  | 1750/2191 [22:26<05:39,  1.30it/s, loss=2.47, v_num=647]Epoch 17:  80%|████████  | 1760/2191 [22:34<05:31,  1.30it/s, loss=2.47, v_num=647]Epoch 17:  80%|████████  | 1760/2191 [22:34<05:31,  1.30it/s, loss=2.43, v_num=647]Epoch 17:  81%|████████  | 1770/2191 [22:41<05:23,  1.30it/s, loss=2.43, v_num=647]Epoch 17:  81%|████████  | 1770/2191 [22:41<05:23,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  81%|████████  | 1780/2191 [22:48<05:15,  1.30it/s, loss=2.41, v_num=647]Epoch 17:  81%|████████  | 1780/2191 [22:48<05:15,  1.30it/s, loss=2.44, v_num=647]Epoch 17:  82%|████████▏ | 1790/2191 [22:56<05:08,  1.30it/s, loss=2.44, v_num=647]Epoch 17:  82%|████████▏ | 1790/2191 [22:56<05:08,  1.30it/s, loss=2.4, v_num=647] Epoch 17:  82%|████████▏ | 1800/2191 [23:03<05:00,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  82%|████████▏ | 1800/2191 [23:03<05:00,  1.30it/s, loss=2.33, v_num=647]Epoch 17:  83%|████████▎ | 1810/2191 [23:10<04:52,  1.30it/s, loss=2.33, v_num=647]Epoch 17:  83%|████████▎ | 1810/2191 [23:10<04:52,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  83%|████████▎ | 1820/2191 [23:17<04:44,  1.30it/s, loss=2.36, v_num=647]Epoch 17:  83%|████████▎ | 1820/2191 [23:17<04:44,  1.30it/s, loss=2.4, v_num=647] Epoch 17:  84%|████████▎ | 1830/2191 [23:25<04:37,  1.30it/s, loss=2.4, v_num=647]Epoch 17:  84%|████████▎ | 1830/2191 [23:25<04:37,  1.30it/s, loss=2.38, v_num=647]Epoch 17:  84%|████████▍ | 1840/2191 [23:29<04:28,  1.31it/s, loss=2.38, v_num=647]Epoch 17:  84%|████████▍ | 1840/2191 [23:29<04:28,  1.31it/s, loss=2.36, v_num=647]Epoch 17:  84%|████████▍ | 1850/2191 [23:31<04:20,  1.31it/s, loss=2.36, v_num=647]Epoch 17:  84%|████████▍ | 1850/2191 [23:31<04:20,  1.31it/s, loss=2.4, v_num=647] Epoch 17:  85%|████████▍ | 1860/2191 [23:34<04:11,  1.32it/s, loss=2.4, v_num=647]Epoch 17:  85%|████████▍ | 1860/2191 [23:34<04:11,  1.32it/s, loss=2.45, v_num=647]validation_epoch_end
graph acc: 0.33865814696485624
valid accuracy: 0.9747278094291687
alidation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9740979671478271
validation_epoch_end
graph acc: 0.36741214057507987
valid accuracy: 0.9752295613288879
id accuracy: 0.9733694791793823
validation_epoch_end
graph acc: 0.35782747603833864
valid accuracy: 0.9769737124443054
Epoch 17:  86%|████████▌ | 1880/2191 [23:39<03:54,  1.33it/s, loss=2.42, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.36421725239616615
valid accuracy: 0.9765106439590454

Validating:   3%|▎         | 10/313 [00:01<00:46,  6.46it/s][AEpoch 17:  86%|████████▋ | 1890/2191 [23:40<03:46,  1.33it/s, loss=2.42, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:49,  5.87it/s][AEpoch 17:  87%|████████▋ | 1900/2191 [23:42<03:37,  1.34it/s, loss=2.42, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:38,  7.39it/s][AEpoch 17:  87%|████████▋ | 1910/2191 [23:43<03:29,  1.34it/s, loss=2.42, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:29,  9.30it/s][AEpoch 17:  88%|████████▊ | 1920/2191 [23:44<03:20,  1.35it/s, loss=2.42, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26, 10.06it/s][AEpoch 17:  88%|████████▊ | 1930/2191 [23:45<03:12,  1.36it/s, loss=2.42, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.45it/s][AEpoch 17:  89%|████████▊ | 1940/2191 [23:45<03:04,  1.36it/s, loss=2.42, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24,  9.86it/s][AEpoch 17:  89%|████████▉ | 1950/2191 [23:47<02:56,  1.37it/s, loss=2.42, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.29it/s][AEpoch 17:  89%|████████▉ | 1960/2191 [23:47<02:48,  1.37it/s, loss=2.42, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.58it/s][AEpoch 17:  90%|████████▉ | 1970/2191 [23:48<02:40,  1.38it/s, loss=2.42, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.61it/s][AEpoch 17:  90%|█████████ | 1980/2191 [23:49<02:32,  1.39it/s, loss=2.42, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:19, 10.20it/s][AEpoch 17:  91%|█████████ | 1990/2191 [23:50<02:24,  1.39it/s, loss=2.42, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:20,  9.58it/s][AEpoch 17:  91%|█████████▏| 2000/2191 [23:51<02:16,  1.40it/s, loss=2.42, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.20it/s][AEpoch 17:  92%|█████████▏| 2010/2191 [23:53<02:08,  1.40it/s, loss=2.42, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  7.95it/s][AEpoch 17:  92%|█████████▏| 2020/2191 [23:54<02:01,  1.41it/s, loss=2.42, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:18,  8.97it/s][AEpoch 17:  93%|█████████▎| 2030/2191 [23:55<01:53,  1.41it/s, loss=2.42, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:16,  9.55it/s][AEpoch 17:  93%|█████████▎| 2040/2191 [23:56<01:46,  1.42it/s, loss=2.42, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.59it/s][AEpoch 17:  94%|█████████▎| 2050/2191 [23:57<01:38,  1.43it/s, loss=2.42, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.41it/s][AEpoch 17:  94%|█████████▍| 2060/2191 [23:58<01:31,  1.43it/s, loss=2.42, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.63it/s][AEpoch 17:  94%|█████████▍| 2070/2191 [23:58<01:24,  1.44it/s, loss=2.42, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.57it/s][AEpoch 17:  95%|█████████▍| 2080/2191 [23:59<01:16,  1.45it/s, loss=2.42, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 11.08it/s][AEpoch 17:  95%|█████████▌| 2090/2191 [24:00<01:09,  1.45it/s, loss=2.42, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.93it/s][AEpoch 17:  96%|█████████▌| 2100/2191 [24:01<01:02,  1.46it/s, loss=2.42, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.64it/s][AEpoch 17:  96%|█████████▋| 2110/2191 [24:02<00:55,  1.46it/s, loss=2.42, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.92it/s][AEpoch 17:  97%|█████████▋| 2120/2191 [24:03<00:48,  1.47it/s, loss=2.42, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.40it/s][AEpoch 17:  97%|█████████▋| 2130/2191 [24:04<00:41,  1.48it/s, loss=2.42, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.95it/s][AEpoch 17:  98%|█████████▊| 2140/2191 [24:05<00:34,  1.48it/s, loss=2.42, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.50it/s][AEpoch 17:  98%|█████████▊| 2150/2191 [24:06<00:27,  1.49it/s, loss=2.42, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 13.37it/s][AEpoch 17:  99%|█████████▊| 2160/2191 [24:06<00:20,  1.49it/s, loss=2.42, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.35it/s][AEpoch 17:  99%|█████████▉| 2170/2191 [24:07<00:14,  1.50it/s, loss=2.42, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.32it/s][AEpoch 17:  99%|█████████▉| 2180/2191 [24:08<00:07,  1.51it/s, loss=2.42, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.62it/s][AEpoch 17: 100%|█████████▉| 2190/2191 [24:09<00:00,  1.51it/s, loss=2.42, v_num=647]validation_epoch_end
graph acc: 0.4057507987220447
valid accuracy: 0.9785344004631042
Epoch 17: 100%|██████████| 2191/2191 [24:10<00:00,  1.51it/s, loss=2.39, v_num=647]
                                                             [AEpoch 17:   0%|          | 0/2191 [00:00<00:00, 11397.57it/s, loss=2.39, v_num=647]Epoch 18:   0%|          | 0/2191 [00:00<00:00, 2768.52it/s, loss=2.39, v_num=647] Epoch 18:   0%|          | 0/2191 [00:13<8:02:39, 13.22s/it, loss=2.39, v_num=647]Epoch 18:   0%|          | 10/2191 [00:13<43:59,  1.21s/it, loss=2.39, v_num=647] Epoch 18:   0%|          | 10/2191 [00:13<43:59,  1.21s/it, loss=2.36, v_num=647]Epoch 18:   1%|          | 20/2191 [00:20<36:07,  1.00it/s, loss=2.36, v_num=647]Epoch 18:   1%|          | 20/2191 [00:20<36:07,  1.00it/s, loss=2.33, v_num=647]Epoch 18:   1%|▏         | 30/2191 [00:29<34:21,  1.05it/s, loss=2.33, v_num=647]Epoch 18:   1%|▏         | 30/2191 [00:29<34:21,  1.05it/s, loss=2.32, v_num=647]Epoch 18:   2%|▏         | 40/2191 [00:36<32:10,  1.11it/s, loss=2.32, v_num=647]Epoch 18:   2%|▏         | 40/2191 [00:36<32:11,  1.11it/s, loss=2.35, v_num=647]Epoch 18:   2%|▏         | 50/2191 [00:49<34:21,  1.04it/s, loss=2.35, v_num=647]Epoch 18:   2%|▏         | 50/2191 [00:49<34:21,  1.04it/s, loss=2.41, v_num=647]Epoch 18:   3%|▎         | 60/2191 [00:56<33:02,  1.08it/s, loss=2.41, v_num=647]Epoch 18:   3%|▎         | 60/2191 [00:56<33:02,  1.08it/s, loss=2.38, v_num=647]Epoch 18:   3%|▎         | 70/2191 [01:04<31:56,  1.11it/s, loss=2.38, v_num=647]Epoch 18:   3%|▎         | 70/2191 [01:04<31:57,  1.11it/s, loss=2.36, v_num=647]Epoch 18:   4%|▎         | 80/2191 [01:12<31:30,  1.12it/s, loss=2.36, v_num=647]Epoch 18:   4%|▎         | 80/2191 [01:12<31:30,  1.12it/s, loss=2.37, v_num=647]Epoch 18:   4%|▍         | 90/2191 [01:19<30:30,  1.15it/s, loss=2.37, v_num=647]Epoch 18:   4%|▍         | 90/2191 [01:19<30:30,  1.15it/s, loss=2.37, v_num=647]Epoch 18:   5%|▍         | 100/2191 [01:27<30:05,  1.16it/s, loss=2.37, v_num=647]Epoch 18:   5%|▍         | 100/2191 [01:27<30:05,  1.16it/s, loss=2.38, v_num=647]Epoch 18:   5%|▌         | 110/2191 [01:34<29:23,  1.18it/s, loss=2.38, v_num=647]Epoch 18:   5%|▌         | 110/2191 [01:34<29:24,  1.18it/s, loss=2.36, v_num=647]Epoch 18:   5%|▌         | 120/2191 [01:41<28:56,  1.19it/s, loss=2.36, v_num=647]Epoch 18:   5%|▌         | 120/2191 [01:41<28:56,  1.19it/s, loss=2.34, v_num=647]Epoch 18:   6%|▌         | 130/2191 [01:50<29:06,  1.18it/s, loss=2.34, v_num=647]Epoch 18:   6%|▌         | 130/2191 [01:50<29:06,  1.18it/s, loss=2.33, v_num=647]Epoch 18:   6%|▋         | 140/2191 [01:59<28:55,  1.18it/s, loss=2.33, v_num=647]Epoch 18:   6%|▋         | 140/2191 [01:59<28:55,  1.18it/s, loss=2.34, v_num=647]Epoch 18:   7%|▋         | 150/2191 [02:07<28:42,  1.19it/s, loss=2.34, v_num=647]Epoch 18:   7%|▋         | 150/2191 [02:07<28:42,  1.19it/s, loss=2.33, v_num=647]Epoch 18:   7%|▋         | 160/2191 [02:14<28:21,  1.19it/s, loss=2.33, v_num=647]Epoch 18:   7%|▋         | 160/2191 [02:14<28:21,  1.19it/s, loss=2.34, v_num=647]Epoch 18:   8%|▊         | 170/2191 [02:21<27:53,  1.21it/s, loss=2.34, v_num=647]Epoch 18:   8%|▊         | 170/2191 [02:21<27:53,  1.21it/s, loss=2.34, v_num=647]Epoch 18:   8%|▊         | 180/2191 [02:29<27:38,  1.21it/s, loss=2.34, v_num=647]Epoch 18:   8%|▊         | 180/2191 [02:29<27:38,  1.21it/s, loss=2.32, v_num=647]Epoch 18:   9%|▊         | 190/2191 [02:36<27:18,  1.22it/s, loss=2.32, v_num=647]Epoch 18:   9%|▊         | 190/2191 [02:36<27:18,  1.22it/s, loss=2.35, v_num=647]Epoch 18:   9%|▉         | 200/2191 [02:43<27:00,  1.23it/s, loss=2.35, v_num=647]Epoch 18:   9%|▉         | 200/2191 [02:43<27:00,  1.23it/s, loss=2.41, v_num=647]Epoch 18:  10%|▉         | 210/2191 [02:50<26:43,  1.24it/s, loss=2.41, v_num=647]Epoch 18:  10%|▉         | 210/2191 [02:50<26:43,  1.24it/s, loss=2.4, v_num=647] Epoch 18:  10%|█         | 220/2191 [02:59<26:39,  1.23it/s, loss=2.4, v_num=647]Epoch 18:  10%|█         | 220/2191 [02:59<26:39,  1.23it/s, loss=2.4, v_num=647]Epoch 18:  10%|█         | 230/2191 [03:07<26:29,  1.23it/s, loss=2.4, v_num=647]Epoch 18:  10%|█         | 230/2191 [03:07<26:29,  1.23it/s, loss=2.37, v_num=647]Epoch 18:  11%|█         | 240/2191 [03:15<26:23,  1.23it/s, loss=2.37, v_num=647]Epoch 18:  11%|█         | 240/2191 [03:15<26:23,  1.23it/s, loss=2.4, v_num=647] Epoch 18:  11%|█▏        | 250/2191 [03:23<26:14,  1.23it/s, loss=2.4, v_num=647]Epoch 18:  11%|█▏        | 250/2191 [03:23<26:14,  1.23it/s, loss=2.43, v_num=647]Epoch 18:  12%|█▏        | 260/2191 [03:31<26:06,  1.23it/s, loss=2.43, v_num=647]Epoch 18:  12%|█▏        | 260/2191 [03:31<26:06,  1.23it/s, loss=2.36, v_num=647]Epoch 18:  12%|█▏        | 270/2191 [03:39<25:58,  1.23it/s, loss=2.36, v_num=647]Epoch 18:  12%|█▏        | 270/2191 [03:39<25:58,  1.23it/s, loss=2.34, v_num=647]Epoch 18:  13%|█▎        | 280/2191 [03:46<25:41,  1.24it/s, loss=2.34, v_num=647]Epoch 18:  13%|█▎        | 280/2191 [03:46<25:41,  1.24it/s, loss=2.34, v_num=647]Epoch 18:  13%|█▎        | 290/2191 [03:55<25:41,  1.23it/s, loss=2.34, v_num=647]Epoch 18:  13%|█▎        | 290/2191 [03:55<25:41,  1.23it/s, loss=2.32, v_num=647]Epoch 18:  14%|█▎        | 300/2191 [04:02<25:24,  1.24it/s, loss=2.32, v_num=647]Epoch 18:  14%|█▎        | 300/2191 [04:02<25:24,  1.24it/s, loss=2.31, v_num=647]Epoch 18:  14%|█▍        | 310/2191 [04:10<25:12,  1.24it/s, loss=2.31, v_num=647]Epoch 18:  14%|█▍        | 310/2191 [04:10<25:12,  1.24it/s, loss=2.32, v_num=647]Epoch 18:  15%|█▍        | 320/2191 [04:18<25:04,  1.24it/s, loss=2.32, v_num=647]Epoch 18:  15%|█▍        | 320/2191 [04:18<25:04,  1.24it/s, loss=2.31, v_num=647]Epoch 18:  15%|█▌        | 330/2191 [04:25<24:52,  1.25it/s, loss=2.31, v_num=647]Epoch 18:  15%|█▌        | 330/2191 [04:25<24:52,  1.25it/s, loss=2.34, v_num=647]Epoch 18:  16%|█▌        | 340/2191 [04:34<24:47,  1.24it/s, loss=2.34, v_num=647]Epoch 18:  16%|█▌        | 340/2191 [04:34<24:47,  1.24it/s, loss=2.39, v_num=647]Epoch 18:  16%|█▌        | 350/2191 [04:41<24:35,  1.25it/s, loss=2.39, v_num=647]Epoch 18:  16%|█▌        | 350/2191 [04:41<24:35,  1.25it/s, loss=2.43, v_num=647]Epoch 18:  16%|█▋        | 360/2191 [04:50<24:31,  1.24it/s, loss=2.43, v_num=647]Epoch 18:  16%|█▋        | 360/2191 [04:50<24:31,  1.24it/s, loss=2.42, v_num=647]Epoch 18:  17%|█▋        | 370/2191 [04:58<24:26,  1.24it/s, loss=2.42, v_num=647]Epoch 18:  17%|█▋        | 370/2191 [04:58<24:26,  1.24it/s, loss=2.4, v_num=647] Epoch 18:  17%|█▋        | 380/2191 [05:04<24:09,  1.25it/s, loss=2.4, v_num=647]Epoch 18:  17%|█▋        | 380/2191 [05:04<24:09,  1.25it/s, loss=2.42, v_num=647]Epoch 18:  18%|█▊        | 390/2191 [05:12<23:57,  1.25it/s, loss=2.42, v_num=647]Epoch 18:  18%|█▊        | 390/2191 [05:12<23:57,  1.25it/s, loss=2.4, v_num=647] Epoch 18:  18%|█▊        | 400/2191 [05:19<23:46,  1.26it/s, loss=2.4, v_num=647]Epoch 18:  18%|█▊        | 400/2191 [05:19<23:46,  1.26it/s, loss=2.36, v_num=647]Epoch 18:  19%|█▊        | 410/2191 [05:26<23:35,  1.26it/s, loss=2.36, v_num=647]Epoch 18:  19%|█▊        | 410/2191 [05:26<23:35,  1.26it/s, loss=2.32, v_num=647]Epoch 18:  19%|█▉        | 420/2191 [05:34<23:25,  1.26it/s, loss=2.32, v_num=647]Epoch 18:  19%|█▉        | 420/2191 [05:34<23:25,  1.26it/s, loss=2.32, v_num=647]Epoch 18:  20%|█▉        | 430/2191 [05:40<23:11,  1.27it/s, loss=2.32, v_num=647]Epoch 18:  20%|█▉        | 430/2191 [05:40<23:11,  1.27it/s, loss=2.35, v_num=647]Epoch 18:  20%|██        | 440/2191 [05:48<23:04,  1.26it/s, loss=2.35, v_num=647]Epoch 18:  20%|██        | 440/2191 [05:48<23:04,  1.26it/s, loss=2.34, v_num=647]Epoch 18:  21%|██        | 450/2191 [05:56<22:56,  1.27it/s, loss=2.34, v_num=647]Epoch 18:  21%|██        | 450/2191 [05:56<22:56,  1.27it/s, loss=2.33, v_num=647]Epoch 18:  21%|██        | 460/2191 [06:03<22:46,  1.27it/s, loss=2.33, v_num=647]Epoch 18:  21%|██        | 460/2191 [06:03<22:46,  1.27it/s, loss=2.33, v_num=647]Epoch 18:  21%|██▏       | 470/2191 [06:12<22:42,  1.26it/s, loss=2.33, v_num=647]Epoch 18:  21%|██▏       | 470/2191 [06:12<22:42,  1.26it/s, loss=2.38, v_num=647]Epoch 18:  22%|██▏       | 480/2191 [06:20<22:34,  1.26it/s, loss=2.38, v_num=647]Epoch 18:  22%|██▏       | 480/2191 [06:20<22:34,  1.26it/s, loss=2.41, v_num=647]Epoch 18:  22%|██▏       | 490/2191 [06:28<22:26,  1.26it/s, loss=2.41, v_num=647]Epoch 18:  22%|██▏       | 490/2191 [06:28<22:26,  1.26it/s, loss=2.37, v_num=647]Epoch 18:  23%|██▎       | 500/2191 [06:35<22:16,  1.27it/s, loss=2.37, v_num=647]Epoch 18:  23%|██▎       | 500/2191 [06:35<22:16,  1.27it/s, loss=2.34, v_num=647]Epoch 18:  23%|██▎       | 510/2191 [06:42<22:04,  1.27it/s, loss=2.34, v_num=647]Epoch 18:  23%|██▎       | 510/2191 [06:42<22:04,  1.27it/s, loss=2.36, v_num=647]Epoch 18:  24%|██▎       | 520/2191 [06:50<21:55,  1.27it/s, loss=2.36, v_num=647]Epoch 18:  24%|██▎       | 520/2191 [06:50<21:55,  1.27it/s, loss=2.38, v_num=647]Epoch 18:  24%|██▍       | 530/2191 [06:56<21:42,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  24%|██▍       | 530/2191 [06:56<21:42,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  25%|██▍       | 540/2191 [07:05<21:39,  1.27it/s, loss=2.38, v_num=647]Epoch 18:  25%|██▍       | 540/2191 [07:05<21:39,  1.27it/s, loss=2.35, v_num=647]Epoch 18:  25%|██▌       | 550/2191 [07:13<21:29,  1.27it/s, loss=2.35, v_num=647]Epoch 18:  25%|██▌       | 550/2191 [07:13<21:29,  1.27it/s, loss=2.33, v_num=647]Epoch 18:  26%|██▌       | 560/2191 [07:21<21:22,  1.27it/s, loss=2.33, v_num=647]Epoch 18:  26%|██▌       | 560/2191 [07:21<21:22,  1.27it/s, loss=2.35, v_num=647]Epoch 18:  26%|██▌       | 570/2191 [07:28<21:11,  1.27it/s, loss=2.35, v_num=647]Epoch 18:  26%|██▌       | 570/2191 [07:28<21:11,  1.27it/s, loss=2.36, v_num=647]Epoch 18:  26%|██▋       | 580/2191 [07:35<21:03,  1.28it/s, loss=2.36, v_num=647]Epoch 18:  26%|██▋       | 580/2191 [07:35<21:03,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  27%|██▋       | 590/2191 [07:42<20:53,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  27%|██▋       | 590/2191 [07:42<20:53,  1.28it/s, loss=2.37, v_num=647]Epoch 18:  27%|██▋       | 600/2191 [07:49<20:42,  1.28it/s, loss=2.37, v_num=647]Epoch 18:  27%|██▋       | 600/2191 [07:49<20:42,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  28%|██▊       | 610/2191 [07:56<20:33,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  28%|██▊       | 610/2191 [07:56<20:33,  1.28it/s, loss=2.4, v_num=647] Epoch 18:  28%|██▊       | 620/2191 [08:05<20:26,  1.28it/s, loss=2.4, v_num=647]Epoch 18:  28%|██▊       | 620/2191 [08:05<20:26,  1.28it/s, loss=2.39, v_num=647]Epoch 18:  29%|██▉       | 630/2191 [08:11<20:16,  1.28it/s, loss=2.39, v_num=647]Epoch 18:  29%|██▉       | 630/2191 [08:11<20:16,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  29%|██▉       | 640/2191 [08:20<20:11,  1.28it/s, loss=2.38, v_num=647]Epoch 18:  29%|██▉       | 640/2191 [08:20<20:11,  1.28it/s, loss=2.35, v_num=647]Epoch 18:  30%|██▉       | 650/2191 [08:27<20:00,  1.28it/s, loss=2.35, v_num=647]Epoch 18:  30%|██▉       | 650/2191 [08:27<20:00,  1.28it/s, loss=2.37, v_num=647]Epoch 18:  30%|███       | 660/2191 [08:34<19:51,  1.28it/s, loss=2.37, v_num=647]Epoch 18:  30%|███       | 660/2191 [08:34<19:51,  1.28it/s, loss=2.41, v_num=647]Epoch 18:  31%|███       | 670/2191 [08:41<19:41,  1.29it/s, loss=2.41, v_num=647]Epoch 18:  31%|███       | 670/2191 [08:41<19:41,  1.29it/s, loss=2.41, v_num=647]Epoch 18:  31%|███       | 680/2191 [08:49<19:35,  1.29it/s, loss=2.41, v_num=647]Epoch 18:  31%|███       | 680/2191 [08:49<19:35,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  31%|███▏      | 690/2191 [08:57<19:27,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  31%|███▏      | 690/2191 [08:57<19:27,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  32%|███▏      | 700/2191 [09:04<19:17,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  32%|███▏      | 700/2191 [09:04<19:17,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  32%|███▏      | 710/2191 [09:11<19:09,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  32%|███▏      | 710/2191 [09:11<19:09,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  33%|███▎      | 720/2191 [09:19<19:01,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  33%|███▎      | 720/2191 [09:19<19:01,  1.29it/s, loss=2.34, v_num=647]Epoch 18:  33%|███▎      | 730/2191 [09:26<18:52,  1.29it/s, loss=2.34, v_num=647]Epoch 18:  33%|███▎      | 730/2191 [09:26<18:52,  1.29it/s, loss=2.32, v_num=647]Epoch 18:  34%|███▍      | 740/2191 [09:34<18:45,  1.29it/s, loss=2.32, v_num=647]Epoch 18:  34%|███▍      | 740/2191 [09:34<18:45,  1.29it/s, loss=2.38, v_num=647]Epoch 18:  34%|███▍      | 750/2191 [09:41<18:35,  1.29it/s, loss=2.38, v_num=647]Epoch 18:  34%|███▍      | 750/2191 [09:41<18:35,  1.29it/s, loss=2.4, v_num=647] Epoch 18:  35%|███▍      | 760/2191 [09:49<18:28,  1.29it/s, loss=2.4, v_num=647]Epoch 18:  35%|███▍      | 760/2191 [09:49<18:28,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  35%|███▌      | 770/2191 [09:57<18:20,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  35%|███▌      | 770/2191 [09:57<18:20,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  36%|███▌      | 780/2191 [10:04<18:12,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  36%|███▌      | 780/2191 [10:04<18:12,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  36%|███▌      | 790/2191 [10:11<18:02,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  36%|███▌      | 790/2191 [10:11<18:02,  1.29it/s, loss=2.38, v_num=647]Epoch 18:  37%|███▋      | 800/2191 [10:17<17:52,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  37%|███▋      | 800/2191 [10:17<17:52,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  37%|███▋      | 810/2191 [10:25<17:45,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  37%|███▋      | 810/2191 [10:25<17:45,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  37%|███▋      | 820/2191 [10:33<17:37,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  37%|███▋      | 820/2191 [10:33<17:37,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  38%|███▊      | 830/2191 [10:39<17:27,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  38%|███▊      | 830/2191 [10:39<17:27,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  38%|███▊      | 840/2191 [10:48<17:21,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  38%|███▊      | 840/2191 [10:48<17:21,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  39%|███▉      | 850/2191 [10:56<17:14,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  39%|███▉      | 850/2191 [10:56<17:14,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  39%|███▉      | 860/2191 [11:04<17:07,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  39%|███▉      | 860/2191 [11:04<17:07,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  40%|███▉      | 870/2191 [11:12<17:00,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  40%|███▉      | 870/2191 [11:12<17:00,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  40%|████      | 880/2191 [11:21<16:53,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  40%|████      | 880/2191 [11:21<16:53,  1.29it/s, loss=2.36, v_num=647]Epoch 18:  41%|████      | 890/2191 [11:30<16:47,  1.29it/s, loss=2.36, v_num=647]Epoch 18:  41%|████      | 890/2191 [11:30<16:47,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  41%|████      | 900/2191 [11:37<16:38,  1.29it/s, loss=2.37, v_num=647]Epoch 18:  41%|████      | 900/2191 [11:37<16:38,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  42%|████▏     | 910/2191 [11:43<16:29,  1.29it/s, loss=2.39, v_num=647]Epoch 18:  42%|████▏     | 910/2191 [11:43<16:29,  1.29it/s, loss=2.35, v_num=647]Epoch 18:  42%|████▏     | 920/2191 [11:50<16:19,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  42%|████▏     | 920/2191 [11:50<16:19,  1.30it/s, loss=2.33, v_num=647]Epoch 18:  42%|████▏     | 930/2191 [11:57<16:11,  1.30it/s, loss=2.33, v_num=647]Epoch 18:  42%|████▏     | 930/2191 [11:57<16:11,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  43%|████▎     | 940/2191 [12:04<16:03,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  43%|████▎     | 940/2191 [12:04<16:03,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  43%|████▎     | 950/2191 [12:11<15:54,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  43%|████▎     | 950/2191 [12:11<15:54,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  44%|████▍     | 960/2191 [12:18<15:45,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  44%|████▍     | 960/2191 [12:18<15:45,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  44%|████▍     | 970/2191 [12:24<15:36,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  44%|████▍     | 970/2191 [12:24<15:36,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  45%|████▍     | 980/2191 [12:33<15:30,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  45%|████▍     | 980/2191 [12:33<15:30,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  45%|████▌     | 990/2191 [12:42<15:23,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  45%|████▌     | 990/2191 [12:42<15:23,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  46%|████▌     | 1000/2191 [12:50<15:16,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  46%|████▌     | 1000/2191 [12:50<15:16,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  46%|████▌     | 1010/2191 [12:57<15:08,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  46%|████▌     | 1010/2191 [12:57<15:08,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  47%|████▋     | 1020/2191 [13:06<15:01,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  47%|████▋     | 1020/2191 [13:06<15:01,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  47%|████▋     | 1030/2191 [13:13<14:53,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  47%|████▋     | 1030/2191 [13:13<14:53,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  47%|████▋     | 1040/2191 [13:22<14:46,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  47%|████▋     | 1040/2191 [13:22<14:46,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  48%|████▊     | 1050/2191 [13:29<14:38,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  48%|████▊     | 1050/2191 [13:29<14:38,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  48%|████▊     | 1060/2191 [13:36<14:30,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  48%|████▊     | 1060/2191 [13:36<14:30,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  49%|████▉     | 1070/2191 [13:43<14:21,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  49%|████▉     | 1070/2191 [13:43<14:21,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  49%|████▉     | 1080/2191 [13:51<14:14,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  49%|████▉     | 1080/2191 [13:51<14:14,  1.30it/s, loss=2.34, v_num=647]Epoch 18:  50%|████▉     | 1090/2191 [13:59<14:07,  1.30it/s, loss=2.34, v_num=647]Epoch 18:  50%|████▉     | 1090/2191 [13:59<14:07,  1.30it/s, loss=2.32, v_num=647]Epoch 18:  50%|█████     | 1100/2191 [14:07<13:59,  1.30it/s, loss=2.32, v_num=647]Epoch 18:  50%|█████     | 1100/2191 [14:07<13:59,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  51%|█████     | 1110/2191 [14:14<13:51,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  51%|█████     | 1110/2191 [14:14<13:51,  1.30it/s, loss=2.44, v_num=647]Epoch 18:  51%|█████     | 1120/2191 [14:22<13:44,  1.30it/s, loss=2.44, v_num=647]Epoch 18:  51%|█████     | 1120/2191 [14:22<13:44,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  52%|█████▏    | 1130/2191 [14:30<13:36,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  52%|█████▏    | 1130/2191 [14:30<13:36,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  52%|█████▏    | 1140/2191 [14:37<13:28,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  52%|█████▏    | 1140/2191 [14:37<13:28,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  52%|█████▏    | 1150/2191 [14:45<13:21,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  52%|█████▏    | 1150/2191 [14:45<13:21,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  53%|█████▎    | 1160/2191 [14:52<13:12,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  53%|█████▎    | 1160/2191 [14:52<13:12,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  53%|█████▎    | 1170/2191 [14:59<13:04,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  53%|█████▎    | 1170/2191 [14:59<13:04,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  54%|█████▍    | 1180/2191 [15:09<12:58,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  54%|█████▍    | 1180/2191 [15:09<12:58,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  54%|█████▍    | 1190/2191 [15:16<12:50,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  54%|█████▍    | 1190/2191 [15:16<12:50,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  55%|█████▍    | 1200/2191 [15:23<12:41,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  55%|█████▍    | 1200/2191 [15:23<12:41,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  55%|█████▌    | 1210/2191 [15:30<12:34,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  55%|█████▌    | 1210/2191 [15:31<12:34,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  56%|█████▌    | 1230/2191 [15:45<12:18,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  56%|█████▌    | 1230/2191 [15:45<12:18,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  57%|█████▋    | 1240/2191 [15:53<12:10,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  57%|█████▋    | 1240/2191 [15:53<12:10,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  57%|█████▋    | 1250/2191 [16:00<12:02,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  57%|█████▋    | 1250/2191 [16:00<12:02,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  58%|█████▊    | 1260/2191 [16:07<11:54,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  58%|█████▊    | 1260/2191 [16:07<11:54,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  58%|█████▊    | 1270/2191 [16:14<11:46,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  58%|█████▊    | 1270/2191 [16:14<11:46,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  58%|█████▊    | 1280/2191 [16:22<11:38,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  58%|█████▊    | 1280/2191 [16:22<11:38,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  59%|█████▉    | 1290/2191 [16:30<11:31,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  59%|█████▉    | 1290/2191 [16:30<11:31,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  59%|█████▉    | 1300/2191 [16:38<11:24,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  59%|█████▉    | 1300/2191 [16:38<11:24,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  60%|█████▉    | 1310/2191 [16:46<11:16,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  60%|█████▉    | 1310/2191 [16:46<11:16,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  60%|██████    | 1320/2191 [16:53<11:08,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  60%|██████    | 1320/2191 [16:53<11:08,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  61%|██████    | 1330/2191 [17:02<11:01,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  61%|██████    | 1330/2191 [17:02<11:01,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  61%|██████    | 1340/2191 [17:09<10:53,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  61%|██████    | 1340/2191 [17:09<10:53,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  62%|██████▏   | 1350/2191 [17:15<10:44,  1.30it/s, loss=2.42, v_num=647]Epoch 18:  62%|██████▏   | 1350/2191 [17:15<10:44,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  62%|██████▏   | 1360/2191 [17:25<10:38,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  62%|██████▏   | 1360/2191 [17:25<10:38,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  63%|██████▎   | 1370/2191 [17:32<10:30,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  63%|██████▎   | 1370/2191 [17:32<10:30,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  63%|██████▎   | 1380/2191 [17:39<10:22,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  63%|██████▎   | 1380/2191 [17:39<10:22,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  63%|██████▎   | 1390/2191 [17:47<10:14,  1.30it/s, loss=2.35, v_num=647]Epoch 18:  63%|██████▎   | 1390/2191 [17:47<10:14,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  64%|██████▍   | 1400/2191 [17:53<10:06,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  64%|██████▍   | 1400/2191 [17:53<10:06,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  64%|██████▍   | 1410/2191 [18:02<09:58,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  64%|██████▍   | 1410/2191 [18:02<09:58,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  65%|██████▍   | 1420/2191 [18:10<09:51,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  65%|██████▍   | 1420/2191 [18:10<09:51,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  65%|██████▌   | 1430/2191 [18:17<09:43,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  65%|██████▌   | 1430/2191 [18:17<09:43,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  66%|██████▌   | 1440/2191 [18:24<09:35,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  66%|██████▌   | 1440/2191 [18:24<09:35,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  66%|██████▌   | 1450/2191 [18:32<09:27,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  66%|██████▌   | 1450/2191 [18:32<09:27,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  67%|██████▋   | 1460/2191 [18:38<09:19,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  67%|██████▋   | 1460/2191 [18:38<09:19,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  67%|██████▋   | 1470/2191 [18:47<09:12,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  67%|██████▋   | 1470/2191 [18:47<09:12,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  68%|██████▊   | 1480/2191 [18:54<09:04,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  68%|██████▊   | 1480/2191 [18:54<09:04,  1.30it/s, loss=2.43, v_num=647]Epoch 18:  68%|██████▊   | 1490/2191 [19:02<08:57,  1.31it/s, loss=2.43, v_num=647]Epoch 18:  68%|██████▊   | 1490/2191 [19:02<08:57,  1.31it/s, loss=2.43, v_num=647]Epoch 18:  68%|██████▊   | 1500/2191 [19:09<08:49,  1.31it/s, loss=2.43, v_num=647]Epoch 18:  68%|██████▊   | 1500/2191 [19:09<08:49,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  69%|██████▉   | 1510/2191 [19:17<08:41,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  69%|██████▉   | 1510/2191 [19:17<08:41,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  69%|██████▉   | 1520/2191 [19:26<08:34,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  69%|██████▉   | 1520/2191 [19:26<08:34,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  70%|███████   | 1540/2191 [19:41<08:18,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  70%|███████   | 1540/2191 [19:41<08:18,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  71%|███████   | 1550/2191 [19:49<08:11,  1.30it/s, loss=2.4, v_num=647]Epoch 18:  71%|███████   | 1550/2191 [19:49<08:11,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  71%|███████   | 1560/2191 [19:56<08:03,  1.30it/s, loss=2.38, v_num=647]Epoch 18:  71%|███████   | 1560/2191 [19:56<08:03,  1.30it/s, loss=2.4, v_num=647] Epoch 18:  72%|███████▏  | 1570/2191 [20:03<07:55,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  72%|███████▏  | 1570/2191 [20:03<07:55,  1.31it/s, loss=2.46, v_num=647]Epoch 18:  72%|███████▏  | 1580/2191 [20:11<07:48,  1.31it/s, loss=2.46, v_num=647]Epoch 18:  72%|███████▏  | 1580/2191 [20:11<07:48,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  73%|███████▎  | 1590/2191 [20:19<07:40,  1.30it/s, loss=2.41, v_num=647]Epoch 18:  73%|███████▎  | 1590/2191 [20:19<07:40,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  73%|███████▎  | 1600/2191 [20:27<07:32,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  73%|███████▎  | 1600/2191 [20:27<07:32,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  73%|███████▎  | 1610/2191 [20:34<07:25,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  73%|███████▎  | 1610/2191 [20:34<07:25,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  74%|███████▍  | 1620/2191 [20:43<07:18,  1.30it/s, loss=2.36, v_num=647]Epoch 18:  74%|███████▍  | 1620/2191 [20:43<07:18,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  74%|███████▍  | 1630/2191 [20:50<07:10,  1.30it/s, loss=2.37, v_num=647]Epoch 18:  74%|███████▍  | 1630/2191 [20:50<07:10,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  75%|███████▍  | 1640/2191 [20:58<07:02,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  75%|███████▍  | 1640/2191 [20:58<07:02,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  75%|███████▌  | 1650/2191 [21:05<06:54,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  75%|███████▌  | 1650/2191 [21:05<06:54,  1.30it/s, loss=2.39, v_num=647]Epoch 18:  76%|███████▌  | 1660/2191 [21:12<06:46,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  76%|███████▌  | 1660/2191 [21:12<06:46,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  76%|███████▌  | 1670/2191 [21:19<06:38,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  76%|███████▌  | 1670/2191 [21:19<06:38,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  77%|███████▋  | 1680/2191 [21:26<06:31,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  77%|███████▋  | 1680/2191 [21:26<06:31,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  77%|███████▋  | 1690/2191 [21:34<06:23,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  77%|███████▋  | 1690/2191 [21:34<06:23,  1.31it/s, loss=2.38, v_num=647]Epoch 18:  78%|███████▊  | 1700/2191 [21:41<06:15,  1.31it/s, loss=2.38, v_num=647]Epoch 18:  78%|███████▊  | 1700/2191 [21:41<06:15,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  78%|███████▊  | 1710/2191 [21:48<06:07,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  78%|███████▊  | 1710/2191 [21:48<06:07,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  79%|███████▊  | 1720/2191 [21:55<06:00,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  79%|███████▊  | 1720/2191 [21:55<06:00,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  79%|███████▉  | 1730/2191 [22:03<05:52,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  79%|███████▉  | 1730/2191 [22:03<05:52,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  79%|███████▉  | 1740/2191 [22:09<05:44,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  79%|███████▉  | 1740/2191 [22:09<05:44,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  80%|███████▉  | 1750/2191 [22:16<05:36,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  80%|███████▉  | 1750/2191 [22:16<05:36,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  80%|████████  | 1760/2191 [22:23<05:28,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  80%|████████  | 1760/2191 [22:23<05:28,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  81%|████████  | 1770/2191 [22:31<05:21,  1.31it/s, loss=2.37, v_num=647]Epoch 18:  81%|████████  | 1770/2191 [22:31<05:21,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  81%|████████  | 1780/2191 [22:39<05:13,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  81%|████████  | 1780/2191 [22:39<05:13,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  82%|████████▏ | 1790/2191 [22:47<05:06,  1.31it/s, loss=2.41, v_num=647]Epoch 18:  82%|████████▏ | 1790/2191 [22:47<05:06,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  82%|████████▏ | 1800/2191 [22:54<04:58,  1.31it/s, loss=2.39, v_num=647]Epoch 18:  82%|████████▏ | 1800/2191 [22:54<04:58,  1.31it/s, loss=2.4, v_num=647] Epoch 18:  83%|████████▎ | 1810/2191 [23:00<04:50,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  83%|████████▎ | 1810/2191 [23:00<04:50,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  83%|████████▎ | 1820/2191 [23:07<04:42,  1.31it/s, loss=2.4, v_num=647]Epoch 18:  83%|████████▎ | 1820/2191 [23:07<04:42,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  84%|████████▎ | 1830/2191 [23:14<04:34,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  84%|████████▎ | 1830/2191 [23:14<04:34,  1.31it/s, loss=2.36, v_num=647]Epoch 18:  84%|████████▍ | 1840/2191 [23:18<04:26,  1.32it/s, loss=2.36, v_num=647]Epoch 18:  84%|████████▍ | 1840/2191 [23:18<04:26,  1.32it/s, loss=2.42, v_num=647]Epoch 18:  84%|████████▍ | 1850/2191 [23:21<04:18,  1.32it/s, loss=2.42, v_num=647]Epoch 18:  84%|████████▍ | 1850/2191 [23:21<04:18,  1.32it/s, loss=2.39, v_num=647]Epoch 18:  85%|████████▍ | 1860/2191 [23:25<04:09,  1.32it/s, loss=2.39, v_num=647]Epoch 18:  85%|████████▍ | 1860/2191 [23:25<04:09,  1.32it/s, loss=2.32, v_num=647]validation_epoch_end
graph acc: 0.3514376996805112
valid accuracy: 0.9747278094291687
validation_epoch_end
graph acc: 0.4281150159744409
valid accuracy: 0.9744667410850525
validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9744445085525513
id accuracy: 0.9744720458984375
Epoch 18:  86%|████████▌ | 1880/2191 [23:29<03:53,  1.33it/s, loss=2.35, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9775481820106506
validation_epoch_end
graph acc: 0.34185303514376997
valid accuracy: 0.97664475440979

Validating:   3%|▎         | 10/313 [00:01<00:48,  6.27it/s][AEpoch 18:  86%|████████▋ | 1890/2191 [23:31<03:44,  1.34it/s, loss=2.35, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:52,  5.62it/s][AEpoch 18:  87%|████████▋ | 1900/2191 [23:33<03:36,  1.35it/s, loss=2.35, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:37,  7.49it/s][AEpoch 18:  87%|████████▋ | 1910/2191 [23:33<03:27,  1.35it/s, loss=2.35, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:29,  9.41it/s][AEpoch 18:  88%|████████▊ | 1920/2191 [23:34<03:19,  1.36it/s, loss=2.35, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26, 10.12it/s][AEpoch 18:  88%|████████▊ | 1930/2191 [23:35<03:11,  1.36it/s, loss=2.35, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.55it/s][AEpoch 18:  89%|████████▊ | 1940/2191 [23:36<03:03,  1.37it/s, loss=2.35, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24, 10.02it/s][AEpoch 18:  89%|████████▉ | 1950/2191 [23:37<02:55,  1.38it/s, loss=2.35, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:24,  9.61it/s][AEpoch 18:  89%|████████▉ | 1960/2191 [23:38<02:47,  1.38it/s, loss=2.35, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:22, 10.05it/s][AEpoch 18:  90%|████████▉ | 1970/2191 [23:39<02:39,  1.39it/s, loss=2.35, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.16it/s][AEpoch 18:  90%|█████████ | 1980/2191 [23:40<02:31,  1.39it/s, loss=2.35, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20, 10.00it/s][AEpoch 18:  91%|█████████ | 1990/2191 [23:41<02:23,  1.40it/s, loss=2.35, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.81it/s][AEpoch 18:  91%|█████████▏| 2000/2191 [23:42<02:15,  1.41it/s, loss=2.35, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:21,  8.68it/s][AEpoch 18:  92%|█████████▏| 2010/2191 [23:43<02:08,  1.41it/s, loss=2.35, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  8.00it/s][AEpoch 18:  92%|█████████▏| 2020/2191 [23:45<02:00,  1.42it/s, loss=2.35, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:18,  8.92it/s][AEpoch 18:  93%|█████████▎| 2030/2191 [23:46<01:53,  1.42it/s, loss=2.35, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:17,  8.98it/s][AEpoch 18:  93%|█████████▎| 2040/2191 [23:47<01:45,  1.43it/s, loss=2.35, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:13, 10.25it/s][AEpoch 18:  94%|█████████▎| 2050/2191 [23:48<01:38,  1.44it/s, loss=2.35, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:19<00:13,  9.91it/s][AEpoch 18:  94%|█████████▍| 2060/2191 [23:49<01:30,  1.44it/s, loss=2.35, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:09, 12.34it/s][AEpoch 18:  94%|█████████▍| 2070/2191 [23:49<01:23,  1.45it/s, loss=2.35, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.40it/s][AEpoch 18:  95%|█████████▍| 2080/2191 [23:50<01:16,  1.45it/s, loss=2.35, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:08, 11.48it/s][AEpoch 18:  95%|█████████▌| 2090/2191 [23:51<01:09,  1.46it/s, loss=2.35, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09, 10.05it/s][AEpoch 18:  96%|█████████▌| 2100/2191 [23:52<01:02,  1.47it/s, loss=2.35, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.80it/s][AEpoch 18:  96%|█████████▋| 2110/2191 [23:53<00:55,  1.47it/s, loss=2.35, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:25<00:07,  9.67it/s][AEpoch 18:  97%|█████████▋| 2120/2191 [23:54<00:48,  1.48it/s, loss=2.35, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.12it/s][AEpoch 18:  97%|█████████▋| 2130/2191 [23:55<00:41,  1.48it/s, loss=2.35, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:27<00:05,  9.55it/s][AEpoch 18:  98%|█████████▊| 2140/2191 [23:56<00:34,  1.49it/s, loss=2.35, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 10.85it/s][AEpoch 18:  98%|█████████▊| 2150/2191 [23:57<00:27,  1.50it/s, loss=2.35, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:28<00:02, 11.67it/s][AEpoch 18:  99%|█████████▊| 2160/2191 [23:58<00:20,  1.50it/s, loss=2.35, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:29<00:01, 11.85it/s][AEpoch 18:  99%|█████████▉| 2170/2191 [23:58<00:13,  1.51it/s, loss=2.35, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.08it/s][AEpoch 18:  99%|█████████▉| 2180/2191 [23:59<00:07,  1.52it/s, loss=2.35, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 13.54it/s][AEpoch 18: 100%|█████████▉| 2190/2191 [24:00<00:00,  1.52it/s, loss=2.35, v_num=647]validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.977776825428009
Epoch 18: 100%|██████████| 2191/2191 [24:01<00:00,  1.52it/s, loss=2.35, v_num=647]
                                                             [AEpoch 18:   0%|          | 0/2191 [00:00<00:00, 14315.03it/s, loss=2.35, v_num=647]Epoch 19:   0%|          | 0/2191 [00:00<00:00, 3569.62it/s, loss=2.35, v_num=647] Epoch 19:   0%|          | 0/2191 [00:11<7:07:56, 11.72s/it, loss=2.35, v_num=647]Epoch 19:   0%|          | 10/2191 [00:12<41:17,  1.14s/it, loss=2.35, v_num=647] Epoch 19:   0%|          | 10/2191 [00:12<41:17,  1.14s/it, loss=2.34, v_num=647]Epoch 19:   1%|          | 20/2191 [00:19<33:58,  1.06it/s, loss=2.34, v_num=647]Epoch 19:   1%|          | 20/2191 [00:19<33:58,  1.06it/s, loss=2.36, v_num=647]Epoch 19:   1%|▏         | 30/2191 [00:27<32:21,  1.11it/s, loss=2.36, v_num=647]Epoch 19:   1%|▏         | 30/2191 [00:27<32:22,  1.11it/s, loss=2.35, v_num=647]Epoch 19:   2%|▏         | 40/2191 [00:37<32:47,  1.09it/s, loss=2.35, v_num=647]Epoch 19:   2%|▏         | 40/2191 [00:37<32:47,  1.09it/s, loss=2.37, v_num=647]Epoch 19:   2%|▏         | 50/2191 [00:46<32:18,  1.10it/s, loss=2.37, v_num=647]Epoch 19:   2%|▏         | 50/2191 [00:46<32:18,  1.10it/s, loss=2.38, v_num=647]Epoch 19:   3%|▎         | 60/2191 [00:53<31:24,  1.13it/s, loss=2.38, v_num=647]Epoch 19:   3%|▎         | 60/2191 [00:53<31:24,  1.13it/s, loss=2.34, v_num=647]Epoch 19:   3%|▎         | 70/2191 [01:01<30:46,  1.15it/s, loss=2.34, v_num=647]Epoch 19:   3%|▎         | 70/2191 [01:01<30:46,  1.15it/s, loss=2.34, v_num=647]Epoch 19:   4%|▎         | 80/2191 [01:12<31:23,  1.12it/s, loss=2.34, v_num=647]Epoch 19:   4%|▎         | 80/2191 [01:12<31:23,  1.12it/s, loss=2.37, v_num=647]Epoch 19:   4%|▍         | 90/2191 [01:19<30:34,  1.15it/s, loss=2.37, v_num=647]Epoch 19:   4%|▍         | 90/2191 [01:19<30:34,  1.14it/s, loss=2.39, v_num=647]Epoch 19:   5%|▍         | 100/2191 [01:26<29:56,  1.16it/s, loss=2.39, v_num=647]Epoch 19:   5%|▍         | 100/2191 [01:26<29:56,  1.16it/s, loss=2.38, v_num=647]Epoch 19:   5%|▌         | 110/2191 [01:34<29:26,  1.18it/s, loss=2.38, v_num=647]Epoch 19:   5%|▌         | 110/2191 [01:34<29:27,  1.18it/s, loss=2.37, v_num=647]Epoch 19:   5%|▌         | 120/2191 [01:41<28:54,  1.19it/s, loss=2.37, v_num=647]Epoch 19:   5%|▌         | 120/2191 [01:41<28:54,  1.19it/s, loss=2.34, v_num=647]Epoch 19:   6%|▌         | 130/2191 [01:48<28:26,  1.21it/s, loss=2.34, v_num=647]Epoch 19:   6%|▌         | 130/2191 [01:48<28:26,  1.21it/s, loss=2.33, v_num=647]Epoch 19:   6%|▋         | 140/2191 [01:55<28:02,  1.22it/s, loss=2.33, v_num=647]Epoch 19:   6%|▋         | 140/2191 [01:55<28:02,  1.22it/s, loss=2.34, v_num=647]Epoch 19:   7%|▋         | 150/2191 [02:02<27:42,  1.23it/s, loss=2.34, v_num=647]Epoch 19:   7%|▋         | 150/2191 [02:02<27:42,  1.23it/s, loss=2.34, v_num=647]Epoch 19:   7%|▋         | 160/2191 [02:11<27:38,  1.22it/s, loss=2.34, v_num=647]Epoch 19:   7%|▋         | 160/2191 [02:11<27:38,  1.22it/s, loss=2.34, v_num=647]Epoch 19:   8%|▊         | 170/2191 [02:19<27:26,  1.23it/s, loss=2.34, v_num=647]Epoch 19:   8%|▊         | 170/2191 [02:19<27:26,  1.23it/s, loss=2.37, v_num=647]Epoch 19:   8%|▊         | 180/2191 [02:26<27:06,  1.24it/s, loss=2.37, v_num=647]Epoch 19:   8%|▊         | 180/2191 [02:26<27:06,  1.24it/s, loss=2.39, v_num=647]Epoch 19:   9%|▊         | 190/2191 [02:34<27:01,  1.23it/s, loss=2.39, v_num=647]Epoch 19:   9%|▊         | 190/2191 [02:34<27:02,  1.23it/s, loss=2.38, v_num=647]Epoch 19:   9%|▉         | 200/2191 [02:41<26:43,  1.24it/s, loss=2.38, v_num=647]Epoch 19:   9%|▉         | 200/2191 [02:41<26:43,  1.24it/s, loss=2.4, v_num=647] Epoch 19:  10%|▉         | 210/2191 [02:48<26:22,  1.25it/s, loss=2.4, v_num=647]Epoch 19:  10%|▉         | 210/2191 [02:48<26:22,  1.25it/s, loss=2.4, v_num=647]Epoch 19:  10%|█         | 220/2191 [02:55<26:03,  1.26it/s, loss=2.4, v_num=647]Epoch 19:  10%|█         | 220/2191 [02:55<26:03,  1.26it/s, loss=2.39, v_num=647]Epoch 19:  10%|█         | 230/2191 [03:03<25:56,  1.26it/s, loss=2.39, v_num=647]Epoch 19:  10%|█         | 230/2191 [03:03<25:56,  1.26it/s, loss=2.36, v_num=647]Epoch 19:  11%|█         | 240/2191 [03:11<25:47,  1.26it/s, loss=2.36, v_num=647]Epoch 19:  11%|█         | 240/2191 [03:11<25:48,  1.26it/s, loss=2.39, v_num=647]Epoch 19:  11%|█▏        | 250/2191 [03:18<25:37,  1.26it/s, loss=2.39, v_num=647]Epoch 19:  11%|█▏        | 250/2191 [03:18<25:37,  1.26it/s, loss=2.37, v_num=647]Epoch 19:  12%|█▏        | 260/2191 [03:26<25:28,  1.26it/s, loss=2.37, v_num=647]Epoch 19:  12%|█▏        | 260/2191 [03:26<25:28,  1.26it/s, loss=2.34, v_num=647]Epoch 19:  12%|█▏        | 270/2191 [03:33<25:10,  1.27it/s, loss=2.34, v_num=647]Epoch 19:  12%|█▏        | 270/2191 [03:33<25:10,  1.27it/s, loss=2.35, v_num=647]Epoch 19:  13%|█▎        | 280/2191 [03:40<25:01,  1.27it/s, loss=2.35, v_num=647]Epoch 19:  13%|█▎        | 280/2191 [03:40<25:01,  1.27it/s, loss=2.33, v_num=647]Epoch 19:  13%|█▎        | 290/2191 [03:48<24:54,  1.27it/s, loss=2.33, v_num=647]Epoch 19:  13%|█▎        | 290/2191 [03:48<24:54,  1.27it/s, loss=2.35, v_num=647]Epoch 19:  14%|█▎        | 300/2191 [03:56<24:44,  1.27it/s, loss=2.35, v_num=647]Epoch 19:  14%|█▎        | 300/2191 [03:56<24:44,  1.27it/s, loss=2.36, v_num=647]Epoch 19:  14%|█▍        | 310/2191 [04:03<24:30,  1.28it/s, loss=2.36, v_num=647]Epoch 19:  14%|█▍        | 310/2191 [04:03<24:30,  1.28it/s, loss=2.37, v_num=647]Epoch 19:  15%|█▍        | 320/2191 [04:09<24:13,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  15%|█▍        | 320/2191 [04:09<24:13,  1.29it/s, loss=2.35, v_num=647]Epoch 19:  15%|█▌        | 330/2191 [04:17<24:08,  1.28it/s, loss=2.35, v_num=647]Epoch 19:  15%|█▌        | 330/2191 [04:17<24:08,  1.28it/s, loss=2.32, v_num=647]Epoch 19:  16%|█▌        | 340/2191 [04:26<24:07,  1.28it/s, loss=2.32, v_num=647]Epoch 19:  16%|█▌        | 340/2191 [04:26<24:07,  1.28it/s, loss=2.36, v_num=647]Epoch 19:  16%|█▌        | 350/2191 [04:34<23:59,  1.28it/s, loss=2.36, v_num=647]Epoch 19:  16%|█▌        | 350/2191 [04:34<23:59,  1.28it/s, loss=2.37, v_num=647]Epoch 19:  16%|█▋        | 360/2191 [04:41<23:46,  1.28it/s, loss=2.37, v_num=647]Epoch 19:  16%|█▋        | 360/2191 [04:41<23:46,  1.28it/s, loss=2.38, v_num=647]Epoch 19:  17%|█▋        | 370/2191 [04:48<23:34,  1.29it/s, loss=2.38, v_num=647]Epoch 19:  17%|█▋        | 370/2191 [04:48<23:34,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  17%|█▋        | 380/2191 [04:56<23:28,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  17%|█▋        | 380/2191 [04:56<23:28,  1.29it/s, loss=2.38, v_num=647]Epoch 19:  18%|█▊        | 390/2191 [05:03<23:19,  1.29it/s, loss=2.38, v_num=647]Epoch 19:  18%|█▊        | 390/2191 [05:03<23:19,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  18%|█▊        | 400/2191 [05:10<23:07,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  18%|█▊        | 400/2191 [05:10<23:07,  1.29it/s, loss=2.34, v_num=647]Epoch 19:  19%|█▊        | 410/2191 [05:19<23:06,  1.28it/s, loss=2.34, v_num=647]Epoch 19:  19%|█▊        | 410/2191 [05:20<23:06,  1.28it/s, loss=2.37, v_num=647]Epoch 19:  19%|█▉        | 420/2191 [05:27<22:56,  1.29it/s, loss=2.37, v_num=647]Epoch 19:  19%|█▉        | 420/2191 [05:27<22:56,  1.29it/s, loss=2.36, v_num=647]Epoch 19:  20%|█▉        | 430/2191 [05:34<22:45,  1.29it/s, loss=2.36, v_num=647]Epoch 19:  20%|█▉        | 430/2191 [05:34<22:45,  1.29it/s, loss=2.38, v_num=647]Epoch 19:  20%|██        | 440/2191 [05:40<22:32,  1.29it/s, loss=2.38, v_num=647]Epoch 19:  20%|██        | 440/2191 [05:40<22:32,  1.29it/s, loss=2.36, v_num=647]Epoch 19:  21%|██        | 450/2191 [05:47<22:22,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  21%|██        | 450/2191 [05:47<22:22,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  21%|██        | 460/2191 [05:55<22:13,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  21%|██        | 460/2191 [05:55<22:13,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  21%|██▏       | 470/2191 [06:01<22:02,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  21%|██▏       | 470/2191 [06:01<22:02,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  22%|██▏       | 480/2191 [06:08<21:52,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  22%|██▏       | 480/2191 [06:08<21:52,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  22%|██▏       | 490/2191 [06:19<21:53,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  22%|██▏       | 490/2191 [06:19<21:53,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  23%|██▎       | 500/2191 [06:25<21:40,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  23%|██▎       | 500/2191 [06:25<21:40,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  23%|██▎       | 510/2191 [06:34<21:36,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  23%|██▎       | 510/2191 [06:34<21:36,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  24%|██▎       | 520/2191 [06:41<21:27,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  24%|██▎       | 520/2191 [06:41<21:27,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  24%|██▍       | 530/2191 [06:47<21:15,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  24%|██▍       | 530/2191 [06:47<21:15,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  25%|██▍       | 540/2191 [06:55<21:06,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  25%|██▍       | 540/2191 [06:55<21:06,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  25%|██▌       | 550/2191 [07:02<20:59,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  25%|██▌       | 550/2191 [07:02<20:59,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  26%|██▌       | 560/2191 [07:10<20:50,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  26%|██▌       | 560/2191 [07:10<20:50,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  26%|██▌       | 570/2191 [07:18<20:45,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  26%|██▌       | 570/2191 [07:18<20:45,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  26%|██▋       | 580/2191 [07:27<20:39,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  26%|██▋       | 580/2191 [07:27<20:39,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  27%|██▋       | 590/2191 [07:35<20:32,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  27%|██▋       | 590/2191 [07:35<20:32,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  27%|██▋       | 600/2191 [07:43<20:25,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  27%|██▋       | 600/2191 [07:43<20:25,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  28%|██▊       | 610/2191 [07:49<20:15,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  28%|██▊       | 610/2191 [07:49<20:15,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  28%|██▊       | 620/2191 [07:57<20:06,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  28%|██▊       | 620/2191 [07:57<20:06,  1.30it/s, loss=2.43, v_num=647]Epoch 19:  29%|██▉       | 630/2191 [08:04<19:59,  1.30it/s, loss=2.43, v_num=647]Epoch 19:  29%|██▉       | 630/2191 [08:04<19:59,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  29%|██▉       | 640/2191 [08:12<19:51,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  29%|██▉       | 640/2191 [08:12<19:51,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  30%|██▉       | 650/2191 [08:20<19:44,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  30%|██▉       | 650/2191 [08:20<19:44,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  30%|███       | 660/2191 [08:28<19:37,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  30%|███       | 660/2191 [08:28<19:37,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  31%|███       | 670/2191 [08:38<19:35,  1.29it/s, loss=2.34, v_num=647]Epoch 19:  31%|███       | 670/2191 [08:38<19:35,  1.29it/s, loss=2.36, v_num=647]Epoch 19:  31%|███       | 680/2191 [08:44<19:24,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  31%|███       | 680/2191 [08:44<19:24,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  31%|███▏      | 690/2191 [08:52<19:15,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  31%|███▏      | 690/2191 [08:52<19:15,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  32%|███▏      | 700/2191 [08:59<19:07,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  32%|███▏      | 700/2191 [08:59<19:07,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  32%|███▏      | 710/2191 [09:07<18:59,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  32%|███▏      | 710/2191 [09:07<18:59,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  33%|███▎      | 720/2191 [09:15<18:53,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  33%|███▎      | 720/2191 [09:15<18:53,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  33%|███▎      | 730/2191 [09:21<18:43,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  33%|███▎      | 730/2191 [09:21<18:43,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  34%|███▍      | 740/2191 [09:28<18:33,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  34%|███▍      | 740/2191 [09:28<18:33,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  34%|███▍      | 750/2191 [09:37<18:27,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  34%|███▍      | 750/2191 [09:37<18:27,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  35%|███▍      | 760/2191 [09:45<18:20,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  35%|███▍      | 760/2191 [09:45<18:20,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  35%|███▌      | 770/2191 [09:53<18:13,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  35%|███▌      | 770/2191 [09:53<18:13,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  36%|███▌      | 780/2191 [10:00<18:04,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  36%|███▌      | 780/2191 [10:00<18:04,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  36%|███▌      | 790/2191 [10:07<17:55,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  36%|███▌      | 790/2191 [10:07<17:55,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  37%|███▋      | 800/2191 [10:14<17:47,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  37%|███▋      | 800/2191 [10:14<17:47,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  37%|███▋      | 810/2191 [10:22<17:40,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  37%|███▋      | 810/2191 [10:22<17:40,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  37%|███▋      | 820/2191 [10:31<17:34,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  37%|███▋      | 820/2191 [10:31<17:34,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  38%|███▊      | 830/2191 [10:39<17:27,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  38%|███▊      | 830/2191 [10:39<17:27,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  38%|███▊      | 840/2191 [10:47<17:20,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  38%|███▊      | 840/2191 [10:47<17:20,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  39%|███▉      | 850/2191 [10:55<17:12,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  39%|███▉      | 850/2191 [10:55<17:12,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  39%|███▉      | 860/2191 [11:03<17:06,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  39%|███▉      | 860/2191 [11:03<17:06,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  40%|███▉      | 870/2191 [11:10<16:56,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  40%|███▉      | 870/2191 [11:10<16:56,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  40%|████      | 880/2191 [11:19<16:50,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  40%|████      | 880/2191 [11:19<16:50,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  41%|████      | 890/2191 [11:27<16:43,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  41%|████      | 890/2191 [11:27<16:43,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  41%|████      | 900/2191 [11:34<16:35,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  41%|████      | 900/2191 [11:34<16:35,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  42%|████▏     | 910/2191 [11:41<16:26,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  42%|████▏     | 910/2191 [11:41<16:26,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  42%|████▏     | 920/2191 [11:50<16:20,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  42%|████▏     | 920/2191 [11:50<16:20,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  42%|████▏     | 930/2191 [11:58<16:13,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  42%|████▏     | 930/2191 [11:58<16:13,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  43%|████▎     | 940/2191 [12:05<16:04,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  43%|████▎     | 940/2191 [12:05<16:04,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  43%|████▎     | 950/2191 [12:14<15:58,  1.29it/s, loss=2.34, v_num=647]Epoch 19:  43%|████▎     | 950/2191 [12:14<15:58,  1.29it/s, loss=2.32, v_num=647]Epoch 19:  44%|████▍     | 960/2191 [12:21<15:49,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  44%|████▍     | 960/2191 [12:21<15:49,  1.30it/s, loss=2.31, v_num=647]Epoch 19:  44%|████▍     | 970/2191 [12:28<15:41,  1.30it/s, loss=2.31, v_num=647]Epoch 19:  44%|████▍     | 970/2191 [12:28<15:41,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  45%|████▍     | 980/2191 [12:36<15:34,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  45%|████▍     | 980/2191 [12:36<15:34,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  45%|████▌     | 990/2191 [12:43<15:25,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  45%|████▌     | 990/2191 [12:43<15:25,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  46%|████▌     | 1000/2191 [12:50<15:16,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  46%|████▌     | 1000/2191 [12:50<15:16,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  46%|████▌     | 1010/2191 [12:59<15:10,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  46%|████▌     | 1010/2191 [12:59<15:10,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  47%|████▋     | 1020/2191 [13:06<15:01,  1.30it/s, loss=2.32, v_num=647]Epoch 19:  47%|████▋     | 1020/2191 [13:06<15:01,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  47%|████▋     | 1030/2191 [13:13<14:53,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  47%|████▋     | 1030/2191 [13:13<14:53,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  47%|████▋     | 1040/2191 [13:21<14:45,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  47%|████▋     | 1040/2191 [13:21<14:45,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  48%|████▊     | 1050/2191 [13:29<14:38,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  48%|████▊     | 1050/2191 [13:29<14:38,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  48%|████▊     | 1060/2191 [13:35<14:29,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  48%|████▊     | 1060/2191 [13:35<14:29,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  49%|████▉     | 1070/2191 [13:43<14:21,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  49%|████▉     | 1070/2191 [13:43<14:21,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  49%|████▉     | 1080/2191 [13:51<14:15,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  49%|████▉     | 1080/2191 [13:51<14:15,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  50%|████▉     | 1090/2191 [13:58<14:06,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  50%|████▉     | 1090/2191 [13:58<14:06,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  50%|█████     | 1100/2191 [14:05<13:57,  1.30it/s, loss=2.35, v_num=647]Epoch 19:  50%|█████     | 1100/2191 [14:05<13:57,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  51%|█████     | 1110/2191 [14:14<13:50,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  51%|█████     | 1110/2191 [14:14<13:50,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  51%|█████     | 1120/2191 [14:22<13:43,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  51%|█████     | 1120/2191 [14:22<13:43,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  52%|█████▏    | 1130/2191 [14:30<13:36,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  52%|█████▏    | 1130/2191 [14:30<13:36,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  52%|█████▏    | 1140/2191 [14:38<13:28,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  52%|█████▏    | 1140/2191 [14:38<13:28,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  52%|█████▏    | 1150/2191 [14:45<13:20,  1.30it/s, loss=2.33, v_num=647]Epoch 19:  52%|█████▏    | 1150/2191 [14:45<13:20,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  53%|█████▎    | 1160/2191 [14:52<13:12,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  53%|█████▎    | 1160/2191 [14:52<13:12,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  53%|█████▎    | 1170/2191 [15:00<13:04,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  53%|█████▎    | 1170/2191 [15:00<13:04,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  54%|█████▍    | 1180/2191 [15:07<12:56,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  54%|█████▍    | 1180/2191 [15:07<12:56,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  54%|█████▍    | 1190/2191 [15:15<12:49,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  54%|█████▍    | 1190/2191 [15:15<12:49,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  55%|█████▍    | 1200/2191 [15:22<12:41,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  55%|█████▍    | 1200/2191 [15:22<12:41,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  55%|█████▌    | 1210/2191 [15:29<12:33,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  55%|█████▌    | 1210/2191 [15:29<12:33,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  56%|█████▌    | 1230/2191 [15:45<12:18,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  56%|█████▌    | 1230/2191 [15:45<12:18,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  57%|█████▋    | 1240/2191 [15:53<12:10,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  57%|█████▋    | 1240/2191 [15:53<12:10,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  57%|█████▋    | 1250/2191 [16:00<12:02,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  57%|█████▋    | 1250/2191 [16:00<12:02,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1260/2191 [16:09<11:55,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1260/2191 [16:09<11:55,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1270/2191 [16:17<11:48,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1270/2191 [16:17<11:48,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1280/2191 [16:24<11:40,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  58%|█████▊    | 1280/2191 [16:24<11:40,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  59%|█████▉    | 1290/2191 [16:31<11:31,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  59%|█████▉    | 1290/2191 [16:31<11:31,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  59%|█████▉    | 1300/2191 [16:37<11:23,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  59%|█████▉    | 1300/2191 [16:37<11:23,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  60%|█████▉    | 1310/2191 [16:44<11:15,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  60%|█████▉    | 1310/2191 [16:44<11:15,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  60%|██████    | 1320/2191 [16:52<11:07,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  60%|██████    | 1320/2191 [16:52<11:07,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  61%|██████    | 1330/2191 [16:59<10:59,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  61%|██████    | 1330/2191 [16:59<10:59,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  61%|██████    | 1340/2191 [17:07<10:51,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  61%|██████    | 1340/2191 [17:07<10:51,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  62%|██████▏   | 1350/2191 [17:13<10:43,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  62%|██████▏   | 1350/2191 [17:13<10:43,  1.31it/s, loss=2.4, v_num=647] Epoch 19:  62%|██████▏   | 1360/2191 [17:21<10:35,  1.31it/s, loss=2.4, v_num=647]Epoch 19:  62%|██████▏   | 1360/2191 [17:21<10:35,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  63%|██████▎   | 1370/2191 [17:27<10:27,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  63%|██████▎   | 1370/2191 [17:27<10:27,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  63%|██████▎   | 1380/2191 [17:36<10:20,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  63%|██████▎   | 1380/2191 [17:36<10:20,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  63%|██████▎   | 1390/2191 [17:47<10:14,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  63%|██████▎   | 1390/2191 [17:47<10:14,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  64%|██████▍   | 1400/2191 [17:54<10:06,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  64%|██████▍   | 1400/2191 [17:54<10:06,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  64%|██████▍   | 1410/2191 [18:03<09:59,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  64%|██████▍   | 1410/2191 [18:03<09:59,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  65%|██████▍   | 1420/2191 [18:11<09:51,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  65%|██████▍   | 1420/2191 [18:11<09:51,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  65%|██████▌   | 1430/2191 [18:18<09:43,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  65%|██████▌   | 1430/2191 [18:18<09:43,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  66%|██████▌   | 1440/2191 [18:26<09:36,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  66%|██████▌   | 1440/2191 [18:26<09:36,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  66%|██████▌   | 1450/2191 [18:34<09:29,  1.30it/s, loss=2.39, v_num=647]Epoch 19:  66%|██████▌   | 1450/2191 [18:34<09:29,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  67%|██████▋   | 1460/2191 [18:42<09:21,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  67%|██████▋   | 1460/2191 [18:42<09:21,  1.30it/s, loss=2.43, v_num=647]Epoch 19:  67%|██████▋   | 1470/2191 [18:49<09:13,  1.30it/s, loss=2.43, v_num=647]Epoch 19:  67%|██████▋   | 1470/2191 [18:49<09:13,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  68%|██████▊   | 1480/2191 [18:56<09:05,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  68%|██████▊   | 1480/2191 [18:56<09:05,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  68%|██████▊   | 1490/2191 [19:05<08:58,  1.30it/s, loss=2.34, v_num=647]Epoch 19:  68%|██████▊   | 1490/2191 [19:05<08:58,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  68%|██████▊   | 1500/2191 [19:11<08:49,  1.30it/s, loss=2.37, v_num=647]Epoch 19:  68%|██████▊   | 1500/2191 [19:11<08:49,  1.30it/s, loss=2.4, v_num=647] Epoch 19:  69%|██████▉   | 1510/2191 [19:18<08:41,  1.30it/s, loss=2.4, v_num=647]Epoch 19:  69%|██████▉   | 1510/2191 [19:18<08:41,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  69%|██████▉   | 1520/2191 [19:25<08:34,  1.30it/s, loss=2.38, v_num=647]Epoch 19:  69%|██████▉   | 1520/2191 [19:25<08:34,  1.30it/s, loss=2.36, v_num=647]Epoch 19:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  70%|██████▉   | 1530/2191 [19:32<08:26,  1.31it/s, loss=2.41, v_num=647]Epoch 19:  70%|███████   | 1540/2191 [19:41<08:19,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  70%|███████   | 1540/2191 [19:41<08:19,  1.30it/s, loss=2.41, v_num=647]Epoch 19:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.41, v_num=647]Epoch 19:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  71%|███████   | 1560/2191 [19:54<08:02,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  71%|███████   | 1560/2191 [19:54<08:02,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  72%|███████▏  | 1570/2191 [20:01<07:54,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  72%|███████▏  | 1570/2191 [20:01<07:54,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  72%|███████▏  | 1580/2191 [20:07<07:46,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  72%|███████▏  | 1580/2191 [20:07<07:46,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  73%|███████▎  | 1590/2191 [20:15<07:39,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  73%|███████▎  | 1590/2191 [20:15<07:39,  1.31it/s, loss=2.4, v_num=647] Epoch 19:  73%|███████▎  | 1600/2191 [20:23<07:31,  1.31it/s, loss=2.4, v_num=647]Epoch 19:  73%|███████▎  | 1600/2191 [20:23<07:31,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  74%|███████▍  | 1620/2191 [20:38<07:16,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  74%|███████▍  | 1620/2191 [20:38<07:16,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  74%|███████▍  | 1630/2191 [20:46<07:08,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  74%|███████▍  | 1630/2191 [20:46<07:08,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.42, v_num=647]Epoch 19:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  75%|███████▌  | 1650/2191 [21:01<06:53,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  75%|███████▌  | 1650/2191 [21:01<06:53,  1.31it/s, loss=2.34, v_num=647]Epoch 19:  76%|███████▌  | 1660/2191 [21:07<06:45,  1.31it/s, loss=2.34, v_num=647]Epoch 19:  76%|███████▌  | 1660/2191 [21:07<06:45,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  76%|███████▌  | 1670/2191 [21:15<06:37,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  76%|███████▌  | 1670/2191 [21:15<06:37,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  77%|███████▋  | 1680/2191 [21:23<06:30,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  77%|███████▋  | 1680/2191 [21:23<06:30,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  77%|███████▋  | 1690/2191 [21:30<06:22,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  77%|███████▋  | 1690/2191 [21:30<06:22,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  78%|███████▊  | 1700/2191 [21:38<06:14,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  78%|███████▊  | 1700/2191 [21:38<06:14,  1.31it/s, loss=2.33, v_num=647]Epoch 19:  78%|███████▊  | 1710/2191 [21:47<06:07,  1.31it/s, loss=2.33, v_num=647]Epoch 19:  78%|███████▊  | 1710/2191 [21:47<06:07,  1.31it/s, loss=2.34, v_num=647]Epoch 19:  79%|███████▊  | 1720/2191 [21:54<05:59,  1.31it/s, loss=2.34, v_num=647]Epoch 19:  79%|███████▊  | 1720/2191 [21:54<05:59,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  79%|███████▉  | 1730/2191 [22:02<05:52,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  79%|███████▉  | 1730/2191 [22:02<05:52,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  79%|███████▉  | 1740/2191 [22:09<05:44,  1.31it/s, loss=2.37, v_num=647]Epoch 19:  79%|███████▉  | 1740/2191 [22:09<05:44,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  80%|███████▉  | 1750/2191 [22:17<05:36,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  80%|███████▉  | 1750/2191 [22:17<05:36,  1.31it/s, loss=2.35, v_num=647]Epoch 19:  80%|████████  | 1760/2191 [22:24<05:29,  1.31it/s, loss=2.35, v_num=647]Epoch 19:  80%|████████  | 1760/2191 [22:24<05:29,  1.31it/s, loss=2.35, v_num=647]Epoch 19:  81%|████████  | 1770/2191 [22:32<05:21,  1.31it/s, loss=2.35, v_num=647]Epoch 19:  81%|████████  | 1770/2191 [22:32<05:21,  1.31it/s, loss=2.41, v_num=647]Epoch 19:  81%|████████  | 1780/2191 [22:38<05:13,  1.31it/s, loss=2.41, v_num=647]Epoch 19:  81%|████████  | 1780/2191 [22:38<05:13,  1.31it/s, loss=2.45, v_num=647]Epoch 19:  82%|████████▏ | 1790/2191 [22:45<05:05,  1.31it/s, loss=2.45, v_num=647]Epoch 19:  82%|████████▏ | 1790/2191 [22:45<05:05,  1.31it/s, loss=2.43, v_num=647]Epoch 19:  82%|████████▏ | 1800/2191 [22:52<04:57,  1.31it/s, loss=2.43, v_num=647]Epoch 19:  82%|████████▏ | 1800/2191 [22:52<04:57,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  83%|████████▎ | 1810/2191 [23:00<04:50,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  83%|████████▎ | 1810/2191 [23:00<04:50,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  83%|████████▎ | 1820/2191 [23:06<04:42,  1.31it/s, loss=2.38, v_num=647]Epoch 19:  83%|████████▎ | 1820/2191 [23:06<04:42,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  84%|████████▎ | 1830/2191 [23:13<04:34,  1.31it/s, loss=2.39, v_num=647]Epoch 19:  84%|████████▎ | 1830/2191 [23:13<04:34,  1.31it/s, loss=2.36, v_num=647]Epoch 19:  84%|████████▍ | 1840/2191 [23:17<04:26,  1.32it/s, loss=2.36, v_num=647]Epoch 19:  84%|████████▍ | 1840/2191 [23:17<04:26,  1.32it/s, loss=2.34, v_num=647]Epoch 19:  84%|████████▍ | 1850/2191 [23:20<04:18,  1.32it/s, loss=2.34, v_num=647]Epoch 19:  84%|████████▍ | 1850/2191 [23:20<04:18,  1.32it/s, loss=2.38, v_num=647]Epoch 19:  85%|████████▍ | 1860/2191 [23:22<04:09,  1.33it/s, loss=2.38, v_num=647]Epoch 19:  85%|████████▍ | 1860/2191 [23:22<04:09,  1.33it/s, loss=2.37, v_num=647]Epoch 19:  85%|████████▌ | 1870/2191 [23:24<04:01,  1.33it/s, loss=2.37, v_num=647]Epoch 19:  85%|████████▌ | 1870/2191 [23:24<04:01,  1.33it/s, loss=2.35, v_num=647]validation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.976515531539917

Epoch 19:  86%|████████▌ | 1880/2191 [23:26<03:52,  1.34it/s, loss=2.35, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3450479233226837
valid accuracy: 0.9737087488174438
validation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9775481820106506

Validating:   3%|▎         | 10/313 [00:01<00:45,  6.61it/s][AEpoch 19:  86%|████████▋ | 1890/2191 [23:27<03:44,  1.34it/s, loss=2.35, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:52,  5.53it/s][AEpoch 19:  87%|████████▋ | 1900/2191 [23:29<03:35,  1.35it/s, loss=2.35, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:37,  7.49it/s][AEpoch 19:  87%|████████▋ | 1910/2191 [23:30<03:27,  1.35it/s, loss=2.35, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:27,  9.76it/s][AEpoch 19:  88%|████████▊ | 1920/2191 [23:31<03:19,  1.36it/s, loss=2.35, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26,  9.98it/s][AEpoch 19:  88%|████████▊ | 1930/2191 [23:32<03:10,  1.37it/s, loss=2.35, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.15it/s][AEpoch 19:  89%|████████▊ | 1940/2191 [23:33<03:02,  1.37it/s, loss=2.35, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24,  9.91it/s][AEpoch 19:  89%|████████▉ | 1950/2191 [23:34<02:54,  1.38it/s, loss=2.35, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:23, 10.09it/s][AEpoch 19:  89%|████████▉ | 1960/2191 [23:35<02:46,  1.39it/s, loss=2.35, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.18it/s][AEpoch 19:  90%|████████▉ | 1970/2191 [23:36<02:38,  1.39it/s, loss=2.35, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.18it/s][AEpoch 19:  90%|█████████ | 1980/2191 [23:37<02:30,  1.40it/s, loss=2.35, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:21,  9.40it/s][AEpoch 19:  91%|█████████ | 1990/2191 [23:38<02:23,  1.40it/s, loss=2.35, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:20,  9.63it/s][AEpoch 19:  91%|█████████▏| 2000/2191 [23:39<02:15,  1.41it/s, loss=2.35, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:19,  9.28it/s][AEpoch 19:  92%|█████████▏| 2010/2191 [23:40<02:07,  1.42it/s, loss=2.35, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.35it/s][AEpoch 19:  92%|█████████▏| 2020/2191 [23:41<02:00,  1.42it/s, loss=2.35, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.20it/s][AEpoch 19:  93%|█████████▎| 2030/2191 [23:42<01:52,  1.43it/s, loss=2.35, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:14, 10.40it/s][AEpoch 19:  93%|█████████▎| 2040/2191 [23:43<01:45,  1.43it/s, loss=2.35, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.72it/s][AEpoch 19:  94%|█████████▎| 2050/2191 [23:44<01:37,  1.44it/s, loss=2.35, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.43it/s][AEpoch 19:  94%|█████████▍| 2060/2191 [23:45<01:30,  1.45it/s, loss=2.35, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.73it/s][AEpoch 19:  94%|█████████▍| 2070/2191 [23:45<01:23,  1.45it/s, loss=2.35, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 12.08it/s][AEpoch 19:  95%|█████████▍| 2080/2191 [23:46<01:16,  1.46it/s, loss=2.35, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:08, 11.84it/s][AEpoch 19:  95%|█████████▌| 2090/2191 [23:47<01:08,  1.46it/s, loss=2.35, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.97it/s][AEpoch 19:  96%|█████████▌| 2100/2191 [23:48<01:01,  1.47it/s, loss=2.35, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 11.06it/s][AEpoch 19:  96%|█████████▋| 2110/2191 [23:49<00:54,  1.48it/s, loss=2.35, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:06, 10.54it/s][AEpoch 19:  97%|█████████▋| 2120/2191 [23:50<00:47,  1.48it/s, loss=2.35, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:05, 10.71it/s][AEpoch 19:  97%|█████████▋| 2130/2191 [23:51<00:40,  1.49it/s, loss=2.35, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.41it/s][AEpoch 19:  98%|█████████▊| 2140/2191 [23:52<00:34,  1.49it/s, loss=2.35, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 10.94it/s][AEpoch 19:  98%|█████████▊| 2150/2191 [23:53<00:27,  1.50it/s, loss=2.35, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.69it/s][AEpoch 19:  99%|█████████▊| 2160/2191 [23:53<00:20,  1.51it/s, loss=2.35, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:02, 11.50it/s][AEpoch 19:  99%|█████████▉| 2170/2191 [23:55<00:13,  1.51it/s, loss=2.35, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.97it/s][AEpoch 19:  99%|█████████▉| 2180/2191 [23:55<00:07,  1.52it/s, loss=2.35, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.51it/s][AEpoch 19: 100%|█████████▉| 2190/2191 [23:56<00:00,  1.53it/s, loss=2.35, v_num=647]validation_epoch_end
graph acc: 0.3961661341853035
valid accuracy: 0.9763878583908081
Epoch 19: 100%|██████████| 2191/2191 [23:57<00:00,  1.52it/s, loss=2.37, v_num=647]
                                                             [AEpoch 19:   0%|          | 0/2191 [00:00<00:00, 12018.06it/s, loss=2.37, v_num=647]Epoch 20:   0%|          | 0/2191 [00:00<00:00, 2822.55it/s, loss=2.37, v_num=647] Epoch 20:   0%|          | 10/2191 [00:12<41:43,  1.15s/it, loss=2.37, v_num=647] Epoch 20:   0%|          | 10/2191 [00:12<41:43,  1.15s/it, loss=2.34, v_num=647]Epoch 20:   1%|          | 20/2191 [00:20<35:29,  1.02it/s, loss=2.34, v_num=647]Epoch 20:   1%|          | 20/2191 [00:20<35:29,  1.02it/s, loss=2.3, v_num=647] Epoch 20:   1%|▏         | 30/2191 [00:28<32:45,  1.10it/s, loss=2.3, v_num=647]Epoch 20:   1%|▏         | 30/2191 [00:28<32:45,  1.10it/s, loss=2.28, v_num=647]Epoch 20:   2%|▏         | 40/2191 [00:36<31:44,  1.13it/s, loss=2.28, v_num=647]Epoch 20:   2%|▏         | 40/2191 [00:36<31:44,  1.13it/s, loss=2.31, v_num=647]Epoch 20:   2%|▏         | 50/2191 [00:44<31:15,  1.14it/s, loss=2.31, v_num=647]Epoch 20:   2%|▏         | 50/2191 [00:44<31:15,  1.14it/s, loss=2.35, v_num=647]Epoch 20:   3%|▎         | 60/2191 [00:51<30:02,  1.18it/s, loss=2.35, v_num=647]Epoch 20:   3%|▎         | 60/2191 [00:51<30:02,  1.18it/s, loss=2.37, v_num=647]Epoch 20:   3%|▎         | 70/2191 [01:00<30:01,  1.18it/s, loss=2.37, v_num=647]Epoch 20:   3%|▎         | 70/2191 [01:00<30:01,  1.18it/s, loss=2.39, v_num=647]Epoch 20:   4%|▎         | 80/2191 [01:08<29:47,  1.18it/s, loss=2.39, v_num=647]Epoch 20:   4%|▎         | 80/2191 [01:08<29:47,  1.18it/s, loss=2.37, v_num=647]Epoch 20:   4%|▍         | 90/2191 [01:15<28:58,  1.21it/s, loss=2.37, v_num=647]Epoch 20:   4%|▍         | 90/2191 [01:15<28:58,  1.21it/s, loss=2.38, v_num=647]Epoch 20:   5%|▍         | 100/2191 [01:23<28:52,  1.21it/s, loss=2.38, v_num=647]Epoch 20:   5%|▍         | 100/2191 [01:23<28:52,  1.21it/s, loss=2.41, v_num=647]Epoch 20:   5%|▌         | 110/2191 [01:30<28:21,  1.22it/s, loss=2.41, v_num=647]Epoch 20:   5%|▌         | 110/2191 [01:30<28:21,  1.22it/s, loss=2.39, v_num=647]Epoch 20:   5%|▌         | 120/2191 [01:39<28:16,  1.22it/s, loss=2.39, v_num=647]Epoch 20:   5%|▌         | 120/2191 [01:39<28:16,  1.22it/s, loss=2.32, v_num=647]Epoch 20:   6%|▌         | 130/2191 [01:48<28:31,  1.20it/s, loss=2.32, v_num=647]Epoch 20:   6%|▌         | 130/2191 [01:48<28:31,  1.20it/s, loss=2.31, v_num=647]Epoch 20:   6%|▋         | 140/2191 [01:56<28:19,  1.21it/s, loss=2.31, v_num=647]Epoch 20:   6%|▋         | 140/2191 [01:56<28:19,  1.21it/s, loss=2.33, v_num=647]Epoch 20:   7%|▋         | 150/2191 [02:04<27:56,  1.22it/s, loss=2.33, v_num=647]Epoch 20:   7%|▋         | 150/2191 [02:04<27:56,  1.22it/s, loss=2.33, v_num=647]Epoch 20:   7%|▋         | 160/2191 [02:12<27:55,  1.21it/s, loss=2.33, v_num=647]Epoch 20:   7%|▋         | 160/2191 [02:12<27:55,  1.21it/s, loss=2.31, v_num=647]Epoch 20:   8%|▊         | 170/2191 [02:21<27:46,  1.21it/s, loss=2.31, v_num=647]Epoch 20:   8%|▊         | 170/2191 [02:21<27:46,  1.21it/s, loss=2.32, v_num=647]Epoch 20:   8%|▊         | 180/2191 [02:28<27:29,  1.22it/s, loss=2.32, v_num=647]Epoch 20:   8%|▊         | 180/2191 [02:28<27:29,  1.22it/s, loss=2.36, v_num=647]Epoch 20:   9%|▊         | 190/2191 [02:36<27:23,  1.22it/s, loss=2.36, v_num=647]Epoch 20:   9%|▊         | 190/2191 [02:36<27:23,  1.22it/s, loss=2.38, v_num=647]Epoch 20:   9%|▉         | 200/2191 [02:43<27:02,  1.23it/s, loss=2.38, v_num=647]Epoch 20:   9%|▉         | 200/2191 [02:43<27:02,  1.23it/s, loss=2.36, v_num=647]Epoch 20:  10%|▉         | 210/2191 [02:50<26:45,  1.23it/s, loss=2.36, v_num=647]Epoch 20:  10%|▉         | 210/2191 [02:50<26:45,  1.23it/s, loss=2.32, v_num=647]Epoch 20:  10%|█         | 220/2191 [02:58<26:27,  1.24it/s, loss=2.32, v_num=647]Epoch 20:  10%|█         | 220/2191 [02:58<26:27,  1.24it/s, loss=2.3, v_num=647] Epoch 20:  10%|█         | 230/2191 [03:05<26:12,  1.25it/s, loss=2.3, v_num=647]Epoch 20:  10%|█         | 230/2191 [03:05<26:12,  1.25it/s, loss=2.33, v_num=647]Epoch 20:  11%|█         | 240/2191 [03:12<25:58,  1.25it/s, loss=2.33, v_num=647]Epoch 20:  11%|█         | 240/2191 [03:12<25:58,  1.25it/s, loss=2.36, v_num=647]Epoch 20:  11%|█▏        | 250/2191 [03:20<25:47,  1.25it/s, loss=2.36, v_num=647]Epoch 20:  11%|█▏        | 250/2191 [03:20<25:47,  1.25it/s, loss=2.38, v_num=647]Epoch 20:  12%|█▏        | 260/2191 [03:27<25:38,  1.25it/s, loss=2.38, v_num=647]Epoch 20:  12%|█▏        | 260/2191 [03:27<25:38,  1.25it/s, loss=2.35, v_num=647]Epoch 20:  12%|█▏        | 270/2191 [03:36<25:34,  1.25it/s, loss=2.35, v_num=647]Epoch 20:  12%|█▏        | 270/2191 [03:36<25:34,  1.25it/s, loss=2.34, v_num=647]Epoch 20:  13%|█▎        | 280/2191 [03:43<25:17,  1.26it/s, loss=2.34, v_num=647]Epoch 20:  13%|█▎        | 280/2191 [03:43<25:17,  1.26it/s, loss=2.34, v_num=647]Epoch 20:  13%|█▎        | 290/2191 [03:50<25:05,  1.26it/s, loss=2.34, v_num=647]Epoch 20:  13%|█▎        | 290/2191 [03:50<25:05,  1.26it/s, loss=2.32, v_num=647]Epoch 20:  14%|█▎        | 300/2191 [03:58<25:00,  1.26it/s, loss=2.32, v_num=647]Epoch 20:  14%|█▎        | 300/2191 [03:58<25:00,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  14%|█▍        | 310/2191 [04:06<24:51,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  14%|█▍        | 310/2191 [04:06<24:51,  1.26it/s, loss=2.35, v_num=647]Epoch 20:  15%|█▍        | 320/2191 [04:14<24:41,  1.26it/s, loss=2.35, v_num=647]Epoch 20:  15%|█▍        | 320/2191 [04:14<24:41,  1.26it/s, loss=2.35, v_num=647]Epoch 20:  15%|█▌        | 330/2191 [04:23<24:41,  1.26it/s, loss=2.35, v_num=647]Epoch 20:  15%|█▌        | 330/2191 [04:23<24:41,  1.26it/s, loss=2.36, v_num=647]Epoch 20:  16%|█▌        | 340/2191 [04:31<24:33,  1.26it/s, loss=2.36, v_num=647]Epoch 20:  16%|█▌        | 340/2191 [04:31<24:33,  1.26it/s, loss=2.37, v_num=647]Epoch 20:  16%|█▌        | 350/2191 [04:38<24:20,  1.26it/s, loss=2.37, v_num=647]Epoch 20:  16%|█▌        | 350/2191 [04:38<24:20,  1.26it/s, loss=2.36, v_num=647]Epoch 20:  16%|█▋        | 360/2191 [04:46<24:10,  1.26it/s, loss=2.36, v_num=647]Epoch 20:  16%|█▋        | 360/2191 [04:46<24:10,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  17%|█▋        | 370/2191 [04:54<24:06,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  17%|█▋        | 370/2191 [04:54<24:06,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  17%|█▋        | 380/2191 [05:01<23:51,  1.26it/s, loss=2.33, v_num=647]Epoch 20:  17%|█▋        | 380/2191 [05:01<23:51,  1.26it/s, loss=2.35, v_num=647]Epoch 20:  18%|█▊        | 390/2191 [05:08<23:40,  1.27it/s, loss=2.35, v_num=647]Epoch 20:  18%|█▊        | 390/2191 [05:08<23:40,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  18%|█▊        | 400/2191 [05:14<23:24,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  18%|█▊        | 400/2191 [05:14<23:24,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  19%|█▊        | 410/2191 [05:23<23:23,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  19%|█▊        | 410/2191 [05:23<23:23,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  19%|█▉        | 420/2191 [05:32<23:18,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  19%|█▉        | 420/2191 [05:32<23:18,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  20%|█▉        | 430/2191 [05:39<23:05,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  20%|█▉        | 430/2191 [05:39<23:05,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  20%|██        | 440/2191 [05:46<22:56,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  20%|██        | 440/2191 [05:46<22:56,  1.27it/s, loss=2.4, v_num=647] Epoch 20:  21%|██        | 450/2191 [05:53<22:46,  1.27it/s, loss=2.4, v_num=647]Epoch 20:  21%|██        | 450/2191 [05:53<22:46,  1.27it/s, loss=2.35, v_num=647]Epoch 20:  21%|██        | 460/2191 [06:01<22:37,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  21%|██        | 460/2191 [06:01<22:37,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  21%|██▏       | 470/2191 [06:08<22:25,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  21%|██▏       | 470/2191 [06:08<22:25,  1.28it/s, loss=2.38, v_num=647]Epoch 20:  22%|██▏       | 480/2191 [06:16<22:20,  1.28it/s, loss=2.38, v_num=647]Epoch 20:  22%|██▏       | 480/2191 [06:16<22:20,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  22%|██▏       | 490/2191 [06:23<22:07,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  22%|██▏       | 490/2191 [06:23<22:07,  1.28it/s, loss=2.31, v_num=647]Epoch 20:  23%|██▎       | 500/2191 [06:31<22:00,  1.28it/s, loss=2.31, v_num=647]Epoch 20:  23%|██▎       | 500/2191 [06:31<22:00,  1.28it/s, loss=2.32, v_num=647]Epoch 20:  23%|██▎       | 510/2191 [06:38<21:51,  1.28it/s, loss=2.32, v_num=647]Epoch 20:  23%|██▎       | 510/2191 [06:38<21:51,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  24%|██▎       | 520/2191 [06:47<21:46,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  24%|██▎       | 520/2191 [06:47<21:46,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  24%|██▍       | 530/2191 [06:55<21:38,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  24%|██▍       | 530/2191 [06:55<21:38,  1.28it/s, loss=2.36, v_num=647]Epoch 20:  25%|██▍       | 540/2191 [07:02<21:28,  1.28it/s, loss=2.36, v_num=647]Epoch 20:  25%|██▍       | 540/2191 [07:02<21:28,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  25%|██▌       | 550/2191 [07:09<21:19,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  25%|██▌       | 550/2191 [07:09<21:19,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▌       | 560/2191 [07:16<21:10,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▌       | 560/2191 [07:16<21:10,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▌       | 570/2191 [07:24<21:01,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▌       | 570/2191 [07:24<21:01,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▋       | 580/2191 [07:32<20:53,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  26%|██▋       | 580/2191 [07:32<20:53,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  27%|██▋       | 590/2191 [07:38<20:43,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  27%|██▋       | 590/2191 [07:38<20:43,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  27%|██▋       | 600/2191 [07:47<20:38,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  27%|██▋       | 600/2191 [07:47<20:38,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  28%|██▊       | 610/2191 [07:56<20:32,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  28%|██▊       | 610/2191 [07:56<20:32,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  28%|██▊       | 620/2191 [08:05<20:27,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  28%|██▊       | 620/2191 [08:05<20:27,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  29%|██▉       | 630/2191 [08:13<20:20,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  29%|██▉       | 630/2191 [08:13<20:20,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  29%|██▉       | 640/2191 [08:22<20:14,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  29%|██▉       | 640/2191 [08:22<20:14,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  30%|██▉       | 650/2191 [08:30<20:08,  1.27it/s, loss=2.33, v_num=647]Epoch 20:  30%|██▉       | 650/2191 [08:30<20:08,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  30%|███       | 660/2191 [08:38<20:00,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  30%|███       | 660/2191 [08:38<20:00,  1.27it/s, loss=2.37, v_num=647]Epoch 20:  31%|███       | 670/2191 [08:46<19:52,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  31%|███       | 670/2191 [08:46<19:52,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  31%|███       | 680/2191 [08:55<19:47,  1.27it/s, loss=2.34, v_num=647]Epoch 20:  31%|███       | 680/2191 [08:55<19:47,  1.27it/s, loss=2.3, v_num=647] Epoch 20:  31%|███▏      | 690/2191 [09:02<19:37,  1.27it/s, loss=2.3, v_num=647]Epoch 20:  31%|███▏      | 690/2191 [09:02<19:37,  1.27it/s, loss=2.32, v_num=647]Epoch 20:  32%|███▏      | 700/2191 [09:11<19:33,  1.27it/s, loss=2.32, v_num=647]Epoch 20:  32%|███▏      | 700/2191 [09:11<19:33,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  32%|███▏      | 710/2191 [09:19<19:26,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  32%|███▏      | 710/2191 [09:19<19:26,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  33%|███▎      | 720/2191 [09:26<19:16,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  33%|███▎      | 720/2191 [09:26<19:16,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  33%|███▎      | 730/2191 [09:34<19:08,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  33%|███▎      | 730/2191 [09:34<19:08,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  34%|███▍      | 740/2191 [09:43<19:02,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  34%|███▍      | 740/2191 [09:43<19:02,  1.27it/s, loss=2.35, v_num=647]Epoch 20:  34%|███▍      | 750/2191 [09:50<18:53,  1.27it/s, loss=2.35, v_num=647]Epoch 20:  34%|███▍      | 750/2191 [09:50<18:53,  1.27it/s, loss=2.34, v_num=647]Epoch 20:  35%|███▍      | 760/2191 [09:58<18:45,  1.27it/s, loss=2.34, v_num=647]Epoch 20:  35%|███▍      | 760/2191 [09:58<18:45,  1.27it/s, loss=2.38, v_num=647]Epoch 20:  35%|███▌      | 770/2191 [10:04<18:34,  1.28it/s, loss=2.38, v_num=647]Epoch 20:  35%|███▌      | 770/2191 [10:04<18:34,  1.28it/s, loss=2.36, v_num=647]Epoch 20:  36%|███▌      | 780/2191 [10:13<18:27,  1.27it/s, loss=2.36, v_num=647]Epoch 20:  36%|███▌      | 780/2191 [10:13<18:27,  1.27it/s, loss=2.33, v_num=647]Epoch 20:  36%|███▌      | 790/2191 [10:19<18:17,  1.28it/s, loss=2.33, v_num=647]Epoch 20:  36%|███▌      | 790/2191 [10:19<18:17,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  37%|███▋      | 800/2191 [10:26<18:07,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  37%|███▋      | 800/2191 [10:26<18:07,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  37%|███▋      | 810/2191 [10:33<17:58,  1.28it/s, loss=2.37, v_num=647]Epoch 20:  37%|███▋      | 810/2191 [10:33<17:58,  1.28it/s, loss=2.36, v_num=647]Epoch 20:  37%|███▋      | 820/2191 [10:40<17:48,  1.28it/s, loss=2.36, v_num=647]Epoch 20:  37%|███▋      | 820/2191 [10:40<17:48,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  38%|███▊      | 830/2191 [10:46<17:39,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  38%|███▊      | 830/2191 [10:46<17:39,  1.28it/s, loss=2.39, v_num=647]Epoch 20:  38%|███▊      | 840/2191 [10:53<17:29,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  38%|███▊      | 840/2191 [10:53<17:29,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  39%|███▉      | 850/2191 [11:00<17:20,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  39%|███▉      | 850/2191 [11:00<17:20,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  39%|███▉      | 860/2191 [11:07<17:12,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  39%|███▉      | 860/2191 [11:07<17:12,  1.29it/s, loss=2.38, v_num=647]Epoch 20:  40%|███▉      | 870/2191 [11:18<17:08,  1.28it/s, loss=2.38, v_num=647]Epoch 20:  40%|███▉      | 870/2191 [11:18<17:08,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  40%|████      | 880/2191 [11:26<17:02,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  40%|████      | 880/2191 [11:26<17:02,  1.28it/s, loss=2.29, v_num=647]Epoch 20:  41%|████      | 890/2191 [11:34<16:53,  1.28it/s, loss=2.29, v_num=647]Epoch 20:  41%|████      | 890/2191 [11:34<16:53,  1.28it/s, loss=2.35, v_num=647]Epoch 20:  41%|████      | 900/2191 [11:41<16:44,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  41%|████      | 900/2191 [11:41<16:44,  1.29it/s, loss=2.38, v_num=647]Epoch 20:  42%|████▏     | 910/2191 [11:48<16:36,  1.29it/s, loss=2.38, v_num=647]Epoch 20:  42%|████▏     | 910/2191 [11:48<16:36,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  42%|████▏     | 920/2191 [11:56<16:28,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  42%|████▏     | 920/2191 [11:56<16:28,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  42%|████▏     | 930/2191 [12:03<16:20,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  42%|████▏     | 930/2191 [12:03<16:20,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  43%|████▎     | 940/2191 [12:11<16:13,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  43%|████▎     | 940/2191 [12:11<16:13,  1.29it/s, loss=2.34, v_num=647]Epoch 20:  43%|████▎     | 950/2191 [12:20<16:05,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  43%|████▎     | 950/2191 [12:20<16:05,  1.28it/s, loss=2.34, v_num=647]Epoch 20:  44%|████▍     | 960/2191 [12:26<15:56,  1.29it/s, loss=2.34, v_num=647]Epoch 20:  44%|████▍     | 960/2191 [12:26<15:56,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  44%|████▍     | 970/2191 [12:35<15:49,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  44%|████▍     | 970/2191 [12:35<15:49,  1.29it/s, loss=2.4, v_num=647] Epoch 20:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.4, v_num=647]Epoch 20:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  45%|████▌     | 990/2191 [12:51<15:34,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  45%|████▌     | 990/2191 [12:51<15:34,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  46%|████▌     | 1000/2191 [12:57<15:25,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  46%|████▌     | 1000/2191 [12:57<15:25,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  46%|████▌     | 1010/2191 [13:04<15:16,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  46%|████▌     | 1010/2191 [13:04<15:16,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  47%|████▋     | 1020/2191 [13:11<15:08,  1.29it/s, loss=2.39, v_num=647]Epoch 20:  47%|████▋     | 1020/2191 [13:11<15:08,  1.29it/s, loss=2.34, v_num=647]Epoch 20:  47%|████▋     | 1030/2191 [13:20<15:00,  1.29it/s, loss=2.34, v_num=647]Epoch 20:  47%|████▋     | 1030/2191 [13:20<15:00,  1.29it/s, loss=2.3, v_num=647] Epoch 20:  47%|████▋     | 1040/2191 [13:27<14:52,  1.29it/s, loss=2.3, v_num=647]Epoch 20:  47%|████▋     | 1040/2191 [13:27<14:52,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  48%|████▊     | 1050/2191 [13:35<14:45,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  48%|████▊     | 1050/2191 [13:35<14:45,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  48%|████▊     | 1060/2191 [13:43<14:37,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  48%|████▊     | 1060/2191 [13:43<14:37,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  49%|████▉     | 1070/2191 [13:50<14:29,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  49%|████▉     | 1070/2191 [13:51<14:29,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  49%|████▉     | 1080/2191 [13:57<14:20,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  49%|████▉     | 1080/2191 [13:57<14:20,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  50%|████▉     | 1090/2191 [14:03<14:11,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  50%|████▉     | 1090/2191 [14:03<14:11,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  50%|█████     | 1100/2191 [14:10<14:02,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  50%|█████     | 1100/2191 [14:10<14:02,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  51%|█████     | 1110/2191 [14:18<13:55,  1.29it/s, loss=2.37, v_num=647]Epoch 20:  51%|█████     | 1110/2191 [14:18<13:55,  1.29it/s, loss=2.33, v_num=647]Epoch 20:  51%|█████     | 1120/2191 [14:24<13:46,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  51%|█████     | 1120/2191 [14:24<13:46,  1.30it/s, loss=2.32, v_num=647]Epoch 20:  52%|█████▏    | 1130/2191 [14:32<13:38,  1.30it/s, loss=2.32, v_num=647]Epoch 20:  52%|█████▏    | 1130/2191 [14:32<13:38,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  52%|█████▏    | 1140/2191 [14:40<13:30,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  52%|█████▏    | 1140/2191 [14:40<13:30,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  52%|█████▏    | 1150/2191 [14:48<13:23,  1.29it/s, loss=2.35, v_num=647]Epoch 20:  52%|█████▏    | 1150/2191 [14:48<13:23,  1.29it/s, loss=2.36, v_num=647]Epoch 20:  53%|█████▎    | 1160/2191 [14:55<13:15,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  53%|█████▎    | 1160/2191 [14:55<13:15,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  53%|█████▎    | 1170/2191 [15:02<13:06,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  53%|█████▎    | 1170/2191 [15:02<13:06,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  54%|█████▍    | 1180/2191 [15:09<12:58,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  54%|█████▍    | 1180/2191 [15:09<12:58,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  54%|█████▍    | 1190/2191 [15:18<12:51,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  54%|█████▍    | 1190/2191 [15:18<12:51,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  55%|█████▍    | 1200/2191 [15:24<12:43,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  55%|█████▍    | 1200/2191 [15:24<12:43,  1.30it/s, loss=2.32, v_num=647]Epoch 20:  55%|█████▌    | 1210/2191 [15:32<12:35,  1.30it/s, loss=2.32, v_num=647]Epoch 20:  55%|█████▌    | 1210/2191 [15:32<12:35,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  56%|█████▌    | 1220/2191 [15:38<12:26,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  56%|█████▌    | 1230/2191 [15:46<12:18,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  56%|█████▌    | 1230/2191 [15:46<12:18,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  57%|█████▋    | 1240/2191 [15:54<12:11,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  57%|█████▋    | 1240/2191 [15:54<12:11,  1.30it/s, loss=2.39, v_num=647]Epoch 20:  57%|█████▋    | 1250/2191 [16:02<12:03,  1.30it/s, loss=2.39, v_num=647]Epoch 20:  57%|█████▋    | 1250/2191 [16:02<12:03,  1.30it/s, loss=2.39, v_num=647]Epoch 20:  58%|█████▊    | 1260/2191 [16:08<11:55,  1.30it/s, loss=2.39, v_num=647]Epoch 20:  58%|█████▊    | 1260/2191 [16:08<11:55,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  58%|█████▊    | 1270/2191 [16:17<11:48,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  58%|█████▊    | 1270/2191 [16:17<11:48,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  58%|█████▊    | 1280/2191 [16:25<11:41,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  58%|█████▊    | 1280/2191 [16:25<11:41,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  59%|█████▉    | 1290/2191 [16:33<11:33,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  59%|█████▉    | 1290/2191 [16:33<11:33,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  59%|█████▉    | 1300/2191 [16:40<11:24,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  59%|█████▉    | 1300/2191 [16:40<11:24,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  60%|█████▉    | 1310/2191 [16:48<11:17,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  60%|█████▉    | 1310/2191 [16:48<11:17,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  60%|██████    | 1320/2191 [16:55<11:09,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  60%|██████    | 1320/2191 [16:55<11:09,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  61%|██████    | 1330/2191 [17:02<11:01,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  61%|██████    | 1330/2191 [17:02<11:01,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  61%|██████    | 1340/2191 [17:09<10:53,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  61%|██████    | 1340/2191 [17:09<10:53,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  62%|██████▏   | 1350/2191 [17:16<10:45,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  62%|██████▏   | 1350/2191 [17:16<10:45,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  62%|██████▏   | 1360/2191 [17:23<10:37,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  62%|██████▏   | 1360/2191 [17:23<10:37,  1.30it/s, loss=2.38, v_num=647]Epoch 20:  63%|██████▎   | 1370/2191 [17:30<10:28,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  63%|██████▎   | 1370/2191 [17:30<10:28,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  63%|██████▎   | 1380/2191 [17:37<10:20,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  63%|██████▎   | 1380/2191 [17:37<10:20,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  63%|██████▎   | 1390/2191 [17:45<10:13,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  63%|██████▎   | 1390/2191 [17:45<10:13,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  64%|██████▍   | 1400/2191 [17:52<10:05,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  64%|██████▍   | 1400/2191 [17:52<10:05,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  64%|██████▍   | 1410/2191 [18:02<09:59,  1.30it/s, loss=2.37, v_num=647]Epoch 20:  64%|██████▍   | 1410/2191 [18:02<09:59,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  65%|██████▍   | 1420/2191 [18:08<09:50,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  65%|██████▍   | 1420/2191 [18:08<09:50,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  65%|██████▌   | 1430/2191 [18:18<09:44,  1.30it/s, loss=2.36, v_num=647]Epoch 20:  65%|██████▌   | 1430/2191 [18:18<09:44,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  66%|██████▌   | 1440/2191 [18:25<09:35,  1.30it/s, loss=2.35, v_num=647]Epoch 20:  66%|██████▌   | 1440/2191 [18:25<09:35,  1.30it/s, loss=2.33, v_num=647]Epoch 20:  66%|██████▌   | 1450/2191 [18:31<09:27,  1.31it/s, loss=2.33, v_num=647]Epoch 20:  66%|██████▌   | 1450/2191 [18:31<09:27,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  67%|██████▋   | 1460/2191 [18:38<09:19,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  67%|██████▋   | 1460/2191 [18:38<09:19,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  67%|██████▋   | 1470/2191 [18:45<09:11,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  67%|██████▋   | 1470/2191 [18:45<09:11,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  68%|██████▊   | 1480/2191 [18:52<09:03,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  68%|██████▊   | 1480/2191 [18:52<09:03,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  68%|██████▊   | 1490/2191 [19:00<08:56,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  68%|██████▊   | 1490/2191 [19:00<08:56,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  68%|██████▊   | 1500/2191 [19:07<08:48,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  68%|██████▊   | 1500/2191 [19:07<08:48,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  69%|██████▉   | 1510/2191 [19:14<08:40,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  69%|██████▉   | 1510/2191 [19:14<08:40,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  69%|██████▉   | 1520/2191 [19:22<08:32,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  69%|██████▉   | 1520/2191 [19:22<08:32,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  70%|██████▉   | 1530/2191 [19:30<08:25,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  70%|██████▉   | 1530/2191 [19:30<08:25,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  70%|███████   | 1540/2191 [19:38<08:17,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  70%|███████   | 1540/2191 [19:38<08:17,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  71%|███████   | 1550/2191 [19:47<08:10,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  71%|███████   | 1560/2191 [19:55<08:03,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  71%|███████   | 1560/2191 [19:55<08:03,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  72%|███████▏  | 1570/2191 [20:01<07:55,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  72%|███████▏  | 1570/2191 [20:01<07:55,  1.31it/s, loss=2.39, v_num=647]Epoch 20:  72%|███████▏  | 1580/2191 [20:09<07:47,  1.31it/s, loss=2.39, v_num=647]Epoch 20:  72%|███████▏  | 1580/2191 [20:09<07:47,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  73%|███████▎  | 1590/2191 [20:17<07:40,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  73%|███████▎  | 1590/2191 [20:17<07:40,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  73%|███████▎  | 1600/2191 [20:25<07:32,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  73%|███████▎  | 1600/2191 [20:25<07:32,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  73%|███████▎  | 1610/2191 [20:34<07:25,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  73%|███████▎  | 1610/2191 [20:34<07:25,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  74%|███████▍  | 1620/2191 [20:40<07:17,  1.31it/s, loss=2.41, v_num=647]Epoch 20:  74%|███████▍  | 1620/2191 [20:40<07:17,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  74%|███████▍  | 1630/2191 [20:47<07:09,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  74%|███████▍  | 1630/2191 [20:47<07:09,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  75%|███████▌  | 1650/2191 [21:01<06:53,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  75%|███████▌  | 1650/2191 [21:01<06:53,  1.31it/s, loss=2.39, v_num=647]Epoch 20:  76%|███████▌  | 1660/2191 [21:08<06:45,  1.31it/s, loss=2.39, v_num=647]Epoch 20:  76%|███████▌  | 1660/2191 [21:08<06:45,  1.31it/s, loss=2.4, v_num=647] Epoch 20:  76%|███████▌  | 1670/2191 [21:16<06:38,  1.31it/s, loss=2.4, v_num=647]Epoch 20:  76%|███████▌  | 1670/2191 [21:16<06:38,  1.31it/s, loss=2.4, v_num=647]Epoch 20:  77%|███████▋  | 1680/2191 [21:23<06:30,  1.31it/s, loss=2.4, v_num=647]Epoch 20:  77%|███████▋  | 1680/2191 [21:23<06:30,  1.31it/s, loss=2.4, v_num=647]Epoch 20:  77%|███████▋  | 1690/2191 [21:30<06:22,  1.31it/s, loss=2.4, v_num=647]Epoch 20:  77%|███████▋  | 1690/2191 [21:30<06:22,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  78%|███████▊  | 1700/2191 [21:37<06:14,  1.31it/s, loss=2.37, v_num=647]Epoch 20:  78%|███████▊  | 1700/2191 [21:37<06:14,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  78%|███████▊  | 1710/2191 [21:44<06:06,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  78%|███████▊  | 1710/2191 [21:44<06:06,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  79%|███████▊  | 1720/2191 [21:52<05:59,  1.31it/s, loss=2.32, v_num=647]Epoch 20:  79%|███████▊  | 1720/2191 [21:52<05:59,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  79%|███████▉  | 1730/2191 [21:59<05:51,  1.31it/s, loss=2.34, v_num=647]Epoch 20:  79%|███████▉  | 1730/2191 [21:59<05:51,  1.31it/s, loss=2.33, v_num=647]Epoch 20:  79%|███████▉  | 1740/2191 [22:06<05:43,  1.31it/s, loss=2.33, v_num=647]Epoch 20:  79%|███████▉  | 1740/2191 [22:06<05:43,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  80%|███████▉  | 1750/2191 [22:13<05:35,  1.31it/s, loss=2.35, v_num=647]Epoch 20:  80%|███████▉  | 1750/2191 [22:13<05:35,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  80%|████████  | 1760/2191 [22:20<05:27,  1.31it/s, loss=2.36, v_num=647]Epoch 20:  80%|████████  | 1760/2191 [22:20<05:27,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  81%|████████  | 1770/2191 [22:27<05:20,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  81%|████████  | 1770/2191 [22:27<05:20,  1.31it/s, loss=2.38, v_num=647]Epoch 20:  81%|████████  | 1780/2191 [22:33<05:12,  1.32it/s, loss=2.38, v_num=647]Epoch 20:  81%|████████  | 1780/2191 [22:33<05:12,  1.32it/s, loss=2.35, v_num=647]Epoch 20:  82%|████████▏ | 1790/2191 [22:41<05:04,  1.32it/s, loss=2.35, v_num=647]Epoch 20:  82%|████████▏ | 1790/2191 [22:41<05:04,  1.32it/s, loss=2.35, v_num=647]Epoch 20:  82%|████████▏ | 1800/2191 [22:47<04:56,  1.32it/s, loss=2.35, v_num=647]Epoch 20:  82%|████████▏ | 1800/2191 [22:47<04:56,  1.32it/s, loss=2.34, v_num=647]Epoch 20:  83%|████████▎ | 1810/2191 [22:55<04:49,  1.32it/s, loss=2.34, v_num=647]Epoch 20:  83%|████████▎ | 1810/2191 [22:55<04:49,  1.32it/s, loss=2.4, v_num=647] Epoch 20:  83%|████████▎ | 1820/2191 [23:03<04:41,  1.32it/s, loss=2.4, v_num=647]Epoch 20:  83%|████████▎ | 1820/2191 [23:03<04:41,  1.32it/s, loss=2.39, v_num=647]Epoch 20:  84%|████████▎ | 1830/2191 [23:11<04:34,  1.32it/s, loss=2.39, v_num=647]Epoch 20:  84%|████████▎ | 1830/2191 [23:11<04:34,  1.32it/s, loss=2.38, v_num=647]Epoch 20:  84%|████████▍ | 1840/2191 [23:16<04:26,  1.32it/s, loss=2.38, v_num=647]Epoch 20:  84%|████████▍ | 1840/2191 [23:16<04:26,  1.32it/s, loss=2.4, v_num=647] Epoch 20:  84%|████████▍ | 1850/2191 [23:20<04:18,  1.32it/s, loss=2.4, v_num=647]Epoch 20:  84%|████████▍ | 1850/2191 [23:20<04:18,  1.32it/s, loss=2.37, v_num=647]validation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9750934839248657

validation_epoch_end
graph acc: 0.3865814696485623
valid accuracy: 0.9758305549621582
validation_epoch_end
graph acc: 0.3354632587859425
valid accuracy: 0.9751031994819641
 [23:25<04:01,  1.33it/s, loss=2.33, v_num=647]Epoch 20:  85%|████████▌ | 1870/2191 [23:25<04:01,  1.33it/s, loss=2.34, v_num=647]validation_epoch_end
graph acc: 0.33226837060702874
valid accuracy: 0.9746416211128235
Epoch 20:  86%|████████▌ | 1880/2191 [23:27<03:52,  1.34it/s, loss=2.34, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3610223642172524
valid accuracy: 0.9765214323997498
validation_epoch_end
graph acc: 0.3865814696485623
valid accuracy: 0.9765521287918091

Validating:   3%|▎         | 10/313 [00:01<00:48,  6.23it/s][AEpoch 20:  86%|████████▋ | 1890/2191 [23:28<03:44,  1.34it/s, loss=2.34, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:51,  5.71it/s][AEpoch 20:  87%|████████▋ | 1900/2191 [23:30<03:35,  1.35it/s, loss=2.34, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:35,  7.87it/s][AEpoch 20:  87%|████████▋ | 1910/2191 [23:31<03:27,  1.35it/s, loss=2.34, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:25, 10.50it/s][AEpoch 20:  88%|████████▊ | 1920/2191 [23:31<03:19,  1.36it/s, loss=2.34, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.12it/s][AEpoch 20:  88%|████████▊ | 1930/2191 [23:32<03:10,  1.37it/s, loss=2.34, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:22, 11.40it/s][AEpoch 20:  89%|████████▊ | 1940/2191 [23:33<03:02,  1.37it/s, loss=2.34, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:22, 10.65it/s][AEpoch 20:  89%|████████▉ | 1950/2191 [23:34<02:54,  1.38it/s, loss=2.34, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.24it/s][AEpoch 20:  89%|████████▉ | 1960/2191 [23:35<02:46,  1.39it/s, loss=2.34, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.51it/s][AEpoch 20:  90%|████████▉ | 1970/2191 [23:36<02:38,  1.39it/s, loss=2.34, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.29it/s][AEpoch 20:  90%|█████████ | 1980/2191 [23:37<02:30,  1.40it/s, loss=2.34, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:18, 11.01it/s][AEpoch 20:  91%|█████████ | 1990/2191 [23:38<02:23,  1.40it/s, loss=2.34, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:21,  9.04it/s][AEpoch 20:  91%|█████████▏| 2000/2191 [23:39<02:15,  1.41it/s, loss=2.34, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:20,  9.12it/s][AEpoch 20:  92%|█████████▏| 2010/2191 [23:41<02:07,  1.42it/s, loss=2.34, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  8.17it/s][AEpoch 20:  92%|█████████▏| 2020/2191 [23:42<02:00,  1.42it/s, loss=2.34, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.23it/s][AEpoch 20:  93%|█████████▎| 2030/2191 [23:43<01:52,  1.43it/s, loss=2.34, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:16,  9.36it/s][AEpoch 20:  93%|█████████▎| 2040/2191 [23:44<01:45,  1.43it/s, loss=2.34, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.71it/s][AEpoch 20:  94%|█████████▎| 2050/2191 [23:44<01:37,  1.44it/s, loss=2.34, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.29it/s][AEpoch 20:  94%|█████████▍| 2060/2191 [23:46<01:30,  1.45it/s, loss=2.34, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:09, 12.44it/s][AEpoch 20:  94%|█████████▍| 2070/2191 [23:46<01:23,  1.45it/s, loss=2.34, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.52it/s][AEpoch 20:  95%|█████████▍| 2080/2191 [23:47<01:16,  1.46it/s, loss=2.34, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 10.75it/s][AEpoch 20:  95%|█████████▌| 2090/2191 [23:48<01:09,  1.46it/s, loss=2.34, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.40it/s][AEpoch 20:  96%|█████████▌| 2100/2191 [23:49<01:01,  1.47it/s, loss=2.34, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.92it/s][AEpoch 20:  96%|█████████▋| 2110/2191 [23:50<00:54,  1.48it/s, loss=2.34, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.26it/s][AEpoch 20:  97%|█████████▋| 2120/2191 [23:51<00:47,  1.48it/s, loss=2.34, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06,  9.82it/s][AEpoch 20:  97%|█████████▋| 2130/2191 [23:52<00:41,  1.49it/s, loss=2.34, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.70it/s][AEpoch 20:  98%|█████████▊| 2140/2191 [23:53<00:34,  1.49it/s, loss=2.34, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:04, 10.63it/s][AEpoch 20:  98%|█████████▊| 2150/2191 [23:54<00:27,  1.50it/s, loss=2.34, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 11.50it/s][AEpoch 20:  99%|█████████▊| 2160/2191 [23:55<00:20,  1.51it/s, loss=2.34, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 11.53it/s][AEpoch 20:  99%|█████████▉| 2170/2191 [23:56<00:13,  1.51it/s, loss=2.34, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.64it/s][AEpoch 20:  99%|█████████▉| 2180/2191 [23:56<00:07,  1.52it/s, loss=2.34, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 13.39it/s][AEpoch 20: 100%|█████████▉| 2190/2191 [23:57<00:00,  1.52it/s, loss=2.34, v_num=647]validation_epoch_end
graph acc: 0.4217252396166134
valid accuracy: 0.9779030680656433
Epoch 20: 100%|██████████| 2191/2191 [23:58<00:00,  1.52it/s, loss=2.39, v_num=647]
                                                             [AEpoch 20:   0%|          | 0/2191 [00:00<00:00, 12446.01it/s, loss=2.39, v_num=647]Epoch 21:   0%|          | 0/2191 [00:00<00:00, 2964.17it/s, loss=2.39, v_num=647] Epoch 21:   0%|          | 10/2191 [00:12<42:38,  1.17s/it, loss=2.39, v_num=647] Epoch 21:   0%|          | 10/2191 [00:12<42:38,  1.17s/it, loss=2.34, v_num=647]Epoch 21:   1%|          | 20/2191 [00:21<36:17,  1.00s/it, loss=2.34, v_num=647]Epoch 21:   1%|          | 20/2191 [00:21<36:17,  1.00s/it, loss=2.32, v_num=647]Epoch 21:   1%|▏         | 30/2191 [00:29<34:27,  1.05it/s, loss=2.32, v_num=647]Epoch 21:   1%|▏         | 30/2191 [00:29<34:27,  1.05it/s, loss=2.38, v_num=647]Epoch 21:   2%|▏         | 40/2191 [00:37<32:44,  1.10it/s, loss=2.38, v_num=647]Epoch 21:   2%|▏         | 40/2191 [00:37<32:44,  1.10it/s, loss=2.36, v_num=647]Epoch 21:   2%|▏         | 50/2191 [00:45<31:54,  1.12it/s, loss=2.36, v_num=647]Epoch 21:   2%|▏         | 50/2191 [00:45<31:54,  1.12it/s, loss=2.35, v_num=647]Epoch 21:   3%|▎         | 60/2191 [00:53<31:12,  1.14it/s, loss=2.35, v_num=647]Epoch 21:   3%|▎         | 60/2191 [00:53<31:12,  1.14it/s, loss=2.37, v_num=647]Epoch 21:   3%|▎         | 70/2191 [01:02<30:55,  1.14it/s, loss=2.37, v_num=647]Epoch 21:   3%|▎         | 70/2191 [01:02<30:55,  1.14it/s, loss=2.35, v_num=647]Epoch 21:   4%|▎         | 80/2191 [01:10<30:40,  1.15it/s, loss=2.35, v_num=647]Epoch 21:   4%|▎         | 80/2191 [01:10<30:40,  1.15it/s, loss=2.3, v_num=647] Epoch 21:   4%|▍         | 90/2191 [01:19<30:35,  1.14it/s, loss=2.3, v_num=647]Epoch 21:   4%|▍         | 90/2191 [01:19<30:35,  1.14it/s, loss=2.32, v_num=647]Epoch 21:   5%|▍         | 100/2191 [01:27<30:07,  1.16it/s, loss=2.32, v_num=647]Epoch 21:   5%|▍         | 100/2191 [01:27<30:07,  1.16it/s, loss=2.32, v_num=647]Epoch 21:   5%|▌         | 110/2191 [01:36<29:59,  1.16it/s, loss=2.32, v_num=647]Epoch 21:   5%|▌         | 110/2191 [01:36<29:59,  1.16it/s, loss=2.35, v_num=647]Epoch 21:   5%|▌         | 120/2191 [01:44<29:45,  1.16it/s, loss=2.35, v_num=647]Epoch 21:   5%|▌         | 120/2191 [01:44<29:45,  1.16it/s, loss=2.37, v_num=647]Epoch 21:   6%|▌         | 130/2191 [01:52<29:35,  1.16it/s, loss=2.37, v_num=647]Epoch 21:   6%|▌         | 130/2191 [01:52<29:35,  1.16it/s, loss=2.31, v_num=647]Epoch 21:   6%|▋         | 140/2191 [02:00<29:09,  1.17it/s, loss=2.31, v_num=647]Epoch 21:   6%|▋         | 140/2191 [02:00<29:09,  1.17it/s, loss=2.31, v_num=647]Epoch 21:   7%|▋         | 150/2191 [02:08<28:56,  1.18it/s, loss=2.31, v_num=647]Epoch 21:   7%|▋         | 150/2191 [02:08<28:56,  1.18it/s, loss=2.33, v_num=647]Epoch 21:   7%|▋         | 160/2191 [02:16<28:37,  1.18it/s, loss=2.33, v_num=647]Epoch 21:   7%|▋         | 160/2191 [02:16<28:37,  1.18it/s, loss=2.36, v_num=647]Epoch 21:   8%|▊         | 170/2191 [02:24<28:24,  1.19it/s, loss=2.36, v_num=647]Epoch 21:   8%|▊         | 170/2191 [02:24<28:24,  1.19it/s, loss=2.36, v_num=647]Epoch 21:   8%|▊         | 180/2191 [02:31<28:05,  1.19it/s, loss=2.36, v_num=647]Epoch 21:   8%|▊         | 180/2191 [02:31<28:05,  1.19it/s, loss=2.31, v_num=647]Epoch 21:   9%|▊         | 190/2191 [02:42<28:17,  1.18it/s, loss=2.31, v_num=647]Epoch 21:   9%|▊         | 190/2191 [02:42<28:17,  1.18it/s, loss=2.27, v_num=647]Epoch 21:   9%|▉         | 200/2191 [02:49<27:59,  1.19it/s, loss=2.27, v_num=647]Epoch 21:   9%|▉         | 200/2191 [02:49<27:59,  1.19it/s, loss=2.28, v_num=647]Epoch 21:  10%|▉         | 210/2191 [02:57<27:49,  1.19it/s, loss=2.28, v_num=647]Epoch 21:  10%|▉         | 210/2191 [02:57<27:49,  1.19it/s, loss=2.32, v_num=647]Epoch 21:  10%|█         | 220/2191 [03:05<27:37,  1.19it/s, loss=2.32, v_num=647]Epoch 21:  10%|█         | 220/2191 [03:05<27:37,  1.19it/s, loss=2.32, v_num=647]Epoch 21:  10%|█         | 230/2191 [03:13<27:25,  1.19it/s, loss=2.32, v_num=647]Epoch 21:  10%|█         | 230/2191 [03:13<27:25,  1.19it/s, loss=2.34, v_num=647]Epoch 21:  11%|█         | 240/2191 [03:21<27:11,  1.20it/s, loss=2.34, v_num=647]Epoch 21:  11%|█         | 240/2191 [03:21<27:11,  1.20it/s, loss=2.35, v_num=647]Epoch 21:  11%|█▏        | 250/2191 [03:29<26:57,  1.20it/s, loss=2.35, v_num=647]Epoch 21:  11%|█▏        | 250/2191 [03:29<26:57,  1.20it/s, loss=2.32, v_num=647]Epoch 21:  12%|█▏        | 260/2191 [03:36<26:44,  1.20it/s, loss=2.32, v_num=647]Epoch 21:  12%|█▏        | 260/2191 [03:36<26:44,  1.20it/s, loss=2.29, v_num=647]Epoch 21:  12%|█▏        | 270/2191 [03:44<26:34,  1.20it/s, loss=2.29, v_num=647]Epoch 21:  12%|█▏        | 270/2191 [03:44<26:34,  1.20it/s, loss=2.32, v_num=647]Epoch 21:  13%|█▎        | 280/2191 [03:52<26:18,  1.21it/s, loss=2.32, v_num=647]Epoch 21:  13%|█▎        | 280/2191 [03:52<26:18,  1.21it/s, loss=2.36, v_num=647]Epoch 21:  13%|█▎        | 290/2191 [03:58<26:00,  1.22it/s, loss=2.36, v_num=647]Epoch 21:  13%|█▎        | 290/2191 [03:58<26:00,  1.22it/s, loss=2.33, v_num=647]Epoch 21:  14%|█▎        | 300/2191 [04:06<25:50,  1.22it/s, loss=2.33, v_num=647]Epoch 21:  14%|█▎        | 300/2191 [04:06<25:50,  1.22it/s, loss=2.32, v_num=647]Epoch 21:  14%|█▍        | 310/2191 [04:14<25:36,  1.22it/s, loss=2.32, v_num=647]Epoch 21:  14%|█▍        | 310/2191 [04:14<25:36,  1.22it/s, loss=2.34, v_num=647]Epoch 21:  15%|█▍        | 320/2191 [04:23<25:33,  1.22it/s, loss=2.34, v_num=647]Epoch 21:  15%|█▍        | 320/2191 [04:23<25:33,  1.22it/s, loss=2.34, v_num=647]Epoch 21:  15%|█▌        | 330/2191 [04:30<25:21,  1.22it/s, loss=2.34, v_num=647]Epoch 21:  15%|█▌        | 330/2191 [04:30<25:21,  1.22it/s, loss=2.32, v_num=647]Epoch 21:  16%|█▌        | 340/2191 [04:37<25:07,  1.23it/s, loss=2.32, v_num=647]Epoch 21:  16%|█▌        | 340/2191 [04:37<25:07,  1.23it/s, loss=2.31, v_num=647]Epoch 21:  16%|█▌        | 350/2191 [04:45<24:55,  1.23it/s, loss=2.31, v_num=647]Epoch 21:  16%|█▌        | 350/2191 [04:45<24:55,  1.23it/s, loss=2.35, v_num=647]Epoch 21:  16%|█▋        | 360/2191 [04:52<24:41,  1.24it/s, loss=2.35, v_num=647]Epoch 21:  16%|█▋        | 360/2191 [04:52<24:41,  1.24it/s, loss=2.36, v_num=647]Epoch 21:  17%|█▋        | 370/2191 [05:01<24:38,  1.23it/s, loss=2.36, v_num=647]Epoch 21:  17%|█▋        | 370/2191 [05:01<24:38,  1.23it/s, loss=2.35, v_num=647]Epoch 21:  17%|█▋        | 380/2191 [05:08<24:27,  1.23it/s, loss=2.35, v_num=647]Epoch 21:  17%|█▋        | 380/2191 [05:08<24:27,  1.23it/s, loss=2.34, v_num=647]Epoch 21:  18%|█▊        | 390/2191 [05:17<24:23,  1.23it/s, loss=2.34, v_num=647]Epoch 21:  18%|█▊        | 390/2191 [05:17<24:23,  1.23it/s, loss=2.29, v_num=647]Epoch 21:  18%|█▊        | 400/2191 [05:26<24:17,  1.23it/s, loss=2.29, v_num=647]Epoch 21:  18%|█▊        | 400/2191 [05:26<24:17,  1.23it/s, loss=2.3, v_num=647] Epoch 21:  19%|█▊        | 410/2191 [05:32<24:02,  1.23it/s, loss=2.3, v_num=647]Epoch 21:  19%|█▊        | 410/2191 [05:32<24:02,  1.23it/s, loss=2.32, v_num=647]Epoch 21:  19%|█▉        | 420/2191 [05:40<23:51,  1.24it/s, loss=2.32, v_num=647]Epoch 21:  19%|█▉        | 420/2191 [05:40<23:51,  1.24it/s, loss=2.34, v_num=647]Epoch 21:  20%|█▉        | 430/2191 [05:48<23:43,  1.24it/s, loss=2.34, v_num=647]Epoch 21:  20%|█▉        | 430/2191 [05:48<23:43,  1.24it/s, loss=2.36, v_num=647]Epoch 21:  20%|██        | 440/2191 [05:55<23:32,  1.24it/s, loss=2.36, v_num=647]Epoch 21:  20%|██        | 440/2191 [05:55<23:32,  1.24it/s, loss=2.35, v_num=647]Epoch 21:  21%|██        | 450/2191 [06:02<23:21,  1.24it/s, loss=2.35, v_num=647]Epoch 21:  21%|██        | 450/2191 [06:02<23:21,  1.24it/s, loss=2.32, v_num=647]Epoch 21:  21%|██        | 460/2191 [06:11<23:15,  1.24it/s, loss=2.32, v_num=647]Epoch 21:  21%|██        | 460/2191 [06:11<23:15,  1.24it/s, loss=2.35, v_num=647]Epoch 21:  21%|██▏       | 470/2191 [06:19<23:06,  1.24it/s, loss=2.35, v_num=647]Epoch 21:  21%|██▏       | 470/2191 [06:19<23:06,  1.24it/s, loss=2.37, v_num=647]Epoch 21:  22%|██▏       | 480/2191 [06:26<22:53,  1.25it/s, loss=2.37, v_num=647]Epoch 21:  22%|██▏       | 480/2191 [06:26<22:53,  1.25it/s, loss=2.37, v_num=647]Epoch 21:  22%|██▏       | 490/2191 [06:33<22:41,  1.25it/s, loss=2.37, v_num=647]Epoch 21:  22%|██▏       | 490/2191 [06:33<22:41,  1.25it/s, loss=2.38, v_num=647]Epoch 21:  23%|██▎       | 500/2191 [06:40<22:31,  1.25it/s, loss=2.38, v_num=647]Epoch 21:  23%|██▎       | 500/2191 [06:40<22:31,  1.25it/s, loss=2.37, v_num=647]Epoch 21:  23%|██▎       | 510/2191 [06:47<22:20,  1.25it/s, loss=2.37, v_num=647]Epoch 21:  23%|██▎       | 510/2191 [06:47<22:20,  1.25it/s, loss=2.33, v_num=647]Epoch 21:  24%|██▎       | 520/2191 [06:54<22:10,  1.26it/s, loss=2.33, v_num=647]Epoch 21:  24%|██▎       | 520/2191 [06:54<22:10,  1.26it/s, loss=2.35, v_num=647]Epoch 21:  24%|██▍       | 530/2191 [07:03<22:03,  1.25it/s, loss=2.35, v_num=647]Epoch 21:  24%|██▍       | 530/2191 [07:03<22:03,  1.25it/s, loss=2.35, v_num=647]Epoch 21:  25%|██▍       | 540/2191 [07:09<21:51,  1.26it/s, loss=2.35, v_num=647]Epoch 21:  25%|██▍       | 540/2191 [07:09<21:51,  1.26it/s, loss=2.33, v_num=647]Epoch 21:  25%|██▌       | 550/2191 [07:16<21:41,  1.26it/s, loss=2.33, v_num=647]Epoch 21:  25%|██▌       | 550/2191 [07:16<21:41,  1.26it/s, loss=2.35, v_num=647]Epoch 21:  26%|██▌       | 560/2191 [07:23<21:29,  1.26it/s, loss=2.35, v_num=647]Epoch 21:  26%|██▌       | 560/2191 [07:23<21:29,  1.26it/s, loss=2.36, v_num=647]Epoch 21:  26%|██▌       | 570/2191 [07:30<21:18,  1.27it/s, loss=2.36, v_num=647]Epoch 21:  26%|██▌       | 570/2191 [07:30<21:18,  1.27it/s, loss=2.37, v_num=647]Epoch 21:  26%|██▋       | 580/2191 [07:37<21:09,  1.27it/s, loss=2.37, v_num=647]Epoch 21:  26%|██▋       | 580/2191 [07:37<21:09,  1.27it/s, loss=2.35, v_num=647]Epoch 21:  27%|██▋       | 590/2191 [07:45<21:00,  1.27it/s, loss=2.35, v_num=647]Epoch 21:  27%|██▋       | 590/2191 [07:45<21:00,  1.27it/s, loss=2.33, v_num=647]Epoch 21:  27%|██▋       | 600/2191 [07:52<20:51,  1.27it/s, loss=2.33, v_num=647]Epoch 21:  27%|██▋       | 600/2191 [07:52<20:51,  1.27it/s, loss=2.31, v_num=647]Epoch 21:  28%|██▊       | 610/2191 [08:00<20:43,  1.27it/s, loss=2.31, v_num=647]Epoch 21:  28%|██▊       | 610/2191 [08:00<20:43,  1.27it/s, loss=2.33, v_num=647]Epoch 21:  28%|██▊       | 620/2191 [08:07<20:33,  1.27it/s, loss=2.33, v_num=647]Epoch 21:  28%|██▊       | 620/2191 [08:07<20:33,  1.27it/s, loss=2.35, v_num=647]Epoch 21:  29%|██▉       | 630/2191 [08:16<20:27,  1.27it/s, loss=2.35, v_num=647]Epoch 21:  29%|██▉       | 630/2191 [08:16<20:27,  1.27it/s, loss=2.32, v_num=647]Epoch 21:  29%|██▉       | 640/2191 [08:22<20:16,  1.27it/s, loss=2.32, v_num=647]Epoch 21:  29%|██▉       | 640/2191 [08:22<20:16,  1.27it/s, loss=2.33, v_num=647]Epoch 21:  30%|██▉       | 650/2191 [08:30<20:07,  1.28it/s, loss=2.33, v_num=647]Epoch 21:  30%|██▉       | 650/2191 [08:30<20:07,  1.28it/s, loss=2.4, v_num=647] Epoch 21:  30%|███       | 660/2191 [08:39<20:03,  1.27it/s, loss=2.4, v_num=647]Epoch 21:  30%|███       | 660/2191 [08:39<20:03,  1.27it/s, loss=2.41, v_num=647]Epoch 21:  31%|███       | 670/2191 [08:46<19:53,  1.27it/s, loss=2.41, v_num=647]Epoch 21:  31%|███       | 670/2191 [08:46<19:53,  1.27it/s, loss=2.36, v_num=647]Epoch 21:  31%|███       | 680/2191 [08:55<19:47,  1.27it/s, loss=2.36, v_num=647]Epoch 21:  31%|███       | 680/2191 [08:55<19:47,  1.27it/s, loss=2.4, v_num=647] Epoch 21:  31%|███▏      | 690/2191 [09:03<19:39,  1.27it/s, loss=2.4, v_num=647]Epoch 21:  31%|███▏      | 690/2191 [09:03<19:39,  1.27it/s, loss=2.38, v_num=647]Epoch 21:  32%|███▏      | 700/2191 [09:09<19:29,  1.27it/s, loss=2.38, v_num=647]Epoch 21:  32%|███▏      | 700/2191 [09:09<19:29,  1.27it/s, loss=2.35, v_num=647]Epoch 21:  32%|███▏      | 710/2191 [09:17<19:21,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  32%|███▏      | 710/2191 [09:17<19:21,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  33%|███▎      | 720/2191 [09:24<19:12,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  33%|███▎      | 720/2191 [09:24<19:12,  1.28it/s, loss=2.34, v_num=647]Epoch 21:  33%|███▎      | 730/2191 [09:32<19:05,  1.28it/s, loss=2.34, v_num=647]Epoch 21:  33%|███▎      | 730/2191 [09:32<19:05,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  34%|███▍      | 740/2191 [09:40<18:57,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  34%|███▍      | 740/2191 [09:40<18:57,  1.28it/s, loss=2.41, v_num=647]Epoch 21:  34%|███▍      | 750/2191 [09:48<18:48,  1.28it/s, loss=2.41, v_num=647]Epoch 21:  34%|███▍      | 750/2191 [09:48<18:48,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  35%|███▍      | 760/2191 [09:55<18:39,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  35%|███▍      | 760/2191 [09:55<18:39,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  35%|███▌      | 770/2191 [10:02<18:29,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  35%|███▌      | 770/2191 [10:02<18:29,  1.28it/s, loss=2.39, v_num=647]Epoch 21:  36%|███▌      | 780/2191 [10:10<18:23,  1.28it/s, loss=2.39, v_num=647]Epoch 21:  36%|███▌      | 780/2191 [10:10<18:23,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  36%|███▌      | 790/2191 [10:17<18:13,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  36%|███▌      | 790/2191 [10:17<18:13,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  37%|███▋      | 800/2191 [10:24<18:04,  1.28it/s, loss=2.35, v_num=647]Epoch 21:  37%|███▋      | 800/2191 [10:24<18:04,  1.28it/s, loss=2.33, v_num=647]Epoch 21:  37%|███▋      | 810/2191 [10:34<18:00,  1.28it/s, loss=2.33, v_num=647]Epoch 21:  37%|███▋      | 810/2191 [10:34<18:00,  1.28it/s, loss=2.32, v_num=647]Epoch 21:  37%|███▋      | 820/2191 [10:40<17:50,  1.28it/s, loss=2.32, v_num=647]Epoch 21:  37%|███▋      | 820/2191 [10:40<17:50,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  38%|███▊      | 830/2191 [10:49<17:43,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  38%|███▊      | 830/2191 [10:49<17:43,  1.28it/s, loss=2.37, v_num=647]Epoch 21:  38%|███▊      | 840/2191 [10:55<17:33,  1.28it/s, loss=2.37, v_num=647]Epoch 21:  38%|███▊      | 840/2191 [10:55<17:33,  1.28it/s, loss=2.32, v_num=647]Epoch 21:  39%|███▉      | 850/2191 [11:02<17:24,  1.28it/s, loss=2.32, v_num=647]Epoch 21:  39%|███▉      | 850/2191 [11:02<17:24,  1.28it/s, loss=2.31, v_num=647]Epoch 21:  39%|███▉      | 860/2191 [11:10<17:15,  1.28it/s, loss=2.31, v_num=647]Epoch 21:  39%|███▉      | 860/2191 [11:10<17:15,  1.28it/s, loss=2.36, v_num=647]Epoch 21:  40%|███▉      | 870/2191 [11:17<17:07,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  40%|███▉      | 870/2191 [11:17<17:07,  1.29it/s, loss=2.4, v_num=647] Epoch 21:  40%|████      | 880/2191 [11:25<16:59,  1.29it/s, loss=2.4, v_num=647]Epoch 21:  40%|████      | 880/2191 [11:25<16:59,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  41%|████      | 890/2191 [11:32<16:51,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  41%|████      | 890/2191 [11:32<16:51,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  41%|████      | 900/2191 [11:41<16:45,  1.28it/s, loss=2.34, v_num=647]Epoch 21:  41%|████      | 900/2191 [11:41<16:45,  1.28it/s, loss=2.39, v_num=647]Epoch 21:  42%|████▏     | 910/2191 [11:48<16:36,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  42%|████▏     | 910/2191 [11:48<16:36,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  42%|████▏     | 920/2191 [11:56<16:29,  1.28it/s, loss=2.39, v_num=647]Epoch 21:  42%|████▏     | 920/2191 [11:56<16:29,  1.28it/s, loss=2.38, v_num=647]Epoch 21:  42%|████▏     | 930/2191 [12:04<16:20,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  42%|████▏     | 930/2191 [12:04<16:20,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  43%|████▎     | 940/2191 [12:11<16:12,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  43%|████▎     | 940/2191 [12:11<16:12,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  43%|████▎     | 950/2191 [12:19<16:05,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  43%|████▎     | 950/2191 [12:19<16:05,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  44%|████▍     | 960/2191 [12:26<15:56,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  44%|████▍     | 960/2191 [12:26<15:56,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  44%|████▍     | 970/2191 [12:34<15:49,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  44%|████▍     | 970/2191 [12:34<15:49,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  45%|████▍     | 980/2191 [12:42<15:41,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  45%|████▌     | 990/2191 [12:50<15:33,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  45%|████▌     | 990/2191 [12:50<15:33,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  46%|████▌     | 1000/2191 [12:58<15:25,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  46%|████▌     | 1000/2191 [12:58<15:25,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  46%|████▌     | 1010/2191 [13:05<15:17,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  46%|████▌     | 1010/2191 [13:05<15:17,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  47%|████▋     | 1020/2191 [13:13<15:09,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  47%|████▋     | 1020/2191 [13:13<15:09,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  47%|████▋     | 1030/2191 [13:20<15:01,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  47%|████▋     | 1030/2191 [13:20<15:01,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  47%|████▋     | 1040/2191 [13:28<14:53,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  47%|████▋     | 1040/2191 [13:28<14:53,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  48%|████▊     | 1050/2191 [13:35<14:44,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  48%|████▊     | 1050/2191 [13:35<14:44,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  48%|████▊     | 1060/2191 [13:42<14:36,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  48%|████▊     | 1060/2191 [13:42<14:36,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  49%|████▉     | 1070/2191 [13:49<14:28,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  49%|████▉     | 1070/2191 [13:49<14:28,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  49%|████▉     | 1080/2191 [13:58<14:21,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  49%|████▉     | 1080/2191 [13:58<14:21,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  50%|████▉     | 1090/2191 [14:05<14:13,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  50%|████▉     | 1090/2191 [14:05<14:13,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  50%|█████     | 1100/2191 [14:14<14:06,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  50%|█████     | 1100/2191 [14:14<14:06,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  51%|█████     | 1110/2191 [14:22<13:58,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  51%|█████     | 1110/2191 [14:22<13:58,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  51%|█████     | 1120/2191 [14:29<13:50,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  51%|█████     | 1120/2191 [14:29<13:50,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  52%|█████▏    | 1130/2191 [14:36<13:42,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  52%|█████▏    | 1130/2191 [14:36<13:42,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  52%|█████▏    | 1140/2191 [14:44<13:34,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  52%|█████▏    | 1140/2191 [14:44<13:34,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  52%|█████▏    | 1150/2191 [14:52<13:27,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  52%|█████▏    | 1150/2191 [14:52<13:27,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  53%|█████▎    | 1160/2191 [14:58<13:18,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  53%|█████▎    | 1160/2191 [14:58<13:18,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  53%|█████▎    | 1170/2191 [15:06<13:10,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  53%|█████▎    | 1170/2191 [15:06<13:10,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  54%|█████▍    | 1180/2191 [15:13<13:01,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  54%|█████▍    | 1180/2191 [15:13<13:01,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  54%|█████▍    | 1190/2191 [15:20<12:54,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  54%|█████▍    | 1190/2191 [15:20<12:54,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  55%|█████▍    | 1200/2191 [15:28<12:46,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  55%|█████▍    | 1200/2191 [15:28<12:46,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  55%|█████▌    | 1210/2191 [15:36<12:38,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  55%|█████▌    | 1210/2191 [15:36<12:38,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  56%|█████▌    | 1220/2191 [15:44<12:30,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  56%|█████▌    | 1220/2191 [15:44<12:30,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  56%|█████▌    | 1230/2191 [15:51<12:22,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  56%|█████▌    | 1230/2191 [15:51<12:22,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  57%|█████▋    | 1240/2191 [15:59<12:15,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  57%|█████▋    | 1240/2191 [15:59<12:15,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  57%|█████▋    | 1250/2191 [16:07<12:07,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  57%|█████▋    | 1250/2191 [16:07<12:07,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  58%|█████▊    | 1260/2191 [16:14<11:59,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  58%|█████▊    | 1260/2191 [16:14<11:59,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  58%|█████▊    | 1270/2191 [16:22<11:52,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  58%|█████▊    | 1270/2191 [16:22<11:52,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  58%|█████▊    | 1280/2191 [16:30<11:44,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  58%|█████▊    | 1280/2191 [16:30<11:44,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  59%|█████▉    | 1290/2191 [16:38<11:36,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  59%|█████▉    | 1290/2191 [16:38<11:36,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  59%|█████▉    | 1300/2191 [16:45<11:28,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  59%|█████▉    | 1300/2191 [16:45<11:28,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  60%|█████▉    | 1310/2191 [16:54<11:21,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  60%|█████▉    | 1310/2191 [16:54<11:21,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  60%|██████    | 1320/2191 [17:02<11:14,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  60%|██████    | 1320/2191 [17:02<11:14,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  61%|██████    | 1330/2191 [17:09<11:05,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  61%|██████    | 1330/2191 [17:09<11:05,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  61%|██████    | 1340/2191 [17:16<10:57,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  61%|██████    | 1340/2191 [17:16<10:57,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  62%|██████▏   | 1350/2191 [17:24<10:50,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  62%|██████▏   | 1350/2191 [17:24<10:50,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  62%|██████▏   | 1360/2191 [17:32<10:42,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  62%|██████▏   | 1360/2191 [17:32<10:42,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  63%|██████▎   | 1370/2191 [17:39<10:34,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  63%|██████▎   | 1370/2191 [17:39<10:34,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  63%|██████▎   | 1380/2191 [17:48<10:27,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  63%|██████▎   | 1380/2191 [17:48<10:27,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  63%|██████▎   | 1390/2191 [17:56<10:20,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  63%|██████▎   | 1390/2191 [17:56<10:20,  1.29it/s, loss=2.4, v_num=647] Epoch 21:  64%|██████▍   | 1400/2191 [18:03<10:11,  1.29it/s, loss=2.4, v_num=647]Epoch 21:  64%|██████▍   | 1400/2191 [18:03<10:11,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  64%|██████▍   | 1410/2191 [18:11<10:03,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  64%|██████▍   | 1410/2191 [18:11<10:03,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  65%|██████▍   | 1420/2191 [18:20<09:56,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  65%|██████▍   | 1420/2191 [18:20<09:56,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  65%|██████▌   | 1430/2191 [18:29<09:49,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  65%|██████▌   | 1430/2191 [18:29<09:49,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  66%|██████▌   | 1440/2191 [18:35<09:41,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  66%|██████▌   | 1440/2191 [18:35<09:41,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  66%|██████▌   | 1450/2191 [18:44<09:34,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  66%|██████▌   | 1450/2191 [18:44<09:34,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  67%|██████▋   | 1460/2191 [18:51<09:26,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  67%|██████▋   | 1460/2191 [18:51<09:26,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  67%|██████▋   | 1470/2191 [18:59<09:18,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  67%|██████▋   | 1470/2191 [18:59<09:18,  1.29it/s, loss=2.42, v_num=647]Epoch 21:  68%|██████▊   | 1480/2191 [19:07<09:10,  1.29it/s, loss=2.42, v_num=647]Epoch 21:  68%|██████▊   | 1480/2191 [19:07<09:10,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  68%|██████▊   | 1490/2191 [19:14<09:02,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  68%|██████▊   | 1490/2191 [19:14<09:02,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  68%|██████▊   | 1500/2191 [19:21<08:54,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  68%|██████▊   | 1500/2191 [19:21<08:54,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  69%|██████▉   | 1510/2191 [19:29<08:47,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  69%|██████▉   | 1510/2191 [19:29<08:47,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  69%|██████▉   | 1520/2191 [19:37<08:39,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  69%|██████▉   | 1520/2191 [19:37<08:39,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  70%|██████▉   | 1530/2191 [19:45<08:31,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  70%|██████▉   | 1530/2191 [19:45<08:31,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  70%|███████   | 1540/2191 [19:52<08:23,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  70%|███████   | 1540/2191 [19:52<08:23,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  71%|███████   | 1550/2191 [19:59<08:15,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  71%|███████   | 1550/2191 [19:59<08:15,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  71%|███████   | 1560/2191 [20:06<08:07,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  71%|███████   | 1560/2191 [20:06<08:07,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  72%|███████▏  | 1570/2191 [20:14<07:59,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  72%|███████▏  | 1570/2191 [20:14<07:59,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  72%|███████▏  | 1580/2191 [20:22<07:52,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  72%|███████▏  | 1580/2191 [20:22<07:52,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  73%|███████▎  | 1590/2191 [20:30<07:44,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  73%|███████▎  | 1590/2191 [20:30<07:44,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  73%|███████▎  | 1600/2191 [20:37<07:36,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  73%|███████▎  | 1600/2191 [20:37<07:36,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  73%|███████▎  | 1610/2191 [20:44<07:28,  1.29it/s, loss=2.31, v_num=647]Epoch 21:  73%|███████▎  | 1610/2191 [20:44<07:28,  1.29it/s, loss=2.28, v_num=647]Epoch 21:  74%|███████▍  | 1620/2191 [20:53<07:21,  1.29it/s, loss=2.28, v_num=647]Epoch 21:  74%|███████▍  | 1620/2191 [20:53<07:21,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  74%|███████▍  | 1630/2191 [21:00<07:13,  1.29it/s, loss=2.32, v_num=647]Epoch 21:  74%|███████▍  | 1630/2191 [21:00<07:13,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  75%|███████▍  | 1640/2191 [21:08<07:05,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  75%|███████▍  | 1640/2191 [21:08<07:05,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  75%|███████▌  | 1650/2191 [21:16<06:58,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  75%|███████▌  | 1650/2191 [21:16<06:58,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  76%|███████▌  | 1660/2191 [21:24<06:50,  1.29it/s, loss=2.35, v_num=647]Epoch 21:  76%|███████▌  | 1660/2191 [21:24<06:50,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  76%|███████▌  | 1670/2191 [21:32<06:42,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  76%|███████▌  | 1670/2191 [21:32<06:42,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  77%|███████▋  | 1680/2191 [21:42<06:35,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  77%|███████▋  | 1680/2191 [21:42<06:35,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  77%|███████▋  | 1690/2191 [21:49<06:27,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  77%|███████▋  | 1690/2191 [21:49<06:27,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  78%|███████▊  | 1700/2191 [21:57<06:20,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  78%|███████▊  | 1700/2191 [21:57<06:20,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  78%|███████▊  | 1710/2191 [22:04<06:12,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  78%|███████▊  | 1710/2191 [22:04<06:12,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  79%|███████▊  | 1720/2191 [22:11<06:04,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  79%|███████▊  | 1720/2191 [22:11<06:04,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  79%|███████▉  | 1730/2191 [22:19<05:56,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  79%|███████▉  | 1730/2191 [22:19<05:56,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  79%|███████▉  | 1740/2191 [22:27<05:48,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  79%|███████▉  | 1740/2191 [22:27<05:48,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  80%|███████▉  | 1750/2191 [22:36<05:41,  1.29it/s, loss=2.38, v_num=647]Epoch 21:  80%|███████▉  | 1750/2191 [22:36<05:41,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  80%|████████  | 1760/2191 [22:42<05:33,  1.29it/s, loss=2.34, v_num=647]Epoch 21:  80%|████████  | 1760/2191 [22:42<05:33,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  81%|████████  | 1770/2191 [22:50<05:25,  1.29it/s, loss=2.33, v_num=647]Epoch 21:  81%|████████  | 1770/2191 [22:50<05:25,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  81%|████████  | 1780/2191 [22:57<05:17,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  81%|████████  | 1780/2191 [22:57<05:17,  1.29it/s, loss=2.4, v_num=647] Epoch 21:  82%|████████▏ | 1790/2191 [23:04<05:09,  1.29it/s, loss=2.4, v_num=647]Epoch 21:  82%|████████▏ | 1790/2191 [23:04<05:09,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  82%|████████▏ | 1800/2191 [23:11<05:02,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  82%|████████▏ | 1800/2191 [23:11<05:02,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  83%|████████▎ | 1810/2191 [23:19<04:54,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  83%|████████▎ | 1810/2191 [23:19<04:54,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  83%|████████▎ | 1820/2191 [23:28<04:46,  1.29it/s, loss=2.37, v_num=647]Epoch 21:  83%|████████▎ | 1820/2191 [23:28<04:46,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  84%|████████▎ | 1830/2191 [23:34<04:38,  1.29it/s, loss=2.39, v_num=647]Epoch 21:  84%|████████▎ | 1830/2191 [23:34<04:38,  1.29it/s, loss=2.36, v_num=647]Epoch 21:  84%|████████▍ | 1840/2191 [23:39<04:30,  1.30it/s, loss=2.36, v_num=647]Epoch 21:  84%|████████▍ | 1840/2191 [23:39<04:30,  1.30it/s, loss=2.37, v_num=647]Epoch 21:  84%|████████▍ | 1850/2191 [23:42<04:22,  1.30it/s, loss=2.37, v_num=647]Epoch 21:  84%|████████▍ | 1850/2191 [23:42<04:22,  1.30it/s, loss=2.39, v_num=647]Epoch 21:  85%|████████▍ | 1860/2191 [23:45<04:13,  1.31it/s, loss=2.39, v_num=647]Epoch 21:  85%|████████▍ | 1860/2191 [23:45<04:13,  1.31it/s, loss=2.41, v_num=647]Epoch 21:  85%|████████▌ | 1870/2191 [23:47<04:04,  1.31it/s, loss=2.41, v_num=647]Epoch 21:  85%|████████▌ | 1870/2191 [23:47<04:04,  1.31it/s, loss=2.39, v_num=647]validation_epoch_end
graph acc: 0.3546325878594249
valid accuracy: 0.9752966165542603
validation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9752674698829651
validation_epoch_end
graph acc: 0.36741214057507987
valid accuracy: 0.975735068321228
=2.39, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.36421725239616615
valid accuracy: 0.9780839085578918

Validating:   3%|▎         | 10/313 [00:01<00:48,  6.28it/s][AEpoch 21:  86%|████████▋ | 1890/2191 [23:50<03:47,  1.32it/s, loss=2.39, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:47,  6.17it/s][AEpoch 21:  87%|████████▋ | 1900/2191 [23:51<03:39,  1.33it/s, loss=2.39, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:36,  7.65it/s][AEpoch 21:  87%|████████▋ | 1910/2191 [23:52<03:30,  1.33it/s, loss=2.39, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:26, 10.18it/s][AEpoch 21:  88%|████████▊ | 1920/2191 [23:53<03:22,  1.34it/s, loss=2.39, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:24, 10.52it/s][AEpoch 21:  88%|████████▊ | 1930/2191 [23:54<03:13,  1.35it/s, loss=2.39, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.55it/s][AEpoch 21:  89%|████████▊ | 1940/2191 [23:55<03:05,  1.35it/s, loss=2.39, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:23, 10.23it/s][AEpoch 21:  89%|████████▉ | 1950/2191 [23:56<02:57,  1.36it/s, loss=2.39, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:23,  9.85it/s][AEpoch 21:  89%|████████▉ | 1960/2191 [23:57<02:49,  1.36it/s, loss=2.39, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:22,  9.89it/s][AEpoch 21:  90%|████████▉ | 1970/2191 [23:58<02:41,  1.37it/s, loss=2.39, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.31it/s][AEpoch 21:  90%|█████████ | 1980/2191 [23:59<02:33,  1.38it/s, loss=2.39, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:19, 10.40it/s][AEpoch 21:  91%|█████████ | 1990/2191 [24:00<02:25,  1.38it/s, loss=2.39, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:21,  8.85it/s][AEpoch 21:  91%|█████████▏| 2000/2191 [24:01<02:17,  1.39it/s, loss=2.39, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:20,  8.83it/s][AEpoch 21:  92%|█████████▏| 2010/2191 [24:02<02:09,  1.39it/s, loss=2.39, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  8.24it/s][AEpoch 21:  92%|█████████▏| 2020/2191 [24:04<02:02,  1.40it/s, loss=2.39, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:18,  8.92it/s][AEpoch 21:  93%|█████████▎| 2030/2191 [24:05<01:54,  1.41it/s, loss=2.39, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:17,  8.96it/s][AEpoch 21:  93%|█████████▎| 2040/2191 [24:06<01:46,  1.41it/s, loss=2.39, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:14, 10.17it/s][AEpoch 21:  94%|█████████▎| 2050/2191 [24:06<01:39,  1.42it/s, loss=2.39, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:19<00:12, 10.41it/s][AEpoch 21:  94%|█████████▍| 2060/2191 [24:07<01:32,  1.42it/s, loss=2.39, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.58it/s][AEpoch 21:  94%|█████████▍| 2070/2191 [24:08<01:24,  1.43it/s, loss=2.39, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:08, 12.76it/s][AEpoch 21:  95%|█████████▍| 2080/2191 [24:08<01:17,  1.44it/s, loss=2.39, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 10.40it/s][AEpoch 21:  95%|█████████▌| 2090/2191 [24:10<01:10,  1.44it/s, loss=2.39, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.43it/s][AEpoch 21:  96%|█████████▌| 2100/2191 [24:11<01:02,  1.45it/s, loss=2.39, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:08,  9.74it/s][AEpoch 21:  96%|█████████▋| 2110/2191 [24:12<00:55,  1.45it/s, loss=2.39, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.05it/s][AEpoch 21:  97%|█████████▋| 2120/2191 [24:13<00:48,  1.46it/s, loss=2.39, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.02it/s][AEpoch 21:  97%|█████████▋| 2130/2191 [24:14<00:41,  1.47it/s, loss=2.39, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.82it/s][AEpoch 21:  98%|█████████▊| 2140/2191 [24:15<00:34,  1.47it/s, loss=2.39, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.37it/s][AEpoch 21:  98%|█████████▊| 2150/2191 [24:16<00:27,  1.48it/s, loss=2.39, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:28<00:02, 11.81it/s][AEpoch 21:  99%|█████████▊| 2160/2191 [24:16<00:20,  1.48it/s, loss=2.39, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:29<00:01, 11.73it/s][AEpoch 21:  99%|█████████▉| 2170/2191 [24:17<00:14,  1.49it/s, loss=2.39, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.42it/s][AEpoch 21:  99%|█████████▉| 2180/2191 [24:18<00:07,  1.50it/s, loss=2.39, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 14.34it/s][AEpoch 21: 100%|█████████▉| 2190/2191 [24:18<00:00,  1.50it/s, loss=2.39, v_num=647]validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.977229654788971
Epoch 21: 100%|██████████| 2191/2191 [24:21<00:00,  1.50it/s, loss=2.35, v_num=647]
                                                             [AEpoch 21:   0%|          | 0/2191 [00:00<00:00, 14074.85it/s, loss=2.35, v_num=647]Epoch 22:   0%|          | 0/2191 [00:00<00:00, 3368.92it/s, loss=2.35, v_num=647] Epoch 22:   0%|          | 10/2191 [00:11<39:06,  1.08s/it, loss=2.35, v_num=647] Epoch 22:   0%|          | 10/2191 [00:11<39:06,  1.08s/it, loss=2.33, v_num=647]Epoch 22:   1%|          | 20/2191 [00:19<34:04,  1.06it/s, loss=2.33, v_num=647]Epoch 22:   1%|          | 20/2191 [00:19<34:05,  1.06it/s, loss=2.29, v_num=647]Epoch 22:   1%|▏         | 30/2191 [00:27<32:13,  1.12it/s, loss=2.29, v_num=647]Epoch 22:   1%|▏         | 30/2191 [00:27<32:13,  1.12it/s, loss=2.31, v_num=647]Epoch 22:   2%|▏         | 40/2191 [00:37<32:34,  1.10it/s, loss=2.31, v_num=647]Epoch 22:   2%|▏         | 40/2191 [00:37<32:34,  1.10it/s, loss=2.29, v_num=647]Epoch 22:   2%|▏         | 50/2191 [00:45<31:38,  1.13it/s, loss=2.29, v_num=647]Epoch 22:   2%|▏         | 50/2191 [00:45<31:38,  1.13it/s, loss=2.27, v_num=647]Epoch 22:   3%|▎         | 60/2191 [00:53<31:09,  1.14it/s, loss=2.27, v_num=647]Epoch 22:   3%|▎         | 60/2191 [00:53<31:09,  1.14it/s, loss=2.33, v_num=647]Epoch 22:   3%|▎         | 70/2191 [01:01<30:23,  1.16it/s, loss=2.33, v_num=647]Epoch 22:   3%|▎         | 70/2191 [01:01<30:23,  1.16it/s, loss=2.33, v_num=647]Epoch 22:   4%|▎         | 80/2191 [01:08<29:33,  1.19it/s, loss=2.33, v_num=647]Epoch 22:   4%|▎         | 80/2191 [01:08<29:33,  1.19it/s, loss=2.29, v_num=647]Epoch 22:   4%|▍         | 90/2191 [01:15<29:06,  1.20it/s, loss=2.29, v_num=647]Epoch 22:   4%|▍         | 90/2191 [01:15<29:06,  1.20it/s, loss=2.3, v_num=647] Epoch 22:   5%|▍         | 100/2191 [01:22<28:28,  1.22it/s, loss=2.3, v_num=647]Epoch 22:   5%|▍         | 100/2191 [01:22<28:28,  1.22it/s, loss=2.33, v_num=647]Epoch 22:   5%|▍         | 100/2191 [01:32<32:00,  1.09it/s, loss=2.33, v_num=647]Epoch 22:   5%|▌         | 110/2191 [01:33<29:03,  1.19it/s, loss=2.33, v_num=647]Epoch 22:   5%|▌         | 110/2191 [01:33<29:03,  1.19it/s, loss=2.31, v_num=647]Epoch 22:   5%|▌         | 120/2191 [01:42<29:06,  1.19it/s, loss=2.31, v_num=647]Epoch 22:   5%|▌         | 120/2191 [01:42<29:06,  1.19it/s, loss=2.29, v_num=647]Epoch 22:   6%|▌         | 130/2191 [01:51<29:12,  1.18it/s, loss=2.29, v_num=647]Epoch 22:   6%|▌         | 130/2191 [01:51<29:12,  1.18it/s, loss=2.3, v_num=647] Epoch 22:   6%|▋         | 140/2191 [01:59<29:03,  1.18it/s, loss=2.3, v_num=647]Epoch 22:   6%|▋         | 140/2191 [01:59<29:03,  1.18it/s, loss=2.32, v_num=647]Epoch 22:   7%|▋         | 150/2191 [02:07<28:38,  1.19it/s, loss=2.32, v_num=647]Epoch 22:   7%|▋         | 150/2191 [02:07<28:38,  1.19it/s, loss=2.33, v_num=647]Epoch 22:   7%|▋         | 160/2191 [02:14<28:14,  1.20it/s, loss=2.33, v_num=647]Epoch 22:   7%|▋         | 160/2191 [02:14<28:14,  1.20it/s, loss=2.31, v_num=647]Epoch 22:   8%|▊         | 170/2191 [02:22<27:59,  1.20it/s, loss=2.31, v_num=647]Epoch 22:   8%|▊         | 170/2191 [02:22<27:59,  1.20it/s, loss=2.34, v_num=647]Epoch 22:   8%|▊         | 180/2191 [02:29<27:36,  1.21it/s, loss=2.34, v_num=647]Epoch 22:   8%|▊         | 180/2191 [02:29<27:36,  1.21it/s, loss=2.37, v_num=647]Epoch 22:   9%|▊         | 190/2191 [02:37<27:31,  1.21it/s, loss=2.37, v_num=647]Epoch 22:   9%|▊         | 190/2191 [02:37<27:31,  1.21it/s, loss=2.33, v_num=647]Epoch 22:   9%|▉         | 200/2191 [02:47<27:37,  1.20it/s, loss=2.33, v_num=647]Epoch 22:   9%|▉         | 200/2191 [02:47<27:37,  1.20it/s, loss=2.32, v_num=647]Epoch 22:  10%|▉         | 210/2191 [02:56<27:34,  1.20it/s, loss=2.32, v_num=647]Epoch 22:  10%|▉         | 210/2191 [02:56<27:35,  1.20it/s, loss=2.33, v_num=647]Epoch 22:  10%|█         | 220/2191 [03:05<27:38,  1.19it/s, loss=2.33, v_num=647]Epoch 22:  10%|█         | 220/2191 [03:05<27:38,  1.19it/s, loss=2.31, v_num=647]Epoch 22:  10%|█         | 230/2191 [03:13<27:22,  1.19it/s, loss=2.31, v_num=647]Epoch 22:  10%|█         | 230/2191 [03:13<27:22,  1.19it/s, loss=2.27, v_num=647]Epoch 22:  11%|█         | 240/2191 [03:20<26:59,  1.20it/s, loss=2.27, v_num=647]Epoch 22:  11%|█         | 240/2191 [03:20<26:59,  1.20it/s, loss=2.32, v_num=647]Epoch 22:  11%|█▏        | 250/2191 [03:28<26:52,  1.20it/s, loss=2.32, v_num=647]Epoch 22:  11%|█▏        | 250/2191 [03:28<26:52,  1.20it/s, loss=2.37, v_num=647]Epoch 22:  12%|█▏        | 260/2191 [03:35<26:33,  1.21it/s, loss=2.37, v_num=647]Epoch 22:  12%|█▏        | 260/2191 [03:35<26:33,  1.21it/s, loss=2.34, v_num=647]Epoch 22:  12%|█▏        | 270/2191 [03:43<26:26,  1.21it/s, loss=2.34, v_num=647]Epoch 22:  12%|█▏        | 270/2191 [03:43<26:26,  1.21it/s, loss=2.32, v_num=647]Epoch 22:  13%|█▎        | 280/2191 [03:51<26:13,  1.21it/s, loss=2.32, v_num=647]Epoch 22:  13%|█▎        | 280/2191 [03:51<26:13,  1.21it/s, loss=2.3, v_num=647] Epoch 22:  13%|█▎        | 290/2191 [03:58<25:57,  1.22it/s, loss=2.3, v_num=647]Epoch 22:  13%|█▎        | 290/2191 [03:58<25:57,  1.22it/s, loss=2.33, v_num=647]Epoch 22:  14%|█▎        | 300/2191 [04:05<25:41,  1.23it/s, loss=2.33, v_num=647]Epoch 22:  14%|█▎        | 300/2191 [04:05<25:41,  1.23it/s, loss=2.33, v_num=647]Epoch 22:  14%|█▍        | 310/2191 [04:12<25:26,  1.23it/s, loss=2.33, v_num=647]Epoch 22:  14%|█▍        | 310/2191 [04:12<25:26,  1.23it/s, loss=2.3, v_num=647] Epoch 22:  15%|█▍        | 320/2191 [04:20<25:19,  1.23it/s, loss=2.3, v_num=647]Epoch 22:  15%|█▍        | 320/2191 [04:20<25:19,  1.23it/s, loss=2.33, v_num=647]Epoch 22:  15%|█▌        | 330/2191 [04:28<25:07,  1.23it/s, loss=2.33, v_num=647]Epoch 22:  15%|█▌        | 330/2191 [04:28<25:07,  1.23it/s, loss=2.36, v_num=647]Epoch 22:  16%|█▌        | 340/2191 [04:35<24:53,  1.24it/s, loss=2.36, v_num=647]Epoch 22:  16%|█▌        | 340/2191 [04:35<24:53,  1.24it/s, loss=2.35, v_num=647]Epoch 22:  16%|█▌        | 350/2191 [04:43<24:46,  1.24it/s, loss=2.35, v_num=647]Epoch 22:  16%|█▌        | 350/2191 [04:43<24:46,  1.24it/s, loss=2.35, v_num=647]Epoch 22:  16%|█▋        | 360/2191 [04:50<24:34,  1.24it/s, loss=2.35, v_num=647]Epoch 22:  16%|█▋        | 360/2191 [04:50<24:34,  1.24it/s, loss=2.37, v_num=647]Epoch 22:  17%|█▋        | 370/2191 [04:57<24:21,  1.25it/s, loss=2.37, v_num=647]Epoch 22:  17%|█▋        | 370/2191 [04:57<24:21,  1.25it/s, loss=2.36, v_num=647]Epoch 22:  17%|█▋        | 380/2191 [05:06<24:17,  1.24it/s, loss=2.36, v_num=647]Epoch 22:  17%|█▋        | 380/2191 [05:06<24:17,  1.24it/s, loss=2.31, v_num=647]Epoch 22:  18%|█▊        | 390/2191 [05:13<24:04,  1.25it/s, loss=2.31, v_num=647]Epoch 22:  18%|█▊        | 390/2191 [05:13<24:04,  1.25it/s, loss=2.3, v_num=647] Epoch 22:  18%|█▊        | 400/2191 [05:21<23:55,  1.25it/s, loss=2.3, v_num=647]Epoch 22:  18%|█▊        | 400/2191 [05:21<23:55,  1.25it/s, loss=2.31, v_num=647]Epoch 22:  19%|█▊        | 410/2191 [05:29<23:46,  1.25it/s, loss=2.31, v_num=647]Epoch 22:  19%|█▊        | 410/2191 [05:29<23:46,  1.25it/s, loss=2.35, v_num=647]Epoch 22:  19%|█▉        | 420/2191 [05:36<23:34,  1.25it/s, loss=2.35, v_num=647]Epoch 22:  19%|█▉        | 420/2191 [05:36<23:34,  1.25it/s, loss=2.35, v_num=647]Epoch 22:  20%|█▉        | 430/2191 [05:44<23:25,  1.25it/s, loss=2.35, v_num=647]Epoch 22:  20%|█▉        | 430/2191 [05:44<23:25,  1.25it/s, loss=2.34, v_num=647]Epoch 22:  20%|██        | 440/2191 [05:51<23:16,  1.25it/s, loss=2.34, v_num=647]Epoch 22:  20%|██        | 440/2191 [05:51<23:16,  1.25it/s, loss=2.36, v_num=647]Epoch 22:  21%|██        | 450/2191 [05:59<23:09,  1.25it/s, loss=2.36, v_num=647]Epoch 22:  21%|██        | 450/2191 [05:59<23:09,  1.25it/s, loss=2.36, v_num=647]Epoch 22:  21%|██        | 460/2191 [06:07<22:58,  1.26it/s, loss=2.36, v_num=647]Epoch 22:  21%|██        | 460/2191 [06:07<22:58,  1.26it/s, loss=2.36, v_num=647]Epoch 22:  21%|██▏       | 470/2191 [06:16<22:54,  1.25it/s, loss=2.36, v_num=647]Epoch 22:  21%|██▏       | 470/2191 [06:16<22:54,  1.25it/s, loss=2.35, v_num=647]Epoch 22:  22%|██▏       | 480/2191 [06:22<22:42,  1.26it/s, loss=2.35, v_num=647]Epoch 22:  22%|██▏       | 480/2191 [06:22<22:42,  1.26it/s, loss=2.34, v_num=647]Epoch 22:  22%|██▏       | 490/2191 [06:30<22:31,  1.26it/s, loss=2.34, v_num=647]Epoch 22:  22%|██▏       | 490/2191 [06:30<22:31,  1.26it/s, loss=2.33, v_num=647]Epoch 22:  23%|██▎       | 500/2191 [06:37<22:22,  1.26it/s, loss=2.33, v_num=647]Epoch 22:  23%|██▎       | 500/2191 [06:37<22:22,  1.26it/s, loss=2.3, v_num=647] Epoch 22:  23%|██▎       | 510/2191 [06:44<22:09,  1.26it/s, loss=2.3, v_num=647]Epoch 22:  23%|██▎       | 510/2191 [06:44<22:09,  1.26it/s, loss=2.33, v_num=647]Epoch 22:  24%|██▎       | 520/2191 [06:51<21:58,  1.27it/s, loss=2.33, v_num=647]Epoch 22:  24%|██▎       | 520/2191 [06:51<21:58,  1.27it/s, loss=2.37, v_num=647]Epoch 22:  24%|██▍       | 530/2191 [06:57<21:46,  1.27it/s, loss=2.37, v_num=647]Epoch 22:  24%|██▍       | 530/2191 [06:57<21:46,  1.27it/s, loss=2.37, v_num=647]Epoch 22:  25%|██▍       | 540/2191 [07:04<21:36,  1.27it/s, loss=2.37, v_num=647]Epoch 22:  25%|██▍       | 540/2191 [07:04<21:36,  1.27it/s, loss=2.36, v_num=647]Epoch 22:  25%|██▌       | 550/2191 [07:12<21:29,  1.27it/s, loss=2.36, v_num=647]Epoch 22:  25%|██▌       | 550/2191 [07:12<21:29,  1.27it/s, loss=2.33, v_num=647]Epoch 22:  26%|██▌       | 560/2191 [07:19<21:17,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  26%|██▌       | 560/2191 [07:19<21:17,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  26%|██▌       | 570/2191 [07:26<21:06,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  26%|██▌       | 570/2191 [07:26<21:06,  1.28it/s, loss=2.37, v_num=647]Epoch 22:  26%|██▋       | 580/2191 [07:34<21:01,  1.28it/s, loss=2.37, v_num=647]Epoch 22:  26%|██▋       | 580/2191 [07:34<21:01,  1.28it/s, loss=2.36, v_num=647]Epoch 22:  27%|██▋       | 590/2191 [07:42<20:52,  1.28it/s, loss=2.36, v_num=647]Epoch 22:  27%|██▋       | 590/2191 [07:42<20:52,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  27%|██▋       | 600/2191 [07:49<20:43,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  27%|██▋       | 600/2191 [07:49<20:43,  1.28it/s, loss=2.31, v_num=647]Epoch 22:  28%|██▊       | 610/2191 [07:56<20:32,  1.28it/s, loss=2.31, v_num=647]Epoch 22:  28%|██▊       | 610/2191 [07:56<20:32,  1.28it/s, loss=2.32, v_num=647]Epoch 22:  28%|██▊       | 620/2191 [08:05<20:27,  1.28it/s, loss=2.32, v_num=647]Epoch 22:  28%|██▊       | 620/2191 [08:05<20:27,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  29%|██▉       | 630/2191 [08:11<20:15,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  29%|██▉       | 630/2191 [08:11<20:15,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  29%|██▉       | 640/2191 [08:20<20:10,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  29%|██▉       | 640/2191 [08:20<20:10,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  30%|██▉       | 650/2191 [08:26<19:59,  1.28it/s, loss=2.33, v_num=647]Epoch 22:  30%|██▉       | 650/2191 [08:26<19:59,  1.28it/s, loss=2.29, v_num=647]Epoch 22:  30%|███       | 660/2191 [08:33<19:49,  1.29it/s, loss=2.29, v_num=647]Epoch 22:  30%|███       | 660/2191 [08:33<19:49,  1.29it/s, loss=2.31, v_num=647]Epoch 22:  31%|███       | 670/2191 [08:40<19:39,  1.29it/s, loss=2.31, v_num=647]Epoch 22:  31%|███       | 670/2191 [08:40<19:39,  1.29it/s, loss=2.35, v_num=647]Epoch 22:  31%|███       | 680/2191 [08:47<19:30,  1.29it/s, loss=2.35, v_num=647]Epoch 22:  31%|███       | 680/2191 [08:47<19:30,  1.29it/s, loss=2.38, v_num=647]Epoch 22:  31%|███▏      | 690/2191 [08:53<19:19,  1.29it/s, loss=2.38, v_num=647]Epoch 22:  31%|███▏      | 690/2191 [08:53<19:19,  1.29it/s, loss=2.38, v_num=647]Epoch 22:  32%|███▏      | 700/2191 [09:01<19:11,  1.29it/s, loss=2.38, v_num=647]Epoch 22:  32%|███▏      | 700/2191 [09:01<19:11,  1.29it/s, loss=2.34, v_num=647]Epoch 22:  32%|███▏      | 710/2191 [09:08<19:03,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  32%|███▏      | 710/2191 [09:08<19:03,  1.30it/s, loss=2.28, v_num=647]Epoch 22:  33%|███▎      | 720/2191 [09:16<18:55,  1.30it/s, loss=2.28, v_num=647]Epoch 22:  33%|███▎      | 720/2191 [09:16<18:55,  1.30it/s, loss=2.26, v_num=647]Epoch 22:  33%|███▎      | 730/2191 [09:24<18:47,  1.30it/s, loss=2.26, v_num=647]Epoch 22:  33%|███▎      | 730/2191 [09:24<18:47,  1.30it/s, loss=2.29, v_num=647]Epoch 22:  34%|███▍      | 740/2191 [09:31<18:39,  1.30it/s, loss=2.29, v_num=647]Epoch 22:  34%|███▍      | 740/2191 [09:31<18:39,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  34%|███▍      | 750/2191 [09:39<18:31,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  34%|███▍      | 750/2191 [09:39<18:31,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  35%|███▍      | 760/2191 [09:46<18:22,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  35%|███▍      | 760/2191 [09:46<18:22,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  35%|███▌      | 770/2191 [09:54<18:15,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  35%|███▌      | 770/2191 [09:54<18:15,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  36%|███▌      | 780/2191 [10:01<18:06,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  36%|███▌      | 780/2191 [10:01<18:06,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  36%|███▌      | 790/2191 [10:09<17:58,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  36%|███▌      | 790/2191 [10:09<17:58,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  37%|███▋      | 800/2191 [10:16<17:50,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  37%|███▋      | 800/2191 [10:16<17:50,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  37%|███▋      | 810/2191 [10:24<17:43,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  37%|███▋      | 810/2191 [10:24<17:43,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  37%|███▋      | 820/2191 [10:32<17:36,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  37%|███▋      | 820/2191 [10:32<17:36,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  38%|███▊      | 830/2191 [10:39<17:28,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  38%|███▊      | 830/2191 [10:39<17:28,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  38%|███▊      | 840/2191 [10:47<17:20,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  38%|███▊      | 840/2191 [10:47<17:20,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  39%|███▉      | 850/2191 [10:54<17:11,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  39%|███▉      | 850/2191 [10:54<17:11,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  39%|███▉      | 860/2191 [11:04<17:06,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  39%|███▉      | 860/2191 [11:04<17:06,  1.30it/s, loss=2.29, v_num=647]Epoch 22:  40%|███▉      | 870/2191 [11:10<16:57,  1.30it/s, loss=2.29, v_num=647]Epoch 22:  40%|███▉      | 870/2191 [11:10<16:57,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  40%|████      | 880/2191 [11:18<16:49,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  40%|████      | 880/2191 [11:18<16:49,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  41%|████      | 890/2191 [11:26<16:41,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  41%|████      | 890/2191 [11:26<16:41,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  41%|████      | 900/2191 [11:33<16:34,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  41%|████      | 900/2191 [11:33<16:34,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  42%|████▏     | 910/2191 [11:41<16:26,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  42%|████▏     | 910/2191 [11:41<16:26,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  42%|████▏     | 920/2191 [11:49<16:18,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  42%|████▏     | 920/2191 [11:49<16:18,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  42%|████▏     | 930/2191 [11:56<16:10,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  42%|████▏     | 930/2191 [11:56<16:10,  1.30it/s, loss=2.3, v_num=647] Epoch 22:  43%|████▎     | 940/2191 [12:04<16:02,  1.30it/s, loss=2.3, v_num=647]Epoch 22:  43%|████▎     | 940/2191 [12:04<16:02,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  43%|████▎     | 950/2191 [12:11<15:54,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  43%|████▎     | 950/2191 [12:11<15:54,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  44%|████▍     | 960/2191 [12:18<15:46,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  44%|████▍     | 960/2191 [12:18<15:46,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  44%|████▍     | 970/2191 [12:25<15:37,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  44%|████▍     | 970/2191 [12:25<15:37,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  45%|████▍     | 980/2191 [12:34<15:30,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  45%|████▍     | 980/2191 [12:34<15:30,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  45%|████▌     | 990/2191 [12:42<15:23,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  45%|████▌     | 990/2191 [12:42<15:23,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  46%|████▌     | 1000/2191 [12:48<15:14,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  46%|████▌     | 1000/2191 [12:48<15:14,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  46%|████▌     | 1010/2191 [12:57<15:07,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  46%|████▌     | 1010/2191 [12:57<15:07,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  47%|████▋     | 1020/2191 [13:04<14:59,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  47%|████▋     | 1020/2191 [13:04<14:59,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  47%|████▋     | 1030/2191 [13:12<14:52,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  47%|████▋     | 1030/2191 [13:12<14:52,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  47%|████▋     | 1040/2191 [13:20<14:45,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  47%|████▋     | 1040/2191 [13:20<14:45,  1.30it/s, loss=2.31, v_num=647]Epoch 22:  48%|████▊     | 1050/2191 [13:27<14:36,  1.30it/s, loss=2.31, v_num=647]Epoch 22:  48%|████▊     | 1050/2191 [13:27<14:36,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  48%|████▊     | 1060/2191 [13:34<14:27,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  48%|████▊     | 1060/2191 [13:34<14:27,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  49%|████▉     | 1070/2191 [13:41<14:19,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  49%|████▉     | 1070/2191 [13:41<14:19,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  49%|████▉     | 1080/2191 [13:50<14:13,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  49%|████▉     | 1080/2191 [13:50<14:13,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  50%|████▉     | 1090/2191 [13:58<14:06,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  50%|████▉     | 1090/2191 [13:58<14:06,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  50%|█████     | 1100/2191 [14:05<13:57,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  50%|█████     | 1100/2191 [14:05<13:57,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  51%|█████     | 1110/2191 [14:13<13:50,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  51%|█████     | 1110/2191 [14:13<13:50,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  51%|█████     | 1120/2191 [14:20<13:41,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  51%|█████     | 1120/2191 [14:20<13:41,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  52%|█████▏    | 1130/2191 [14:26<13:33,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  52%|█████▏    | 1130/2191 [14:26<13:33,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  52%|█████▏    | 1140/2191 [14:33<13:24,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  52%|█████▏    | 1140/2191 [14:33<13:24,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  52%|█████▏    | 1150/2191 [14:41<13:17,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  52%|█████▏    | 1150/2191 [14:41<13:17,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  53%|█████▎    | 1160/2191 [14:48<13:09,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  53%|█████▎    | 1160/2191 [14:48<13:09,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  53%|█████▎    | 1170/2191 [14:55<13:00,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  53%|█████▎    | 1170/2191 [14:55<13:00,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  54%|█████▍    | 1180/2191 [15:02<12:52,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  54%|█████▍    | 1180/2191 [15:02<12:52,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  54%|█████▍    | 1190/2191 [15:09<12:44,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  54%|█████▍    | 1190/2191 [15:09<12:44,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  55%|█████▍    | 1200/2191 [15:16<12:36,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  55%|█████▍    | 1200/2191 [15:16<12:36,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  55%|█████▌    | 1210/2191 [15:25<12:29,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  55%|█████▌    | 1210/2191 [15:25<12:29,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  56%|█████▌    | 1220/2191 [15:33<12:22,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  56%|█████▌    | 1220/2191 [15:33<12:22,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  56%|█████▌    | 1230/2191 [15:40<12:14,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  56%|█████▌    | 1230/2191 [15:40<12:14,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  57%|█████▋    | 1240/2191 [15:48<12:06,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  57%|█████▋    | 1240/2191 [15:48<12:06,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  57%|█████▋    | 1250/2191 [15:55<11:58,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  57%|█████▋    | 1250/2191 [15:55<11:58,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  58%|█████▊    | 1260/2191 [16:03<11:51,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  58%|█████▊    | 1260/2191 [16:03<11:51,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  58%|█████▊    | 1270/2191 [16:10<11:43,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  58%|█████▊    | 1270/2191 [16:10<11:43,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  58%|█████▊    | 1280/2191 [16:18<11:35,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  58%|█████▊    | 1280/2191 [16:18<11:35,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  59%|█████▉    | 1290/2191 [16:27<11:29,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  59%|█████▉    | 1290/2191 [16:27<11:29,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  59%|█████▉    | 1300/2191 [16:35<11:21,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  59%|█████▉    | 1300/2191 [16:35<11:21,  1.31it/s, loss=2.3, v_num=647] Epoch 22:  60%|█████▉    | 1310/2191 [16:44<11:14,  1.31it/s, loss=2.3, v_num=647]Epoch 22:  60%|█████▉    | 1310/2191 [16:44<11:14,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  60%|██████    | 1320/2191 [16:51<11:07,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  60%|██████    | 1320/2191 [16:51<11:07,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  61%|██████    | 1330/2191 [17:00<10:59,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  61%|██████    | 1330/2191 [17:00<10:59,  1.30it/s, loss=2.39, v_num=647]Epoch 22:  61%|██████    | 1340/2191 [17:07<10:52,  1.31it/s, loss=2.39, v_num=647]Epoch 22:  61%|██████    | 1340/2191 [17:07<10:52,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  62%|██████▏   | 1350/2191 [17:14<10:44,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  62%|██████▏   | 1350/2191 [17:14<10:44,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  62%|██████▏   | 1360/2191 [17:26<10:38,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  62%|██████▏   | 1360/2191 [17:26<10:38,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  63%|██████▎   | 1370/2191 [17:33<10:30,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  63%|██████▎   | 1370/2191 [17:33<10:30,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  63%|██████▎   | 1380/2191 [17:43<10:24,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  63%|██████▎   | 1380/2191 [17:43<10:24,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  63%|██████▎   | 1390/2191 [17:49<10:16,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  63%|██████▎   | 1390/2191 [17:49<10:16,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  64%|██████▍   | 1400/2191 [17:56<10:07,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  64%|██████▍   | 1400/2191 [17:56<10:07,  1.30it/s, loss=2.3, v_num=647] Epoch 22:  64%|██████▍   | 1410/2191 [18:02<09:59,  1.30it/s, loss=2.3, v_num=647]Epoch 22:  64%|██████▍   | 1410/2191 [18:02<09:59,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  65%|██████▍   | 1420/2191 [18:10<09:51,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  65%|██████▍   | 1420/2191 [18:10<09:51,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  65%|██████▌   | 1430/2191 [18:17<09:43,  1.30it/s, loss=2.37, v_num=647]Epoch 22:  65%|██████▌   | 1430/2191 [18:17<09:43,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  66%|██████▌   | 1440/2191 [18:28<09:37,  1.30it/s, loss=2.34, v_num=647]Epoch 22:  66%|██████▌   | 1440/2191 [18:28<09:37,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  66%|██████▌   | 1450/2191 [18:34<09:29,  1.30it/s, loss=2.32, v_num=647]Epoch 22:  66%|██████▌   | 1450/2191 [18:34<09:29,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  67%|██████▋   | 1460/2191 [18:41<09:21,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  67%|██████▋   | 1460/2191 [18:41<09:21,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  67%|██████▋   | 1470/2191 [18:49<09:13,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  67%|██████▋   | 1470/2191 [18:49<09:13,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  68%|██████▊   | 1480/2191 [18:55<09:05,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  68%|██████▊   | 1480/2191 [18:55<09:05,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  68%|██████▊   | 1490/2191 [19:04<08:58,  1.30it/s, loss=2.33, v_num=647]Epoch 22:  68%|██████▊   | 1490/2191 [19:04<08:58,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  68%|██████▊   | 1500/2191 [19:10<08:49,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  68%|██████▊   | 1500/2191 [19:10<08:49,  1.30it/s, loss=2.39, v_num=647]Epoch 22:  69%|██████▉   | 1510/2191 [19:19<08:42,  1.30it/s, loss=2.39, v_num=647]Epoch 22:  69%|██████▉   | 1510/2191 [19:19<08:42,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  69%|██████▉   | 1520/2191 [19:26<08:34,  1.30it/s, loss=2.35, v_num=647]Epoch 22:  69%|██████▉   | 1520/2191 [19:26<08:34,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  70%|██████▉   | 1530/2191 [19:34<08:26,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  70%|██████▉   | 1530/2191 [19:34<08:26,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  70%|███████   | 1540/2191 [19:42<08:19,  1.30it/s, loss=2.36, v_num=647]Epoch 22:  70%|███████   | 1540/2191 [19:42<08:19,  1.30it/s, loss=2.39, v_num=647]Epoch 22:  71%|███████   | 1550/2191 [19:49<08:11,  1.30it/s, loss=2.39, v_num=647]Epoch 22:  71%|███████   | 1550/2191 [19:49<08:11,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  71%|███████   | 1560/2191 [19:56<08:03,  1.30it/s, loss=2.38, v_num=647]Epoch 22:  71%|███████   | 1560/2191 [19:56<08:03,  1.30it/s, loss=2.31, v_num=647]Epoch 22:  72%|███████▏  | 1570/2191 [20:02<07:55,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  72%|███████▏  | 1570/2191 [20:02<07:55,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  72%|███████▏  | 1580/2191 [20:10<07:47,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  72%|███████▏  | 1580/2191 [20:10<07:47,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  73%|███████▎  | 1590/2191 [20:17<07:39,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  73%|███████▎  | 1590/2191 [20:17<07:39,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  73%|███████▎  | 1600/2191 [20:25<07:32,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  73%|███████▎  | 1600/2191 [20:25<07:32,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  73%|███████▎  | 1610/2191 [20:32<07:24,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  74%|███████▍  | 1620/2191 [20:39<07:16,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  74%|███████▍  | 1620/2191 [20:39<07:16,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  74%|███████▍  | 1630/2191 [20:46<07:08,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  74%|███████▍  | 1630/2191 [20:46<07:08,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  75%|███████▍  | 1640/2191 [20:54<07:01,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  75%|███████▌  | 1650/2191 [21:02<06:53,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  75%|███████▌  | 1650/2191 [21:02<06:53,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  76%|███████▌  | 1660/2191 [21:09<06:45,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  76%|███████▌  | 1660/2191 [21:09<06:45,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  76%|███████▌  | 1670/2191 [21:17<06:38,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  76%|███████▌  | 1670/2191 [21:17<06:38,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  77%|███████▋  | 1680/2191 [21:24<06:30,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  77%|███████▋  | 1680/2191 [21:24<06:30,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  77%|███████▋  | 1690/2191 [21:31<06:22,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  77%|███████▋  | 1690/2191 [21:31<06:22,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  78%|███████▊  | 1700/2191 [21:39<06:14,  1.31it/s, loss=2.31, v_num=647]Epoch 22:  78%|███████▊  | 1700/2191 [21:39<06:14,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  78%|███████▊  | 1710/2191 [21:46<06:07,  1.31it/s, loss=2.33, v_num=647]Epoch 22:  78%|███████▊  | 1710/2191 [21:46<06:07,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  79%|███████▊  | 1720/2191 [21:54<05:59,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  79%|███████▊  | 1720/2191 [21:54<05:59,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  79%|███████▉  | 1730/2191 [22:02<05:52,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  79%|███████▉  | 1730/2191 [22:02<05:52,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  79%|███████▉  | 1740/2191 [22:10<05:44,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  79%|███████▉  | 1740/2191 [22:10<05:44,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  80%|███████▉  | 1750/2191 [22:19<05:37,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  80%|███████▉  | 1750/2191 [22:19<05:37,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  80%|████████  | 1760/2191 [22:26<05:29,  1.31it/s, loss=2.32, v_num=647]Epoch 22:  80%|████████  | 1760/2191 [22:26<05:29,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  81%|████████  | 1770/2191 [22:33<05:21,  1.31it/s, loss=2.35, v_num=647]Epoch 22:  81%|████████  | 1770/2191 [22:33<05:21,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  81%|████████  | 1780/2191 [22:40<05:14,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  81%|████████  | 1780/2191 [22:40<05:14,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  82%|████████▏ | 1790/2191 [22:47<05:06,  1.31it/s, loss=2.36, v_num=647]Epoch 22:  82%|████████▏ | 1790/2191 [22:47<05:06,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  82%|████████▏ | 1800/2191 [22:56<04:58,  1.31it/s, loss=2.38, v_num=647]Epoch 22:  82%|████████▏ | 1800/2191 [22:56<04:58,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  83%|████████▎ | 1810/2191 [23:04<04:51,  1.31it/s, loss=2.34, v_num=647]Epoch 22:  83%|████████▎ | 1810/2191 [23:04<04:51,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  83%|████████▎ | 1820/2191 [23:13<04:43,  1.31it/s, loss=2.37, v_num=647]Epoch 22:  83%|████████▎ | 1820/2191 [23:13<04:43,  1.31it/s, loss=2.41, v_num=647]Epoch 22:  84%|████████▎ | 1830/2191 [23:21<04:36,  1.31it/s, loss=2.41, v_num=647]Epoch 22:  84%|████████▎ | 1830/2191 [23:21<04:36,  1.31it/s, loss=2.42, v_num=647]Epoch 22:  84%|████████▍ | 1840/2191 [23:24<04:27,  1.31it/s, loss=2.42, v_num=647]Epoch 22:  84%|████████▍ | 1840/2191 [23:24<04:27,  1.31it/s, loss=2.39, v_num=647]validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9765968322753906
.39, v_num=647]Epoch 22:  84%|████████▍ | 1850/2191 [23:27<04:19,  1.32it/s, loss=2.34, v_num=647]Epoch 22:  85%|████████▍ | 1860/2191 [23:30<04:10,  1.32it/s, loss=2.34, v_num=647]Epoch 22:  85%|████████▍ | 1860/2191 [23:30<04:10,  1.32it/s, loss=2.34, v_num=647]Epoch 22:  85%|████████▌ | 1870/2191 [23:32<04:02,  1.32it/s, loss=2.34, v_num=647]Epoch 22:  85%|████████▌ | 1870/2191 [23:32<04:02,  1.32it/s, loss=2.38, v_num=647]Epoch 22:  86%|████████▌ | 1880/2191 [23:33<03:53,  1.33it/s, loss=2.38, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.38338658146964855
valid accuracy: 0.9765521287918091
validation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9765625596046448
validation_epoch_end
graph acc: 0.34185303514376997
valid accuracy: 0.9752777218818665

Validating:   3%|▎         | 10/313 [00:01<00:45,  6.63it/s][AEpoch 22:  86%|████████▋ | 1890/2191 [23:35<03:45,  1.34it/s, loss=2.38, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:47,  6.13it/s][AEpoch 22:  87%|████████▋ | 1900/2191 [23:37<03:36,  1.34it/s, loss=2.38, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:37,  7.65it/s][AEpoch 22:  87%|████████▋ | 1910/2191 [23:37<03:28,  1.35it/s, loss=2.38, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:27,  9.83it/s][AEpoch 22:  88%|████████▊ | 1920/2191 [23:38<03:20,  1.35it/s, loss=2.38, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26,  9.93it/s][AEpoch 22:  88%|████████▊ | 1930/2191 [23:39<03:11,  1.36it/s, loss=2.38, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.46it/s][AEpoch 22:  89%|████████▊ | 1940/2191 [23:40<03:03,  1.37it/s, loss=2.38, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:22, 10.60it/s][AEpoch 22:  89%|████████▉ | 1950/2191 [23:41<02:55,  1.37it/s, loss=2.38, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.24it/s][AEpoch 22:  89%|████████▉ | 1960/2191 [23:42<02:47,  1.38it/s, loss=2.38, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.21it/s][AEpoch 22:  90%|████████▉ | 1970/2191 [23:43<02:39,  1.38it/s, loss=2.38, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.38it/s][AEpoch 22:  90%|█████████ | 1980/2191 [23:44<02:31,  1.39it/s, loss=2.38, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20, 10.00it/s][AEpoch 22:  91%|█████████ | 1990/2191 [23:45<02:23,  1.40it/s, loss=2.38, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.66it/s][AEpoch 22:  91%|█████████▏| 2000/2191 [23:46<02:16,  1.40it/s, loss=2.38, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:18, 10.16it/s][AEpoch 22:  92%|█████████▏| 2010/2191 [23:47<02:08,  1.41it/s, loss=2.38, v_num=647]
Validating:  45%|████▍     | 140/313 [00:14<00:19,  8.99it/s][AEpoch 22:  92%|█████████▏| 2020/2191 [23:48<02:00,  1.41it/s, loss=2.38, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:17,  9.57it/s][AEpoch 22:  93%|█████████▎| 2030/2191 [23:49<01:53,  1.42it/s, loss=2.38, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15,  9.95it/s][AEpoch 22:  93%|█████████▎| 2040/2191 [23:50<01:45,  1.43it/s, loss=2.38, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:12, 11.29it/s][AEpoch 22:  94%|█████████▎| 2050/2191 [23:51<01:38,  1.43it/s, loss=2.38, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.45it/s][AEpoch 22:  94%|█████████▍| 2060/2191 [23:52<01:31,  1.44it/s, loss=2.38, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.34it/s][AEpoch 22:  94%|█████████▍| 2070/2191 [23:53<01:23,  1.45it/s, loss=2.38, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:19<00:09, 11.99it/s][AEpoch 22:  95%|█████████▍| 2080/2191 [23:53<01:16,  1.45it/s, loss=2.38, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:20<00:09, 11.17it/s][AEpoch 22:  95%|█████████▌| 2090/2191 [23:54<01:09,  1.46it/s, loss=2.38, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.55it/s][AEpoch 22:  96%|█████████▌| 2100/2191 [23:56<01:02,  1.46it/s, loss=2.38, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:22<00:07, 11.40it/s][AEpoch 22:  96%|█████████▋| 2110/2191 [23:56<00:55,  1.47it/s, loss=2.38, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:23<00:07, 10.30it/s][AEpoch 22:  97%|█████████▋| 2120/2191 [23:57<00:48,  1.48it/s, loss=2.38, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06,  9.18it/s][AEpoch 22:  97%|█████████▋| 2130/2191 [23:59<00:41,  1.48it/s, loss=2.38, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.66it/s][AEpoch 22:  98%|█████████▊| 2140/2191 [24:00<00:34,  1.49it/s, loss=2.38, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:04, 10.72it/s][AEpoch 22:  98%|█████████▊| 2150/2191 [24:00<00:27,  1.49it/s, loss=2.38, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.72it/s][AEpoch 22:  99%|█████████▊| 2160/2191 [24:01<00:20,  1.50it/s, loss=2.38, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:02, 11.49it/s][AEpoch 22:  99%|█████████▉| 2170/2191 [24:02<00:13,  1.51it/s, loss=2.38, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.99it/s][AEpoch 22:  99%|█████████▉| 2180/2191 [24:02<00:07,  1.51it/s, loss=2.38, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.27it/s][AEpoch 22: 100%|█████████▉| 2190/2191 [24:03<00:00,  1.52it/s, loss=2.38, v_num=647]validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9789553284645081
Epoch 22: 100%|██████████| 2191/2191 [24:05<00:00,  1.52it/s, loss=2.41, v_num=647]
                                                             [AEpoch 22:   0%|          | 0/2191 [00:00<00:00, 12192.74it/s, loss=2.41, v_num=647]Epoch 23:   0%|          | 0/2191 [00:00<00:00, 3024.01it/s, loss=2.41, v_num=647] Epoch 23:   0%|          | 10/2191 [00:13<43:54,  1.21s/it, loss=2.41, v_num=647] Epoch 23:   0%|          | 10/2191 [00:13<43:54,  1.21s/it, loss=2.37, v_num=647]Epoch 23:   1%|          | 20/2191 [00:21<37:00,  1.02s/it, loss=2.37, v_num=647]Epoch 23:   1%|          | 20/2191 [00:21<37:00,  1.02s/it, loss=2.29, v_num=647]Epoch 23:   1%|▏         | 30/2191 [00:30<36:00,  1.00it/s, loss=2.29, v_num=647]Epoch 23:   1%|▏         | 30/2191 [00:30<36:00,  1.00it/s, loss=2.31, v_num=647]Epoch 23:   2%|▏         | 40/2191 [00:38<33:22,  1.07it/s, loss=2.31, v_num=647]Epoch 23:   2%|▏         | 40/2191 [00:38<33:22,  1.07it/s, loss=2.28, v_num=647]Epoch 23:   2%|▏         | 50/2191 [00:45<31:42,  1.13it/s, loss=2.28, v_num=647]Epoch 23:   2%|▏         | 50/2191 [00:45<31:42,  1.13it/s, loss=2.26, v_num=647]Epoch 23:   3%|▎         | 60/2191 [00:52<30:48,  1.15it/s, loss=2.26, v_num=647]Epoch 23:   3%|▎         | 60/2191 [00:52<30:48,  1.15it/s, loss=2.31, v_num=647]Epoch 23:   3%|▎         | 70/2191 [01:00<30:03,  1.18it/s, loss=2.31, v_num=647]Epoch 23:   3%|▎         | 70/2191 [01:00<30:03,  1.18it/s, loss=2.33, v_num=647]Epoch 23:   4%|▎         | 80/2191 [01:08<29:33,  1.19it/s, loss=2.33, v_num=647]Epoch 23:   4%|▎         | 80/2191 [01:08<29:33,  1.19it/s, loss=2.3, v_num=647] Epoch 23:   4%|▍         | 90/2191 [01:15<29:12,  1.20it/s, loss=2.3, v_num=647]Epoch 23:   4%|▍         | 90/2191 [01:15<29:12,  1.20it/s, loss=2.32, v_num=647]Epoch 23:   5%|▍         | 100/2191 [01:23<28:57,  1.20it/s, loss=2.32, v_num=647]Epoch 23:   5%|▍         | 100/2191 [01:23<28:57,  1.20it/s, loss=2.33, v_num=647]Epoch 23:   5%|▌         | 110/2191 [01:31<28:44,  1.21it/s, loss=2.33, v_num=647]Epoch 23:   5%|▌         | 110/2191 [01:31<28:44,  1.21it/s, loss=2.29, v_num=647]Epoch 23:   5%|▌         | 110/2191 [01:46<33:20,  1.04it/s, loss=2.29, v_num=647]Epoch 23:   5%|▌         | 120/2191 [01:48<31:01,  1.11it/s, loss=2.29, v_num=647]Epoch 23:   5%|▌         | 120/2191 [01:48<31:01,  1.11it/s, loss=2.29, v_num=647]Epoch 23:   6%|▌         | 130/2191 [01:56<30:36,  1.12it/s, loss=2.29, v_num=647]Epoch 23:   6%|▌         | 130/2191 [01:56<30:36,  1.12it/s, loss=2.29, v_num=647]Epoch 23:   6%|▋         | 140/2191 [02:06<30:37,  1.12it/s, loss=2.29, v_num=647]Epoch 23:   6%|▋         | 140/2191 [02:06<30:37,  1.12it/s, loss=2.3, v_num=647] Epoch 23:   7%|▋         | 150/2191 [02:13<30:04,  1.13it/s, loss=2.3, v_num=647]Epoch 23:   7%|▋         | 150/2191 [02:13<30:04,  1.13it/s, loss=2.33, v_num=647]Epoch 23:   7%|▋         | 160/2191 [02:21<29:43,  1.14it/s, loss=2.33, v_num=647]Epoch 23:   7%|▋         | 160/2191 [02:21<29:43,  1.14it/s, loss=2.34, v_num=647]Epoch 23:   8%|▊         | 170/2191 [02:28<29:20,  1.15it/s, loss=2.34, v_num=647]Epoch 23:   8%|▊         | 170/2191 [02:28<29:20,  1.15it/s, loss=2.32, v_num=647]Epoch 23:   8%|▊         | 180/2191 [02:36<28:55,  1.16it/s, loss=2.32, v_num=647]Epoch 23:   8%|▊         | 180/2191 [02:36<28:55,  1.16it/s, loss=2.35, v_num=647]Epoch 23:   9%|▊         | 190/2191 [02:42<28:27,  1.17it/s, loss=2.35, v_num=647]Epoch 23:   9%|▊         | 190/2191 [02:42<28:27,  1.17it/s, loss=2.36, v_num=647]Epoch 23:   9%|▉         | 200/2191 [02:49<28:02,  1.18it/s, loss=2.36, v_num=647]Epoch 23:   9%|▉         | 200/2191 [02:49<28:02,  1.18it/s, loss=2.36, v_num=647]Epoch 23:  10%|▉         | 210/2191 [02:59<28:08,  1.17it/s, loss=2.36, v_num=647]Epoch 23:  10%|▉         | 210/2191 [02:59<28:08,  1.17it/s, loss=2.33, v_num=647]Epoch 23:  10%|█         | 220/2191 [03:07<27:53,  1.18it/s, loss=2.33, v_num=647]Epoch 23:  10%|█         | 220/2191 [03:07<27:53,  1.18it/s, loss=2.31, v_num=647]Epoch 23:  10%|█         | 230/2191 [03:15<27:43,  1.18it/s, loss=2.31, v_num=647]Epoch 23:  10%|█         | 230/2191 [03:15<27:43,  1.18it/s, loss=2.35, v_num=647]Epoch 23:  11%|█         | 240/2191 [03:23<27:27,  1.18it/s, loss=2.35, v_num=647]Epoch 23:  11%|█         | 240/2191 [03:23<27:27,  1.18it/s, loss=2.35, v_num=647]Epoch 23:  11%|█▏        | 250/2191 [03:31<27:16,  1.19it/s, loss=2.35, v_num=647]Epoch 23:  11%|█▏        | 250/2191 [03:31<27:16,  1.19it/s, loss=2.35, v_num=647]Epoch 23:  12%|█▏        | 260/2191 [03:39<27:03,  1.19it/s, loss=2.35, v_num=647]Epoch 23:  12%|█▏        | 260/2191 [03:39<27:03,  1.19it/s, loss=2.33, v_num=647]Epoch 23:  12%|█▏        | 270/2191 [03:47<26:49,  1.19it/s, loss=2.33, v_num=647]Epoch 23:  12%|█▏        | 270/2191 [03:47<26:49,  1.19it/s, loss=2.32, v_num=647]Epoch 23:  13%|█▎        | 280/2191 [03:54<26:36,  1.20it/s, loss=2.32, v_num=647]Epoch 23:  13%|█▎        | 280/2191 [03:54<26:36,  1.20it/s, loss=2.36, v_num=647]Epoch 23:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.36, v_num=647]Epoch 23:  13%|█▎        | 290/2191 [04:02<26:23,  1.20it/s, loss=2.33, v_num=647]Epoch 23:  14%|█▎        | 300/2191 [04:08<26:02,  1.21it/s, loss=2.33, v_num=647]Epoch 23:  14%|█▎        | 300/2191 [04:08<26:02,  1.21it/s, loss=2.29, v_num=647]Epoch 23:  14%|█▍        | 310/2191 [04:15<25:45,  1.22it/s, loss=2.29, v_num=647]Epoch 23:  14%|█▍        | 310/2191 [04:15<25:45,  1.22it/s, loss=2.29, v_num=647]Epoch 23:  15%|█▍        | 320/2191 [04:23<25:33,  1.22it/s, loss=2.29, v_num=647]Epoch 23:  15%|█▍        | 320/2191 [04:23<25:33,  1.22it/s, loss=2.32, v_num=647]Epoch 23:  15%|█▌        | 330/2191 [04:29<25:14,  1.23it/s, loss=2.32, v_num=647]Epoch 23:  15%|█▌        | 330/2191 [04:29<25:14,  1.23it/s, loss=2.31, v_num=647]Epoch 23:  16%|█▌        | 340/2191 [04:35<24:57,  1.24it/s, loss=2.31, v_num=647]Epoch 23:  16%|█▌        | 340/2191 [04:35<24:57,  1.24it/s, loss=2.29, v_num=647]Epoch 23:  16%|█▌        | 350/2191 [04:43<24:48,  1.24it/s, loss=2.29, v_num=647]Epoch 23:  16%|█▌        | 350/2191 [04:43<24:48,  1.24it/s, loss=2.31, v_num=647]Epoch 23:  16%|█▋        | 360/2191 [04:51<24:36,  1.24it/s, loss=2.31, v_num=647]Epoch 23:  16%|█▋        | 360/2191 [04:51<24:36,  1.24it/s, loss=2.34, v_num=647]Epoch 23:  17%|█▋        | 370/2191 [04:59<24:30,  1.24it/s, loss=2.34, v_num=647]Epoch 23:  17%|█▋        | 370/2191 [04:59<24:30,  1.24it/s, loss=2.35, v_num=647]Epoch 23:  17%|█▋        | 380/2191 [05:07<24:23,  1.24it/s, loss=2.35, v_num=647]Epoch 23:  17%|█▋        | 380/2191 [05:07<24:23,  1.24it/s, loss=2.32, v_num=647]Epoch 23:  18%|█▊        | 390/2191 [05:15<24:15,  1.24it/s, loss=2.32, v_num=647]Epoch 23:  18%|█▊        | 390/2191 [05:15<24:15,  1.24it/s, loss=2.28, v_num=647]Epoch 23:  18%|█▊        | 400/2191 [05:22<24:00,  1.24it/s, loss=2.28, v_num=647]Epoch 23:  18%|█▊        | 400/2191 [05:22<24:00,  1.24it/s, loss=2.31, v_num=647]Epoch 23:  19%|█▊        | 410/2191 [05:28<23:45,  1.25it/s, loss=2.31, v_num=647]Epoch 23:  19%|█▊        | 410/2191 [05:28<23:45,  1.25it/s, loss=2.39, v_num=647]Epoch 23:  19%|█▉        | 420/2191 [05:35<23:32,  1.25it/s, loss=2.39, v_num=647]Epoch 23:  19%|█▉        | 420/2191 [05:35<23:32,  1.25it/s, loss=2.38, v_num=647]Epoch 23:  20%|█▉        | 430/2191 [05:44<23:28,  1.25it/s, loss=2.38, v_num=647]Epoch 23:  20%|█▉        | 430/2191 [05:44<23:28,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  20%|██        | 440/2191 [05:53<23:22,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  20%|██        | 440/2191 [05:53<23:22,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  21%|██        | 450/2191 [06:00<23:10,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  21%|██        | 450/2191 [06:00<23:10,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  21%|██        | 460/2191 [06:07<22:59,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  21%|██        | 460/2191 [06:07<22:59,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  21%|██▏       | 470/2191 [06:15<22:50,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  21%|██▏       | 470/2191 [06:15<22:50,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  22%|██▏       | 480/2191 [06:23<22:45,  1.25it/s, loss=2.32, v_num=647]Epoch 23:  22%|██▏       | 480/2191 [06:23<22:45,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  22%|██▏       | 490/2191 [06:32<22:38,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  22%|██▏       | 490/2191 [06:32<22:38,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  23%|██▎       | 500/2191 [06:38<22:26,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  23%|██▎       | 500/2191 [06:38<22:26,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  23%|██▎       | 510/2191 [06:45<22:15,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  23%|██▎       | 510/2191 [06:45<22:15,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  24%|██▎       | 520/2191 [06:52<22:02,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  24%|██▎       | 520/2191 [06:52<22:02,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  24%|██▍       | 530/2191 [07:01<21:59,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  24%|██▍       | 530/2191 [07:01<21:59,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  25%|██▍       | 540/2191 [07:09<21:51,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  25%|██▍       | 540/2191 [07:09<21:51,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  25%|██▌       | 550/2191 [07:16<21:38,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  25%|██▌       | 550/2191 [07:16<21:38,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  26%|██▌       | 560/2191 [07:23<21:30,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  26%|██▌       | 560/2191 [07:23<21:30,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  26%|██▌       | 570/2191 [07:31<21:22,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  26%|██▌       | 570/2191 [07:31<21:22,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  26%|██▋       | 580/2191 [07:39<21:13,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  26%|██▋       | 580/2191 [07:39<21:13,  1.26it/s, loss=2.37, v_num=647]Epoch 23:  27%|██▋       | 590/2191 [07:46<21:04,  1.27it/s, loss=2.37, v_num=647]Epoch 23:  27%|██▋       | 590/2191 [07:46<21:04,  1.27it/s, loss=2.36, v_num=647]Epoch 23:  27%|██▋       | 600/2191 [07:54<20:56,  1.27it/s, loss=2.36, v_num=647]Epoch 23:  27%|██▋       | 600/2191 [07:54<20:56,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  28%|██▊       | 610/2191 [08:02<20:49,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  28%|██▊       | 610/2191 [08:02<20:49,  1.27it/s, loss=2.35, v_num=647]Epoch 23:  28%|██▊       | 620/2191 [08:11<20:43,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  28%|██▊       | 620/2191 [08:11<20:43,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  29%|██▉       | 630/2191 [08:18<20:34,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  29%|██▉       | 630/2191 [08:18<20:34,  1.26it/s, loss=2.3, v_num=647] Epoch 23:  29%|██▉       | 640/2191 [08:26<20:25,  1.27it/s, loss=2.3, v_num=647]Epoch 23:  29%|██▉       | 640/2191 [08:26<20:25,  1.27it/s, loss=2.3, v_num=647]Epoch 23:  30%|██▉       | 650/2191 [08:33<20:15,  1.27it/s, loss=2.3, v_num=647]Epoch 23:  30%|██▉       | 650/2191 [08:33<20:15,  1.27it/s, loss=2.29, v_num=647]Epoch 23:  30%|███       | 660/2191 [08:40<20:06,  1.27it/s, loss=2.29, v_num=647]Epoch 23:  30%|███       | 660/2191 [08:40<20:06,  1.27it/s, loss=2.3, v_num=647] Epoch 23:  31%|███       | 670/2191 [08:47<19:55,  1.27it/s, loss=2.3, v_num=647]Epoch 23:  31%|███       | 670/2191 [08:47<19:55,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  31%|███       | 680/2191 [08:55<19:49,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  31%|███       | 680/2191 [08:55<19:49,  1.27it/s, loss=2.29, v_num=647]Epoch 23:  31%|███▏      | 690/2191 [09:03<19:40,  1.27it/s, loss=2.29, v_num=647]Epoch 23:  31%|███▏      | 690/2191 [09:03<19:40,  1.27it/s, loss=2.3, v_num=647] Epoch 23:  32%|███▏      | 700/2191 [09:09<19:28,  1.28it/s, loss=2.3, v_num=647]Epoch 23:  32%|███▏      | 700/2191 [09:09<19:28,  1.28it/s, loss=2.34, v_num=647]Epoch 23:  32%|███▏      | 710/2191 [09:16<19:19,  1.28it/s, loss=2.34, v_num=647]Epoch 23:  32%|███▏      | 710/2191 [09:16<19:19,  1.28it/s, loss=2.34, v_num=647]Epoch 23:  33%|███▎      | 720/2191 [09:25<19:12,  1.28it/s, loss=2.34, v_num=647]Epoch 23:  33%|███▎      | 720/2191 [09:25<19:12,  1.28it/s, loss=2.34, v_num=647]Epoch 23:  33%|███▎      | 730/2191 [09:34<19:07,  1.27it/s, loss=2.34, v_num=647]Epoch 23:  33%|███▎      | 730/2191 [09:34<19:07,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  34%|███▍      | 740/2191 [09:42<18:59,  1.27it/s, loss=2.32, v_num=647]Epoch 23:  34%|███▍      | 740/2191 [09:42<18:59,  1.27it/s, loss=2.34, v_num=647]Epoch 23:  34%|███▍      | 750/2191 [09:51<18:55,  1.27it/s, loss=2.34, v_num=647]Epoch 23:  34%|███▍      | 750/2191 [09:51<18:55,  1.27it/s, loss=2.36, v_num=647]Epoch 23:  35%|███▍      | 760/2191 [10:01<18:51,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  35%|███▍      | 760/2191 [10:01<18:51,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  35%|███▌      | 770/2191 [10:10<18:45,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  35%|███▌      | 770/2191 [10:10<18:45,  1.26it/s, loss=2.3, v_num=647] Epoch 23:  36%|███▌      | 780/2191 [10:18<18:37,  1.26it/s, loss=2.3, v_num=647]Epoch 23:  36%|███▌      | 780/2191 [10:18<18:37,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  36%|███▌      | 790/2191 [10:25<18:27,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  36%|███▌      | 790/2191 [10:25<18:27,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  37%|███▋      | 800/2191 [10:32<18:18,  1.27it/s, loss=2.35, v_num=647]Epoch 23:  37%|███▋      | 800/2191 [10:32<18:18,  1.27it/s, loss=2.36, v_num=647]Epoch 23:  37%|███▋      | 810/2191 [10:41<18:12,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  37%|███▋      | 810/2191 [10:41<18:12,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  37%|███▋      | 820/2191 [10:48<18:03,  1.27it/s, loss=2.34, v_num=647]Epoch 23:  37%|███▋      | 820/2191 [10:48<18:03,  1.27it/s, loss=2.3, v_num=647] Epoch 23:  38%|███▊      | 830/2191 [10:57<17:57,  1.26it/s, loss=2.3, v_num=647]Epoch 23:  38%|███▊      | 830/2191 [10:57<17:57,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  38%|███▊      | 840/2191 [11:07<17:52,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  38%|███▊      | 840/2191 [11:07<17:52,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  39%|███▉      | 850/2191 [11:16<17:46,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  39%|███▉      | 850/2191 [11:16<17:46,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  39%|███▉      | 860/2191 [11:23<17:37,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  39%|███▉      | 860/2191 [11:23<17:37,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  40%|███▉      | 870/2191 [11:31<17:28,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  40%|███▉      | 870/2191 [11:31<17:28,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  40%|████      | 880/2191 [11:38<17:19,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  40%|████      | 880/2191 [11:38<17:19,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  41%|████      | 890/2191 [11:47<17:12,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  41%|████      | 890/2191 [11:47<17:12,  1.26it/s, loss=2.29, v_num=647]Epoch 23:  41%|████      | 900/2191 [11:55<17:04,  1.26it/s, loss=2.29, v_num=647]Epoch 23:  41%|████      | 900/2191 [11:55<17:04,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  42%|████▏     | 910/2191 [12:04<16:58,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  42%|████▏     | 910/2191 [12:04<16:58,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  42%|████▏     | 920/2191 [12:12<16:51,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  42%|████▏     | 920/2191 [12:12<16:51,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  42%|████▏     | 930/2191 [12:22<16:45,  1.25it/s, loss=2.32, v_num=647]Epoch 23:  42%|████▏     | 930/2191 [12:22<16:45,  1.25it/s, loss=2.31, v_num=647]Epoch 23:  43%|████▎     | 940/2191 [12:29<16:36,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  43%|████▎     | 940/2191 [12:29<16:36,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  43%|████▎     | 950/2191 [12:36<16:26,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  43%|████▎     | 950/2191 [12:36<16:26,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  44%|████▍     | 960/2191 [12:44<16:19,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  44%|████▍     | 960/2191 [12:44<16:19,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  44%|████▍     | 970/2191 [12:51<16:10,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  44%|████▍     | 970/2191 [12:51<16:10,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  45%|████▍     | 980/2191 [13:00<16:03,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  45%|████▍     | 980/2191 [13:00<16:03,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  45%|████▌     | 990/2191 [13:11<15:58,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  45%|████▌     | 990/2191 [13:11<15:58,  1.25it/s, loss=2.38, v_num=647]Epoch 23:  46%|████▌     | 1000/2191 [13:19<15:51,  1.25it/s, loss=2.38, v_num=647]Epoch 23:  46%|████▌     | 1000/2191 [13:19<15:51,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  46%|████▌     | 1010/2191 [13:28<15:43,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  46%|████▌     | 1010/2191 [13:28<15:43,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  47%|████▋     | 1020/2191 [13:37<15:37,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  47%|████▋     | 1020/2191 [13:37<15:37,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  47%|████▋     | 1030/2191 [13:44<15:28,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  47%|████▋     | 1030/2191 [13:44<15:28,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  47%|████▋     | 1040/2191 [13:52<15:20,  1.25it/s, loss=2.36, v_num=647]Epoch 23:  47%|████▋     | 1040/2191 [13:52<15:20,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  48%|████▊     | 1050/2191 [14:00<15:12,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  48%|████▊     | 1050/2191 [14:00<15:12,  1.25it/s, loss=2.29, v_num=647]Epoch 23:  48%|████▊     | 1060/2191 [14:08<15:03,  1.25it/s, loss=2.29, v_num=647]Epoch 23:  48%|████▊     | 1060/2191 [14:08<15:03,  1.25it/s, loss=2.3, v_num=647] Epoch 23:  49%|████▉     | 1070/2191 [14:15<14:55,  1.25it/s, loss=2.3, v_num=647]Epoch 23:  49%|████▉     | 1070/2191 [14:15<14:55,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  49%|████▉     | 1080/2191 [14:24<14:48,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  49%|████▉     | 1080/2191 [14:24<14:48,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  50%|████▉     | 1090/2191 [14:32<14:40,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  50%|████▉     | 1090/2191 [14:32<14:40,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  50%|█████     | 1100/2191 [14:38<14:30,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  50%|█████     | 1100/2191 [14:38<14:30,  1.25it/s, loss=2.32, v_num=647]Epoch 23:  51%|█████     | 1110/2191 [14:46<14:22,  1.25it/s, loss=2.32, v_num=647]Epoch 23:  51%|█████     | 1110/2191 [14:46<14:22,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  51%|█████     | 1120/2191 [14:55<14:15,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  51%|█████     | 1120/2191 [14:55<14:15,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  52%|█████▏    | 1130/2191 [15:03<14:07,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  52%|█████▏    | 1130/2191 [15:03<14:07,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  52%|█████▏    | 1140/2191 [15:10<13:58,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  52%|█████▏    | 1140/2191 [15:10<13:58,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  52%|█████▏    | 1150/2191 [15:18<13:50,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  52%|█████▏    | 1150/2191 [15:18<13:50,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  53%|█████▎    | 1160/2191 [15:25<13:41,  1.25it/s, loss=2.34, v_num=647]Epoch 23:  53%|█████▎    | 1160/2191 [15:25<13:41,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  53%|█████▎    | 1170/2191 [15:34<13:35,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  53%|█████▎    | 1170/2191 [15:34<13:35,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  54%|█████▍    | 1180/2191 [15:41<13:26,  1.25it/s, loss=2.35, v_num=647]Epoch 23:  54%|█████▍    | 1180/2191 [15:41<13:26,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  54%|█████▍    | 1190/2191 [15:49<13:17,  1.25it/s, loss=2.33, v_num=647]Epoch 23:  54%|█████▍    | 1190/2191 [15:49<13:17,  1.25it/s, loss=2.31, v_num=647]Epoch 23:  55%|█████▍    | 1200/2191 [15:56<13:09,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  55%|█████▍    | 1200/2191 [15:56<13:09,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  55%|█████▌    | 1210/2191 [16:03<13:00,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  55%|█████▌    | 1210/2191 [16:03<13:00,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  56%|█████▌    | 1220/2191 [16:11<12:52,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  56%|█████▌    | 1220/2191 [16:11<12:52,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  56%|█████▌    | 1230/2191 [16:18<12:44,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  56%|█████▌    | 1230/2191 [16:18<12:44,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  57%|█████▋    | 1240/2191 [16:26<12:35,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  57%|█████▋    | 1240/2191 [16:26<12:35,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  57%|█████▋    | 1250/2191 [16:33<12:27,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  57%|█████▋    | 1250/2191 [16:33<12:27,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  58%|█████▊    | 1260/2191 [16:39<12:18,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  58%|█████▊    | 1260/2191 [16:39<12:18,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  58%|█████▊    | 1270/2191 [16:47<12:10,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  58%|█████▊    | 1270/2191 [16:47<12:10,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  58%|█████▊    | 1280/2191 [16:55<12:02,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  58%|█████▊    | 1280/2191 [16:55<12:02,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  59%|█████▉    | 1290/2191 [17:05<11:55,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  59%|█████▉    | 1290/2191 [17:05<11:55,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  59%|█████▉    | 1300/2191 [17:12<11:47,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  59%|█████▉    | 1300/2191 [17:12<11:47,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  60%|█████▉    | 1310/2191 [17:19<11:38,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  60%|█████▉    | 1310/2191 [17:19<11:38,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  60%|██████    | 1320/2191 [17:27<11:30,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  60%|██████    | 1320/2191 [17:27<11:30,  1.26it/s, loss=2.3, v_num=647] Epoch 23:  61%|██████    | 1330/2191 [17:34<11:22,  1.26it/s, loss=2.3, v_num=647]Epoch 23:  61%|██████    | 1330/2191 [17:34<11:22,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  61%|██████    | 1340/2191 [17:43<11:14,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  61%|██████    | 1340/2191 [17:43<11:14,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  62%|██████▏   | 1350/2191 [17:50<11:06,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  62%|██████▏   | 1350/2191 [17:50<11:06,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  62%|██████▏   | 1360/2191 [17:58<10:58,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  62%|██████▏   | 1360/2191 [17:58<10:58,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  63%|██████▎   | 1370/2191 [18:06<10:50,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  63%|██████▎   | 1370/2191 [18:06<10:50,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  63%|██████▎   | 1380/2191 [18:14<10:42,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  63%|██████▎   | 1380/2191 [18:14<10:42,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  63%|██████▎   | 1390/2191 [18:22<10:35,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  63%|██████▎   | 1390/2191 [18:22<10:35,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  64%|██████▍   | 1400/2191 [18:30<10:26,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  64%|██████▍   | 1400/2191 [18:30<10:26,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  64%|██████▍   | 1410/2191 [18:37<10:18,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  64%|██████▍   | 1410/2191 [18:37<10:18,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  65%|██████▍   | 1420/2191 [18:44<10:10,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  65%|██████▍   | 1420/2191 [18:44<10:10,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  65%|██████▌   | 1430/2191 [18:52<10:02,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  65%|██████▌   | 1430/2191 [18:52<10:02,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  66%|██████▌   | 1440/2191 [18:59<09:53,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  66%|██████▌   | 1440/2191 [18:59<09:53,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  66%|██████▌   | 1450/2191 [19:08<09:46,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  66%|██████▌   | 1450/2191 [19:08<09:46,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  67%|██████▋   | 1460/2191 [19:17<09:38,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  67%|██████▋   | 1460/2191 [19:17<09:38,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  67%|██████▋   | 1470/2191 [19:25<09:31,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  67%|██████▋   | 1470/2191 [19:25<09:31,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  68%|██████▊   | 1480/2191 [19:33<09:23,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  68%|██████▊   | 1480/2191 [19:33<09:23,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  68%|██████▊   | 1490/2191 [19:41<09:15,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  68%|██████▊   | 1490/2191 [19:41<09:15,  1.26it/s, loss=2.27, v_num=647]Epoch 23:  68%|██████▊   | 1500/2191 [19:49<09:07,  1.26it/s, loss=2.27, v_num=647]Epoch 23:  68%|██████▊   | 1500/2191 [19:49<09:07,  1.26it/s, loss=2.29, v_num=647]Epoch 23:  69%|██████▉   | 1510/2191 [19:58<09:00,  1.26it/s, loss=2.29, v_num=647]Epoch 23:  69%|██████▉   | 1510/2191 [19:58<09:00,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  69%|██████▉   | 1520/2191 [20:06<08:52,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  69%|██████▉   | 1520/2191 [20:06<08:52,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  70%|██████▉   | 1530/2191 [20:13<08:43,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  70%|██████▉   | 1530/2191 [20:13<08:43,  1.26it/s, loss=2.28, v_num=647]Epoch 23:  70%|███████   | 1540/2191 [20:20<08:35,  1.26it/s, loss=2.28, v_num=647]Epoch 23:  70%|███████   | 1540/2191 [20:20<08:35,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  71%|███████   | 1550/2191 [20:28<08:27,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  71%|███████   | 1550/2191 [20:28<08:27,  1.26it/s, loss=2.4, v_num=647] Epoch 23:  71%|███████   | 1560/2191 [20:37<08:20,  1.26it/s, loss=2.4, v_num=647]Epoch 23:  71%|███████   | 1560/2191 [20:37<08:20,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  72%|███████▏  | 1570/2191 [20:45<08:12,  1.26it/s, loss=2.38, v_num=647]Epoch 23:  72%|███████▏  | 1570/2191 [20:45<08:12,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  72%|███████▏  | 1580/2191 [20:53<08:04,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  72%|███████▏  | 1580/2191 [20:53<08:04,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  73%|███████▎  | 1590/2191 [21:02<07:56,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  73%|███████▎  | 1590/2191 [21:02<07:56,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  73%|███████▎  | 1600/2191 [21:10<07:49,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  73%|███████▎  | 1600/2191 [21:10<07:49,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  73%|███████▎  | 1610/2191 [21:19<07:41,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  73%|███████▎  | 1610/2191 [21:19<07:41,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  74%|███████▍  | 1620/2191 [21:27<07:33,  1.26it/s, loss=2.34, v_num=647]Epoch 23:  74%|███████▍  | 1620/2191 [21:27<07:33,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  74%|███████▍  | 1630/2191 [21:35<07:25,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  74%|███████▍  | 1630/2191 [21:35<07:25,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  75%|███████▍  | 1640/2191 [21:42<07:17,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  75%|███████▍  | 1640/2191 [21:42<07:17,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  75%|███████▌  | 1650/2191 [21:49<07:09,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  75%|███████▌  | 1650/2191 [21:49<07:09,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  76%|███████▌  | 1660/2191 [21:57<07:01,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  76%|███████▌  | 1660/2191 [21:57<07:01,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  76%|███████▌  | 1670/2191 [22:07<06:53,  1.26it/s, loss=2.32, v_num=647]Epoch 23:  76%|███████▌  | 1670/2191 [22:07<06:53,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  77%|███████▋  | 1680/2191 [22:14<06:45,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  77%|███████▋  | 1680/2191 [22:14<06:45,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  77%|███████▋  | 1690/2191 [22:24<06:38,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  77%|███████▋  | 1690/2191 [22:24<06:38,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  78%|███████▊  | 1700/2191 [22:32<06:30,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  78%|███████▊  | 1700/2191 [22:32<06:30,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  78%|███████▊  | 1710/2191 [22:40<06:22,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  78%|███████▊  | 1710/2191 [22:40<06:22,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▊  | 1720/2191 [22:48<06:14,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▊  | 1720/2191 [22:48<06:14,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▉  | 1730/2191 [22:56<06:06,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▉  | 1730/2191 [22:56<06:06,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▉  | 1740/2191 [23:03<05:58,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  79%|███████▉  | 1740/2191 [23:03<05:58,  1.26it/s, loss=2.3, v_num=647] Epoch 23:  80%|███████▉  | 1750/2191 [23:10<05:50,  1.26it/s, loss=2.3, v_num=647]Epoch 23:  80%|███████▉  | 1750/2191 [23:10<05:50,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  80%|████████  | 1760/2191 [23:18<05:42,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  80%|████████  | 1760/2191 [23:18<05:42,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  81%|████████  | 1770/2191 [23:25<05:34,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  81%|████████  | 1770/2191 [23:25<05:34,  1.26it/s, loss=2.3, v_num=647] Epoch 23:  81%|████████  | 1780/2191 [23:32<05:26,  1.26it/s, loss=2.3, v_num=647]Epoch 23:  81%|████████  | 1780/2191 [23:32<05:26,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  82%|████████▏ | 1790/2191 [23:41<05:18,  1.26it/s, loss=2.36, v_num=647]Epoch 23:  82%|████████▏ | 1790/2191 [23:41<05:18,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  82%|████████▏ | 1800/2191 [23:50<05:10,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  82%|████████▏ | 1800/2191 [23:50<05:10,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  83%|████████▎ | 1810/2191 [23:58<05:02,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  83%|████████▎ | 1810/2191 [23:58<05:02,  1.26it/s, loss=2.37, v_num=647]Epoch 23:  83%|████████▎ | 1820/2191 [24:06<04:54,  1.26it/s, loss=2.37, v_num=647]Epoch 23:  83%|████████▎ | 1820/2191 [24:06<04:54,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  84%|████████▎ | 1830/2191 [24:14<04:46,  1.26it/s, loss=2.35, v_num=647]Epoch 23:  84%|████████▎ | 1830/2191 [24:14<04:46,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  84%|████████▍ | 1840/2191 [24:23<04:38,  1.26it/s, loss=2.33, v_num=647]Epoch 23:  84%|████████▍ | 1840/2191 [24:23<04:38,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  84%|████████▍ | 1850/2191 [24:29<04:30,  1.26it/s, loss=2.31, v_num=647]Epoch 23:  84%|████████▍ | 1850/2191 [24:29<04:30,  1.26it/s, loss=2.29, v_num=647]validation_epoch_end
graph acc: 0.36741214057507987
valid accuracy: 0.9765775799751282
validation_epoch_end
graph acc: 0.3993610223642173
valid accuracy: 0.9754302501678467
validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9756140112876892
[24:37<04:13,  1.27it/s, loss=2.33, v_num=647]Epoch 23:  85%|████████▌ | 1870/2191 [24:37<04:13,  1.27it/s, loss=2.34, v_num=647]Epoch 23:  86%|████████▌ | 1880/2191 [24:39<04:04,  1.27it/s, loss=2.34, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9752353429794312
validation_epoch_end
graph acc: 0.33865814696485624
valid accuracy: 0.9771792888641357
validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9777141809463501

Validating:   3%|▎         | 10/313 [00:01<00:50,  6.04it/s][AEpoch 23:  86%|████████▋ | 1890/2191 [24:41<03:55,  1.28it/s, loss=2.34, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:46,  6.25it/s][AEpoch 23:  87%|████████▋ | 1900/2191 [24:42<03:46,  1.28it/s, loss=2.34, v_num=647]
Validating:  10%|▉         | 30/313 [00:03<00:33,  8.56it/s][AEpoch 23:  87%|████████▋ | 1910/2191 [24:43<03:38,  1.29it/s, loss=2.34, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:26, 10.30it/s][AEpoch 23:  88%|████████▊ | 1920/2191 [24:43<03:29,  1.29it/s, loss=2.34, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.29it/s][AEpoch 23:  88%|████████▊ | 1930/2191 [24:44<03:20,  1.30it/s, loss=2.34, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.84it/s][AEpoch 23:  89%|████████▊ | 1940/2191 [24:45<03:12,  1.31it/s, loss=2.34, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24, 10.10it/s][AEpoch 23:  89%|████████▉ | 1950/2191 [24:46<03:03,  1.31it/s, loss=2.34, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:21, 10.60it/s][AEpoch 23:  89%|████████▉ | 1960/2191 [24:47<02:55,  1.32it/s, loss=2.34, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.33it/s][AEpoch 23:  90%|████████▉ | 1970/2191 [24:48<02:46,  1.32it/s, loss=2.34, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:18, 11.21it/s][AEpoch 23:  90%|█████████ | 1980/2191 [24:49<02:38,  1.33it/s, loss=2.34, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.78it/s][AEpoch 23:  91%|█████████ | 1990/2191 [24:50<02:30,  1.34it/s, loss=2.34, v_num=647]
Validating:  38%|███▊      | 120/313 [00:11<00:17, 11.18it/s][AEpoch 23:  91%|█████████▏| 2000/2191 [24:51<02:22,  1.34it/s, loss=2.34, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.41it/s][AEpoch 23:  92%|█████████▏| 2010/2191 [24:52<02:14,  1.35it/s, loss=2.34, v_num=647]
Validating:  45%|████▍     | 140/313 [00:14<00:20,  8.55it/s][AEpoch 23:  92%|█████████▏| 2020/2191 [24:54<02:06,  1.35it/s, loss=2.34, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:17,  9.28it/s][AEpoch 23:  93%|█████████▎| 2030/2191 [24:55<01:58,  1.36it/s, loss=2.34, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15,  9.96it/s][AEpoch 23:  93%|█████████▎| 2040/2191 [24:55<01:50,  1.36it/s, loss=2.34, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:12, 11.11it/s][AEpoch 23:  94%|█████████▎| 2050/2191 [24:56<01:42,  1.37it/s, loss=2.34, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.42it/s][AEpoch 23:  94%|█████████▍| 2060/2191 [24:57<01:35,  1.38it/s, loss=2.34, v_num=647]
Validating:  61%|██████    | 190/313 [00:18<00:10, 11.81it/s][AEpoch 23:  94%|█████████▍| 2070/2191 [24:58<01:27,  1.38it/s, loss=2.34, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:19<00:09, 11.58it/s][AEpoch 23:  95%|█████████▍| 2080/2191 [24:59<01:19,  1.39it/s, loss=2.34, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:20<00:09, 10.81it/s][AEpoch 23:  95%|█████████▌| 2090/2191 [25:00<01:12,  1.39it/s, loss=2.34, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.51it/s][AEpoch 23:  96%|█████████▌| 2100/2191 [25:01<01:05,  1.40it/s, loss=2.34, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:22<00:08, 10.31it/s][AEpoch 23:  96%|█████████▋| 2110/2191 [25:02<00:57,  1.41it/s, loss=2.34, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.60it/s][AEpoch 23:  97%|█████████▋| 2120/2191 [25:03<00:50,  1.41it/s, loss=2.34, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.18it/s][AEpoch 23:  97%|█████████▋| 2130/2191 [25:04<00:43,  1.42it/s, loss=2.34, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.57it/s][AEpoch 23:  98%|█████████▊| 2140/2191 [25:05<00:35,  1.42it/s, loss=2.34, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 10.81it/s][AEpoch 23:  98%|█████████▊| 2150/2191 [25:06<00:28,  1.43it/s, loss=2.34, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.16it/s][AEpoch 23:  99%|█████████▊| 2160/2191 [25:06<00:21,  1.43it/s, loss=2.34, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:02, 11.48it/s][AEpoch 23:  99%|█████████▉| 2170/2191 [25:07<00:14,  1.44it/s, loss=2.34, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:28<00:00, 13.12it/s][AEpoch 23:  99%|█████████▉| 2180/2191 [25:08<00:07,  1.45it/s, loss=2.34, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.56it/s][AEpoch 23: 100%|█████████▉| 2190/2191 [25:09<00:00,  1.45it/s, loss=2.34, v_num=647]validation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9770612716674805
Epoch 23: 100%|██████████| 2191/2191 [25:11<00:00,  1.45it/s, loss=2.32, v_num=647]
                                                             [AEpoch 23:   0%|          | 0/2191 [00:00<00:00, 16256.99it/s, loss=2.32, v_num=647]Epoch 24:   0%|          | 0/2191 [00:00<00:00, 4185.93it/s, loss=2.32, v_num=647] Epoch 24:   0%|          | 10/2191 [00:13<45:51,  1.26s/it, loss=2.32, v_num=647] Epoch 24:   0%|          | 10/2191 [00:13<45:51,  1.26s/it, loss=2.32, v_num=647]Epoch 24:   1%|          | 20/2191 [00:22<38:36,  1.07s/it, loss=2.32, v_num=647]Epoch 24:   1%|          | 20/2191 [00:22<38:36,  1.07s/it, loss=2.31, v_num=647]Epoch 24:   1%|▏         | 30/2191 [00:29<34:38,  1.04it/s, loss=2.31, v_num=647]Epoch 24:   1%|▏         | 30/2191 [00:29<34:38,  1.04it/s, loss=2.29, v_num=647]Epoch 24:   2%|▏         | 40/2191 [00:38<33:30,  1.07it/s, loss=2.29, v_num=647]Epoch 24:   2%|▏         | 40/2191 [00:38<33:30,  1.07it/s, loss=2.3, v_num=647] Epoch 24:   2%|▏         | 50/2191 [00:46<32:20,  1.10it/s, loss=2.3, v_num=647]Epoch 24:   2%|▏         | 50/2191 [00:46<32:20,  1.10it/s, loss=2.31, v_num=647]Epoch 24:   3%|▎         | 60/2191 [00:54<31:26,  1.13it/s, loss=2.31, v_num=647]Epoch 24:   3%|▎         | 60/2191 [00:54<31:26,  1.13it/s, loss=2.27, v_num=647]Epoch 24:   3%|▎         | 70/2191 [01:01<30:50,  1.15it/s, loss=2.27, v_num=647]Epoch 24:   3%|▎         | 70/2191 [01:01<30:50,  1.15it/s, loss=2.27, v_num=647]Epoch 24:   4%|▎         | 80/2191 [01:09<30:06,  1.17it/s, loss=2.27, v_num=647]Epoch 24:   4%|▎         | 80/2191 [01:09<30:06,  1.17it/s, loss=2.31, v_num=647]Epoch 24:   4%|▍         | 90/2191 [01:17<29:44,  1.18it/s, loss=2.31, v_num=647]Epoch 24:   4%|▍         | 90/2191 [01:17<29:44,  1.18it/s, loss=2.34, v_num=647]Epoch 24:   5%|▍         | 100/2191 [01:27<30:09,  1.16it/s, loss=2.34, v_num=647]Epoch 24:   5%|▍         | 100/2191 [01:27<30:09,  1.16it/s, loss=2.31, v_num=647]Epoch 24:   5%|▌         | 110/2191 [01:35<29:46,  1.16it/s, loss=2.31, v_num=647]Epoch 24:   5%|▌         | 110/2191 [01:35<29:46,  1.16it/s, loss=2.26, v_num=647]Epoch 24:   5%|▌         | 120/2191 [01:44<29:41,  1.16it/s, loss=2.26, v_num=647]Epoch 24:   5%|▌         | 120/2191 [01:44<29:41,  1.16it/s, loss=2.27, v_num=647]Epoch 24:   6%|▌         | 130/2191 [01:54<29:53,  1.15it/s, loss=2.27, v_num=647]Epoch 24:   6%|▌         | 130/2191 [01:54<29:53,  1.15it/s, loss=2.29, v_num=647]Epoch 24:   6%|▋         | 140/2191 [02:02<29:35,  1.15it/s, loss=2.29, v_num=647]Epoch 24:   6%|▋         | 140/2191 [02:02<29:35,  1.15it/s, loss=2.28, v_num=647]Epoch 24:   7%|▋         | 150/2191 [02:12<29:45,  1.14it/s, loss=2.28, v_num=647]Epoch 24:   7%|▋         | 150/2191 [02:12<29:45,  1.14it/s, loss=2.26, v_num=647]Epoch 24:   7%|▋         | 160/2191 [02:22<29:58,  1.13it/s, loss=2.26, v_num=647]Epoch 24:   7%|▋         | 160/2191 [02:22<29:58,  1.13it/s, loss=2.28, v_num=647]Epoch 24:   8%|▊         | 170/2191 [02:32<30:05,  1.12it/s, loss=2.28, v_num=647]Epoch 24:   8%|▊         | 170/2191 [02:32<30:05,  1.12it/s, loss=2.31, v_num=647]Epoch 24:   8%|▊         | 180/2191 [02:41<29:59,  1.12it/s, loss=2.31, v_num=647]Epoch 24:   8%|▊         | 180/2191 [02:41<29:59,  1.12it/s, loss=2.33, v_num=647]Epoch 24:   9%|▊         | 190/2191 [02:49<29:39,  1.12it/s, loss=2.33, v_num=647]Epoch 24:   9%|▊         | 190/2191 [02:49<29:39,  1.12it/s, loss=2.32, v_num=647]Epoch 24:   9%|▉         | 200/2191 [02:57<29:18,  1.13it/s, loss=2.32, v_num=647]Epoch 24:   9%|▉         | 200/2191 [02:57<29:18,  1.13it/s, loss=2.3, v_num=647] Epoch 24:  10%|▉         | 210/2191 [03:06<29:12,  1.13it/s, loss=2.3, v_num=647]Epoch 24:  10%|▉         | 210/2191 [03:06<29:12,  1.13it/s, loss=2.36, v_num=647]Epoch 24:  10%|█         | 220/2191 [03:13<28:46,  1.14it/s, loss=2.36, v_num=647]Epoch 24:  10%|█         | 220/2191 [03:13<28:46,  1.14it/s, loss=2.35, v_num=647]Epoch 24:  10%|█         | 230/2191 [03:21<28:32,  1.15it/s, loss=2.35, v_num=647]Epoch 24:  10%|█         | 230/2191 [03:21<28:32,  1.15it/s, loss=2.3, v_num=647] Epoch 24:  11%|█         | 240/2191 [03:29<28:13,  1.15it/s, loss=2.3, v_num=647]Epoch 24:  11%|█         | 240/2191 [03:29<28:13,  1.15it/s, loss=2.27, v_num=647]Epoch 24:  11%|█▏        | 250/2191 [03:36<27:51,  1.16it/s, loss=2.27, v_num=647]Epoch 24:  11%|█▏        | 250/2191 [03:36<27:51,  1.16it/s, loss=2.26, v_num=647]Epoch 24:  12%|█▏        | 260/2191 [03:44<27:40,  1.16it/s, loss=2.26, v_num=647]Epoch 24:  12%|█▏        | 260/2191 [03:44<27:40,  1.16it/s, loss=2.3, v_num=647] Epoch 24:  12%|█▏        | 270/2191 [03:51<27:20,  1.17it/s, loss=2.3, v_num=647]Epoch 24:  12%|█▏        | 270/2191 [03:51<27:20,  1.17it/s, loss=2.34, v_num=647]Epoch 24:  13%|█▎        | 280/2191 [03:57<26:58,  1.18it/s, loss=2.34, v_num=647]Epoch 24:  13%|█▎        | 280/2191 [03:57<26:58,  1.18it/s, loss=2.33, v_num=647]Epoch 24:  13%|█▎        | 290/2191 [04:05<26:44,  1.18it/s, loss=2.33, v_num=647]Epoch 24:  13%|█▎        | 290/2191 [04:05<26:44,  1.18it/s, loss=2.31, v_num=647]Epoch 24:  14%|█▎        | 300/2191 [04:14<26:38,  1.18it/s, loss=2.31, v_num=647]Epoch 24:  14%|█▎        | 300/2191 [04:14<26:38,  1.18it/s, loss=2.34, v_num=647]Epoch 24:  14%|█▍        | 310/2191 [04:23<26:36,  1.18it/s, loss=2.34, v_num=647]Epoch 24:  14%|█▍        | 310/2191 [04:23<26:36,  1.18it/s, loss=2.35, v_num=647]Epoch 24:  15%|█▍        | 320/2191 [04:32<26:26,  1.18it/s, loss=2.35, v_num=647]Epoch 24:  15%|█▍        | 320/2191 [04:32<26:26,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  15%|█▌        | 330/2191 [04:41<26:20,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  15%|█▌        | 330/2191 [04:41<26:20,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  16%|█▌        | 340/2191 [04:48<26:04,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  16%|█▌        | 340/2191 [04:48<26:04,  1.18it/s, loss=2.33, v_num=647]Epoch 24:  16%|█▌        | 350/2191 [04:55<25:50,  1.19it/s, loss=2.33, v_num=647]Epoch 24:  16%|█▌        | 350/2191 [04:55<25:50,  1.19it/s, loss=2.36, v_num=647]Epoch 24:  16%|█▋        | 360/2191 [05:04<25:44,  1.19it/s, loss=2.36, v_num=647]Epoch 24:  16%|█▋        | 360/2191 [05:04<25:44,  1.19it/s, loss=2.36, v_num=647]Epoch 24:  17%|█▋        | 370/2191 [05:12<25:33,  1.19it/s, loss=2.36, v_num=647]Epoch 24:  17%|█▋        | 370/2191 [05:12<25:33,  1.19it/s, loss=2.3, v_num=647] Epoch 24:  17%|█▋        | 380/2191 [05:21<25:27,  1.19it/s, loss=2.3, v_num=647]Epoch 24:  17%|█▋        | 380/2191 [05:21<25:27,  1.19it/s, loss=2.3, v_num=647]Epoch 24:  18%|█▊        | 390/2191 [05:31<25:25,  1.18it/s, loss=2.3, v_num=647]Epoch 24:  18%|█▊        | 390/2191 [05:31<25:25,  1.18it/s, loss=2.33, v_num=647]Epoch 24:  18%|█▊        | 400/2191 [05:37<25:09,  1.19it/s, loss=2.33, v_num=647]Epoch 24:  18%|█▊        | 400/2191 [05:37<25:09,  1.19it/s, loss=2.32, v_num=647]Epoch 24:  19%|█▊        | 410/2191 [05:46<25:03,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  19%|█▊        | 410/2191 [05:46<25:03,  1.18it/s, loss=2.31, v_num=647]Epoch 24:  19%|█▉        | 420/2191 [05:54<24:52,  1.19it/s, loss=2.31, v_num=647]Epoch 24:  19%|█▉        | 420/2191 [05:54<24:52,  1.19it/s, loss=2.32, v_num=647]Epoch 24:  20%|█▉        | 430/2191 [06:02<24:40,  1.19it/s, loss=2.32, v_num=647]Epoch 24:  20%|█▉        | 430/2191 [06:02<24:40,  1.19it/s, loss=2.33, v_num=647]Epoch 24:  20%|██        | 440/2191 [06:12<24:40,  1.18it/s, loss=2.33, v_num=647]Epoch 24:  20%|██        | 440/2191 [06:12<24:40,  1.18it/s, loss=2.32, v_num=647]Epoch 24:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.32, v_num=647]Epoch 24:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.35, v_num=647]Epoch 24:  21%|██        | 460/2191 [06:28<24:17,  1.19it/s, loss=2.35, v_num=647]Epoch 24:  21%|██        | 460/2191 [06:28<24:17,  1.19it/s, loss=2.38, v_num=647]Epoch 24:  21%|██▏       | 470/2191 [06:35<24:03,  1.19it/s, loss=2.38, v_num=647]Epoch 24:  21%|██▏       | 470/2191 [06:35<24:03,  1.19it/s, loss=2.32, v_num=647]Epoch 24:  22%|██▏       | 480/2191 [06:41<23:49,  1.20it/s, loss=2.32, v_num=647]Epoch 24:  22%|██▏       | 480/2191 [06:41<23:49,  1.20it/s, loss=2.27, v_num=647]Epoch 24:  22%|██▏       | 490/2191 [06:48<23:36,  1.20it/s, loss=2.27, v_num=647]Epoch 24:  22%|██▏       | 490/2191 [06:48<23:36,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  23%|██▎       | 500/2191 [06:56<23:24,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  23%|██▎       | 500/2191 [06:56<23:24,  1.20it/s, loss=2.34, v_num=647]Epoch 24:  23%|██▎       | 510/2191 [07:06<23:21,  1.20it/s, loss=2.34, v_num=647]Epoch 24:  23%|██▎       | 510/2191 [07:06<23:21,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  24%|██▎       | 520/2191 [07:13<23:10,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  24%|██▎       | 520/2191 [07:13<23:10,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  24%|██▍       | 530/2191 [07:23<23:06,  1.20it/s, loss=2.31, v_num=647]Epoch 24:  24%|██▍       | 530/2191 [07:23<23:06,  1.20it/s, loss=2.33, v_num=647]Epoch 24:  25%|██▍       | 540/2191 [07:30<22:55,  1.20it/s, loss=2.33, v_num=647]Epoch 24:  25%|██▍       | 540/2191 [07:30<22:55,  1.20it/s, loss=2.3, v_num=647] Epoch 24:  25%|██▌       | 550/2191 [07:37<22:43,  1.20it/s, loss=2.3, v_num=647]Epoch 24:  25%|██▌       | 550/2191 [07:37<22:43,  1.20it/s, loss=2.32, v_num=647]Epoch 24:  26%|██▌       | 560/2191 [07:46<22:35,  1.20it/s, loss=2.32, v_num=647]Epoch 24:  26%|██▌       | 560/2191 [07:46<22:35,  1.20it/s, loss=2.39, v_num=647]Epoch 24:  26%|██▌       | 570/2191 [07:54<22:25,  1.20it/s, loss=2.39, v_num=647]Epoch 24:  26%|██▌       | 570/2191 [07:54<22:25,  1.20it/s, loss=2.32, v_num=647]Epoch 24:  26%|██▋       | 580/2191 [08:01<22:15,  1.21it/s, loss=2.32, v_num=647]Epoch 24:  26%|██▋       | 580/2191 [08:01<22:15,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  27%|██▋       | 590/2191 [08:10<22:08,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  27%|██▋       | 590/2191 [08:10<22:08,  1.21it/s, loss=2.35, v_num=647]Epoch 24:  27%|██▋       | 600/2191 [08:18<22:00,  1.21it/s, loss=2.35, v_num=647]Epoch 24:  27%|██▋       | 600/2191 [08:18<22:00,  1.21it/s, loss=2.34, v_num=647]Epoch 24:  28%|██▊       | 610/2191 [08:25<21:49,  1.21it/s, loss=2.34, v_num=647]Epoch 24:  28%|██▊       | 610/2191 [08:25<21:49,  1.21it/s, loss=2.28, v_num=647]Epoch 24:  28%|██▊       | 620/2191 [08:34<21:41,  1.21it/s, loss=2.28, v_num=647]Epoch 24:  28%|██▊       | 620/2191 [08:34<21:41,  1.21it/s, loss=2.3, v_num=647] Epoch 24:  29%|██▉       | 630/2191 [08:42<21:33,  1.21it/s, loss=2.3, v_num=647]Epoch 24:  29%|██▉       | 630/2191 [08:42<21:33,  1.21it/s, loss=2.3, v_num=647]Epoch 24:  29%|██▉       | 640/2191 [08:50<21:23,  1.21it/s, loss=2.3, v_num=647]Epoch 24:  29%|██▉       | 640/2191 [08:50<21:23,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  30%|██▉       | 650/2191 [08:58<21:14,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  30%|██▉       | 650/2191 [08:58<21:14,  1.21it/s, loss=2.3, v_num=647] Epoch 24:  30%|███       | 660/2191 [09:06<21:05,  1.21it/s, loss=2.3, v_num=647]Epoch 24:  30%|███       | 660/2191 [09:06<21:05,  1.21it/s, loss=2.34, v_num=647]Epoch 24:  31%|███       | 670/2191 [09:14<20:56,  1.21it/s, loss=2.34, v_num=647]Epoch 24:  31%|███       | 670/2191 [09:14<20:56,  1.21it/s, loss=2.38, v_num=647]Epoch 24:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.38, v_num=647]Epoch 24:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.43, v_num=647]Epoch 24:  31%|███▏      | 690/2191 [09:30<20:38,  1.21it/s, loss=2.43, v_num=647]Epoch 24:  31%|███▏      | 690/2191 [09:30<20:38,  1.21it/s, loss=2.4, v_num=647] Epoch 24:  32%|███▏      | 700/2191 [09:38<20:29,  1.21it/s, loss=2.4, v_num=647]Epoch 24:  32%|███▏      | 700/2191 [09:38<20:29,  1.21it/s, loss=2.31, v_num=647]Epoch 24:  32%|███▏      | 710/2191 [09:46<20:21,  1.21it/s, loss=2.31, v_num=647]Epoch 24:  32%|███▏      | 710/2191 [09:46<20:21,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  33%|███▎      | 720/2191 [09:55<20:14,  1.21it/s, loss=2.27, v_num=647]Epoch 24:  33%|███▎      | 720/2191 [09:55<20:14,  1.21it/s, loss=2.29, v_num=647]Epoch 24:  33%|███▎      | 730/2191 [10:04<20:07,  1.21it/s, loss=2.29, v_num=647]Epoch 24:  33%|███▎      | 730/2191 [10:04<20:07,  1.21it/s, loss=2.3, v_num=647] Epoch 24:  34%|███▍      | 740/2191 [10:11<19:57,  1.21it/s, loss=2.3, v_num=647]Epoch 24:  34%|███▍      | 740/2191 [10:11<19:57,  1.21it/s, loss=2.29, v_num=647]Epoch 24:  34%|███▍      | 750/2191 [10:18<19:47,  1.21it/s, loss=2.29, v_num=647]Epoch 24:  34%|███▍      | 750/2191 [10:18<19:47,  1.21it/s, loss=2.33, v_num=647]Epoch 24:  35%|███▍      | 760/2191 [10:25<19:36,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  35%|███▍      | 760/2191 [10:25<19:36,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  35%|███▌      | 770/2191 [10:35<19:30,  1.21it/s, loss=2.34, v_num=647]Epoch 24:  35%|███▌      | 770/2191 [10:35<19:30,  1.21it/s, loss=2.35, v_num=647]Epoch 24:  36%|███▌      | 780/2191 [10:41<19:19,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  36%|███▌      | 780/2191 [10:41<19:19,  1.22it/s, loss=2.39, v_num=647]Epoch 24:  36%|███▌      | 790/2191 [10:48<19:07,  1.22it/s, loss=2.39, v_num=647]Epoch 24:  36%|███▌      | 790/2191 [10:48<19:07,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  37%|███▋      | 800/2191 [10:56<18:59,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  37%|███▋      | 800/2191 [10:56<18:59,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  37%|███▋      | 810/2191 [11:03<18:49,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  37%|███▋      | 810/2191 [11:03<18:49,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  37%|███▋      | 820/2191 [11:10<18:40,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  37%|███▋      | 820/2191 [11:10<18:40,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  38%|███▊      | 830/2191 [11:20<18:35,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  38%|███▊      | 830/2191 [11:20<18:35,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  38%|███▊      | 840/2191 [11:28<18:25,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  38%|███▊      | 840/2191 [11:28<18:25,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  39%|███▉      | 850/2191 [11:36<18:17,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  39%|███▉      | 850/2191 [11:36<18:17,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  39%|███▉      | 860/2191 [11:43<18:07,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  39%|███▉      | 860/2191 [11:43<18:07,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  40%|███▉      | 870/2191 [11:51<17:58,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  40%|███▉      | 870/2191 [11:51<17:58,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  40%|████      | 880/2191 [12:00<17:52,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  40%|████      | 880/2191 [12:00<17:52,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  41%|████      | 890/2191 [12:07<17:42,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  41%|████      | 890/2191 [12:07<17:42,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  41%|████      | 900/2191 [12:16<17:35,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  41%|████      | 900/2191 [12:16<17:35,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  42%|████▏     | 910/2191 [12:23<17:25,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  42%|████▏     | 910/2191 [12:23<17:25,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  42%|████▏     | 920/2191 [12:30<17:15,  1.23it/s, loss=2.29, v_num=647]Epoch 24:  42%|████▏     | 920/2191 [12:30<17:15,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  42%|████▏     | 930/2191 [12:39<17:08,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  42%|████▏     | 930/2191 [12:39<17:08,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  43%|████▎     | 940/2191 [12:48<17:02,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  43%|████▎     | 940/2191 [12:48<17:02,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  43%|████▎     | 950/2191 [12:56<16:53,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  43%|████▎     | 950/2191 [12:56<16:53,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  44%|████▍     | 960/2191 [13:04<16:45,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  44%|████▍     | 960/2191 [13:04<16:45,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  44%|████▍     | 970/2191 [13:14<16:39,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  44%|████▍     | 970/2191 [13:14<16:39,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  45%|████▍     | 980/2191 [13:23<16:31,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  45%|████▍     | 980/2191 [13:23<16:31,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  45%|████▌     | 990/2191 [13:29<16:21,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  45%|████▌     | 990/2191 [13:29<16:21,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  46%|████▌     | 1000/2191 [13:37<16:12,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  46%|████▌     | 1000/2191 [13:37<16:12,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  46%|████▌     | 1010/2191 [13:45<16:04,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  46%|████▌     | 1010/2191 [13:45<16:04,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  47%|████▋     | 1020/2191 [13:52<15:55,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  47%|████▋     | 1020/2191 [13:52<15:55,  1.23it/s, loss=2.29, v_num=647]Epoch 24:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  47%|████▋     | 1040/2191 [14:10<15:40,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  47%|████▋     | 1040/2191 [14:10<15:40,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  48%|████▊     | 1050/2191 [14:18<15:32,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  48%|████▊     | 1050/2191 [14:18<15:32,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  48%|████▊     | 1060/2191 [14:27<15:24,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  48%|████▊     | 1060/2191 [14:27<15:24,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  49%|████▉     | 1070/2191 [14:35<15:16,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  49%|████▉     | 1070/2191 [14:35<15:16,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  49%|████▉     | 1080/2191 [14:43<15:08,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  49%|████▉     | 1080/2191 [14:43<15:08,  1.22it/s, loss=2.27, v_num=647]Epoch 24:  50%|████▉     | 1090/2191 [14:53<15:01,  1.22it/s, loss=2.27, v_num=647]Epoch 24:  50%|████▉     | 1090/2191 [14:53<15:01,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  50%|█████     | 1100/2191 [15:02<14:54,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  50%|█████     | 1100/2191 [15:02<14:54,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  51%|█████     | 1110/2191 [15:09<14:44,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  51%|█████     | 1110/2191 [15:09<14:44,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  51%|█████     | 1120/2191 [15:18<14:37,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  51%|█████     | 1120/2191 [15:18<14:37,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  52%|█████▏    | 1130/2191 [15:27<14:29,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  52%|█████▏    | 1130/2191 [15:27<14:29,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  52%|█████▏    | 1140/2191 [15:34<14:20,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  52%|█████▏    | 1140/2191 [15:34<14:20,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  52%|█████▏    | 1150/2191 [15:42<14:12,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  52%|█████▏    | 1150/2191 [15:42<14:12,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  53%|█████▎    | 1160/2191 [15:51<14:04,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  53%|█████▎    | 1160/2191 [15:51<14:04,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  53%|█████▎    | 1170/2191 [16:00<13:57,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  53%|█████▎    | 1170/2191 [16:00<13:57,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  54%|█████▍    | 1180/2191 [16:07<13:48,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  54%|█████▍    | 1180/2191 [16:07<13:48,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  54%|█████▍    | 1190/2191 [16:15<13:39,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  54%|█████▍    | 1190/2191 [16:15<13:39,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  55%|█████▍    | 1200/2191 [16:22<13:31,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  55%|█████▍    | 1200/2191 [16:22<13:31,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  55%|█████▌    | 1210/2191 [16:29<13:21,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  55%|█████▌    | 1210/2191 [16:29<13:21,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  56%|█████▌    | 1220/2191 [16:37<13:13,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  56%|█████▌    | 1220/2191 [16:37<13:13,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  56%|█████▌    | 1230/2191 [16:45<13:04,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  56%|█████▌    | 1230/2191 [16:45<13:04,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  57%|█████▋    | 1240/2191 [16:53<12:56,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  57%|█████▋    | 1240/2191 [16:53<12:56,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  57%|█████▋    | 1250/2191 [17:01<12:48,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  57%|█████▋    | 1250/2191 [17:01<12:48,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  58%|█████▊    | 1260/2191 [17:09<12:39,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  58%|█████▊    | 1260/2191 [17:09<12:39,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  58%|█████▊    | 1270/2191 [17:15<12:30,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  58%|█████▊    | 1270/2191 [17:15<12:30,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  58%|█████▊    | 1280/2191 [17:24<12:22,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  58%|█████▊    | 1280/2191 [17:24<12:22,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  59%|█████▉    | 1290/2191 [17:32<12:14,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  59%|█████▉    | 1290/2191 [17:32<12:14,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  59%|█████▉    | 1300/2191 [17:41<12:06,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  59%|█████▉    | 1300/2191 [17:41<12:06,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  60%|█████▉    | 1310/2191 [17:50<11:59,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  60%|█████▉    | 1310/2191 [17:50<11:59,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  60%|██████    | 1320/2191 [17:58<11:51,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  60%|██████    | 1320/2191 [17:58<11:51,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  61%|██████    | 1330/2191 [18:07<11:43,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  61%|██████    | 1330/2191 [18:07<11:43,  1.22it/s, loss=2.26, v_num=647]Epoch 24:  61%|██████    | 1340/2191 [18:15<11:35,  1.22it/s, loss=2.26, v_num=647]Epoch 24:  61%|██████    | 1340/2191 [18:15<11:35,  1.22it/s, loss=2.26, v_num=647]Epoch 24:  62%|██████▏   | 1350/2191 [18:24<11:27,  1.22it/s, loss=2.26, v_num=647]Epoch 24:  62%|██████▏   | 1350/2191 [18:24<11:27,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  62%|██████▏   | 1360/2191 [18:33<11:19,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  62%|██████▏   | 1360/2191 [18:33<11:19,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  63%|██████▎   | 1370/2191 [18:44<11:13,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  63%|██████▎   | 1370/2191 [18:44<11:13,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  63%|██████▎   | 1380/2191 [18:53<11:05,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  63%|██████▎   | 1380/2191 [18:53<11:05,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  63%|██████▎   | 1390/2191 [19:02<10:57,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  63%|██████▎   | 1390/2191 [19:02<10:57,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  64%|██████▍   | 1400/2191 [19:09<10:48,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  64%|██████▍   | 1400/2191 [19:09<10:48,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  64%|██████▍   | 1410/2191 [19:17<10:40,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  64%|██████▍   | 1410/2191 [19:17<10:40,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  65%|██████▍   | 1420/2191 [19:26<10:32,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  65%|██████▍   | 1420/2191 [19:26<10:32,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  65%|██████▌   | 1430/2191 [19:33<10:24,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  65%|██████▌   | 1430/2191 [19:33<10:24,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  66%|██████▌   | 1440/2191 [19:42<10:16,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  66%|██████▌   | 1440/2191 [19:42<10:16,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  66%|██████▌   | 1450/2191 [19:51<10:08,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  66%|██████▌   | 1450/2191 [19:51<10:08,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  67%|██████▋   | 1460/2191 [20:00<10:00,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  67%|██████▋   | 1460/2191 [20:00<10:00,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  67%|██████▋   | 1470/2191 [20:07<09:51,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  67%|██████▋   | 1470/2191 [20:07<09:51,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  68%|██████▊   | 1480/2191 [20:15<09:43,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  68%|██████▊   | 1480/2191 [20:15<09:43,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  68%|██████▊   | 1490/2191 [20:26<09:36,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  68%|██████▊   | 1490/2191 [20:26<09:36,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  68%|██████▊   | 1500/2191 [20:34<09:28,  1.22it/s, loss=2.36, v_num=647]Epoch 24:  68%|██████▊   | 1500/2191 [20:34<09:28,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  69%|██████▉   | 1510/2191 [20:42<09:19,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  69%|██████▉   | 1510/2191 [20:42<09:19,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  69%|██████▉   | 1520/2191 [20:51<09:11,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  69%|██████▉   | 1520/2191 [20:51<09:11,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  70%|██████▉   | 1530/2191 [20:59<09:03,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  70%|██████▉   | 1530/2191 [20:59<09:03,  1.22it/s, loss=2.28, v_num=647]Epoch 24:  70%|███████   | 1540/2191 [21:08<08:56,  1.21it/s, loss=2.28, v_num=647]Epoch 24:  70%|███████   | 1540/2191 [21:08<08:56,  1.21it/s, loss=2.31, v_num=647]Epoch 24:  71%|███████   | 1550/2191 [21:16<08:47,  1.21it/s, loss=2.31, v_num=647]Epoch 24:  71%|███████   | 1550/2191 [21:16<08:47,  1.21it/s, loss=2.3, v_num=647] Epoch 24:  71%|███████   | 1560/2191 [21:24<08:39,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  71%|███████   | 1560/2191 [21:24<08:39,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  72%|███████▏  | 1570/2191 [21:31<08:30,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  72%|███████▏  | 1570/2191 [21:31<08:30,  1.22it/s, loss=2.37, v_num=647]Epoch 24:  72%|███████▏  | 1580/2191 [21:37<08:21,  1.22it/s, loss=2.37, v_num=647]Epoch 24:  72%|███████▏  | 1580/2191 [21:37<08:21,  1.22it/s, loss=2.37, v_num=647]Epoch 24:  73%|███████▎  | 1590/2191 [21:47<08:13,  1.22it/s, loss=2.37, v_num=647]Epoch 24:  73%|███████▎  | 1590/2191 [21:47<08:13,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  73%|███████▎  | 1600/2191 [21:55<08:05,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  73%|███████▎  | 1600/2191 [21:55<08:05,  1.22it/s, loss=2.3, v_num=647] Epoch 24:  73%|███████▎  | 1610/2191 [22:02<07:57,  1.22it/s, loss=2.3, v_num=647]Epoch 24:  73%|███████▎  | 1610/2191 [22:02<07:57,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  74%|███████▍  | 1620/2191 [22:10<07:48,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  74%|███████▍  | 1620/2191 [22:10<07:48,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  74%|███████▍  | 1630/2191 [22:18<07:40,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  74%|███████▍  | 1630/2191 [22:18<07:40,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  75%|███████▍  | 1640/2191 [22:26<07:32,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  75%|███████▍  | 1640/2191 [22:26<07:32,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  75%|███████▌  | 1650/2191 [22:33<07:23,  1.22it/s, loss=2.35, v_num=647]Epoch 24:  75%|███████▌  | 1650/2191 [22:33<07:23,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  76%|███████▌  | 1660/2191 [22:41<07:15,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  76%|███████▌  | 1660/2191 [22:41<07:15,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  76%|███████▌  | 1670/2191 [22:47<07:06,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  76%|███████▌  | 1670/2191 [22:47<07:06,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  77%|███████▋  | 1680/2191 [22:55<06:58,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  77%|███████▋  | 1680/2191 [22:55<06:58,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  77%|███████▋  | 1690/2191 [23:02<06:49,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  77%|███████▋  | 1690/2191 [23:02<06:49,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  78%|███████▊  | 1700/2191 [23:10<06:41,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  78%|███████▊  | 1700/2191 [23:10<06:41,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  78%|███████▊  | 1710/2191 [23:18<06:33,  1.22it/s, loss=2.29, v_num=647]Epoch 24:  78%|███████▊  | 1710/2191 [23:18<06:33,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  79%|███████▊  | 1720/2191 [23:27<06:25,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  79%|███████▊  | 1720/2191 [23:27<06:25,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  79%|███████▉  | 1730/2191 [23:34<06:16,  1.22it/s, loss=2.34, v_num=647]Epoch 24:  79%|███████▉  | 1730/2191 [23:34<06:16,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  79%|███████▉  | 1740/2191 [23:42<06:08,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  79%|███████▉  | 1740/2191 [23:42<06:08,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  80%|███████▉  | 1750/2191 [23:50<06:00,  1.22it/s, loss=2.32, v_num=647]Epoch 24:  80%|███████▉  | 1750/2191 [23:50<06:00,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  80%|████████  | 1760/2191 [23:58<05:52,  1.22it/s, loss=2.31, v_num=647]Epoch 24:  80%|████████  | 1760/2191 [23:58<05:52,  1.22it/s, loss=2.33, v_num=647]Epoch 24:  81%|████████  | 1770/2191 [24:05<05:43,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  81%|████████  | 1770/2191 [24:05<05:43,  1.23it/s, loss=2.35, v_num=647]Epoch 24:  81%|████████  | 1780/2191 [24:12<05:35,  1.23it/s, loss=2.35, v_num=647]Epoch 24:  81%|████████  | 1780/2191 [24:12<05:35,  1.23it/s, loss=2.35, v_num=647]Epoch 24:  82%|████████▏ | 1790/2191 [24:19<05:26,  1.23it/s, loss=2.35, v_num=647]Epoch 24:  82%|████████▏ | 1790/2191 [24:19<05:26,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  82%|████████▏ | 1800/2191 [24:26<05:18,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  82%|████████▏ | 1800/2191 [24:26<05:18,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  83%|████████▎ | 1810/2191 [24:33<05:10,  1.23it/s, loss=2.31, v_num=647]Epoch 24:  83%|████████▎ | 1810/2191 [24:33<05:10,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  83%|████████▎ | 1820/2191 [24:41<05:01,  1.23it/s, loss=2.33, v_num=647]Epoch 24:  83%|████████▎ | 1820/2191 [24:41<05:01,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  84%|████████▎ | 1830/2191 [24:49<04:53,  1.23it/s, loss=2.32, v_num=647]Epoch 24:  84%|████████▎ | 1830/2191 [24:49<04:53,  1.23it/s, loss=2.36, v_num=647]Epoch 24:  84%|████████▍ | 1840/2191 [24:56<04:45,  1.23it/s, loss=2.36, v_num=647]Epoch 24:  84%|████████▍ | 1840/2191 [24:56<04:45,  1.23it/s, loss=2.37, v_num=647]Epoch 24:  84%|████████▍ | 1850/2191 [25:03<04:36,  1.23it/s, loss=2.37, v_num=647]Epoch 24:  84%|████████▍ | 1850/2191 [25:03<04:36,  1.23it/s, loss=2.37, v_num=647]Epoch 24:  85%|████████▍ | 1860/2191 [25:06<04:27,  1.24it/s, loss=2.37, v_num=647]Epoch 24:  85%|████████▍ | 1860/2191 [25:06<04:27,  1.24it/s, loss=2.38, v_num=647]validation_epoch_end
graph acc: 0.38338658146964855
valid accuracy: 0.9758654832839966
alidation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.974704384803772
validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9755616784095764
alid accuracy: 0.9781250357627869
Epoch 24:  86%|████████▌ | 1880/2191 [25:10<04:09,  1.24it/s, loss=2.35, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9768841862678528
validation_epoch_end
graph acc: 0.36421725239616615
valid accuracy: 0.9759986400604248

Validating:   3%|▎         | 10/313 [00:01<00:43,  6.98it/s][AEpoch 24:  86%|████████▋ | 1890/2191 [25:12<04:00,  1.25it/s, loss=2.35, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:48,  6.04it/s][AEpoch 24:  87%|████████▋ | 1900/2191 [25:14<03:51,  1.26it/s, loss=2.35, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:35,  7.93it/s][AEpoch 24:  87%|████████▋ | 1910/2191 [25:14<03:42,  1.26it/s, loss=2.35, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:27, 10.09it/s][AEpoch 24:  88%|████████▊ | 1920/2191 [25:15<03:33,  1.27it/s, loss=2.35, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26,  9.91it/s][AEpoch 24:  88%|████████▊ | 1930/2191 [25:16<03:24,  1.27it/s, loss=2.35, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.37it/s][AEpoch 24:  89%|████████▊ | 1940/2191 [25:17<03:16,  1.28it/s, loss=2.35, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24,  9.85it/s][AEpoch 24:  89%|████████▉ | 1950/2191 [25:18<03:07,  1.28it/s, loss=2.35, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.50it/s][AEpoch 24:  89%|████████▉ | 1960/2191 [25:19<02:58,  1.29it/s, loss=2.35, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.34it/s][AEpoch 24:  90%|████████▉ | 1970/2191 [25:20<02:50,  1.30it/s, loss=2.35, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.30it/s][AEpoch 24:  90%|█████████ | 1980/2191 [25:21<02:42,  1.30it/s, loss=2.35, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.81it/s][AEpoch 24:  91%|█████████ | 1990/2191 [25:22<02:33,  1.31it/s, loss=2.35, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.87it/s][AEpoch 24:  91%|█████████▏| 2000/2191 [25:23<02:25,  1.31it/s, loss=2.35, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.26it/s][AEpoch 24:  92%|█████████▏| 2010/2191 [25:24<02:17,  1.32it/s, loss=2.35, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:19,  8.73it/s][AEpoch 24:  92%|█████████▏| 2020/2191 [25:26<02:09,  1.32it/s, loss=2.35, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.20it/s][AEpoch 24:  93%|█████████▎| 2030/2191 [25:26<02:01,  1.33it/s, loss=2.35, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15,  9.76it/s][AEpoch 24:  93%|█████████▎| 2040/2191 [25:27<01:53,  1.34it/s, loss=2.35, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.78it/s][AEpoch 24:  94%|█████████▎| 2050/2191 [25:28<01:45,  1.34it/s, loss=2.35, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.50it/s][AEpoch 24:  94%|█████████▍| 2060/2191 [25:29<01:37,  1.35it/s, loss=2.35, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.43it/s][AEpoch 24:  94%|█████████▍| 2070/2191 [25:30<01:29,  1.35it/s, loss=2.35, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 12.06it/s][AEpoch 24:  95%|█████████▍| 2080/2191 [25:31<01:21,  1.36it/s, loss=2.35, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 11.05it/s][AEpoch 24:  95%|█████████▌| 2090/2191 [25:32<01:14,  1.36it/s, loss=2.35, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.91it/s][AEpoch 24:  96%|█████████▌| 2100/2191 [25:33<01:06,  1.37it/s, loss=2.35, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:22<00:07, 11.40it/s][AEpoch 24:  96%|█████████▋| 2110/2191 [25:33<00:58,  1.38it/s, loss=2.35, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.17it/s][AEpoch 24:  97%|█████████▋| 2120/2191 [25:35<00:51,  1.38it/s, loss=2.35, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.03it/s][AEpoch 24:  97%|█████████▋| 2130/2191 [25:36<00:43,  1.39it/s, loss=2.35, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.98it/s][AEpoch 24:  98%|█████████▊| 2140/2191 [25:37<00:36,  1.39it/s, loss=2.35, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 11.05it/s][AEpoch 24:  98%|█████████▊| 2150/2191 [25:37<00:29,  1.40it/s, loss=2.35, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.30it/s][AEpoch 24:  99%|█████████▊| 2160/2191 [25:38<00:22,  1.40it/s, loss=2.35, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:02, 11.27it/s][AEpoch 24:  99%|█████████▉| 2170/2191 [25:39<00:14,  1.41it/s, loss=2.35, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.81it/s][AEpoch 24:  99%|█████████▉| 2180/2191 [25:40<00:07,  1.42it/s, loss=2.35, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.45it/s][AEpoch 24: 100%|█████████▉| 2190/2191 [25:40<00:00,  1.42it/s, loss=2.35, v_num=647]validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9792499542236328
Epoch 24: 100%|██████████| 2191/2191 [25:43<00:00,  1.42it/s, loss=2.32, v_num=647]
                                                             [AEpoch 24:   0%|          | 0/2191 [00:00<00:00, 14122.24it/s, loss=2.32, v_num=647]Epoch 25:   0%|          | 0/2191 [00:00<00:00, 3371.63it/s, loss=2.32, v_num=647] Epoch 25:   0%|          | 0/2191 [00:10<6:17:31, 10.34s/it, loss=2.32, v_num=647]Epoch 25:   0%|          | 10/2191 [00:12<41:28,  1.14s/it, loss=2.32, v_num=647] Epoch 25:   0%|          | 10/2191 [00:12<41:28,  1.14s/it, loss=2.3, v_num=647] Epoch 25:   1%|          | 20/2191 [00:23<39:49,  1.10s/it, loss=2.3, v_num=647]Epoch 25:   1%|          | 20/2191 [00:23<39:50,  1.10s/it, loss=2.29, v_num=647]Epoch 25:   1%|▏         | 30/2191 [00:33<38:22,  1.07s/it, loss=2.29, v_num=647]Epoch 25:   1%|▏         | 30/2191 [00:33<38:22,  1.07s/it, loss=2.33, v_num=647]Epoch 25:   2%|▏         | 40/2191 [00:42<37:00,  1.03s/it, loss=2.33, v_num=647]Epoch 25:   2%|▏         | 40/2191 [00:42<37:00,  1.03s/it, loss=2.35, v_num=647]Epoch 25:   2%|▏         | 50/2191 [00:51<36:15,  1.02s/it, loss=2.35, v_num=647]Epoch 25:   2%|▏         | 50/2191 [00:51<36:15,  1.02s/it, loss=2.32, v_num=647]Epoch 25:   3%|▎         | 60/2191 [01:01<35:38,  1.00s/it, loss=2.32, v_num=647]Epoch 25:   3%|▎         | 60/2191 [01:01<35:38,  1.00s/it, loss=2.3, v_num=647] Epoch 25:   3%|▎         | 70/2191 [01:10<35:17,  1.00it/s, loss=2.3, v_num=647]Epoch 25:   3%|▎         | 70/2191 [01:10<35:17,  1.00it/s, loss=2.31, v_num=647]Epoch 25:   4%|▎         | 80/2191 [01:20<34:58,  1.01it/s, loss=2.31, v_num=647]Epoch 25:   4%|▎         | 80/2191 [01:20<34:58,  1.01it/s, loss=2.32, v_num=647]Epoch 25:   4%|▍         | 90/2191 [01:29<34:24,  1.02it/s, loss=2.32, v_num=647]Epoch 25:   4%|▍         | 90/2191 [01:29<34:24,  1.02it/s, loss=2.33, v_num=647]Epoch 25:   5%|▍         | 100/2191 [01:38<34:00,  1.02it/s, loss=2.33, v_num=647]Epoch 25:   5%|▍         | 100/2191 [01:38<34:00,  1.02it/s, loss=2.32, v_num=647]Epoch 25:   5%|▌         | 110/2191 [01:47<33:29,  1.04it/s, loss=2.32, v_num=647]Epoch 25:   5%|▌         | 110/2191 [01:47<33:29,  1.04it/s, loss=2.29, v_num=647]Epoch 25:   5%|▌         | 120/2191 [01:55<33:01,  1.05it/s, loss=2.29, v_num=647]Epoch 25:   5%|▌         | 120/2191 [01:55<33:01,  1.05it/s, loss=2.25, v_num=647]Epoch 25:   6%|▌         | 130/2191 [02:03<32:27,  1.06it/s, loss=2.25, v_num=647]Epoch 25:   6%|▌         | 130/2191 [02:03<32:28,  1.06it/s, loss=2.26, v_num=647]Epoch 25:   6%|▋         | 140/2191 [02:11<31:57,  1.07it/s, loss=2.26, v_num=647]Epoch 25:   6%|▋         | 140/2191 [02:11<31:57,  1.07it/s, loss=2.27, v_num=647]Epoch 25:   7%|▋         | 150/2191 [02:19<31:19,  1.09it/s, loss=2.27, v_num=647]Epoch 25:   7%|▋         | 150/2191 [02:19<31:19,  1.09it/s, loss=2.27, v_num=647]Epoch 25:   7%|▋         | 160/2191 [02:26<30:47,  1.10it/s, loss=2.27, v_num=647]Epoch 25:   7%|▋         | 160/2191 [02:26<30:47,  1.10it/s, loss=2.32, v_num=647]Epoch 25:   8%|▊         | 170/2191 [02:33<30:16,  1.11it/s, loss=2.32, v_num=647]Epoch 25:   8%|▊         | 170/2191 [02:33<30:16,  1.11it/s, loss=2.32, v_num=647]Epoch 25:   8%|▊         | 180/2191 [02:40<29:40,  1.13it/s, loss=2.32, v_num=647]Epoch 25:   8%|▊         | 180/2191 [02:40<29:40,  1.13it/s, loss=2.36, v_num=647]Epoch 25:   9%|▊         | 190/2191 [02:48<29:29,  1.13it/s, loss=2.36, v_num=647]Epoch 25:   9%|▊         | 190/2191 [02:48<29:29,  1.13it/s, loss=2.37, v_num=647]Epoch 25:   9%|▉         | 200/2191 [02:56<29:12,  1.14it/s, loss=2.37, v_num=647]Epoch 25:   9%|▉         | 200/2191 [02:56<29:12,  1.14it/s, loss=2.33, v_num=647]Epoch 25:  10%|▉         | 210/2191 [03:05<29:01,  1.14it/s, loss=2.33, v_num=647]Epoch 25:  10%|▉         | 210/2191 [03:05<29:01,  1.14it/s, loss=2.32, v_num=647]Epoch 25:  10%|█         | 220/2191 [03:14<28:52,  1.14it/s, loss=2.32, v_num=647]Epoch 25:  10%|█         | 220/2191 [03:14<28:52,  1.14it/s, loss=2.28, v_num=647]Epoch 25:  10%|█         | 230/2191 [03:21<28:33,  1.14it/s, loss=2.28, v_num=647]Epoch 25:  10%|█         | 230/2191 [03:21<28:33,  1.14it/s, loss=2.26, v_num=647]Epoch 25:  11%|█         | 240/2191 [03:29<28:14,  1.15it/s, loss=2.26, v_num=647]Epoch 25:  11%|█         | 240/2191 [03:29<28:14,  1.15it/s, loss=2.31, v_num=647]Epoch 25:  11%|█▏        | 250/2191 [03:36<27:54,  1.16it/s, loss=2.31, v_num=647]Epoch 25:  11%|█▏        | 250/2191 [03:36<27:54,  1.16it/s, loss=2.35, v_num=647]Epoch 25:  12%|█▏        | 260/2191 [03:43<27:35,  1.17it/s, loss=2.35, v_num=647]Epoch 25:  12%|█▏        | 260/2191 [03:43<27:35,  1.17it/s, loss=2.33, v_num=647]Epoch 25:  12%|█▏        | 270/2191 [03:51<27:18,  1.17it/s, loss=2.33, v_num=647]Epoch 25:  12%|█▏        | 270/2191 [03:51<27:18,  1.17it/s, loss=2.31, v_num=647]Epoch 25:  13%|█▎        | 280/2191 [03:59<27:07,  1.17it/s, loss=2.31, v_num=647]Epoch 25:  13%|█▎        | 280/2191 [03:59<27:07,  1.17it/s, loss=2.32, v_num=647]Epoch 25:  13%|█▎        | 290/2191 [04:06<26:50,  1.18it/s, loss=2.32, v_num=647]Epoch 25:  13%|█▎        | 290/2191 [04:06<26:50,  1.18it/s, loss=2.31, v_num=647]Epoch 25:  14%|█▎        | 300/2191 [04:15<26:45,  1.18it/s, loss=2.31, v_num=647]Epoch 25:  14%|█▎        | 300/2191 [04:15<26:45,  1.18it/s, loss=2.29, v_num=647]Epoch 25:  14%|█▍        | 310/2191 [04:22<26:28,  1.18it/s, loss=2.29, v_num=647]Epoch 25:  14%|█▍        | 310/2191 [04:22<26:28,  1.18it/s, loss=2.3, v_num=647] Epoch 25:  15%|█▍        | 320/2191 [04:30<26:13,  1.19it/s, loss=2.3, v_num=647]Epoch 25:  15%|█▍        | 320/2191 [04:30<26:13,  1.19it/s, loss=2.3, v_num=647]Epoch 25:  15%|█▌        | 330/2191 [04:38<26:03,  1.19it/s, loss=2.3, v_num=647]Epoch 25:  15%|█▌        | 330/2191 [04:38<26:03,  1.19it/s, loss=2.32, v_num=647]Epoch 25:  16%|█▌        | 340/2191 [04:46<25:52,  1.19it/s, loss=2.32, v_num=647]Epoch 25:  16%|█▌        | 340/2191 [04:46<25:52,  1.19it/s, loss=2.3, v_num=647] Epoch 25:  16%|█▌        | 350/2191 [04:55<25:49,  1.19it/s, loss=2.3, v_num=647]Epoch 25:  16%|█▌        | 350/2191 [04:55<25:49,  1.19it/s, loss=2.29, v_num=647]Epoch 25:  16%|█▋        | 360/2191 [05:02<25:35,  1.19it/s, loss=2.29, v_num=647]Epoch 25:  16%|█▋        | 360/2191 [05:02<25:35,  1.19it/s, loss=2.3, v_num=647] Epoch 25:  17%|█▋        | 370/2191 [05:11<25:28,  1.19it/s, loss=2.3, v_num=647]Epoch 25:  17%|█▋        | 370/2191 [05:11<25:28,  1.19it/s, loss=2.27, v_num=647]Epoch 25:  17%|█▋        | 380/2191 [05:19<25:16,  1.19it/s, loss=2.27, v_num=647]Epoch 25:  17%|█▋        | 380/2191 [05:19<25:16,  1.19it/s, loss=2.3, v_num=647] Epoch 25:  18%|█▊        | 390/2191 [05:26<25:05,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  18%|█▊        | 390/2191 [05:26<25:05,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  18%|█▊        | 400/2191 [05:35<25:00,  1.19it/s, loss=2.32, v_num=647]Epoch 25:  18%|█▊        | 400/2191 [05:35<25:00,  1.19it/s, loss=2.32, v_num=647]Epoch 25:  19%|█▊        | 410/2191 [05:43<24:47,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  19%|█▊        | 410/2191 [05:43<24:47,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  19%|█▉        | 420/2191 [05:51<24:38,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  19%|█▉        | 420/2191 [05:51<24:38,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  20%|█▉        | 430/2191 [05:59<24:27,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  20%|█▉        | 430/2191 [05:59<24:27,  1.20it/s, loss=2.36, v_num=647]Epoch 25:  20%|██        | 440/2191 [06:07<24:17,  1.20it/s, loss=2.36, v_num=647]Epoch 25:  20%|██        | 440/2191 [06:07<24:17,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  21%|██        | 450/2191 [06:14<24:05,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  21%|██        | 450/2191 [06:14<24:05,  1.20it/s, loss=2.26, v_num=647]Epoch 25:  21%|██        | 460/2191 [06:22<23:57,  1.20it/s, loss=2.26, v_num=647]Epoch 25:  21%|██        | 460/2191 [06:22<23:57,  1.20it/s, loss=2.26, v_num=647]Epoch 25:  21%|██▏       | 470/2191 [06:32<23:53,  1.20it/s, loss=2.26, v_num=647]Epoch 25:  21%|██▏       | 470/2191 [06:32<23:53,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  22%|██▏       | 480/2191 [06:41<23:47,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  22%|██▏       | 480/2191 [06:41<23:47,  1.20it/s, loss=2.3, v_num=647] Epoch 25:  22%|██▏       | 490/2191 [06:48<23:35,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  22%|██▏       | 490/2191 [06:48<23:35,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  23%|██▎       | 500/2191 [06:59<23:34,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  23%|██▎       | 500/2191 [06:59<23:34,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  23%|██▎       | 510/2191 [07:06<23:22,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  23%|██▎       | 510/2191 [07:06<23:22,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  24%|██▎       | 520/2191 [07:15<23:16,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  24%|██▎       | 520/2191 [07:15<23:16,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  24%|██▍       | 530/2191 [07:23<23:06,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  24%|██▍       | 530/2191 [07:23<23:06,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  25%|██▍       | 540/2191 [07:31<22:58,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  25%|██▍       | 540/2191 [07:31<22:58,  1.20it/s, loss=2.3, v_num=647] Epoch 25:  25%|██▌       | 550/2191 [07:39<22:49,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  25%|██▌       | 550/2191 [07:39<22:49,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  26%|██▌       | 560/2191 [07:48<22:42,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  26%|██▌       | 560/2191 [07:48<22:42,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  26%|██▌       | 570/2191 [07:56<22:33,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  26%|██▌       | 570/2191 [07:56<22:33,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  26%|██▋       | 580/2191 [08:04<22:23,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  26%|██▋       | 580/2191 [08:04<22:23,  1.20it/s, loss=2.3, v_num=647] Epoch 25:  27%|██▋       | 590/2191 [08:13<22:16,  1.20it/s, loss=2.3, v_num=647]Epoch 25:  27%|██▋       | 590/2191 [08:13<22:16,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  27%|██▋       | 600/2191 [08:21<22:07,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  27%|██▋       | 600/2191 [08:21<22:07,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  28%|██▊       | 610/2191 [08:29<21:58,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  28%|██▊       | 610/2191 [08:29<21:58,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  28%|██▊       | 620/2191 [08:38<21:51,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  28%|██▊       | 620/2191 [08:38<21:51,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  29%|██▉       | 630/2191 [08:47<21:44,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  29%|██▉       | 630/2191 [08:47<21:44,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  29%|██▉       | 640/2191 [08:54<21:32,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  29%|██▉       | 640/2191 [08:54<21:32,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  30%|██▉       | 650/2191 [09:03<21:26,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  30%|██▉       | 650/2191 [09:03<21:26,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  30%|███       | 660/2191 [09:11<21:18,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  30%|███       | 660/2191 [09:11<21:18,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  31%|███       | 670/2191 [09:20<21:10,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  31%|███       | 670/2191 [09:20<21:10,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  31%|███       | 680/2191 [09:29<21:02,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  31%|███       | 680/2191 [09:29<21:02,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  32%|███▏      | 700/2191 [09:46<20:47,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  32%|███▏      | 700/2191 [09:46<20:47,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  32%|███▏      | 710/2191 [09:53<20:36,  1.20it/s, loss=2.34, v_num=647]Epoch 25:  32%|███▏      | 710/2191 [09:53<20:36,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  33%|███▎      | 720/2191 [10:01<20:27,  1.20it/s, loss=2.35, v_num=647]Epoch 25:  33%|███▎      | 720/2191 [10:01<20:27,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  33%|███▎      | 730/2191 [10:10<20:20,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  33%|███▎      | 730/2191 [10:10<20:20,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  34%|███▍      | 740/2191 [10:19<20:12,  1.20it/s, loss=2.33, v_num=647]Epoch 25:  34%|███▍      | 740/2191 [10:19<20:12,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  34%|███▍      | 750/2191 [10:27<20:03,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  34%|███▍      | 750/2191 [10:27<20:03,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  35%|███▍      | 760/2191 [10:34<19:53,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  35%|███▍      | 760/2191 [10:34<19:53,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  35%|███▌      | 770/2191 [10:42<19:43,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  35%|███▌      | 770/2191 [10:42<19:43,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  36%|███▌      | 780/2191 [10:50<19:34,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  36%|███▌      | 780/2191 [10:50<19:34,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  36%|███▌      | 790/2191 [10:58<19:25,  1.20it/s, loss=2.28, v_num=647]Epoch 25:  36%|███▌      | 790/2191 [10:58<19:25,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  37%|███▋      | 800/2191 [11:06<19:17,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  37%|███▋      | 800/2191 [11:06<19:17,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  37%|███▋      | 810/2191 [11:14<19:08,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  37%|███▋      | 810/2191 [11:14<19:08,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  37%|███▋      | 820/2191 [11:22<18:59,  1.20it/s, loss=2.31, v_num=647]Epoch 25:  37%|███▋      | 820/2191 [11:22<18:59,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  38%|███▊      | 830/2191 [11:30<18:50,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  38%|███▊      | 830/2191 [11:30<18:50,  1.20it/s, loss=2.27, v_num=647]Epoch 25:  38%|███▊      | 840/2191 [11:37<18:40,  1.21it/s, loss=2.27, v_num=647]Epoch 25:  38%|███▊      | 840/2191 [11:37<18:40,  1.21it/s, loss=2.27, v_num=647]Epoch 25:  39%|███▉      | 850/2191 [11:46<18:32,  1.21it/s, loss=2.27, v_num=647]Epoch 25:  39%|███▉      | 850/2191 [11:46<18:32,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  39%|███▉      | 860/2191 [11:54<18:24,  1.20it/s, loss=2.32, v_num=647]Epoch 25:  39%|███▉      | 860/2191 [11:54<18:24,  1.20it/s, loss=2.38, v_num=647]Epoch 25:  40%|███▉      | 870/2191 [12:02<18:15,  1.21it/s, loss=2.38, v_num=647]Epoch 25:  40%|███▉      | 870/2191 [12:02<18:15,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  40%|████      | 880/2191 [12:11<18:08,  1.20it/s, loss=2.29, v_num=647]Epoch 25:  40%|████      | 880/2191 [12:11<18:08,  1.20it/s, loss=2.24, v_num=647]Epoch 25:  41%|████      | 890/2191 [12:18<17:58,  1.21it/s, loss=2.24, v_num=647]Epoch 25:  41%|████      | 890/2191 [12:18<17:58,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  41%|████      | 900/2191 [12:26<17:48,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  41%|████      | 900/2191 [12:26<17:48,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  42%|████▏     | 910/2191 [12:33<17:39,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  42%|████▏     | 910/2191 [12:33<17:39,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  42%|████▏     | 920/2191 [12:40<17:29,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  42%|████▏     | 920/2191 [12:40<17:29,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  42%|████▏     | 930/2191 [12:48<17:20,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  42%|████▏     | 930/2191 [12:48<17:20,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  43%|████▎     | 940/2191 [12:55<17:10,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  43%|████▎     | 940/2191 [12:55<17:10,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  43%|████▎     | 950/2191 [13:02<17:01,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  43%|████▎     | 950/2191 [13:02<17:01,  1.22it/s, loss=2.27, v_num=647]Epoch 25:  44%|████▍     | 960/2191 [13:10<16:53,  1.21it/s, loss=2.27, v_num=647]Epoch 25:  44%|████▍     | 960/2191 [13:10<16:53,  1.21it/s, loss=2.28, v_num=647]Epoch 25:  44%|████▍     | 970/2191 [13:20<16:47,  1.21it/s, loss=2.28, v_num=647]Epoch 25:  44%|████▍     | 970/2191 [13:20<16:47,  1.21it/s, loss=2.3, v_num=647] Epoch 25:  45%|████▍     | 980/2191 [13:29<16:38,  1.21it/s, loss=2.3, v_num=647]Epoch 25:  45%|████▍     | 980/2191 [13:29<16:38,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  45%|████▌     | 990/2191 [13:37<16:30,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  45%|████▌     | 990/2191 [13:37<16:30,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  46%|████▌     | 1000/2191 [13:44<16:20,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  46%|████▌     | 1000/2191 [13:44<16:20,  1.21it/s, loss=2.28, v_num=647]Epoch 25:  46%|████▌     | 1010/2191 [13:51<16:11,  1.22it/s, loss=2.28, v_num=647]Epoch 25:  46%|████▌     | 1010/2191 [13:51<16:11,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  47%|████▋     | 1020/2191 [13:59<16:03,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  47%|████▋     | 1020/2191 [13:59<16:03,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  47%|████▋     | 1030/2191 [14:07<15:54,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  47%|████▋     | 1030/2191 [14:07<15:54,  1.22it/s, loss=2.36, v_num=647]Epoch 25:  47%|████▋     | 1040/2191 [14:16<15:46,  1.22it/s, loss=2.36, v_num=647]Epoch 25:  47%|████▋     | 1040/2191 [14:16<15:46,  1.22it/s, loss=2.35, v_num=647]Epoch 25:  48%|████▊     | 1050/2191 [14:24<15:38,  1.22it/s, loss=2.35, v_num=647]Epoch 25:  48%|████▊     | 1050/2191 [14:24<15:38,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  48%|████▊     | 1060/2191 [14:33<15:30,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  48%|████▊     | 1060/2191 [14:33<15:30,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  49%|████▉     | 1070/2191 [14:40<15:21,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  49%|████▉     | 1070/2191 [14:40<15:21,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  49%|████▉     | 1080/2191 [14:47<15:12,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  49%|████▉     | 1080/2191 [14:47<15:12,  1.22it/s, loss=2.28, v_num=647]Epoch 25:  50%|████▉     | 1090/2191 [14:56<15:04,  1.22it/s, loss=2.28, v_num=647]Epoch 25:  50%|████▉     | 1090/2191 [14:56<15:04,  1.22it/s, loss=2.3, v_num=647] Epoch 25:  50%|█████     | 1100/2191 [15:05<14:56,  1.22it/s, loss=2.3, v_num=647]Epoch 25:  50%|█████     | 1100/2191 [15:05<14:56,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  51%|█████     | 1110/2191 [15:12<14:48,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  51%|█████     | 1110/2191 [15:12<14:48,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  51%|█████     | 1120/2191 [15:23<14:42,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  51%|█████     | 1120/2191 [15:23<14:42,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  52%|█████▏    | 1130/2191 [15:32<14:35,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  52%|█████▏    | 1130/2191 [15:32<14:35,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  52%|█████▏    | 1140/2191 [15:40<14:26,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  52%|█████▏    | 1140/2191 [15:40<14:26,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  52%|█████▏    | 1150/2191 [15:49<14:18,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  52%|█████▏    | 1150/2191 [15:49<14:18,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  53%|█████▎    | 1160/2191 [15:57<14:09,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  53%|█████▎    | 1160/2191 [15:57<14:09,  1.21it/s, loss=2.36, v_num=647]Epoch 25:  53%|█████▎    | 1170/2191 [16:04<14:01,  1.21it/s, loss=2.36, v_num=647]Epoch 25:  53%|█████▎    | 1170/2191 [16:04<14:01,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  54%|█████▍    | 1180/2191 [16:14<13:53,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  54%|█████▍    | 1180/2191 [16:14<13:53,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  54%|█████▍    | 1190/2191 [16:21<13:44,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  54%|█████▍    | 1190/2191 [16:21<13:44,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  55%|█████▍    | 1200/2191 [16:29<13:36,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  55%|█████▍    | 1200/2191 [16:29<13:36,  1.21it/s, loss=2.28, v_num=647]Epoch 25:  55%|█████▌    | 1210/2191 [16:37<13:28,  1.21it/s, loss=2.28, v_num=647]Epoch 25:  55%|█████▌    | 1210/2191 [16:37<13:28,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  56%|█████▌    | 1220/2191 [16:47<13:20,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  56%|█████▌    | 1220/2191 [16:47<13:20,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  56%|█████▌    | 1230/2191 [16:55<13:12,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  56%|█████▌    | 1230/2191 [16:55<13:12,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  57%|█████▋    | 1240/2191 [17:02<13:03,  1.21it/s, loss=2.32, v_num=647]Epoch 25:  57%|█████▋    | 1240/2191 [17:02<13:03,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  57%|█████▋    | 1250/2191 [17:12<12:56,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  57%|█████▋    | 1250/2191 [17:12<12:56,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  58%|█████▊    | 1260/2191 [17:20<12:48,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  58%|█████▊    | 1260/2191 [17:20<12:48,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  58%|█████▊    | 1270/2191 [17:28<12:39,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  58%|█████▊    | 1270/2191 [17:28<12:39,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  58%|█████▊    | 1280/2191 [17:36<12:31,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  58%|█████▊    | 1280/2191 [17:36<12:31,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  59%|█████▉    | 1290/2191 [17:45<12:23,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  59%|█████▉    | 1290/2191 [17:45<12:23,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  59%|█████▉    | 1300/2191 [17:53<12:14,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  59%|█████▉    | 1300/2191 [17:53<12:14,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  60%|█████▉    | 1310/2191 [18:00<12:06,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  60%|█████▉    | 1310/2191 [18:00<12:06,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  60%|██████    | 1320/2191 [18:08<11:57,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  60%|██████    | 1320/2191 [18:08<11:57,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  61%|██████    | 1330/2191 [18:16<11:49,  1.21it/s, loss=2.33, v_num=647]Epoch 25:  61%|██████    | 1330/2191 [18:16<11:49,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  61%|██████    | 1340/2191 [18:25<11:41,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  61%|██████    | 1340/2191 [18:25<11:41,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  62%|██████▏   | 1350/2191 [18:32<11:32,  1.21it/s, loss=2.31, v_num=647]Epoch 25:  62%|██████▏   | 1350/2191 [18:32<11:32,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  62%|██████▏   | 1360/2191 [18:40<11:24,  1.21it/s, loss=2.29, v_num=647]Epoch 25:  62%|██████▏   | 1360/2191 [18:40<11:24,  1.21it/s, loss=2.35, v_num=647]Epoch 25:  63%|██████▎   | 1370/2191 [18:49<11:16,  1.21it/s, loss=2.35, v_num=647]Epoch 25:  63%|██████▎   | 1370/2191 [18:49<11:16,  1.21it/s, loss=2.34, v_num=647]Epoch 25:  63%|██████▎   | 1380/2191 [18:56<11:07,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  63%|██████▎   | 1380/2191 [18:56<11:07,  1.22it/s, loss=2.27, v_num=647]Epoch 25:  63%|██████▎   | 1390/2191 [19:03<10:58,  1.22it/s, loss=2.27, v_num=647]Epoch 25:  63%|██████▎   | 1390/2191 [19:03<10:58,  1.22it/s, loss=2.3, v_num=647] Epoch 25:  64%|██████▍   | 1400/2191 [19:10<10:49,  1.22it/s, loss=2.3, v_num=647]Epoch 25:  64%|██████▍   | 1400/2191 [19:10<10:49,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  64%|██████▍   | 1410/2191 [19:18<10:41,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  64%|██████▍   | 1410/2191 [19:18<10:41,  1.22it/s, loss=2.28, v_num=647]Epoch 25:  65%|██████▍   | 1420/2191 [19:27<10:33,  1.22it/s, loss=2.28, v_num=647]Epoch 25:  65%|██████▍   | 1420/2191 [19:27<10:33,  1.22it/s, loss=2.3, v_num=647] Epoch 25:  65%|██████▌   | 1430/2191 [19:34<10:24,  1.22it/s, loss=2.3, v_num=647]Epoch 25:  65%|██████▌   | 1430/2191 [19:34<10:24,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  66%|██████▌   | 1440/2191 [19:43<10:16,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  66%|██████▌   | 1440/2191 [19:43<10:16,  1.22it/s, loss=2.3, v_num=647] Epoch 25:  66%|██████▌   | 1450/2191 [19:51<10:08,  1.22it/s, loss=2.3, v_num=647]Epoch 25:  66%|██████▌   | 1450/2191 [19:51<10:08,  1.22it/s, loss=2.29, v_num=647]Epoch 25:  67%|██████▋   | 1460/2191 [19:59<10:00,  1.22it/s, loss=2.29, v_num=647]Epoch 25:  67%|██████▋   | 1460/2191 [19:59<10:00,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  67%|██████▋   | 1470/2191 [20:06<09:51,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  67%|██████▋   | 1470/2191 [20:06<09:51,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  68%|██████▊   | 1480/2191 [20:14<09:42,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  68%|██████▊   | 1480/2191 [20:14<09:42,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  68%|██████▊   | 1490/2191 [20:22<09:34,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  68%|██████▊   | 1490/2191 [20:22<09:34,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  68%|██████▊   | 1500/2191 [20:29<09:26,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  68%|██████▊   | 1500/2191 [20:29<09:26,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  69%|██████▉   | 1510/2191 [20:39<09:18,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  69%|██████▉   | 1510/2191 [20:39<09:18,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  69%|██████▉   | 1520/2191 [20:48<09:10,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  69%|██████▉   | 1520/2191 [20:48<09:10,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  70%|██████▉   | 1530/2191 [20:57<09:03,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  70%|██████▉   | 1530/2191 [20:57<09:03,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  70%|███████   | 1540/2191 [21:04<08:54,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  70%|███████   | 1540/2191 [21:04<08:54,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  71%|███████   | 1550/2191 [21:13<08:46,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  71%|███████   | 1550/2191 [21:13<08:46,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  71%|███████   | 1560/2191 [21:22<08:38,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  71%|███████   | 1560/2191 [21:22<08:38,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  72%|███████▏  | 1570/2191 [21:29<08:29,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  72%|███████▏  | 1570/2191 [21:29<08:29,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  72%|███████▏  | 1580/2191 [21:36<08:21,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  72%|███████▏  | 1580/2191 [21:36<08:21,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1590/2191 [21:45<08:13,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1590/2191 [21:45<08:13,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1600/2191 [21:53<08:04,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1600/2191 [21:53<08:04,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1610/2191 [22:01<07:56,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  73%|███████▎  | 1610/2191 [22:01<07:56,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  74%|███████▍  | 1620/2191 [22:09<07:48,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  74%|███████▍  | 1620/2191 [22:09<07:48,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  74%|███████▍  | 1630/2191 [22:17<07:40,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  74%|███████▍  | 1630/2191 [22:17<07:40,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  75%|███████▍  | 1640/2191 [22:26<07:31,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  75%|███████▍  | 1640/2191 [22:26<07:31,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  75%|███████▌  | 1650/2191 [22:33<07:23,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  75%|███████▌  | 1650/2191 [22:33<07:23,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  76%|███████▌  | 1660/2191 [22:42<07:15,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  76%|███████▌  | 1660/2191 [22:42<07:15,  1.22it/s, loss=2.37, v_num=647]Epoch 25:  76%|███████▌  | 1670/2191 [22:50<07:07,  1.22it/s, loss=2.37, v_num=647]Epoch 25:  76%|███████▌  | 1670/2191 [22:50<07:07,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  77%|███████▋  | 1680/2191 [22:58<06:58,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  77%|███████▋  | 1680/2191 [22:58<06:58,  1.22it/s, loss=2.29, v_num=647]Epoch 25:  77%|███████▋  | 1690/2191 [23:06<06:50,  1.22it/s, loss=2.29, v_num=647]Epoch 25:  77%|███████▋  | 1690/2191 [23:06<06:50,  1.22it/s, loss=2.35, v_num=647]Epoch 25:  78%|███████▊  | 1700/2191 [23:14<06:42,  1.22it/s, loss=2.35, v_num=647]Epoch 25:  78%|███████▊  | 1700/2191 [23:14<06:42,  1.22it/s, loss=2.38, v_num=647]Epoch 25:  78%|███████▊  | 1710/2191 [23:22<06:34,  1.22it/s, loss=2.38, v_num=647]Epoch 25:  78%|███████▊  | 1710/2191 [23:22<06:34,  1.22it/s, loss=2.36, v_num=647]Epoch 25:  79%|███████▊  | 1720/2191 [23:28<06:25,  1.22it/s, loss=2.36, v_num=647]Epoch 25:  79%|███████▊  | 1720/2191 [23:28<06:25,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  79%|███████▉  | 1730/2191 [23:37<06:17,  1.22it/s, loss=2.34, v_num=647]Epoch 25:  79%|███████▉  | 1730/2191 [23:37<06:17,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  79%|███████▉  | 1740/2191 [23:44<06:08,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  79%|███████▉  | 1740/2191 [23:44<06:08,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  80%|███████▉  | 1750/2191 [23:51<06:00,  1.22it/s, loss=2.31, v_num=647]Epoch 25:  80%|███████▉  | 1750/2191 [23:51<06:00,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  80%|████████  | 1760/2191 [23:59<05:52,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  80%|████████  | 1760/2191 [23:59<05:52,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  81%|████████  | 1770/2191 [24:07<05:44,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  81%|████████  | 1770/2191 [24:07<05:44,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  81%|████████  | 1780/2191 [24:18<05:36,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  81%|████████  | 1780/2191 [24:18<05:36,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  82%|████████▏ | 1790/2191 [24:26<05:28,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  82%|████████▏ | 1790/2191 [24:26<05:28,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  82%|████████▏ | 1800/2191 [24:33<05:19,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  82%|████████▏ | 1800/2191 [24:33<05:19,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  83%|████████▎ | 1810/2191 [24:41<05:11,  1.22it/s, loss=2.32, v_num=647]Epoch 25:  83%|████████▎ | 1810/2191 [24:41<05:11,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  83%|████████▎ | 1820/2191 [24:48<05:03,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  83%|████████▎ | 1820/2191 [24:48<05:03,  1.22it/s, loss=2.3, v_num=647] Epoch 25:  84%|████████▎ | 1830/2191 [24:55<04:54,  1.22it/s, loss=2.3, v_num=647]Epoch 25:  84%|████████▎ | 1830/2191 [24:55<04:54,  1.22it/s, loss=2.33, v_num=647]Epoch 25:  84%|████████▍ | 1840/2191 [24:59<04:45,  1.23it/s, loss=2.33, v_num=647]Epoch 25:  84%|████████▍ | 1840/2191 [24:59<04:45,  1.23it/s, loss=2.35, v_num=647]Epoch 25:  84%|████████▍ | 1850/2191 [25:05<04:37,  1.23it/s, loss=2.35, v_num=647]Epoch 25:  84%|████████▍ | 1850/2191 [25:05<04:37,  1.23it/s, loss=2.3, v_num=647] Epoch 25:  85%|████████▍ | 1860/2191 [25:12<04:29,  1.23it/s, loss=2.3, v_num=647]Epoch 25:  85%|████████▍ | 1860/2191 [25:12<04:29,  1.23it/s, loss=2.31, v_num=647]validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9760904312133789
validation_epoch_end
graph acc: 0.43450479233226835
valid accuracy: 0.9752550721168518
validation_epoch_end
graph acc: 0.34824281150159747
valid accuracy: 0.9760299324989319
25:20<04:11,  1.24it/s, loss=2.31, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9770086407661438
validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9786184430122375
validation_epoch_end
graph acc: 0.3610223642172524
valid accuracy: 0.9759562015533447

Validating:   3%|▎         | 10/313 [00:01<00:45,  6.63it/s][AEpoch 25:  86%|████████▋ | 1890/2191 [25:22<04:02,  1.24it/s, loss=2.31, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:45,  6.47it/s][AEpoch 25:  87%|████████▋ | 1900/2191 [25:23<03:53,  1.25it/s, loss=2.31, v_num=647]
Validating:  10%|▉         | 30/313 [00:03<00:34,  8.15it/s][AEpoch 25:  87%|████████▋ | 1910/2191 [25:24<03:44,  1.25it/s, loss=2.31, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:25, 10.74it/s][AEpoch 25:  88%|████████▊ | 1920/2191 [25:25<03:35,  1.26it/s, loss=2.31, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:24, 10.82it/s][AEpoch 25:  88%|████████▊ | 1930/2191 [25:26<03:26,  1.27it/s, loss=2.31, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:22, 11.06it/s][AEpoch 25:  89%|████████▊ | 1940/2191 [25:26<03:17,  1.27it/s, loss=2.31, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24,  9.82it/s][AEpoch 25:  89%|████████▉ | 1950/2191 [25:28<03:08,  1.28it/s, loss=2.31, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:24,  9.65it/s][AEpoch 25:  89%|████████▉ | 1960/2191 [25:29<03:00,  1.28it/s, loss=2.31, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:23,  9.39it/s][AEpoch 25:  90%|████████▉ | 1970/2191 [25:30<02:51,  1.29it/s, loss=2.31, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.22it/s][AEpoch 25:  90%|█████████ | 1980/2191 [25:31<02:43,  1.29it/s, loss=2.31, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.77it/s][AEpoch 25:  91%|█████████ | 1990/2191 [25:32<02:34,  1.30it/s, loss=2.31, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:21,  9.02it/s][AEpoch 25:  91%|█████████▏| 2000/2191 [25:33<02:26,  1.30it/s, loss=2.31, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.19it/s][AEpoch 25:  92%|█████████▏| 2010/2191 [25:34<02:18,  1.31it/s, loss=2.31, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.28it/s][AEpoch 25:  92%|█████████▏| 2020/2191 [25:36<02:09,  1.32it/s, loss=2.31, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.17it/s][AEpoch 25:  93%|█████████▎| 2030/2191 [25:36<02:01,  1.32it/s, loss=2.31, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:16,  9.15it/s][AEpoch 25:  93%|█████████▎| 2040/2191 [25:38<01:53,  1.33it/s, loss=2.31, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:14, 10.10it/s][AEpoch 25:  94%|█████████▎| 2050/2191 [25:38<01:45,  1.33it/s, loss=2.31, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:19<00:13, 10.10it/s][AEpoch 25:  94%|█████████▍| 2060/2191 [25:39<01:37,  1.34it/s, loss=2.31, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.45it/s][AEpoch 25:  94%|█████████▍| 2070/2191 [25:40<01:29,  1.34it/s, loss=2.31, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:10, 11.23it/s][AEpoch 25:  95%|█████████▍| 2080/2191 [25:41<01:22,  1.35it/s, loss=2.31, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:10, 10.10it/s][AEpoch 25:  95%|█████████▌| 2090/2191 [25:42<01:14,  1.36it/s, loss=2.31, v_num=647]
Validating:  70%|███████   | 220/313 [00:23<00:10,  9.16it/s][AEpoch 25:  96%|█████████▌| 2100/2191 [25:43<01:06,  1.36it/s, loss=2.31, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.39it/s][AEpoch 25:  96%|█████████▋| 2110/2191 [25:44<00:59,  1.37it/s, loss=2.31, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.39it/s][AEpoch 25:  97%|█████████▋| 2120/2191 [25:45<00:51,  1.37it/s, loss=2.31, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:05, 10.69it/s][AEpoch 25:  97%|█████████▋| 2130/2191 [25:46<00:44,  1.38it/s, loss=2.31, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.87it/s][AEpoch 25:  98%|█████████▊| 2140/2191 [25:47<00:36,  1.38it/s, loss=2.31, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 10.98it/s][AEpoch 25:  98%|█████████▊| 2150/2191 [25:48<00:29,  1.39it/s, loss=2.31, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:28<00:02, 12.16it/s][AEpoch 25:  99%|█████████▊| 2160/2191 [25:48<00:22,  1.40it/s, loss=2.31, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:29<00:02, 10.98it/s][AEpoch 25:  99%|█████████▉| 2170/2191 [25:49<00:14,  1.40it/s, loss=2.31, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.92it/s][AEpoch 25:  99%|█████████▉| 2180/2191 [25:50<00:07,  1.41it/s, loss=2.31, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 13.68it/s][AEpoch 25: 100%|█████████▉| 2190/2191 [25:51<00:00,  1.41it/s, loss=2.31, v_num=647]validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.9784502387046814
Epoch 25: 100%|██████████| 2191/2191 [25:52<00:00,  1.41it/s, loss=2.32, v_num=647]
                                                             [AEpoch 25:   0%|          | 0/2191 [00:00<00:00, 13981.01it/s, loss=2.32, v_num=647]Epoch 26:   0%|          | 0/2191 [00:00<00:00, 3572.66it/s, loss=2.32, v_num=647] Epoch 26:   0%|          | 10/2191 [00:13<46:14,  1.27s/it, loss=2.32, v_num=647] Epoch 26:   0%|          | 10/2191 [00:13<46:14,  1.27s/it, loss=2.3, v_num=647] Epoch 26:   1%|          | 20/2191 [00:24<41:40,  1.15s/it, loss=2.3, v_num=647]Epoch 26:   1%|          | 20/2191 [00:24<41:40,  1.15s/it, loss=2.27, v_num=647]Epoch 26:   1%|▏         | 30/2191 [00:34<39:43,  1.10s/it, loss=2.27, v_num=647]Epoch 26:   1%|▏         | 30/2191 [00:34<39:43,  1.10s/it, loss=2.28, v_num=647]Epoch 26:   2%|▏         | 40/2191 [00:45<39:59,  1.12s/it, loss=2.28, v_num=647]Epoch 26:   2%|▏         | 40/2191 [00:45<39:59,  1.12s/it, loss=2.28, v_num=647]Epoch 26:   2%|▏         | 50/2191 [00:55<39:02,  1.09s/it, loss=2.28, v_num=647]Epoch 26:   2%|▏         | 50/2191 [00:55<39:02,  1.09s/it, loss=2.25, v_num=647]Epoch 26:   3%|▎         | 60/2191 [01:06<38:28,  1.08s/it, loss=2.25, v_num=647]Epoch 26:   3%|▎         | 60/2191 [01:06<38:28,  1.08s/it, loss=2.26, v_num=647]Epoch 26:   3%|▎         | 70/2191 [01:15<37:41,  1.07s/it, loss=2.26, v_num=647]Epoch 26:   3%|▎         | 70/2191 [01:15<37:41,  1.07s/it, loss=2.28, v_num=647]Epoch 26:   4%|▎         | 80/2191 [01:25<37:01,  1.05s/it, loss=2.28, v_num=647]Epoch 26:   4%|▎         | 80/2191 [01:25<37:01,  1.05s/it, loss=2.24, v_num=647]Epoch 26:   4%|▍         | 90/2191 [01:34<36:13,  1.03s/it, loss=2.24, v_num=647]Epoch 26:   4%|▍         | 90/2191 [01:34<36:13,  1.03s/it, loss=2.28, v_num=647]Epoch 26:   5%|▍         | 100/2191 [01:43<35:39,  1.02s/it, loss=2.28, v_num=647]Epoch 26:   5%|▍         | 100/2191 [01:43<35:39,  1.02s/it, loss=2.31, v_num=647]Epoch 26:   5%|▌         | 110/2191 [01:52<35:02,  1.01s/it, loss=2.31, v_num=647]Epoch 26:   5%|▌         | 110/2191 [01:52<35:02,  1.01s/it, loss=2.3, v_num=647] Epoch 26:   5%|▌         | 120/2191 [02:00<34:23,  1.00it/s, loss=2.3, v_num=647]Epoch 26:   5%|▌         | 120/2191 [02:00<34:23,  1.00it/s, loss=2.3, v_num=647]Epoch 26:   6%|▌         | 130/2191 [02:08<33:36,  1.02it/s, loss=2.3, v_num=647]Epoch 26:   6%|▌         | 130/2191 [02:08<33:36,  1.02it/s, loss=2.28, v_num=647]Epoch 26:   6%|▋         | 140/2191 [02:16<33:02,  1.03it/s, loss=2.28, v_num=647]Epoch 26:   6%|▋         | 140/2191 [02:16<33:02,  1.03it/s, loss=2.28, v_num=647]Epoch 26:   7%|▋         | 150/2191 [02:24<32:30,  1.05it/s, loss=2.28, v_num=647]Epoch 26:   7%|▋         | 150/2191 [02:24<32:30,  1.05it/s, loss=2.28, v_num=647]Epoch 26:   7%|▋         | 160/2191 [02:32<31:57,  1.06it/s, loss=2.28, v_num=647]Epoch 26:   7%|▋         | 160/2191 [02:32<31:57,  1.06it/s, loss=2.27, v_num=647]Epoch 26:   8%|▊         | 170/2191 [02:39<31:22,  1.07it/s, loss=2.27, v_num=647]Epoch 26:   8%|▊         | 170/2191 [02:39<31:22,  1.07it/s, loss=2.29, v_num=647]Epoch 26:   8%|▊         | 180/2191 [02:46<30:55,  1.08it/s, loss=2.29, v_num=647]Epoch 26:   8%|▊         | 180/2191 [02:46<30:55,  1.08it/s, loss=2.29, v_num=647]Epoch 26:   9%|▊         | 190/2191 [02:53<30:22,  1.10it/s, loss=2.29, v_num=647]Epoch 26:   9%|▊         | 190/2191 [02:53<30:22,  1.10it/s, loss=2.27, v_num=647]Epoch 26:   9%|▉         | 200/2191 [03:01<29:56,  1.11it/s, loss=2.27, v_num=647]Epoch 26:   9%|▉         | 200/2191 [03:01<29:56,  1.11it/s, loss=2.25, v_num=647]Epoch 26:  10%|▉         | 210/2191 [03:08<29:29,  1.12it/s, loss=2.25, v_num=647]Epoch 26:  10%|▉         | 210/2191 [03:08<29:29,  1.12it/s, loss=2.25, v_num=647]Epoch 26:  10%|█         | 220/2191 [03:15<29:05,  1.13it/s, loss=2.25, v_num=647]Epoch 26:  10%|█         | 220/2191 [03:15<29:05,  1.13it/s, loss=2.27, v_num=647]Epoch 26:  10%|█         | 230/2191 [03:22<28:41,  1.14it/s, loss=2.27, v_num=647]Epoch 26:  10%|█         | 230/2191 [03:22<28:41,  1.14it/s, loss=2.3, v_num=647] Epoch 26:  11%|█         | 240/2191 [03:29<28:19,  1.15it/s, loss=2.3, v_num=647]Epoch 26:  11%|█         | 240/2191 [03:29<28:19,  1.15it/s, loss=2.28, v_num=647]Epoch 26:  11%|█▏        | 250/2191 [03:36<27:55,  1.16it/s, loss=2.28, v_num=647]Epoch 26:  11%|█▏        | 250/2191 [03:36<27:55,  1.16it/s, loss=2.29, v_num=647]Epoch 26:  12%|█▏        | 260/2191 [03:43<27:34,  1.17it/s, loss=2.29, v_num=647]Epoch 26:  12%|█▏        | 260/2191 [03:43<27:34,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  12%|█▏        | 270/2191 [03:50<27:14,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  12%|█▏        | 270/2191 [03:50<27:14,  1.17it/s, loss=2.29, v_num=647]Epoch 26:  13%|█▎        | 280/2191 [03:57<26:53,  1.18it/s, loss=2.29, v_num=647]Epoch 26:  13%|█▎        | 280/2191 [03:57<26:53,  1.18it/s, loss=2.32, v_num=647]Epoch 26:  13%|█▎        | 290/2191 [04:04<26:38,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  13%|█▎        | 290/2191 [04:04<26:38,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  14%|█▎        | 300/2191 [04:12<26:25,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  14%|█▎        | 300/2191 [04:12<26:25,  1.19it/s, loss=2.28, v_num=647]Epoch 26:  14%|█▍        | 310/2191 [04:21<26:21,  1.19it/s, loss=2.28, v_num=647]Epoch 26:  14%|█▍        | 310/2191 [04:21<26:21,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  15%|█▍        | 320/2191 [04:31<26:21,  1.18it/s, loss=2.32, v_num=647]Epoch 26:  15%|█▍        | 320/2191 [04:31<26:21,  1.18it/s, loss=2.31, v_num=647]Epoch 26:  15%|█▌        | 330/2191 [04:39<26:13,  1.18it/s, loss=2.31, v_num=647]Epoch 26:  15%|█▌        | 330/2191 [04:39<26:13,  1.18it/s, loss=2.25, v_num=647]Epoch 26:  16%|█▌        | 340/2191 [04:48<26:04,  1.18it/s, loss=2.25, v_num=647]Epoch 26:  16%|█▌        | 340/2191 [04:48<26:04,  1.18it/s, loss=2.26, v_num=647]Epoch 26:  16%|█▌        | 350/2191 [04:57<26:02,  1.18it/s, loss=2.26, v_num=647]Epoch 26:  16%|█▌        | 350/2191 [04:57<26:02,  1.18it/s, loss=2.3, v_num=647] Epoch 26:  16%|█▋        | 360/2191 [05:05<25:48,  1.18it/s, loss=2.3, v_num=647]Epoch 26:  16%|█▋        | 360/2191 [05:05<25:48,  1.18it/s, loss=2.31, v_num=647]Epoch 26:  17%|█▋        | 370/2191 [05:14<25:46,  1.18it/s, loss=2.31, v_num=647]Epoch 26:  17%|█▋        | 370/2191 [05:14<25:46,  1.18it/s, loss=2.29, v_num=647]Epoch 26:  17%|█▋        | 380/2191 [05:23<25:37,  1.18it/s, loss=2.29, v_num=647]Epoch 26:  17%|█▋        | 380/2191 [05:23<25:37,  1.18it/s, loss=2.26, v_num=647]Epoch 26:  18%|█▊        | 390/2191 [05:33<25:34,  1.17it/s, loss=2.26, v_num=647]Epoch 26:  18%|█▊        | 390/2191 [05:33<25:34,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  18%|█▊        | 400/2191 [05:42<25:27,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  18%|█▊        | 400/2191 [05:42<25:27,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  19%|█▊        | 410/2191 [05:50<25:17,  1.17it/s, loss=2.31, v_num=647]Epoch 26:  19%|█▊        | 410/2191 [05:50<25:17,  1.17it/s, loss=2.28, v_num=647]Epoch 26:  19%|█▉        | 420/2191 [05:56<25:01,  1.18it/s, loss=2.28, v_num=647]Epoch 26:  19%|█▉        | 420/2191 [05:56<25:01,  1.18it/s, loss=2.32, v_num=647]Epoch 26:  20%|█▉        | 430/2191 [06:04<24:47,  1.18it/s, loss=2.32, v_num=647]Epoch 26:  20%|█▉        | 430/2191 [06:04<24:47,  1.18it/s, loss=2.31, v_num=647]Epoch 26:  20%|██        | 440/2191 [06:11<24:34,  1.19it/s, loss=2.31, v_num=647]Epoch 26:  20%|██        | 440/2191 [06:11<24:34,  1.19it/s, loss=2.33, v_num=647]Epoch 26:  21%|██        | 450/2191 [06:19<24:26,  1.19it/s, loss=2.33, v_num=647]Epoch 26:  21%|██        | 450/2191 [06:19<24:26,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  21%|██        | 460/2191 [06:28<24:20,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  21%|██        | 460/2191 [06:28<24:20,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  21%|██▏       | 470/2191 [06:35<24:06,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  21%|██▏       | 470/2191 [06:35<24:06,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  22%|██▏       | 480/2191 [06:44<24:00,  1.19it/s, loss=2.32, v_num=647]Epoch 26:  22%|██▏       | 480/2191 [06:44<24:00,  1.19it/s, loss=2.3, v_num=647] Epoch 26:  22%|██▏       | 490/2191 [06:52<23:48,  1.19it/s, loss=2.3, v_num=647]Epoch 26:  22%|██▏       | 490/2191 [06:52<23:48,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  23%|██▎       | 500/2191 [07:00<23:39,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  23%|██▎       | 500/2191 [07:00<23:39,  1.19it/s, loss=2.27, v_num=647]Epoch 26:  23%|██▎       | 510/2191 [07:07<23:26,  1.20it/s, loss=2.27, v_num=647]Epoch 26:  23%|██▎       | 510/2191 [07:07<23:26,  1.20it/s, loss=2.27, v_num=647]Epoch 26:  24%|██▎       | 520/2191 [07:15<23:15,  1.20it/s, loss=2.27, v_num=647]Epoch 26:  24%|██▎       | 520/2191 [07:15<23:15,  1.20it/s, loss=2.29, v_num=647]Epoch 26:  24%|██▍       | 530/2191 [07:22<23:05,  1.20it/s, loss=2.29, v_num=647]Epoch 26:  24%|██▍       | 530/2191 [07:22<23:05,  1.20it/s, loss=2.25, v_num=647]Epoch 26:  25%|██▍       | 540/2191 [07:31<22:56,  1.20it/s, loss=2.25, v_num=647]Epoch 26:  25%|██▍       | 540/2191 [07:31<22:56,  1.20it/s, loss=2.24, v_num=647]Epoch 26:  25%|██▌       | 550/2191 [07:39<22:47,  1.20it/s, loss=2.24, v_num=647]Epoch 26:  25%|██▌       | 550/2191 [07:39<22:47,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  26%|██▌       | 560/2191 [07:46<22:36,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  26%|██▌       | 560/2191 [07:46<22:36,  1.20it/s, loss=2.33, v_num=647]Epoch 26:  26%|██▌       | 570/2191 [07:53<22:24,  1.21it/s, loss=2.33, v_num=647]Epoch 26:  26%|██▌       | 570/2191 [07:53<22:24,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  26%|██▋       | 580/2191 [08:01<22:15,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  26%|██▋       | 580/2191 [08:01<22:15,  1.21it/s, loss=2.27, v_num=647]Epoch 26:  27%|██▋       | 590/2191 [08:09<22:06,  1.21it/s, loss=2.27, v_num=647]Epoch 26:  27%|██▋       | 590/2191 [08:09<22:06,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  27%|██▋       | 600/2191 [08:18<22:00,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  27%|██▋       | 600/2191 [08:18<22:00,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  28%|██▊       | 610/2191 [08:26<21:51,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  28%|██▊       | 610/2191 [08:26<21:51,  1.21it/s, loss=2.28, v_num=647]Epoch 26:  28%|██▊       | 620/2191 [08:36<21:46,  1.20it/s, loss=2.28, v_num=647]Epoch 26:  28%|██▊       | 620/2191 [08:36<21:46,  1.20it/s, loss=2.28, v_num=647]Epoch 26:  29%|██▉       | 630/2191 [08:45<21:40,  1.20it/s, loss=2.28, v_num=647]Epoch 26:  29%|██▉       | 630/2191 [08:45<21:40,  1.20it/s, loss=2.33, v_num=647]Epoch 26:  29%|██▉       | 640/2191 [08:54<21:32,  1.20it/s, loss=2.33, v_num=647]Epoch 26:  29%|██▉       | 640/2191 [08:54<21:32,  1.20it/s, loss=2.36, v_num=647]Epoch 26:  30%|██▉       | 650/2191 [09:02<21:23,  1.20it/s, loss=2.36, v_num=647]Epoch 26:  30%|██▉       | 650/2191 [09:02<21:23,  1.20it/s, loss=2.34, v_num=647]Epoch 26:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.34, v_num=647]Epoch 26:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.34, v_num=647]Epoch 26:  31%|███       | 670/2191 [09:19<21:08,  1.20it/s, loss=2.34, v_num=647]Epoch 26:  31%|███       | 670/2191 [09:19<21:08,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  31%|███       | 680/2191 [09:27<20:58,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  31%|███       | 680/2191 [09:27<20:58,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.3, v_num=647] Epoch 26:  32%|███▏      | 700/2191 [09:44<20:43,  1.20it/s, loss=2.3, v_num=647]Epoch 26:  32%|███▏      | 700/2191 [09:44<20:43,  1.20it/s, loss=2.33, v_num=647]Epoch 26:  32%|███▏      | 710/2191 [09:53<20:35,  1.20it/s, loss=2.33, v_num=647]Epoch 26:  32%|███▏      | 710/2191 [09:53<20:35,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  33%|███▎      | 720/2191 [10:01<20:26,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  33%|███▎      | 720/2191 [10:01<20:26,  1.20it/s, loss=2.3, v_num=647] Epoch 26:  33%|███▎      | 730/2191 [10:10<20:19,  1.20it/s, loss=2.3, v_num=647]Epoch 26:  33%|███▎      | 730/2191 [10:10<20:19,  1.20it/s, loss=2.35, v_num=647]Epoch 26:  34%|███▍      | 740/2191 [10:17<20:08,  1.20it/s, loss=2.35, v_num=647]Epoch 26:  34%|███▍      | 740/2191 [10:17<20:08,  1.20it/s, loss=2.32, v_num=647]Epoch 26:  34%|███▍      | 750/2191 [10:24<19:57,  1.20it/s, loss=2.32, v_num=647]Epoch 26:  34%|███▍      | 750/2191 [10:24<19:57,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  35%|███▍      | 760/2191 [10:32<19:49,  1.20it/s, loss=2.31, v_num=647]Epoch 26:  35%|███▍      | 760/2191 [10:32<19:49,  1.20it/s, loss=2.32, v_num=647]Epoch 26:  35%|███▌      | 770/2191 [10:39<19:39,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  35%|███▌      | 770/2191 [10:39<19:39,  1.21it/s, loss=2.27, v_num=647]Epoch 26:  36%|███▌      | 780/2191 [10:47<19:29,  1.21it/s, loss=2.27, v_num=647]Epoch 26:  36%|███▌      | 780/2191 [10:47<19:29,  1.21it/s, loss=2.28, v_num=647]Epoch 26:  36%|███▌      | 790/2191 [10:54<19:18,  1.21it/s, loss=2.28, v_num=647]Epoch 26:  36%|███▌      | 790/2191 [10:54<19:18,  1.21it/s, loss=2.3, v_num=647] Epoch 26:  37%|███▋      | 800/2191 [11:02<19:09,  1.21it/s, loss=2.3, v_num=647]Epoch 26:  37%|███▋      | 800/2191 [11:02<19:09,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  37%|███▋      | 810/2191 [11:09<18:59,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  37%|███▋      | 810/2191 [11:09<18:59,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  37%|███▋      | 820/2191 [11:16<18:49,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  37%|███▋      | 820/2191 [11:16<18:49,  1.21it/s, loss=2.34, v_num=647]Epoch 26:  38%|███▊      | 830/2191 [11:25<18:43,  1.21it/s, loss=2.34, v_num=647]Epoch 26:  38%|███▊      | 830/2191 [11:25<18:43,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  38%|███▊      | 840/2191 [11:33<18:34,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  38%|███▊      | 840/2191 [11:33<18:34,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  39%|███▉      | 850/2191 [11:41<18:24,  1.21it/s, loss=2.29, v_num=647]Epoch 26:  39%|███▉      | 850/2191 [11:41<18:24,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  39%|███▉      | 860/2191 [11:48<18:15,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  39%|███▉      | 860/2191 [11:48<18:15,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  40%|████      | 880/2191 [12:06<18:01,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  40%|████      | 880/2191 [12:06<18:01,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  41%|████      | 890/2191 [12:15<17:53,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  41%|████      | 890/2191 [12:15<17:53,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  41%|████      | 900/2191 [12:22<17:43,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  41%|████      | 900/2191 [12:22<17:43,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  42%|████▏     | 910/2191 [12:30<17:35,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  42%|████▏     | 910/2191 [12:30<17:35,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  42%|████▏     | 920/2191 [12:39<17:27,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  42%|████▏     | 920/2191 [12:39<17:27,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  42%|████▏     | 930/2191 [12:45<17:17,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  42%|████▏     | 930/2191 [12:45<17:17,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  43%|████▎     | 940/2191 [12:53<17:08,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  43%|████▎     | 940/2191 [12:53<17:08,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  43%|████▎     | 950/2191 [13:02<17:00,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  43%|████▎     | 950/2191 [13:02<17:00,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  44%|████▍     | 960/2191 [13:09<16:51,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  44%|████▍     | 960/2191 [13:09<16:51,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  44%|████▍     | 970/2191 [13:17<16:42,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  44%|████▍     | 970/2191 [13:17<16:42,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  45%|████▍     | 980/2191 [13:25<16:34,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  45%|████▍     | 980/2191 [13:25<16:34,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  45%|████▌     | 990/2191 [13:33<16:25,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  45%|████▌     | 990/2191 [13:33<16:25,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  46%|████▌     | 1000/2191 [13:41<16:17,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  46%|████▌     | 1000/2191 [13:41<16:17,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  46%|████▌     | 1010/2191 [13:48<16:08,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  46%|████▌     | 1010/2191 [13:48<16:08,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  47%|████▋     | 1020/2191 [13:56<15:59,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  47%|████▋     | 1020/2191 [13:56<15:59,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  47%|████▋     | 1040/2191 [14:11<15:41,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  47%|████▋     | 1040/2191 [14:11<15:41,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  48%|████▊     | 1050/2191 [14:19<15:33,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  48%|████▊     | 1050/2191 [14:19<15:33,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  48%|████▊     | 1060/2191 [14:28<15:25,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  48%|████▊     | 1060/2191 [14:28<15:25,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  49%|████▉     | 1070/2191 [14:35<15:16,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  49%|████▉     | 1070/2191 [14:35<15:16,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  49%|████▉     | 1080/2191 [14:43<15:07,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  49%|████▉     | 1080/2191 [14:43<15:07,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  50%|████▉     | 1090/2191 [14:52<15:00,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  50%|████▉     | 1090/2191 [14:52<15:00,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  50%|█████     | 1100/2191 [14:59<14:51,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  50%|█████     | 1100/2191 [14:59<14:51,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  51%|█████     | 1110/2191 [15:06<14:41,  1.23it/s, loss=2.29, v_num=647]Epoch 26:  51%|█████     | 1110/2191 [15:06<14:41,  1.23it/s, loss=2.29, v_num=647]Epoch 26:  51%|█████     | 1120/2191 [15:13<14:32,  1.23it/s, loss=2.29, v_num=647]Epoch 26:  51%|█████     | 1120/2191 [15:13<14:32,  1.23it/s, loss=2.27, v_num=647]Epoch 26:  52%|█████▏    | 1130/2191 [15:21<14:24,  1.23it/s, loss=2.27, v_num=647]Epoch 26:  52%|█████▏    | 1130/2191 [15:21<14:24,  1.23it/s, loss=2.31, v_num=647]Epoch 26:  52%|█████▏    | 1140/2191 [15:29<14:16,  1.23it/s, loss=2.31, v_num=647]Epoch 26:  52%|█████▏    | 1140/2191 [15:29<14:16,  1.23it/s, loss=2.35, v_num=647]Epoch 26:  52%|█████▏    | 1150/2191 [15:37<14:07,  1.23it/s, loss=2.35, v_num=647]Epoch 26:  52%|█████▏    | 1150/2191 [15:37<14:07,  1.23it/s, loss=2.32, v_num=647]Epoch 26:  52%|█████▏    | 1150/2191 [15:47<14:17,  1.21it/s, loss=2.32, v_num=647]Epoch 26:  53%|█████▎    | 1160/2191 [15:55<14:08,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  53%|█████▎    | 1160/2191 [15:55<14:08,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  53%|█████▎    | 1170/2191 [16:04<14:01,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  53%|█████▎    | 1170/2191 [16:04<14:01,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  54%|█████▍    | 1180/2191 [16:12<13:52,  1.21it/s, loss=2.31, v_num=647]Epoch 26:  54%|█████▍    | 1180/2191 [16:12<13:52,  1.21it/s, loss=2.33, v_num=647]Epoch 26:  54%|█████▍    | 1190/2191 [16:20<13:44,  1.21it/s, loss=2.33, v_num=647]Epoch 26:  54%|█████▍    | 1190/2191 [16:20<13:44,  1.21it/s, loss=2.35, v_num=647]Epoch 26:  55%|█████▍    | 1200/2191 [16:28<13:35,  1.22it/s, loss=2.35, v_num=647]Epoch 26:  55%|█████▍    | 1200/2191 [16:28<13:35,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  55%|█████▌    | 1210/2191 [16:36<13:27,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  55%|█████▌    | 1210/2191 [16:36<13:27,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  56%|█████▌    | 1220/2191 [16:43<13:17,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  56%|█████▌    | 1220/2191 [16:43<13:17,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  56%|█████▌    | 1230/2191 [16:52<13:10,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  56%|█████▌    | 1230/2191 [16:52<13:10,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  57%|█████▋    | 1240/2191 [17:01<13:02,  1.21it/s, loss=2.34, v_num=647]Epoch 26:  57%|█████▋    | 1240/2191 [17:01<13:02,  1.21it/s, loss=2.34, v_num=647]Epoch 26:  57%|█████▋    | 1250/2191 [17:09<12:54,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  57%|█████▋    | 1250/2191 [17:09<12:54,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  58%|█████▊    | 1260/2191 [17:17<12:46,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  58%|█████▊    | 1260/2191 [17:17<12:46,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  58%|█████▊    | 1270/2191 [17:24<12:37,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  58%|█████▊    | 1270/2191 [17:24<12:37,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  58%|█████▊    | 1280/2191 [17:32<12:28,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  58%|█████▊    | 1280/2191 [17:32<12:28,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  59%|█████▉    | 1290/2191 [17:41<12:20,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  59%|█████▉    | 1290/2191 [17:41<12:20,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  59%|█████▉    | 1300/2191 [17:49<12:12,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  59%|█████▉    | 1300/2191 [17:49<12:12,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  60%|█████▉    | 1310/2191 [17:58<12:04,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  60%|█████▉    | 1310/2191 [17:58<12:04,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  60%|██████    | 1320/2191 [18:06<11:56,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  60%|██████    | 1320/2191 [18:06<11:56,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  61%|██████    | 1330/2191 [18:14<11:48,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  61%|██████    | 1330/2191 [18:14<11:48,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  61%|██████    | 1340/2191 [18:23<11:40,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  61%|██████    | 1340/2191 [18:23<11:40,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  62%|██████▏   | 1350/2191 [18:30<11:31,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  62%|██████▏   | 1350/2191 [18:30<11:31,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  62%|██████▏   | 1360/2191 [18:37<11:22,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  62%|██████▏   | 1360/2191 [18:37<11:22,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  63%|██████▎   | 1370/2191 [18:44<11:13,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  63%|██████▎   | 1370/2191 [18:44<11:13,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  63%|██████▎   | 1380/2191 [18:51<11:04,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  63%|██████▎   | 1380/2191 [18:51<11:04,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  63%|██████▎   | 1390/2191 [18:58<10:55,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  63%|██████▎   | 1390/2191 [18:58<10:55,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  64%|██████▍   | 1400/2191 [19:05<10:46,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  64%|██████▍   | 1400/2191 [19:05<10:46,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  64%|██████▍   | 1410/2191 [19:13<10:38,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  64%|██████▍   | 1410/2191 [19:13<10:38,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  65%|██████▍   | 1420/2191 [19:20<10:29,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  65%|██████▍   | 1420/2191 [19:20<10:29,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  65%|██████▌   | 1430/2191 [19:29<10:21,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  65%|██████▌   | 1430/2191 [19:29<10:21,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  66%|██████▌   | 1440/2191 [19:39<10:14,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  66%|██████▌   | 1440/2191 [19:39<10:14,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  66%|██████▌   | 1450/2191 [19:48<10:07,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  66%|██████▌   | 1450/2191 [19:48<10:07,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  67%|██████▋   | 1460/2191 [19:55<09:58,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  67%|██████▋   | 1460/2191 [19:55<09:58,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  67%|██████▋   | 1470/2191 [20:03<09:49,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  67%|██████▋   | 1470/2191 [20:03<09:49,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  68%|██████▊   | 1480/2191 [20:10<09:41,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  68%|██████▊   | 1480/2191 [20:10<09:41,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  68%|██████▊   | 1490/2191 [20:18<09:33,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  68%|██████▊   | 1490/2191 [20:18<09:33,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  68%|██████▊   | 1500/2191 [20:26<09:24,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  68%|██████▊   | 1500/2191 [20:26<09:24,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  69%|██████▉   | 1510/2191 [20:34<09:16,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  69%|██████▉   | 1510/2191 [20:34<09:16,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  69%|██████▉   | 1520/2191 [20:42<09:08,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  69%|██████▉   | 1520/2191 [20:42<09:08,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  70%|██████▉   | 1530/2191 [20:52<09:00,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  70%|██████▉   | 1530/2191 [20:52<09:00,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  70%|███████   | 1540/2191 [21:01<08:53,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  70%|███████   | 1540/2191 [21:01<08:53,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  71%|███████   | 1550/2191 [21:10<08:45,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  71%|███████   | 1550/2191 [21:10<08:45,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  71%|███████   | 1560/2191 [21:18<08:36,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  71%|███████   | 1560/2191 [21:18<08:36,  1.22it/s, loss=2.35, v_num=647]Epoch 26:  72%|███████▏  | 1570/2191 [21:25<08:28,  1.22it/s, loss=2.35, v_num=647]Epoch 26:  72%|███████▏  | 1570/2191 [21:25<08:28,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  72%|███████▏  | 1580/2191 [21:32<08:19,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  72%|███████▏  | 1580/2191 [21:32<08:19,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  73%|███████▎  | 1590/2191 [21:41<08:11,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  73%|███████▎  | 1590/2191 [21:41<08:11,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  73%|███████▎  | 1600/2191 [21:51<08:03,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  73%|███████▎  | 1600/2191 [21:51<08:03,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  73%|███████▎  | 1610/2191 [21:59<07:55,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  73%|███████▎  | 1610/2191 [21:59<07:55,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  74%|███████▍  | 1620/2191 [22:07<07:47,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  74%|███████▍  | 1620/2191 [22:07<07:47,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  74%|███████▍  | 1630/2191 [22:14<07:39,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  74%|███████▍  | 1630/2191 [22:14<07:39,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  75%|███████▍  | 1640/2191 [22:22<07:30,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  75%|███████▍  | 1640/2191 [22:22<07:30,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  75%|███████▌  | 1650/2191 [22:30<07:22,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  75%|███████▌  | 1650/2191 [22:30<07:22,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  76%|███████▌  | 1660/2191 [22:39<07:14,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  76%|███████▌  | 1660/2191 [22:39<07:14,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  76%|███████▌  | 1670/2191 [22:48<07:06,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  76%|███████▌  | 1670/2191 [22:48<07:06,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  77%|███████▋  | 1680/2191 [22:57<06:58,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  77%|███████▋  | 1680/2191 [22:57<06:58,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  77%|███████▋  | 1690/2191 [23:04<06:50,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  77%|███████▋  | 1690/2191 [23:04<06:50,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  78%|███████▊  | 1700/2191 [23:11<06:41,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  78%|███████▊  | 1700/2191 [23:11<06:41,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  78%|███████▊  | 1710/2191 [23:19<06:33,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  78%|███████▊  | 1710/2191 [23:19<06:33,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  79%|███████▊  | 1720/2191 [23:26<06:24,  1.22it/s, loss=2.36, v_num=647]Epoch 26:  79%|███████▊  | 1720/2191 [23:26<06:24,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  79%|███████▉  | 1730/2191 [23:34<06:16,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  79%|███████▉  | 1730/2191 [23:34<06:16,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  79%|███████▉  | 1740/2191 [23:42<06:08,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  79%|███████▉  | 1740/2191 [23:42<06:08,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  80%|███████▉  | 1750/2191 [23:50<06:00,  1.22it/s, loss=2.33, v_num=647]Epoch 26:  80%|███████▉  | 1750/2191 [23:50<06:00,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  80%|████████  | 1760/2191 [23:59<05:52,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  80%|████████  | 1760/2191 [23:59<05:52,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  81%|████████  | 1770/2191 [24:07<05:44,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  81%|████████  | 1770/2191 [24:07<05:44,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  81%|████████  | 1780/2191 [24:17<05:36,  1.22it/s, loss=2.37, v_num=647]Epoch 26:  81%|████████  | 1780/2191 [24:17<05:36,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  82%|████████▏ | 1790/2191 [24:23<05:27,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  82%|████████▏ | 1790/2191 [24:23<05:27,  1.22it/s, loss=2.3, v_num=647] Epoch 26:  82%|████████▏ | 1800/2191 [24:31<05:19,  1.22it/s, loss=2.3, v_num=647]Epoch 26:  82%|████████▏ | 1800/2191 [24:31<05:19,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  83%|████████▎ | 1810/2191 [24:41<05:11,  1.22it/s, loss=2.32, v_num=647]Epoch 26:  83%|████████▎ | 1810/2191 [24:41<05:11,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  83%|████████▎ | 1820/2191 [24:50<05:03,  1.22it/s, loss=2.34, v_num=647]Epoch 26:  83%|████████▎ | 1820/2191 [24:50<05:03,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  84%|████████▎ | 1830/2191 [24:58<04:55,  1.22it/s, loss=2.31, v_num=647]Epoch 26:  84%|████████▎ | 1830/2191 [24:58<04:55,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  84%|████████▍ | 1840/2191 [25:06<04:47,  1.22it/s, loss=2.28, v_num=647]Epoch 26:  84%|████████▍ | 1840/2191 [25:06<04:47,  1.22it/s, loss=2.27, v_num=647]Epoch 26:  84%|████████▍ | 1850/2191 [25:12<04:38,  1.22it/s, loss=2.27, v_num=647]Epoch 26:  84%|████████▍ | 1850/2191 [25:12<04:38,  1.22it/s, loss=2.29, v_num=647]Epoch 26:  85%|████████▍ | 1860/2191 [25:16<04:29,  1.23it/s, loss=2.29, v_num=647]Epoch 26:  85%|████████▍ | 1860/2191 [25:16<04:29,  1.23it/s, loss=2.32, v_num=647]validation_epoch_end
graph acc: 0.33865814696485624
valid accuracy: 0.976746141910553
validation_epoch_end
graph acc: 0.43769968051118213
valid accuracy: 0.976087212562561
validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.9765235781669617
 [25:21<04:11,  1.24it/s, loss=2.34, v_num=647]validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.978378176689148

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.36741214057507987
valid accuracy: 0.975786566734314
validation_epoch_end
graph acc: 0.3993610223642173
valid accuracy: 0.9788651466369629

Validating:   3%|▎         | 10/313 [00:01<00:43,  6.98it/s][AEpoch 26:  86%|████████▋ | 1890/2191 [25:22<04:02,  1.24it/s, loss=2.34, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:51,  5.72it/s][AEpoch 26:  87%|████████▋ | 1900/2191 [25:24<03:53,  1.25it/s, loss=2.34, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:34,  8.12it/s][AEpoch 26:  87%|████████▋ | 1910/2191 [25:25<03:44,  1.25it/s, loss=2.34, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:26, 10.12it/s][AEpoch 26:  88%|████████▊ | 1920/2191 [25:26<03:35,  1.26it/s, loss=2.34, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.18it/s][AEpoch 26:  88%|████████▊ | 1930/2191 [25:27<03:26,  1.26it/s, loss=2.34, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.13it/s][AEpoch 26:  89%|████████▊ | 1940/2191 [25:28<03:17,  1.27it/s, loss=2.34, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:23, 10.31it/s][AEpoch 26:  89%|████████▉ | 1950/2191 [25:29<03:08,  1.28it/s, loss=2.34, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:20, 11.23it/s][AEpoch 26:  89%|████████▉ | 1960/2191 [25:29<03:00,  1.28it/s, loss=2.34, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:19, 11.19it/s][AEpoch 26:  90%|████████▉ | 1970/2191 [25:30<02:51,  1.29it/s, loss=2.34, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:21, 10.09it/s][AEpoch 26:  90%|█████████ | 1980/2191 [25:31<02:43,  1.29it/s, loss=2.34, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:21,  9.58it/s][AEpoch 26:  91%|█████████ | 1990/2191 [25:33<02:34,  1.30it/s, loss=2.34, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:20,  9.38it/s][AEpoch 26:  91%|█████████▏| 2000/2191 [25:34<02:26,  1.30it/s, loss=2.34, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:18,  9.94it/s][AEpoch 26:  92%|█████████▏| 2010/2191 [25:34<02:18,  1.31it/s, loss=2.34, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.57it/s][AEpoch 26:  92%|█████████▏| 2020/2191 [25:36<02:10,  1.32it/s, loss=2.34, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:17,  9.30it/s][AEpoch 26:  93%|█████████▎| 2030/2191 [25:37<02:01,  1.32it/s, loss=2.34, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15, 10.10it/s][AEpoch 26:  93%|█████████▎| 2040/2191 [25:38<01:53,  1.33it/s, loss=2.34, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.50it/s][AEpoch 26:  94%|█████████▎| 2050/2191 [25:39<01:45,  1.33it/s, loss=2.34, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:13,  9.97it/s][AEpoch 26:  94%|█████████▍| 2060/2191 [25:40<01:37,  1.34it/s, loss=2.34, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.58it/s][AEpoch 26:  94%|█████████▍| 2070/2191 [25:40<01:30,  1.34it/s, loss=2.34, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:10, 10.88it/s][AEpoch 26:  95%|█████████▍| 2080/2191 [25:41<01:22,  1.35it/s, loss=2.34, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:10, 10.00it/s][AEpoch 26:  95%|█████████▌| 2090/2191 [25:42<01:14,  1.36it/s, loss=2.34, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:10,  9.05it/s][AEpoch 26:  96%|█████████▌| 2100/2191 [25:44<01:06,  1.36it/s, loss=2.34, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.53it/s][AEpoch 26:  96%|█████████▋| 2110/2191 [25:44<00:59,  1.37it/s, loss=2.34, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:06, 10.47it/s][AEpoch 26:  97%|█████████▋| 2120/2191 [25:45<00:51,  1.37it/s, loss=2.34, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.05it/s][AEpoch 26:  97%|█████████▋| 2130/2191 [25:46<00:44,  1.38it/s, loss=2.34, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.85it/s][AEpoch 26:  98%|█████████▊| 2140/2191 [25:47<00:36,  1.38it/s, loss=2.34, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.31it/s][AEpoch 26:  98%|█████████▊| 2150/2191 [25:48<00:29,  1.39it/s, loss=2.34, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.19it/s][AEpoch 26:  99%|█████████▊| 2160/2191 [25:49<00:22,  1.39it/s, loss=2.34, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.22it/s][AEpoch 26:  99%|█████████▉| 2170/2191 [25:50<00:14,  1.40it/s, loss=2.34, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.74it/s][AEpoch 26:  99%|█████████▉| 2180/2191 [25:50<00:07,  1.41it/s, loss=2.34, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.69it/s][AEpoch 26: 100%|█████████▉| 2190/2191 [25:51<00:00,  1.41it/s, loss=2.34, v_num=647]validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9782397747039795
Epoch 26: 100%|██████████| 2191/2191 [25:53<00:00,  1.41it/s, loss=2.33, v_num=647]
                                                             [AEpoch 26:   0%|          | 0/2191 [00:00<00:00, 16777.22it/s, loss=2.33, v_num=647]Epoch 27:   0%|          | 0/2191 [00:00<00:00, 3823.43it/s, loss=2.33, v_num=647] Epoch 27:   0%|          | 10/2191 [00:12<40:35,  1.12s/it, loss=2.33, v_num=647] Epoch 27:   0%|          | 10/2191 [00:12<40:35,  1.12s/it, loss=2.27, v_num=647]Epoch 27:   1%|          | 20/2191 [00:20<36:08,  1.00it/s, loss=2.27, v_num=647]Epoch 27:   1%|          | 20/2191 [00:20<36:08,  1.00it/s, loss=2.26, v_num=647]Epoch 27:   1%|▏         | 30/2191 [00:28<33:23,  1.08it/s, loss=2.26, v_num=647]Epoch 27:   1%|▏         | 30/2191 [00:28<33:23,  1.08it/s, loss=2.26, v_num=647]Epoch 27:   2%|▏         | 40/2191 [00:37<32:33,  1.10it/s, loss=2.26, v_num=647]Epoch 27:   2%|▏         | 40/2191 [00:37<32:33,  1.10it/s, loss=2.25, v_num=647]Epoch 27:   2%|▏         | 50/2191 [00:45<31:31,  1.13it/s, loss=2.25, v_num=647]Epoch 27:   2%|▏         | 50/2191 [00:45<31:31,  1.13it/s, loss=2.29, v_num=647]Epoch 27:   3%|▎         | 60/2191 [00:53<30:59,  1.15it/s, loss=2.29, v_num=647]Epoch 27:   3%|▎         | 60/2191 [00:53<30:59,  1.15it/s, loss=2.28, v_num=647]Epoch 27:   3%|▎         | 70/2191 [01:02<30:59,  1.14it/s, loss=2.28, v_num=647]Epoch 27:   3%|▎         | 70/2191 [01:02<30:59,  1.14it/s, loss=2.25, v_num=647]Epoch 27:   4%|▎         | 80/2191 [01:09<30:21,  1.16it/s, loss=2.25, v_num=647]Epoch 27:   4%|▎         | 80/2191 [01:09<30:21,  1.16it/s, loss=2.28, v_num=647]Epoch 27:   4%|▍         | 90/2191 [01:18<30:01,  1.17it/s, loss=2.28, v_num=647]Epoch 27:   4%|▍         | 90/2191 [01:18<30:01,  1.17it/s, loss=2.28, v_num=647]Epoch 27:   5%|▍         | 100/2191 [01:26<29:54,  1.17it/s, loss=2.28, v_num=647]Epoch 27:   5%|▍         | 100/2191 [01:26<29:54,  1.17it/s, loss=2.27, v_num=647]Epoch 27:   5%|▌         | 110/2191 [01:34<29:32,  1.17it/s, loss=2.27, v_num=647]Epoch 27:   5%|▌         | 110/2191 [01:34<29:32,  1.17it/s, loss=2.27, v_num=647]Epoch 27:   5%|▌         | 120/2191 [01:45<30:09,  1.14it/s, loss=2.27, v_num=647]Epoch 27:   5%|▌         | 120/2191 [01:45<30:09,  1.14it/s, loss=2.3, v_num=647] Epoch 27:   6%|▌         | 130/2191 [01:54<29:59,  1.15it/s, loss=2.3, v_num=647]Epoch 27:   6%|▌         | 130/2191 [01:54<29:59,  1.15it/s, loss=2.29, v_num=647]Epoch 27:   6%|▋         | 140/2191 [02:01<29:27,  1.16it/s, loss=2.29, v_num=647]Epoch 27:   6%|▋         | 140/2191 [02:01<29:27,  1.16it/s, loss=2.25, v_num=647]Epoch 27:   7%|▋         | 150/2191 [02:10<29:17,  1.16it/s, loss=2.25, v_num=647]Epoch 27:   7%|▋         | 150/2191 [02:10<29:17,  1.16it/s, loss=2.27, v_num=647]Epoch 27:   7%|▋         | 160/2191 [02:17<28:59,  1.17it/s, loss=2.27, v_num=647]Epoch 27:   7%|▋         | 160/2191 [02:17<28:59,  1.17it/s, loss=2.29, v_num=647]Epoch 27:   8%|▊         | 170/2191 [02:26<28:54,  1.16it/s, loss=2.29, v_num=647]Epoch 27:   8%|▊         | 170/2191 [02:26<28:54,  1.16it/s, loss=2.33, v_num=647]Epoch 27:   8%|▊         | 180/2191 [02:35<28:42,  1.17it/s, loss=2.33, v_num=647]Epoch 27:   8%|▊         | 180/2191 [02:35<28:42,  1.17it/s, loss=2.31, v_num=647]Epoch 27:   9%|▊         | 190/2191 [02:42<28:25,  1.17it/s, loss=2.31, v_num=647]Epoch 27:   9%|▊         | 190/2191 [02:42<28:25,  1.17it/s, loss=2.28, v_num=647]Epoch 27:   9%|▉         | 200/2191 [02:50<28:05,  1.18it/s, loss=2.28, v_num=647]Epoch 27:   9%|▉         | 200/2191 [02:50<28:05,  1.18it/s, loss=2.29, v_num=647]Epoch 27:  10%|▉         | 210/2191 [02:57<27:50,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  10%|▉         | 210/2191 [02:57<27:50,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  10%|█         | 220/2191 [03:06<27:45,  1.18it/s, loss=2.28, v_num=647]Epoch 27:  10%|█         | 220/2191 [03:06<27:45,  1.18it/s, loss=2.3, v_num=647] Epoch 27:  10%|█         | 230/2191 [03:15<27:40,  1.18it/s, loss=2.3, v_num=647]Epoch 27:  10%|█         | 230/2191 [03:15<27:40,  1.18it/s, loss=2.32, v_num=647]Epoch 27:  11%|█         | 240/2191 [03:23<27:29,  1.18it/s, loss=2.32, v_num=647]Epoch 27:  11%|█         | 240/2191 [03:23<27:29,  1.18it/s, loss=2.28, v_num=647]Epoch 27:  11%|█▏        | 250/2191 [03:31<27:15,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  11%|█▏        | 250/2191 [03:31<27:15,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  12%|█▏        | 260/2191 [03:39<27:04,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  12%|█▏        | 260/2191 [03:39<27:04,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  12%|█▏        | 270/2191 [03:47<26:51,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  12%|█▏        | 270/2191 [03:47<26:51,  1.19it/s, loss=2.24, v_num=647]Epoch 27:  13%|█▎        | 280/2191 [03:55<26:40,  1.19it/s, loss=2.24, v_num=647]Epoch 27:  13%|█▎        | 280/2191 [03:55<26:40,  1.19it/s, loss=2.25, v_num=647]Epoch 27:  13%|█▎        | 290/2191 [04:03<26:31,  1.19it/s, loss=2.25, v_num=647]Epoch 27:  13%|█▎        | 290/2191 [04:03<26:31,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  14%|█▎        | 300/2191 [04:10<26:15,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  14%|█▎        | 300/2191 [04:10<26:15,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  14%|█▍        | 310/2191 [04:18<26:06,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  14%|█▍        | 310/2191 [04:18<26:06,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  15%|█▍        | 320/2191 [04:26<25:53,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  15%|█▍        | 320/2191 [04:26<25:54,  1.20it/s, loss=2.34, v_num=647]Epoch 27:  15%|█▌        | 330/2191 [04:33<25:39,  1.21it/s, loss=2.34, v_num=647]Epoch 27:  15%|█▌        | 330/2191 [04:33<25:39,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  16%|█▌        | 340/2191 [04:42<25:32,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  16%|█▌        | 340/2191 [04:42<25:32,  1.21it/s, loss=2.26, v_num=647]Epoch 27:  16%|█▌        | 350/2191 [04:50<25:23,  1.21it/s, loss=2.26, v_num=647]Epoch 27:  16%|█▌        | 350/2191 [04:50<25:23,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  16%|█▋        | 360/2191 [04:58<25:14,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  16%|█▋        | 360/2191 [04:58<25:14,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  17%|█▋        | 370/2191 [05:07<25:10,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  17%|█▋        | 370/2191 [05:07<25:10,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  17%|█▋        | 380/2191 [05:15<24:58,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  17%|█▋        | 380/2191 [05:15<24:58,  1.21it/s, loss=2.3, v_num=647] Epoch 27:  18%|█▊        | 390/2191 [05:25<25:00,  1.20it/s, loss=2.3, v_num=647]Epoch 27:  18%|█▊        | 390/2191 [05:25<25:00,  1.20it/s, loss=2.31, v_num=647]Epoch 27:  18%|█▊        | 400/2191 [05:34<24:52,  1.20it/s, loss=2.31, v_num=647]Epoch 27:  18%|█▊        | 400/2191 [05:34<24:52,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  19%|█▊        | 410/2191 [05:41<24:41,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  19%|█▊        | 410/2191 [05:41<24:41,  1.20it/s, loss=2.23, v_num=647]Epoch 27:  19%|█▉        | 420/2191 [05:49<24:31,  1.20it/s, loss=2.23, v_num=647]Epoch 27:  19%|█▉        | 420/2191 [05:49<24:31,  1.20it/s, loss=2.25, v_num=647]Epoch 27:  20%|█▉        | 430/2191 [05:58<24:24,  1.20it/s, loss=2.25, v_num=647]Epoch 27:  20%|█▉        | 430/2191 [05:58<24:24,  1.20it/s, loss=2.25, v_num=647]Epoch 27:  20%|██        | 440/2191 [06:07<24:17,  1.20it/s, loss=2.25, v_num=647]Epoch 27:  20%|██        | 440/2191 [06:07<24:17,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  21%|██        | 450/2191 [06:15<24:11,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  21%|██        | 450/2191 [06:15<24:11,  1.20it/s, loss=2.3, v_num=647] Epoch 27:  21%|██        | 460/2191 [06:23<24:00,  1.20it/s, loss=2.3, v_num=647]Epoch 27:  21%|██        | 460/2191 [06:23<24:00,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  21%|██        | 460/2191 [06:33<24:38,  1.17it/s, loss=2.27, v_num=647]Epoch 27:  21%|██▏       | 470/2191 [06:35<24:05,  1.19it/s, loss=2.27, v_num=647]Epoch 27:  21%|██▏       | 470/2191 [06:35<24:05,  1.19it/s, loss=2.24, v_num=647]Epoch 27:  22%|██▏       | 480/2191 [06:43<23:55,  1.19it/s, loss=2.24, v_num=647]Epoch 27:  22%|██▏       | 480/2191 [06:43<23:55,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  22%|██▏       | 490/2191 [06:50<23:41,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  22%|██▏       | 490/2191 [06:50<23:41,  1.20it/s, loss=2.3, v_num=647] Epoch 27:  23%|██▎       | 500/2191 [06:58<23:34,  1.20it/s, loss=2.3, v_num=647]Epoch 27:  23%|██▎       | 500/2191 [06:58<23:34,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  23%|██▎       | 510/2191 [07:07<23:27,  1.19it/s, loss=2.26, v_num=647]Epoch 27:  23%|██▎       | 510/2191 [07:07<23:27,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  24%|██▎       | 520/2191 [07:16<23:19,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  24%|██▎       | 520/2191 [07:16<23:19,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  24%|██▍       | 530/2191 [07:24<23:11,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  24%|██▍       | 530/2191 [07:24<23:11,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  25%|██▍       | 540/2191 [07:33<23:03,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  25%|██▍       | 540/2191 [07:33<23:03,  1.19it/s, loss=2.3, v_num=647] Epoch 27:  25%|██▌       | 550/2191 [07:41<22:55,  1.19it/s, loss=2.3, v_num=647]Epoch 27:  25%|██▌       | 550/2191 [07:41<22:55,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  26%|██▌       | 570/2191 [07:58<22:39,  1.19it/s, loss=2.31, v_num=647]Epoch 27:  26%|██▌       | 570/2191 [07:58<22:39,  1.19it/s, loss=2.3, v_num=647] Epoch 27:  26%|██▋       | 580/2191 [08:06<22:28,  1.19it/s, loss=2.3, v_num=647]Epoch 27:  26%|██▋       | 580/2191 [08:06<22:28,  1.19it/s, loss=2.33, v_num=647]Epoch 27:  27%|██▋       | 590/2191 [08:15<22:21,  1.19it/s, loss=2.33, v_num=647]Epoch 27:  27%|██▋       | 590/2191 [08:15<22:21,  1.19it/s, loss=2.34, v_num=647]Epoch 27:  27%|██▋       | 600/2191 [08:23<22:11,  1.19it/s, loss=2.34, v_num=647]Epoch 27:  27%|██▋       | 600/2191 [08:23<22:11,  1.19it/s, loss=2.3, v_num=647] Epoch 27:  28%|██▊       | 610/2191 [08:32<22:05,  1.19it/s, loss=2.3, v_num=647]Epoch 27:  28%|██▊       | 610/2191 [08:32<22:05,  1.19it/s, loss=2.28, v_num=647]Epoch 27:  28%|██▊       | 620/2191 [08:39<21:54,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  28%|██▊       | 620/2191 [08:39<21:54,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  29%|██▉       | 630/2191 [08:46<21:42,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  29%|██▉       | 630/2191 [08:46<21:42,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  29%|██▉       | 640/2191 [08:55<21:36,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  29%|██▉       | 640/2191 [08:55<21:36,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  30%|██▉       | 650/2191 [09:03<21:27,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  30%|██▉       | 650/2191 [09:03<21:27,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.26, v_num=647]Epoch 27:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  31%|███       | 670/2191 [09:21<21:12,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  31%|███       | 670/2191 [09:21<21:12,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  31%|███       | 680/2191 [09:30<21:04,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  31%|███       | 680/2191 [09:30<21:04,  1.19it/s, loss=2.29, v_num=647]Epoch 27:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  31%|███▏      | 690/2191 [09:37<20:54,  1.20it/s, loss=2.3, v_num=647] Epoch 27:  32%|███▏      | 700/2191 [09:44<20:43,  1.20it/s, loss=2.3, v_num=647]Epoch 27:  32%|███▏      | 700/2191 [09:44<20:43,  1.20it/s, loss=2.32, v_num=647]Epoch 27:  32%|███▏      | 710/2191 [09:51<20:32,  1.20it/s, loss=2.32, v_num=647]Epoch 27:  32%|███▏      | 710/2191 [09:51<20:32,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  33%|███▎      | 720/2191 [09:59<20:22,  1.20it/s, loss=2.29, v_num=647]Epoch 27:  33%|███▎      | 720/2191 [09:59<20:22,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  33%|███▎      | 730/2191 [10:06<20:12,  1.20it/s, loss=2.27, v_num=647]Epoch 27:  33%|███▎      | 730/2191 [10:06<20:12,  1.20it/s, loss=2.28, v_num=647]Epoch 27:  34%|███▍      | 740/2191 [10:13<20:01,  1.21it/s, loss=2.28, v_num=647]Epoch 27:  34%|███▍      | 740/2191 [10:13<20:01,  1.21it/s, loss=2.31, v_num=647]Epoch 27:  34%|███▍      | 750/2191 [10:20<19:50,  1.21it/s, loss=2.31, v_num=647]Epoch 27:  34%|███▍      | 750/2191 [10:20<19:50,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  35%|███▍      | 760/2191 [10:27<19:39,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  35%|███▍      | 760/2191 [10:27<19:40,  1.21it/s, loss=2.31, v_num=647]Epoch 27:  35%|███▌      | 770/2191 [10:34<19:29,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  35%|███▌      | 770/2191 [10:34<19:29,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  36%|███▌      | 780/2191 [10:42<19:20,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  36%|███▌      | 780/2191 [10:42<19:20,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  36%|███▌      | 780/2191 [10:53<19:41,  1.19it/s, loss=2.27, v_num=647]Epoch 27:  36%|███▌      | 790/2191 [10:53<19:18,  1.21it/s, loss=2.27, v_num=647]Epoch 27:  36%|███▌      | 790/2191 [10:53<19:18,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  37%|███▋      | 800/2191 [11:01<19:09,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  37%|███▋      | 800/2191 [11:01<19:09,  1.21it/s, loss=2.32, v_num=647]Epoch 27:  37%|███▋      | 810/2191 [11:08<18:58,  1.21it/s, loss=2.32, v_num=647]Epoch 27:  37%|███▋      | 810/2191 [11:08<18:58,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  37%|███▋      | 820/2191 [11:17<18:51,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  37%|███▋      | 820/2191 [11:17<18:51,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  38%|███▊      | 830/2191 [11:24<18:41,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  38%|███▊      | 830/2191 [11:24<18:41,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  38%|███▊      | 840/2191 [11:32<18:32,  1.21it/s, loss=2.33, v_num=647]Epoch 27:  38%|███▊      | 840/2191 [11:32<18:32,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  39%|███▉      | 850/2191 [11:41<18:25,  1.21it/s, loss=2.29, v_num=647]Epoch 27:  39%|███▉      | 850/2191 [11:41<18:25,  1.21it/s, loss=2.2, v_num=647] Epoch 27:  39%|███▉      | 860/2191 [11:48<18:15,  1.22it/s, loss=2.2, v_num=647]Epoch 27:  39%|███▉      | 860/2191 [11:48<18:15,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  40%|████      | 880/2191 [12:03<17:56,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  40%|████      | 880/2191 [12:03<17:56,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  41%|████      | 890/2191 [12:10<17:47,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  41%|████      | 890/2191 [12:10<17:47,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  41%|████      | 900/2191 [12:18<17:38,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  41%|████      | 900/2191 [12:18<17:38,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  42%|████▏     | 910/2191 [12:26<17:29,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  42%|████▏     | 910/2191 [12:26<17:29,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  42%|████▏     | 920/2191 [12:34<17:20,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  42%|████▏     | 920/2191 [12:34<17:20,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  42%|████▏     | 930/2191 [12:41<17:11,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  42%|████▏     | 930/2191 [12:41<17:11,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  43%|████▎     | 940/2191 [12:48<17:01,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  43%|████▎     | 940/2191 [12:48<17:01,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  43%|████▎     | 950/2191 [12:55<16:51,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  43%|████▎     | 950/2191 [12:55<16:51,  1.23it/s, loss=2.3, v_num=647] Epoch 27:  44%|████▍     | 960/2191 [13:05<16:45,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  44%|████▍     | 960/2191 [13:05<16:45,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  44%|████▍     | 970/2191 [13:12<16:36,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  44%|████▍     | 970/2191 [13:12<16:36,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  45%|████▍     | 980/2191 [13:20<16:28,  1.23it/s, loss=2.3, v_num=647]Epoch 27:  45%|████▍     | 980/2191 [13:20<16:28,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  45%|████▌     | 990/2191 [13:27<16:18,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  45%|████▌     | 990/2191 [13:27<16:18,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  46%|████▌     | 1000/2191 [13:36<16:10,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  46%|████▌     | 1000/2191 [13:36<16:10,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  46%|████▌     | 1010/2191 [13:43<16:01,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  46%|████▌     | 1010/2191 [13:43<16:01,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  47%|████▋     | 1020/2191 [13:50<15:52,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  47%|████▋     | 1020/2191 [13:50<15:52,  1.23it/s, loss=2.35, v_num=647]Epoch 27:  47%|████▋     | 1030/2191 [13:58<15:44,  1.23it/s, loss=2.35, v_num=647]Epoch 27:  47%|████▋     | 1030/2191 [13:58<15:44,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  47%|████▋     | 1040/2191 [14:07<15:37,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  47%|████▋     | 1040/2191 [14:07<15:37,  1.23it/s, loss=2.3, v_num=647] Epoch 27:  48%|████▊     | 1050/2191 [14:15<15:28,  1.23it/s, loss=2.3, v_num=647]Epoch 27:  48%|████▊     | 1050/2191 [14:15<15:28,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  48%|████▊     | 1060/2191 [14:23<15:20,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  48%|████▊     | 1060/2191 [14:23<15:20,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  49%|████▉     | 1070/2191 [14:31<15:12,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  49%|████▉     | 1070/2191 [14:31<15:12,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  49%|████▉     | 1080/2191 [14:40<15:04,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  49%|████▉     | 1080/2191 [14:40<15:04,  1.23it/s, loss=2.3, v_num=647] Epoch 27:  50%|████▉     | 1090/2191 [14:46<14:55,  1.23it/s, loss=2.3, v_num=647]Epoch 27:  50%|████▉     | 1090/2191 [14:46<14:55,  1.23it/s, loss=2.25, v_num=647]Epoch 27:  50%|█████     | 1100/2191 [14:55<14:46,  1.23it/s, loss=2.25, v_num=647]Epoch 27:  50%|█████     | 1100/2191 [14:55<14:46,  1.23it/s, loss=2.24, v_num=647]Epoch 27:  51%|█████     | 1110/2191 [15:03<14:39,  1.23it/s, loss=2.24, v_num=647]Epoch 27:  51%|█████     | 1110/2191 [15:03<14:39,  1.23it/s, loss=2.26, v_num=647]Epoch 27:  51%|█████     | 1120/2191 [15:10<14:29,  1.23it/s, loss=2.26, v_num=647]Epoch 27:  51%|█████     | 1120/2191 [15:10<14:29,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  52%|█████▏    | 1130/2191 [15:18<14:21,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  52%|█████▏    | 1130/2191 [15:18<14:21,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  52%|█████▏    | 1140/2191 [15:25<14:12,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  52%|█████▏    | 1140/2191 [15:25<14:12,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  52%|█████▏    | 1150/2191 [15:34<14:05,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  52%|█████▏    | 1150/2191 [15:34<14:05,  1.23it/s, loss=2.27, v_num=647]Epoch 27:  53%|█████▎    | 1160/2191 [15:43<13:57,  1.23it/s, loss=2.27, v_num=647]Epoch 27:  53%|█████▎    | 1160/2191 [15:43<13:57,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  53%|█████▎    | 1170/2191 [15:49<13:48,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  53%|█████▎    | 1170/2191 [15:49<13:48,  1.23it/s, loss=2.3, v_num=647] Epoch 27:  54%|█████▍    | 1180/2191 [15:58<13:40,  1.23it/s, loss=2.3, v_num=647]Epoch 27:  54%|█████▍    | 1180/2191 [15:58<13:40,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  54%|█████▍    | 1190/2191 [16:05<13:31,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  54%|█████▍    | 1190/2191 [16:05<13:31,  1.23it/s, loss=2.36, v_num=647]Epoch 27:  55%|█████▍    | 1200/2191 [16:14<13:23,  1.23it/s, loss=2.36, v_num=647]Epoch 27:  55%|█████▍    | 1200/2191 [16:14<13:23,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  55%|█████▌    | 1210/2191 [16:22<13:15,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  55%|█████▌    | 1210/2191 [16:22<13:15,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  56%|█████▌    | 1220/2191 [16:30<13:07,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  56%|█████▌    | 1220/2191 [16:30<13:07,  1.23it/s, loss=2.3, v_num=647] Epoch 27:  56%|█████▌    | 1230/2191 [16:38<12:59,  1.23it/s, loss=2.3, v_num=647]Epoch 27:  56%|█████▌    | 1230/2191 [16:38<12:59,  1.23it/s, loss=2.27, v_num=647]Epoch 27:  57%|█████▋    | 1240/2191 [16:46<12:50,  1.23it/s, loss=2.27, v_num=647]Epoch 27:  57%|█████▋    | 1240/2191 [16:46<12:50,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  57%|█████▋    | 1250/2191 [16:54<12:43,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  57%|█████▋    | 1250/2191 [16:54<12:43,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  58%|█████▊    | 1260/2191 [17:04<12:36,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  58%|█████▊    | 1260/2191 [17:04<12:36,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  58%|█████▊    | 1270/2191 [17:12<12:28,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  58%|█████▊    | 1270/2191 [17:12<12:28,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  58%|█████▊    | 1280/2191 [17:20<12:20,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  58%|█████▊    | 1280/2191 [17:20<12:20,  1.23it/s, loss=2.35, v_num=647]Epoch 27:  59%|█████▉    | 1290/2191 [17:29<12:12,  1.23it/s, loss=2.35, v_num=647]Epoch 27:  59%|█████▉    | 1290/2191 [17:29<12:12,  1.23it/s, loss=2.37, v_num=647]Epoch 27:  59%|█████▉    | 1300/2191 [17:38<12:04,  1.23it/s, loss=2.37, v_num=647]Epoch 27:  59%|█████▉    | 1300/2191 [17:38<12:04,  1.23it/s, loss=2.37, v_num=647]Epoch 27:  60%|█████▉    | 1310/2191 [17:45<11:56,  1.23it/s, loss=2.37, v_num=647]Epoch 27:  60%|█████▉    | 1310/2191 [17:45<11:56,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  60%|██████    | 1320/2191 [17:54<11:48,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  60%|██████    | 1320/2191 [17:54<11:48,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  61%|██████    | 1330/2191 [18:02<11:40,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  61%|██████    | 1330/2191 [18:02<11:40,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  61%|██████    | 1340/2191 [18:09<11:31,  1.23it/s, loss=2.28, v_num=647]Epoch 27:  61%|██████    | 1340/2191 [18:09<11:31,  1.23it/s, loss=2.25, v_num=647]Epoch 27:  62%|██████▏   | 1350/2191 [18:19<11:24,  1.23it/s, loss=2.25, v_num=647]Epoch 27:  62%|██████▏   | 1350/2191 [18:19<11:24,  1.23it/s, loss=2.26, v_num=647]Epoch 27:  62%|██████▏   | 1360/2191 [18:26<11:15,  1.23it/s, loss=2.26, v_num=647]Epoch 27:  62%|██████▏   | 1360/2191 [18:26<11:15,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  63%|██████▎   | 1370/2191 [18:36<11:08,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  63%|██████▎   | 1370/2191 [18:36<11:08,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  63%|██████▎   | 1380/2191 [18:45<11:00,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  63%|██████▎   | 1380/2191 [18:45<11:00,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  63%|██████▎   | 1390/2191 [18:54<10:53,  1.23it/s, loss=2.32, v_num=647]Epoch 27:  63%|██████▎   | 1390/2191 [18:54<10:53,  1.23it/s, loss=2.29, v_num=647]Epoch 27:  64%|██████▍   | 1400/2191 [19:03<10:45,  1.22it/s, loss=2.29, v_num=647]Epoch 27:  64%|██████▍   | 1400/2191 [19:03<10:45,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  64%|██████▍   | 1410/2191 [19:12<10:37,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  64%|██████▍   | 1410/2191 [19:12<10:37,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  65%|██████▍   | 1420/2191 [19:20<10:29,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  65%|██████▍   | 1420/2191 [19:20<10:29,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  65%|██████▌   | 1430/2191 [19:28<10:21,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  65%|██████▌   | 1430/2191 [19:28<10:21,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  66%|██████▌   | 1440/2191 [19:38<10:14,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  66%|██████▌   | 1440/2191 [19:38<10:14,  1.22it/s, loss=2.29, v_num=647]Epoch 27:  66%|██████▌   | 1450/2191 [19:45<10:05,  1.22it/s, loss=2.29, v_num=647]Epoch 27:  66%|██████▌   | 1450/2191 [19:45<10:05,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  67%|██████▋   | 1460/2191 [19:54<09:57,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  67%|██████▋   | 1460/2191 [19:54<09:57,  1.22it/s, loss=2.35, v_num=647]Epoch 27:  67%|██████▋   | 1470/2191 [20:03<09:50,  1.22it/s, loss=2.35, v_num=647]Epoch 27:  67%|██████▋   | 1470/2191 [20:03<09:50,  1.22it/s, loss=2.35, v_num=647]Epoch 27:  68%|██████▊   | 1480/2191 [20:12<09:42,  1.22it/s, loss=2.35, v_num=647]Epoch 27:  68%|██████▊   | 1480/2191 [20:12<09:42,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  68%|██████▊   | 1490/2191 [20:21<09:34,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  68%|██████▊   | 1490/2191 [20:21<09:34,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  68%|██████▊   | 1500/2191 [20:30<09:26,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  68%|██████▊   | 1500/2191 [20:30<09:26,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  69%|██████▉   | 1510/2191 [20:37<09:17,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  69%|██████▉   | 1510/2191 [20:37<09:17,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  69%|██████▉   | 1520/2191 [20:45<09:09,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  69%|██████▉   | 1520/2191 [20:45<09:09,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  70%|██████▉   | 1530/2191 [20:54<09:01,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  70%|██████▉   | 1530/2191 [20:54<09:01,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  70%|███████   | 1540/2191 [21:02<08:53,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  70%|███████   | 1540/2191 [21:02<08:53,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  71%|███████   | 1550/2191 [21:09<08:44,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  71%|███████   | 1550/2191 [21:09<08:44,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  71%|███████   | 1560/2191 [21:17<08:36,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  71%|███████   | 1560/2191 [21:17<08:36,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  72%|███████▏  | 1570/2191 [21:25<08:28,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  72%|███████▏  | 1570/2191 [21:25<08:28,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  72%|███████▏  | 1580/2191 [21:34<08:20,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  72%|███████▏  | 1580/2191 [21:34<08:20,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  73%|███████▎  | 1590/2191 [21:43<08:12,  1.22it/s, loss=2.28, v_num=647]Epoch 27:  73%|███████▎  | 1590/2191 [21:43<08:12,  1.22it/s, loss=2.24, v_num=647]Epoch 27:  73%|███████▎  | 1600/2191 [21:50<08:03,  1.22it/s, loss=2.24, v_num=647]Epoch 27:  73%|███████▎  | 1600/2191 [21:50<08:03,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  73%|███████▎  | 1610/2191 [21:59<07:55,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  73%|███████▎  | 1610/2191 [21:59<07:55,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  74%|███████▍  | 1620/2191 [22:07<07:47,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  74%|███████▍  | 1620/2191 [22:07<07:47,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  74%|███████▍  | 1630/2191 [22:14<07:39,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  74%|███████▍  | 1630/2191 [22:14<07:39,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  75%|███████▍  | 1640/2191 [22:22<07:30,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  75%|███████▍  | 1640/2191 [22:22<07:30,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  75%|███████▌  | 1650/2191 [22:29<07:22,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  75%|███████▌  | 1650/2191 [22:29<07:22,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  76%|███████▌  | 1660/2191 [22:36<07:13,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  76%|███████▌  | 1660/2191 [22:36<07:13,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  76%|███████▌  | 1670/2191 [22:45<07:05,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  76%|███████▌  | 1670/2191 [22:45<07:05,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  77%|███████▋  | 1680/2191 [22:53<06:57,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  77%|███████▋  | 1680/2191 [22:53<06:57,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  77%|███████▋  | 1690/2191 [23:01<06:49,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  77%|███████▋  | 1690/2191 [23:01<06:49,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  78%|███████▊  | 1700/2191 [23:09<06:41,  1.22it/s, loss=2.25, v_num=647]Epoch 27:  78%|███████▊  | 1700/2191 [23:09<06:41,  1.22it/s, loss=2.29, v_num=647]Epoch 27:  78%|███████▊  | 1710/2191 [23:17<06:33,  1.22it/s, loss=2.29, v_num=647]Epoch 27:  78%|███████▊  | 1710/2191 [23:17<06:33,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  79%|███████▊  | 1720/2191 [23:27<06:25,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  79%|███████▊  | 1720/2191 [23:27<06:25,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  79%|███████▉  | 1730/2191 [23:36<06:17,  1.22it/s, loss=2.34, v_num=647]Epoch 27:  79%|███████▉  | 1730/2191 [23:36<06:17,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  79%|███████▉  | 1740/2191 [23:46<06:09,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  79%|███████▉  | 1740/2191 [23:46<06:09,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  80%|███████▉  | 1750/2191 [23:54<06:01,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  80%|███████▉  | 1750/2191 [23:54<06:01,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  80%|████████  | 1760/2191 [24:02<05:53,  1.22it/s, loss=2.26, v_num=647]Epoch 27:  80%|████████  | 1760/2191 [24:02<05:53,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  81%|████████  | 1770/2191 [24:10<05:44,  1.22it/s, loss=2.27, v_num=647]Epoch 27:  81%|████████  | 1770/2191 [24:10<05:44,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  81%|████████  | 1780/2191 [24:18<05:36,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  81%|████████  | 1780/2191 [24:18<05:36,  1.22it/s, loss=2.3, v_num=647] Epoch 27:  82%|████████▏ | 1790/2191 [24:25<05:28,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  82%|████████▏ | 1790/2191 [24:25<05:28,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  82%|████████▏ | 1800/2191 [24:32<05:19,  1.22it/s, loss=2.3, v_num=647]Epoch 27:  82%|████████▏ | 1800/2191 [24:32<05:19,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  83%|████████▎ | 1810/2191 [24:43<05:12,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  83%|████████▎ | 1810/2191 [24:43<05:12,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  83%|████████▎ | 1820/2191 [24:50<05:03,  1.22it/s, loss=2.31, v_num=647]Epoch 27:  83%|████████▎ | 1820/2191 [24:50<05:03,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  84%|████████▎ | 1830/2191 [24:57<04:55,  1.22it/s, loss=2.32, v_num=647]Epoch 27:  84%|████████▎ | 1830/2191 [24:57<04:55,  1.22it/s, loss=2.33, v_num=647]Epoch 27:  84%|████████▍ | 1840/2191 [25:02<04:46,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  84%|████████▍ | 1840/2191 [25:02<04:46,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  84%|████████▍ | 1850/2191 [25:07<04:37,  1.23it/s, loss=2.31, v_num=647]Epoch 27:  84%|████████▍ | 1850/2191 [25:07<04:37,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  85%|████████▍ | 1860/2191 [25:09<04:28,  1.23it/s, loss=2.33, v_num=647]Epoch 27:  85%|████████▍ | 1860/2191 [25:09<04:28,  1.23it/s, loss=2.34, v_num=647]Epoch 27:  85%|████████▌ | 1870/2191 [25:11<04:19,  1.24it/s, loss=2.34, v_num=647]Epoch 27:  85%|████████▌ | 1870/2191 [25:11<04:19,  1.24it/s, loss=2.32, v_num=647]validation_epoch_end
graph acc: 0.3801916932907348
valid accuracy: 0.9771249890327454
2.32, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9765498638153076
validation_epoch_end
graph acc: 0.4217252396166134
valid accuracy: 0.979111909866333
validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.9777556657791138

Validating:   3%|▎         | 10/313 [00:01<00:44,  6.75it/s][AEpoch 27:  86%|████████▋ | 1890/2191 [25:15<04:01,  1.25it/s, loss=2.32, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:51,  5.68it/s][AEpoch 27:  87%|████████▋ | 1900/2191 [25:17<03:52,  1.25it/s, loss=2.32, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:40,  7.05it/s][AEpoch 27:  87%|████████▋ | 1910/2191 [25:18<03:43,  1.26it/s, loss=2.32, v_num=647]
Validating:  13%|█▎        | 40/313 [00:05<00:29,  9.14it/s][AEpoch 27:  88%|████████▊ | 1920/2191 [25:18<03:34,  1.26it/s, loss=2.32, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:26, 10.00it/s][AEpoch 27:  88%|████████▊ | 1930/2191 [25:19<03:25,  1.27it/s, loss=2.32, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.40it/s][AEpoch 27:  89%|████████▊ | 1940/2191 [25:20<03:16,  1.28it/s, loss=2.32, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:25,  9.61it/s][AEpoch 27:  89%|████████▉ | 1950/2191 [25:21<03:07,  1.28it/s, loss=2.32, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:23,  9.92it/s][AEpoch 27:  89%|████████▉ | 1960/2191 [25:22<02:59,  1.29it/s, loss=2.32, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.46it/s][AEpoch 27:  90%|████████▉ | 1970/2191 [25:23<02:50,  1.29it/s, loss=2.32, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:19, 10.67it/s][AEpoch 27:  90%|█████████ | 1980/2191 [25:24<02:42,  1.30it/s, loss=2.32, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20, 10.10it/s][AEpoch 27:  91%|█████████ | 1990/2191 [25:25<02:34,  1.31it/s, loss=2.32, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:20,  9.55it/s][AEpoch 27:  91%|█████████▏| 2000/2191 [25:26<02:25,  1.31it/s, loss=2.32, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:19,  9.17it/s][AEpoch 27:  92%|█████████▏| 2010/2191 [25:27<02:17,  1.32it/s, loss=2.32, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.63it/s][AEpoch 27:  92%|█████████▏| 2020/2191 [25:29<02:09,  1.32it/s, loss=2.32, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.27it/s][AEpoch 27:  93%|█████████▎| 2030/2191 [25:30<02:01,  1.33it/s, loss=2.32, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:15,  9.70it/s][AEpoch 27:  93%|█████████▎| 2040/2191 [25:31<01:53,  1.33it/s, loss=2.32, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:13, 10.56it/s][AEpoch 27:  94%|█████████▎| 2050/2191 [25:31<01:45,  1.34it/s, loss=2.32, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.44it/s][AEpoch 27:  94%|█████████▍| 2060/2191 [25:32<01:37,  1.34it/s, loss=2.32, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.96it/s][AEpoch 27:  94%|█████████▍| 2070/2191 [25:33<01:29,  1.35it/s, loss=2.32, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.48it/s][AEpoch 27:  95%|█████████▍| 2080/2191 [25:34<01:21,  1.36it/s, loss=2.32, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 11.10it/s][AEpoch 27:  95%|█████████▌| 2090/2191 [25:35<01:14,  1.36it/s, loss=2.32, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.69it/s][AEpoch 27:  96%|█████████▌| 2100/2191 [25:36<01:06,  1.37it/s, loss=2.32, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 11.13it/s][AEpoch 27:  96%|█████████▋| 2110/2191 [25:37<00:58,  1.37it/s, loss=2.32, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.78it/s][AEpoch 27:  97%|█████████▋| 2120/2191 [25:38<00:51,  1.38it/s, loss=2.32, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:05, 10.71it/s][AEpoch 27:  97%|█████████▋| 2130/2191 [25:39<00:44,  1.38it/s, loss=2.32, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.70it/s][AEpoch 27:  98%|█████████▊| 2140/2191 [25:40<00:36,  1.39it/s, loss=2.32, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 10.80it/s][AEpoch 27:  98%|█████████▊| 2150/2191 [25:41<00:29,  1.40it/s, loss=2.32, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.66it/s][AEpoch 27:  99%|█████████▊| 2160/2191 [25:41<00:22,  1.40it/s, loss=2.32, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.09it/s][AEpoch 27:  99%|█████████▉| 2170/2191 [25:42<00:14,  1.41it/s, loss=2.32, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.31it/s][AEpoch 27:  99%|█████████▉| 2180/2191 [25:43<00:07,  1.41it/s, loss=2.32, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 14.33it/s][AEpoch 27: 100%|█████████▉| 2190/2191 [25:43<00:00,  1.42it/s, loss=2.32, v_num=647]validation_epoch_end
graph acc: 0.402555910543131
valid accuracy: 0.9781135320663452
Epoch 27: 100%|██████████| 2191/2191 [25:49<00:00,  1.41it/s, loss=2.32, v_num=647]
                                                             [AEpoch 27:   0%|          | 0/2191 [00:00<00:00, 13706.88it/s, loss=2.32, v_num=647]Epoch 28:   0%|          | 0/2191 [00:00<00:00, 3622.02it/s, loss=2.32, v_num=647] Epoch 28:   0%|          | 10/2191 [00:13<43:27,  1.20s/it, loss=2.32, v_num=647] Epoch 28:   0%|          | 10/2191 [00:13<43:27,  1.20s/it, loss=2.3, v_num=647] Epoch 28:   1%|          | 20/2191 [00:22<38:11,  1.06s/it, loss=2.3, v_num=647]Epoch 28:   1%|          | 20/2191 [00:22<38:11,  1.06s/it, loss=2.25, v_num=647]Epoch 28:   1%|▏         | 30/2191 [00:31<36:13,  1.01s/it, loss=2.25, v_num=647]Epoch 28:   1%|▏         | 30/2191 [00:31<36:13,  1.01s/it, loss=2.29, v_num=647]Epoch 28:   2%|▏         | 40/2191 [00:40<35:15,  1.02it/s, loss=2.29, v_num=647]Epoch 28:   2%|▏         | 40/2191 [00:40<35:15,  1.02it/s, loss=2.3, v_num=647] Epoch 28:   2%|▏         | 50/2191 [00:49<34:30,  1.03it/s, loss=2.3, v_num=647]Epoch 28:   2%|▏         | 50/2191 [00:49<34:30,  1.03it/s, loss=2.25, v_num=647]Epoch 28:   3%|▎         | 60/2191 [00:58<34:05,  1.04it/s, loss=2.25, v_num=647]Epoch 28:   3%|▎         | 60/2191 [00:58<34:05,  1.04it/s, loss=2.29, v_num=647]Epoch 28:   3%|▎         | 70/2191 [01:07<33:43,  1.05it/s, loss=2.29, v_num=647]Epoch 28:   3%|▎         | 70/2191 [01:07<33:43,  1.05it/s, loss=2.31, v_num=647]Epoch 28:   4%|▎         | 80/2191 [01:16<33:23,  1.05it/s, loss=2.31, v_num=647]Epoch 28:   4%|▎         | 80/2191 [01:16<33:23,  1.05it/s, loss=2.27, v_num=647]Epoch 28:   4%|▍         | 90/2191 [01:26<33:11,  1.05it/s, loss=2.27, v_num=647]Epoch 28:   4%|▍         | 90/2191 [01:26<33:11,  1.05it/s, loss=2.25, v_num=647]Epoch 28:   5%|▍         | 100/2191 [01:35<32:56,  1.06it/s, loss=2.25, v_num=647]Epoch 28:   5%|▍         | 100/2191 [01:35<32:56,  1.06it/s, loss=2.25, v_num=647]Epoch 28:   5%|▌         | 110/2191 [01:44<32:42,  1.06it/s, loss=2.25, v_num=647]Epoch 28:   5%|▌         | 110/2191 [01:44<32:42,  1.06it/s, loss=2.23, v_num=647]Epoch 28:   5%|▌         | 120/2191 [01:53<32:17,  1.07it/s, loss=2.23, v_num=647]Epoch 28:   5%|▌         | 120/2191 [01:53<32:17,  1.07it/s, loss=2.26, v_num=647]Epoch 28:   6%|▌         | 130/2191 [02:01<31:55,  1.08it/s, loss=2.26, v_num=647]Epoch 28:   6%|▌         | 130/2191 [02:01<31:55,  1.08it/s, loss=2.29, v_num=647]Epoch 28:   6%|▋         | 140/2191 [02:09<31:26,  1.09it/s, loss=2.29, v_num=647]Epoch 28:   6%|▋         | 140/2191 [02:09<31:26,  1.09it/s, loss=2.25, v_num=647]Epoch 28:   7%|▋         | 150/2191 [02:18<31:08,  1.09it/s, loss=2.25, v_num=647]Epoch 28:   7%|▋         | 150/2191 [02:18<31:08,  1.09it/s, loss=2.27, v_num=647]Epoch 28:   7%|▋         | 160/2191 [02:26<30:45,  1.10it/s, loss=2.27, v_num=647]Epoch 28:   7%|▋         | 160/2191 [02:26<30:45,  1.10it/s, loss=2.29, v_num=647]Epoch 28:   8%|▊         | 170/2191 [02:33<30:17,  1.11it/s, loss=2.29, v_num=647]Epoch 28:   8%|▊         | 170/2191 [02:33<30:17,  1.11it/s, loss=2.25, v_num=647]Epoch 28:   8%|▊         | 180/2191 [02:41<29:54,  1.12it/s, loss=2.25, v_num=647]Epoch 28:   8%|▊         | 180/2191 [02:41<29:54,  1.12it/s, loss=2.28, v_num=647]Epoch 28:   9%|▊         | 190/2191 [02:49<29:34,  1.13it/s, loss=2.28, v_num=647]Epoch 28:   9%|▊         | 190/2191 [02:49<29:34,  1.13it/s, loss=2.32, v_num=647]Epoch 28:   9%|▉         | 200/2191 [02:56<29:11,  1.14it/s, loss=2.32, v_num=647]Epoch 28:   9%|▉         | 200/2191 [02:56<29:11,  1.14it/s, loss=2.29, v_num=647]Epoch 28:  10%|▉         | 210/2191 [03:04<28:52,  1.14it/s, loss=2.29, v_num=647]Epoch 28:  10%|▉         | 210/2191 [03:04<28:52,  1.14it/s, loss=2.29, v_num=647]Epoch 28:  10%|█         | 220/2191 [03:11<28:28,  1.15it/s, loss=2.29, v_num=647]Epoch 28:  10%|█         | 220/2191 [03:11<28:28,  1.15it/s, loss=2.27, v_num=647]Epoch 28:  10%|█         | 230/2191 [03:18<28:06,  1.16it/s, loss=2.27, v_num=647]Epoch 28:  10%|█         | 230/2191 [03:18<28:06,  1.16it/s, loss=2.25, v_num=647]Epoch 28:  11%|█         | 240/2191 [03:25<27:45,  1.17it/s, loss=2.25, v_num=647]Epoch 28:  11%|█         | 240/2191 [03:25<27:45,  1.17it/s, loss=2.26, v_num=647]Epoch 28:  11%|█▏        | 250/2191 [03:32<27:25,  1.18it/s, loss=2.26, v_num=647]Epoch 28:  11%|█▏        | 250/2191 [03:32<27:25,  1.18it/s, loss=2.26, v_num=647]Epoch 28:  12%|█▏        | 260/2191 [03:39<27:04,  1.19it/s, loss=2.26, v_num=647]Epoch 28:  12%|█▏        | 260/2191 [03:39<27:04,  1.19it/s, loss=2.27, v_num=647]Epoch 28:  12%|█▏        | 270/2191 [03:47<26:55,  1.19it/s, loss=2.27, v_num=647]Epoch 28:  12%|█▏        | 270/2191 [03:47<26:55,  1.19it/s, loss=2.28, v_num=647]Epoch 28:  13%|█▎        | 280/2191 [03:55<26:40,  1.19it/s, loss=2.28, v_num=647]Epoch 28:  13%|█▎        | 280/2191 [03:55<26:40,  1.19it/s, loss=2.27, v_num=647]Epoch 28:  13%|█▎        | 290/2191 [04:01<26:20,  1.20it/s, loss=2.27, v_num=647]Epoch 28:  13%|█▎        | 290/2191 [04:01<26:20,  1.20it/s, loss=2.3, v_num=647] Epoch 28:  14%|█▎        | 300/2191 [04:08<26:03,  1.21it/s, loss=2.3, v_num=647]Epoch 28:  14%|█▎        | 300/2191 [04:08<26:03,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  14%|█▍        | 310/2191 [04:16<25:51,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  14%|█▍        | 310/2191 [04:16<25:51,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  15%|█▍        | 320/2191 [04:24<25:39,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  15%|█▍        | 320/2191 [04:24<25:39,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  15%|█▌        | 330/2191 [04:31<25:26,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  15%|█▌        | 330/2191 [04:31<25:26,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  16%|█▌        | 340/2191 [04:40<25:24,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  16%|█▌        | 340/2191 [04:40<25:24,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  16%|█▌        | 350/2191 [04:49<25:18,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  16%|█▌        | 350/2191 [04:49<25:18,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  16%|█▋        | 360/2191 [04:58<25:12,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  16%|█▋        | 360/2191 [04:58<25:12,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  17%|█▋        | 370/2191 [05:04<24:55,  1.22it/s, loss=2.24, v_num=647]Epoch 28:  17%|█▋        | 370/2191 [05:04<24:55,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  17%|█▋        | 380/2191 [05:13<24:48,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  17%|█▋        | 380/2191 [05:13<24:48,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  18%|█▊        | 390/2191 [05:20<24:37,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  18%|█▊        | 390/2191 [05:20<24:37,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  18%|█▊        | 400/2191 [05:31<24:38,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  18%|█▊        | 400/2191 [05:31<24:38,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  19%|█▊        | 410/2191 [05:39<24:29,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  19%|█▊        | 410/2191 [05:39<24:29,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  19%|█▉        | 420/2191 [05:47<24:21,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  19%|█▉        | 420/2191 [05:47<24:21,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  20%|█▉        | 430/2191 [05:57<24:19,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  20%|█▉        | 430/2191 [05:57<24:19,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  20%|██        | 440/2191 [06:04<24:06,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  20%|██        | 440/2191 [06:04<24:06,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  21%|██        | 450/2191 [06:12<23:59,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  21%|██        | 450/2191 [06:12<23:59,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  21%|██        | 460/2191 [06:20<23:47,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  21%|██        | 460/2191 [06:20<23:47,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  21%|██▏       | 470/2191 [06:28<23:40,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  21%|██▏       | 470/2191 [06:28<23:40,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  22%|██▏       | 480/2191 [06:36<23:30,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  22%|██▏       | 480/2191 [06:36<23:30,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  22%|██▏       | 490/2191 [06:43<23:18,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  22%|██▏       | 490/2191 [06:43<23:18,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  23%|██▎       | 500/2191 [06:51<23:08,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  23%|██▎       | 500/2191 [06:51<23:08,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  23%|██▎       | 510/2191 [07:00<23:03,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  23%|██▎       | 510/2191 [07:00<23:03,  1.22it/s, loss=2.24, v_num=647]Epoch 28:  24%|██▎       | 520/2191 [07:08<22:54,  1.22it/s, loss=2.24, v_num=647]Epoch 28:  24%|██▎       | 520/2191 [07:08<22:54,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  24%|██▍       | 530/2191 [07:17<22:47,  1.21it/s, loss=2.26, v_num=647]Epoch 28:  24%|██▍       | 530/2191 [07:17<22:47,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  25%|██▍       | 540/2191 [07:26<22:41,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  25%|██▍       | 540/2191 [07:26<22:41,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  25%|██▌       | 550/2191 [07:34<22:34,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  25%|██▌       | 550/2191 [07:34<22:34,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  26%|██▌       | 560/2191 [07:43<22:27,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  26%|██▌       | 560/2191 [07:43<22:27,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  26%|██▌       | 570/2191 [07:52<22:20,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  26%|██▌       | 570/2191 [07:52<22:20,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  26%|██▋       | 580/2191 [07:59<22:09,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  26%|██▋       | 580/2191 [07:59<22:09,  1.21it/s, loss=2.33, v_num=647]Epoch 28:  27%|██▋       | 590/2191 [08:08<22:04,  1.21it/s, loss=2.33, v_num=647]Epoch 28:  27%|██▋       | 590/2191 [08:08<22:04,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  27%|██▋       | 600/2191 [08:16<21:54,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  27%|██▋       | 600/2191 [08:16<21:54,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  28%|██▊       | 610/2191 [08:24<21:45,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  28%|██▊       | 610/2191 [08:24<21:45,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  28%|██▊       | 620/2191 [08:32<21:36,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  28%|██▊       | 620/2191 [08:32<21:36,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  29%|██▉       | 630/2191 [08:40<21:27,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  29%|██▉       | 630/2191 [08:40<21:27,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  29%|██▉       | 640/2191 [08:47<21:17,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  29%|██▉       | 640/2191 [08:47<21:17,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  30%|██▉       | 650/2191 [08:55<21:06,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  30%|██▉       | 650/2191 [08:55<21:06,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  30%|███       | 660/2191 [09:04<21:01,  1.21it/s, loss=2.32, v_num=647]Epoch 28:  30%|███       | 660/2191 [09:04<21:01,  1.21it/s, loss=2.34, v_num=647]Epoch 28:  31%|███       | 670/2191 [09:12<20:51,  1.22it/s, loss=2.34, v_num=647]Epoch 28:  31%|███       | 670/2191 [09:12<20:51,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  31%|███       | 680/2191 [09:19<20:41,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  31%|███       | 680/2191 [09:19<20:41,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  31%|███▏      | 690/2191 [09:28<20:34,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  31%|███▏      | 690/2191 [09:28<20:34,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  32%|███▏      | 700/2191 [09:35<20:24,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  32%|███▏      | 700/2191 [09:35<20:24,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  32%|███▏      | 710/2191 [09:44<20:16,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  32%|███▏      | 710/2191 [09:44<20:16,  1.22it/s, loss=2.34, v_num=647]Epoch 28:  33%|███▎      | 720/2191 [09:50<20:05,  1.22it/s, loss=2.34, v_num=647]Epoch 28:  33%|███▎      | 720/2191 [09:50<20:05,  1.22it/s, loss=2.36, v_num=647]Epoch 28:  33%|███▎      | 730/2191 [10:00<20:00,  1.22it/s, loss=2.36, v_num=647]Epoch 28:  33%|███▎      | 730/2191 [10:00<20:00,  1.22it/s, loss=2.37, v_num=647]Epoch 28:  34%|███▍      | 740/2191 [10:08<19:52,  1.22it/s, loss=2.37, v_num=647]Epoch 28:  34%|███▍      | 740/2191 [10:08<19:52,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  34%|███▍      | 750/2191 [10:18<19:46,  1.21it/s, loss=2.31, v_num=647]Epoch 28:  34%|███▍      | 750/2191 [10:18<19:46,  1.21it/s, loss=2.3, v_num=647] Epoch 28:  35%|███▍      | 760/2191 [10:27<19:39,  1.21it/s, loss=2.3, v_num=647]Epoch 28:  35%|███▍      | 760/2191 [10:27<19:39,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  35%|███▌      | 770/2191 [10:34<19:30,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  35%|███▌      | 770/2191 [10:34<19:30,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  36%|███▌      | 780/2191 [10:44<19:23,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  36%|███▌      | 780/2191 [10:44<19:23,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  36%|███▌      | 790/2191 [10:51<19:13,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  36%|███▌      | 790/2191 [10:51<19:13,  1.21it/s, loss=2.24, v_num=647]Epoch 28:  37%|███▋      | 800/2191 [10:58<19:03,  1.22it/s, loss=2.24, v_num=647]Epoch 28:  37%|███▋      | 800/2191 [10:58<19:03,  1.22it/s, loss=2.28, v_num=647]Epoch 28:  37%|███▋      | 810/2191 [11:07<18:57,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  37%|███▋      | 810/2191 [11:07<18:57,  1.21it/s, loss=2.3, v_num=647] Epoch 28:  37%|███▋      | 820/2191 [11:18<18:52,  1.21it/s, loss=2.3, v_num=647]Epoch 28:  37%|███▋      | 820/2191 [11:18<18:52,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  38%|███▊      | 830/2191 [11:25<18:42,  1.21it/s, loss=2.28, v_num=647]Epoch 28:  38%|███▊      | 830/2191 [11:25<18:42,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  38%|███▊      | 840/2191 [11:34<18:35,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  38%|███▊      | 840/2191 [11:34<18:35,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  39%|███▉      | 850/2191 [11:43<18:29,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  39%|███▉      | 850/2191 [11:43<18:29,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  39%|███▉      | 860/2191 [11:51<18:19,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  39%|███▉      | 860/2191 [11:51<18:19,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  40%|███▉      | 870/2191 [11:58<18:09,  1.21it/s, loss=2.25, v_num=647]Epoch 28:  40%|███▉      | 870/2191 [11:58<18:09,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  40%|████      | 880/2191 [12:06<18:00,  1.21it/s, loss=2.29, v_num=647]Epoch 28:  40%|████      | 880/2191 [12:06<18:00,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  41%|████      | 890/2191 [12:13<17:51,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  41%|████      | 890/2191 [12:13<17:51,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  41%|████      | 900/2191 [12:22<17:43,  1.21it/s, loss=2.27, v_num=647]Epoch 28:  41%|████      | 900/2191 [12:22<17:43,  1.21it/s, loss=2.3, v_num=647] Epoch 28:  42%|████▏     | 910/2191 [12:29<17:33,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  42%|████▏     | 910/2191 [12:29<17:33,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  42%|████▏     | 920/2191 [12:36<17:23,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  42%|████▏     | 920/2191 [12:36<17:23,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  42%|████▏     | 930/2191 [12:44<17:15,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  42%|████▏     | 930/2191 [12:44<17:15,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  43%|████▎     | 940/2191 [12:52<17:07,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  43%|████▎     | 940/2191 [12:52<17:07,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  43%|████▎     | 950/2191 [12:59<16:57,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  43%|████▎     | 950/2191 [12:59<16:57,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  44%|████▍     | 960/2191 [13:07<16:48,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  44%|████▍     | 960/2191 [13:07<16:48,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  44%|████▍     | 970/2191 [13:15<16:40,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  44%|████▍     | 970/2191 [13:15<16:40,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  45%|████▍     | 980/2191 [13:23<16:31,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  45%|████▍     | 980/2191 [13:23<16:31,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  45%|████▌     | 990/2191 [13:30<16:21,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  45%|████▌     | 990/2191 [13:30<16:21,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  46%|████▌     | 1000/2191 [13:38<16:13,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  46%|████▌     | 1000/2191 [13:38<16:13,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  46%|████▌     | 1010/2191 [13:47<16:06,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  46%|████▌     | 1010/2191 [13:47<16:06,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  47%|████▋     | 1020/2191 [13:56<15:58,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  47%|████▋     | 1020/2191 [13:56<15:58,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  47%|████▋     | 1030/2191 [14:03<15:49,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  47%|████▋     | 1040/2191 [14:10<15:40,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  47%|████▋     | 1040/2191 [14:10<15:40,  1.22it/s, loss=2.28, v_num=647]Epoch 28:  48%|████▊     | 1050/2191 [14:18<15:31,  1.22it/s, loss=2.28, v_num=647]Epoch 28:  48%|████▊     | 1050/2191 [14:18<15:31,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  48%|████▊     | 1060/2191 [14:26<15:23,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  48%|████▊     | 1060/2191 [14:26<15:23,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  49%|████▉     | 1070/2191 [14:34<15:14,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  49%|████▉     | 1070/2191 [14:34<15:14,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  49%|████▉     | 1080/2191 [14:41<15:06,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  49%|████▉     | 1080/2191 [14:41<15:06,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  50%|████▉     | 1090/2191 [14:49<14:58,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  50%|████▉     | 1090/2191 [14:49<14:58,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  50%|█████     | 1100/2191 [14:59<14:50,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  50%|█████     | 1100/2191 [14:59<14:50,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  51%|█████     | 1110/2191 [15:07<14:43,  1.22it/s, loss=2.26, v_num=647]Epoch 28:  51%|█████     | 1110/2191 [15:07<14:43,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  51%|█████     | 1120/2191 [15:16<14:35,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  51%|█████     | 1120/2191 [15:16<14:35,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  52%|█████▏    | 1130/2191 [15:23<14:26,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  52%|█████▏    | 1130/2191 [15:23<14:26,  1.22it/s, loss=2.28, v_num=647]Epoch 28:  52%|█████▏    | 1140/2191 [15:30<14:17,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  52%|█████▏    | 1140/2191 [15:30<14:17,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  52%|█████▏    | 1150/2191 [15:38<14:09,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  52%|█████▏    | 1150/2191 [15:38<14:09,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  53%|█████▎    | 1160/2191 [15:47<14:01,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  53%|█████▎    | 1160/2191 [15:47<14:01,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  53%|█████▎    | 1170/2191 [15:56<13:53,  1.22it/s, loss=2.32, v_num=647]Epoch 28:  53%|█████▎    | 1170/2191 [15:56<13:53,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  54%|█████▍    | 1180/2191 [16:04<13:45,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  54%|█████▍    | 1180/2191 [16:04<13:45,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  54%|█████▍    | 1190/2191 [16:12<13:37,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  54%|█████▍    | 1190/2191 [16:12<13:37,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  55%|█████▍    | 1200/2191 [16:20<13:29,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  55%|█████▍    | 1200/2191 [16:20<13:29,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  55%|█████▌    | 1210/2191 [16:29<13:21,  1.22it/s, loss=2.25, v_num=647]Epoch 28:  55%|█████▌    | 1210/2191 [16:29<13:21,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  56%|█████▌    | 1220/2191 [16:38<13:13,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  56%|█████▌    | 1220/2191 [16:38<13:13,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  56%|█████▌    | 1230/2191 [16:46<13:05,  1.22it/s, loss=2.33, v_num=647]Epoch 28:  56%|█████▌    | 1230/2191 [16:46<13:05,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  57%|█████▋    | 1240/2191 [16:53<12:56,  1.22it/s, loss=2.3, v_num=647]Epoch 28:  57%|█████▋    | 1240/2191 [16:53<12:56,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  57%|█████▋    | 1250/2191 [17:02<12:49,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  57%|█████▋    | 1250/2191 [17:02<12:49,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  58%|█████▊    | 1260/2191 [17:10<12:40,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  58%|█████▊    | 1260/2191 [17:10<12:40,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  58%|█████▊    | 1270/2191 [17:18<12:32,  1.22it/s, loss=2.31, v_num=647]Epoch 28:  58%|█████▊    | 1270/2191 [17:18<12:32,  1.22it/s, loss=2.3, v_num=647] Epoch 28:  58%|█████▊    | 1280/2191 [17:25<12:23,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  58%|█████▊    | 1280/2191 [17:25<12:23,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  59%|█████▉    | 1290/2191 [17:34<12:15,  1.22it/s, loss=2.27, v_num=647]Epoch 28:  59%|█████▉    | 1290/2191 [17:34<12:15,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  59%|█████▉    | 1300/2191 [17:42<12:07,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  59%|█████▉    | 1300/2191 [17:42<12:07,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  60%|█████▉    | 1310/2191 [17:51<11:59,  1.22it/s, loss=2.29, v_num=647]Epoch 28:  60%|█████▉    | 1310/2191 [17:51<11:59,  1.22it/s, loss=2.24, v_num=647]Epoch 28:  60%|██████    | 1320/2191 [17:58<11:50,  1.23it/s, loss=2.24, v_num=647]Epoch 28:  60%|██████    | 1320/2191 [17:58<11:50,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  61%|██████    | 1330/2191 [18:05<11:42,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  61%|██████    | 1330/2191 [18:05<11:42,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  61%|██████    | 1340/2191 [18:12<11:33,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  61%|██████    | 1340/2191 [18:12<11:33,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  62%|██████▏   | 1350/2191 [18:20<11:25,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  62%|██████▏   | 1350/2191 [18:20<11:25,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  62%|██████▏   | 1360/2191 [18:28<11:17,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  62%|██████▏   | 1360/2191 [18:28<11:17,  1.23it/s, loss=2.33, v_num=647]Epoch 28:  63%|██████▎   | 1370/2191 [18:36<11:08,  1.23it/s, loss=2.33, v_num=647]Epoch 28:  63%|██████▎   | 1370/2191 [18:36<11:08,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  63%|██████▎   | 1380/2191 [18:44<11:00,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  63%|██████▎   | 1380/2191 [18:44<11:00,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  63%|██████▎   | 1390/2191 [18:54<10:53,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  63%|██████▎   | 1390/2191 [18:54<10:53,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  64%|██████▍   | 1400/2191 [19:01<10:44,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  64%|██████▍   | 1400/2191 [19:01<10:44,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  64%|██████▍   | 1410/2191 [19:09<10:36,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  64%|██████▍   | 1410/2191 [19:09<10:36,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  65%|██████▍   | 1420/2191 [19:16<10:27,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  65%|██████▍   | 1420/2191 [19:16<10:27,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  65%|██████▌   | 1430/2191 [19:25<10:19,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  65%|██████▌   | 1430/2191 [19:25<10:19,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  66%|██████▌   | 1440/2191 [19:33<10:11,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  66%|██████▌   | 1440/2191 [19:33<10:11,  1.23it/s, loss=2.22, v_num=647]Epoch 28:  66%|██████▌   | 1450/2191 [19:41<10:03,  1.23it/s, loss=2.22, v_num=647]Epoch 28:  66%|██████▌   | 1450/2191 [19:41<10:03,  1.23it/s, loss=2.23, v_num=647]Epoch 28:  67%|██████▋   | 1460/2191 [19:48<09:54,  1.23it/s, loss=2.23, v_num=647]Epoch 28:  67%|██████▋   | 1460/2191 [19:48<09:54,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  67%|██████▋   | 1470/2191 [19:56<09:46,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  67%|██████▋   | 1470/2191 [19:56<09:46,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  68%|██████▊   | 1480/2191 [20:05<09:38,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  68%|██████▊   | 1480/2191 [20:05<09:38,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  68%|██████▊   | 1490/2191 [20:13<09:30,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  68%|██████▊   | 1490/2191 [20:13<09:30,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  68%|██████▊   | 1500/2191 [20:21<09:22,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  68%|██████▊   | 1500/2191 [20:21<09:22,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  69%|██████▉   | 1510/2191 [20:30<09:14,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  69%|██████▉   | 1510/2191 [20:30<09:14,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  69%|██████▉   | 1520/2191 [20:38<09:06,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  69%|██████▉   | 1520/2191 [20:38<09:06,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  70%|██████▉   | 1530/2191 [20:46<08:58,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  70%|██████▉   | 1530/2191 [20:46<08:58,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  70%|███████   | 1540/2191 [20:54<08:49,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  70%|███████   | 1540/2191 [20:54<08:49,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  71%|███████   | 1550/2191 [21:01<08:41,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  71%|███████   | 1550/2191 [21:01<08:41,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  71%|███████   | 1560/2191 [21:09<08:33,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  71%|███████   | 1560/2191 [21:09<08:33,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  72%|███████▏  | 1570/2191 [21:16<08:24,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  72%|███████▏  | 1570/2191 [21:16<08:24,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  72%|███████▏  | 1580/2191 [21:24<08:16,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  72%|███████▏  | 1580/2191 [21:24<08:16,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  73%|███████▎  | 1590/2191 [21:33<08:08,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  73%|███████▎  | 1590/2191 [21:33<08:08,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  73%|███████▎  | 1600/2191 [21:41<08:00,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  73%|███████▎  | 1600/2191 [21:41<08:00,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  73%|███████▎  | 1610/2191 [21:49<07:52,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  73%|███████▎  | 1610/2191 [21:49<07:52,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  74%|███████▍  | 1620/2191 [21:56<07:43,  1.23it/s, loss=2.27, v_num=647]Epoch 28:  74%|███████▍  | 1620/2191 [21:56<07:43,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  74%|███████▍  | 1630/2191 [22:04<07:35,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  74%|███████▍  | 1630/2191 [22:04<07:35,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  75%|███████▍  | 1640/2191 [22:12<07:27,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  75%|███████▍  | 1640/2191 [22:12<07:27,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  75%|███████▌  | 1650/2191 [22:20<07:19,  1.23it/s, loss=2.26, v_num=647]Epoch 28:  75%|███████▌  | 1650/2191 [22:20<07:19,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  76%|███████▌  | 1660/2191 [22:27<07:10,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  76%|███████▌  | 1660/2191 [22:27<07:10,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  76%|███████▌  | 1670/2191 [22:34<07:02,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  76%|███████▌  | 1670/2191 [22:34<07:02,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  77%|███████▋  | 1680/2191 [22:43<06:54,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  77%|███████▋  | 1680/2191 [22:43<06:54,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  77%|███████▋  | 1690/2191 [22:50<06:46,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  77%|███████▋  | 1690/2191 [22:50<06:46,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  78%|███████▊  | 1700/2191 [23:00<06:38,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  78%|███████▊  | 1700/2191 [23:00<06:38,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  78%|███████▊  | 1710/2191 [23:07<06:30,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  78%|███████▊  | 1710/2191 [23:07<06:30,  1.23it/s, loss=2.33, v_num=647]Epoch 28:  79%|███████▊  | 1720/2191 [23:15<06:22,  1.23it/s, loss=2.33, v_num=647]Epoch 28:  79%|███████▊  | 1720/2191 [23:15<06:22,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  79%|███████▉  | 1730/2191 [23:24<06:14,  1.23it/s, loss=2.32, v_num=647]Epoch 28:  79%|███████▉  | 1730/2191 [23:24<06:14,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  79%|███████▉  | 1740/2191 [23:34<06:06,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  79%|███████▉  | 1740/2191 [23:34<06:06,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  80%|███████▉  | 1750/2191 [23:44<05:58,  1.23it/s, loss=2.29, v_num=647]Epoch 28:  80%|███████▉  | 1750/2191 [23:44<05:58,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  80%|████████  | 1760/2191 [23:53<05:50,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  80%|████████  | 1760/2191 [23:53<05:50,  1.23it/s, loss=2.35, v_num=647]Epoch 28:  81%|████████  | 1770/2191 [24:02<05:42,  1.23it/s, loss=2.35, v_num=647]Epoch 28:  81%|████████  | 1770/2191 [24:02<05:42,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  81%|████████  | 1780/2191 [24:09<05:34,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  81%|████████  | 1780/2191 [24:09<05:34,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  82%|████████▏ | 1790/2191 [24:20<05:27,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  82%|████████▏ | 1790/2191 [24:20<05:27,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  82%|████████▏ | 1800/2191 [24:28<05:18,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  82%|████████▏ | 1800/2191 [24:28<05:18,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  83%|████████▎ | 1810/2191 [24:35<05:10,  1.23it/s, loss=2.34, v_num=647]Epoch 28:  83%|████████▎ | 1810/2191 [24:35<05:10,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  83%|████████▎ | 1820/2191 [24:42<05:02,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  83%|████████▎ | 1820/2191 [24:42<05:02,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  84%|████████▎ | 1830/2191 [24:50<04:53,  1.23it/s, loss=2.28, v_num=647]Epoch 28:  84%|████████▎ | 1830/2191 [24:50<04:53,  1.23it/s, loss=2.3, v_num=647] Epoch 28:  84%|████████▍ | 1840/2191 [24:57<04:45,  1.23it/s, loss=2.3, v_num=647]Epoch 28:  84%|████████▍ | 1840/2191 [24:57<04:45,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  84%|████████▍ | 1850/2191 [25:01<04:36,  1.23it/s, loss=2.31, v_num=647]Epoch 28:  84%|████████▍ | 1850/2191 [25:01<04:36,  1.23it/s, loss=2.33, v_num=647]validation_epoch_end
graph acc: 0.3929712460063898
valid accuracy: 0.9779782295227051
alidation_epoch_end
graph acc: 0.3546325878594249
valid accuracy: 0.9777571558952332
validation_epoch_end
graph acc: 0.46006389776357826
valid accuracy: 0.9756930470466614
1 [25:06<04:18,  1.24it/s, loss=2.3, v_num=647]Epoch 28:  85%|████████▌ | 1870/2191 [25:06<04:18,  1.24it/s, loss=2.26, v_num=647]validation_epoch_end
graph acc: 0.3610223642172524
valid accuracy: 0.975616991519928
validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.9778801798820496
Epoch 28:  86%|████████▌ | 1880/2191 [25:08<04:09,  1.25it/s, loss=2.26, v_num=647]
Validating: 0it [00:00, ?it/s][Avalidation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.979070782661438

Validating:   0%|          | 0/313 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/313 [00:01<00:48,  6.28it/s][AEpoch 28:  86%|████████▋ | 1890/2191 [25:09<04:00,  1.25it/s, loss=2.26, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:51,  5.74it/s][AEpoch 28:  87%|████████▋ | 1900/2191 [25:11<03:51,  1.26it/s, loss=2.26, v_num=647]
Validating:  10%|▉         | 30/313 [00:03<00:33,  8.36it/s][AEpoch 28:  87%|████████▋ | 1910/2191 [25:12<03:42,  1.26it/s, loss=2.26, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:27,  9.90it/s][AEpoch 28:  88%|████████▊ | 1920/2191 [25:12<03:33,  1.27it/s, loss=2.26, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.17it/s][AEpoch 28:  88%|████████▊ | 1930/2191 [25:13<03:24,  1.28it/s, loss=2.26, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.64it/s][AEpoch 28:  89%|████████▊ | 1940/2191 [25:14<03:15,  1.28it/s, loss=2.26, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:24,  9.91it/s][AEpoch 28:  89%|████████▉ | 1950/2191 [25:15<03:07,  1.29it/s, loss=2.26, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.31it/s][AEpoch 28:  89%|████████▉ | 1960/2191 [25:16<02:58,  1.29it/s, loss=2.26, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:22, 10.10it/s][AEpoch 28:  90%|████████▉ | 1970/2191 [25:17<02:50,  1.30it/s, loss=2.26, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.36it/s][AEpoch 28:  90%|█████████ | 1980/2191 [25:18<02:41,  1.30it/s, loss=2.26, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:18, 10.76it/s][AEpoch 28:  91%|█████████ | 1990/2191 [25:19<02:33,  1.31it/s, loss=2.26, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:20,  9.44it/s][AEpoch 28:  91%|█████████▏| 2000/2191 [25:20<02:25,  1.32it/s, loss=2.26, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:20,  8.97it/s][AEpoch 28:  92%|█████████▏| 2010/2191 [25:22<02:16,  1.32it/s, loss=2.26, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:21,  8.24it/s][AEpoch 28:  92%|█████████▏| 2020/2191 [25:23<02:08,  1.33it/s, loss=2.26, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.45it/s][AEpoch 28:  93%|█████████▎| 2030/2191 [25:24<02:00,  1.33it/s, loss=2.26, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15, 10.11it/s][AEpoch 28:  93%|█████████▎| 2040/2191 [25:25<01:52,  1.34it/s, loss=2.26, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.97it/s][AEpoch 28:  94%|█████████▎| 2050/2191 [25:25<01:44,  1.34it/s, loss=2.26, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.55it/s][AEpoch 28:  94%|█████████▍| 2060/2191 [25:26<01:37,  1.35it/s, loss=2.26, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 11.80it/s][AEpoch 28:  94%|█████████▍| 2070/2191 [25:27<01:29,  1.36it/s, loss=2.26, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.48it/s][AEpoch 28:  95%|█████████▍| 2080/2191 [25:28<01:21,  1.36it/s, loss=2.26, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 10.88it/s][AEpoch 28:  95%|█████████▌| 2090/2191 [25:29<01:13,  1.37it/s, loss=2.26, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:10,  9.14it/s][AEpoch 28:  96%|█████████▌| 2100/2191 [25:30<01:06,  1.37it/s, loss=2.26, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:08, 10.29it/s][AEpoch 28:  96%|█████████▋| 2110/2191 [25:31<00:58,  1.38it/s, loss=2.26, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.52it/s][AEpoch 28:  97%|█████████▋| 2120/2191 [25:32<00:51,  1.38it/s, loss=2.26, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.42it/s][AEpoch 28:  97%|█████████▋| 2130/2191 [25:33<00:43,  1.39it/s, loss=2.26, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05, 10.13it/s][AEpoch 28:  98%|█████████▊| 2140/2191 [25:34<00:36,  1.40it/s, loss=2.26, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.49it/s][AEpoch 28:  98%|█████████▊| 2150/2191 [25:35<00:29,  1.40it/s, loss=2.26, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.47it/s][AEpoch 28:  99%|█████████▊| 2160/2191 [25:35<00:22,  1.41it/s, loss=2.26, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.35it/s][AEpoch 28:  99%|█████████▉| 2170/2191 [25:36<00:14,  1.41it/s, loss=2.26, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.44it/s][AEpoch 28:  99%|█████████▉| 2180/2191 [25:37<00:07,  1.42it/s, loss=2.26, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 14.16it/s][AEpoch 28: 100%|█████████▉| 2190/2191 [25:37<00:00,  1.42it/s, loss=2.26, v_num=647]validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.9791657328605652
Epoch 28: 100%|██████████| 2191/2191 [25:43<00:00,  1.42it/s, loss=2.29, v_num=647]
                                                             [AEpoch 28:   0%|          | 0/2191 [00:00<00:00, 12192.74it/s, loss=2.29, v_num=647]Epoch 29:   0%|          | 0/2191 [00:00<00:00, 2884.67it/s, loss=2.29, v_num=647] Epoch 29:   0%|          | 10/2191 [00:14<48:39,  1.34s/it, loss=2.29, v_num=647] Epoch 29:   0%|          | 10/2191 [00:14<48:39,  1.34s/it, loss=2.27, v_num=647]Epoch 29:   1%|          | 20/2191 [00:24<41:34,  1.15s/it, loss=2.27, v_num=647]Epoch 29:   1%|          | 20/2191 [00:24<41:34,  1.15s/it, loss=2.27, v_num=647]Epoch 29:   1%|▏         | 30/2191 [00:33<39:24,  1.09s/it, loss=2.27, v_num=647]Epoch 29:   1%|▏         | 30/2191 [00:33<39:24,  1.09s/it, loss=2.27, v_num=647]Epoch 29:   2%|▏         | 40/2191 [00:43<38:00,  1.06s/it, loss=2.27, v_num=647]Epoch 29:   2%|▏         | 40/2191 [00:43<38:00,  1.06s/it, loss=2.24, v_num=647]Epoch 29:   2%|▏         | 50/2191 [00:52<36:47,  1.03s/it, loss=2.24, v_num=647]Epoch 29:   2%|▏         | 50/2191 [00:52<36:47,  1.03s/it, loss=2.26, v_num=647]Epoch 29:   3%|▎         | 60/2191 [01:02<36:17,  1.02s/it, loss=2.26, v_num=647]Epoch 29:   3%|▎         | 60/2191 [01:02<36:17,  1.02s/it, loss=2.29, v_num=647]Epoch 29:   3%|▎         | 70/2191 [01:11<35:48,  1.01s/it, loss=2.29, v_num=647]Epoch 29:   3%|▎         | 70/2191 [01:11<35:48,  1.01s/it, loss=2.29, v_num=647]Epoch 29:   4%|▎         | 80/2191 [01:21<35:21,  1.01s/it, loss=2.29, v_num=647]Epoch 29:   4%|▎         | 80/2191 [01:21<35:21,  1.01s/it, loss=2.29, v_num=647]Epoch 29:   4%|▍         | 90/2191 [01:30<34:51,  1.00it/s, loss=2.29, v_num=647]Epoch 29:   4%|▍         | 90/2191 [01:30<34:51,  1.00it/s, loss=2.27, v_num=647]Epoch 29:   5%|▍         | 100/2191 [01:38<34:04,  1.02it/s, loss=2.27, v_num=647]Epoch 29:   5%|▍         | 100/2191 [01:38<34:04,  1.02it/s, loss=2.28, v_num=647]Epoch 29:   5%|▌         | 110/2191 [01:46<33:20,  1.04it/s, loss=2.28, v_num=647]Epoch 29:   5%|▌         | 110/2191 [01:46<33:20,  1.04it/s, loss=2.31, v_num=647]Epoch 29:   5%|▌         | 120/2191 [01:54<32:37,  1.06it/s, loss=2.31, v_num=647]Epoch 29:   5%|▌         | 120/2191 [01:54<32:37,  1.06it/s, loss=2.28, v_num=647]Epoch 29:   6%|▌         | 130/2191 [02:02<32:02,  1.07it/s, loss=2.28, v_num=647]Epoch 29:   6%|▌         | 130/2191 [02:02<32:02,  1.07it/s, loss=2.28, v_num=647]Epoch 29:   6%|▋         | 140/2191 [02:09<31:30,  1.08it/s, loss=2.28, v_num=647]Epoch 29:   6%|▋         | 140/2191 [02:09<31:30,  1.08it/s, loss=2.29, v_num=647]Epoch 29:   7%|▋         | 150/2191 [02:17<30:53,  1.10it/s, loss=2.29, v_num=647]Epoch 29:   7%|▋         | 150/2191 [02:17<30:53,  1.10it/s, loss=2.26, v_num=647]Epoch 29:   7%|▋         | 160/2191 [02:24<30:16,  1.12it/s, loss=2.26, v_num=647]Epoch 29:   7%|▋         | 160/2191 [02:24<30:16,  1.12it/s, loss=2.31, v_num=647]Epoch 29:   8%|▊         | 170/2191 [02:31<29:46,  1.13it/s, loss=2.31, v_num=647]Epoch 29:   8%|▊         | 170/2191 [02:31<29:46,  1.13it/s, loss=2.32, v_num=647]Epoch 29:   8%|▊         | 180/2191 [02:38<29:17,  1.14it/s, loss=2.32, v_num=647]Epoch 29:   8%|▊         | 180/2191 [02:38<29:17,  1.14it/s, loss=2.23, v_num=647]Epoch 29:   9%|▊         | 190/2191 [02:45<28:51,  1.16it/s, loss=2.23, v_num=647]Epoch 29:   9%|▊         | 190/2191 [02:45<28:51,  1.16it/s, loss=2.2, v_num=647] Epoch 29:   9%|▉         | 200/2191 [02:52<28:33,  1.16it/s, loss=2.2, v_num=647]Epoch 29:   9%|▉         | 200/2191 [02:52<28:33,  1.16it/s, loss=2.22, v_num=647]Epoch 29:  10%|▉         | 210/2191 [03:00<28:14,  1.17it/s, loss=2.22, v_num=647]Epoch 29:  10%|▉         | 210/2191 [03:00<28:14,  1.17it/s, loss=2.23, v_num=647]Epoch 29:  10%|█         | 220/2191 [03:07<27:49,  1.18it/s, loss=2.23, v_num=647]Epoch 29:  10%|█         | 220/2191 [03:07<27:49,  1.18it/s, loss=2.27, v_num=647]Epoch 29:  10%|█         | 230/2191 [03:15<27:36,  1.18it/s, loss=2.27, v_num=647]Epoch 29:  10%|█         | 230/2191 [03:15<27:36,  1.18it/s, loss=2.29, v_num=647]Epoch 29:  11%|█         | 240/2191 [03:22<27:20,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  11%|█         | 240/2191 [03:22<27:20,  1.19it/s, loss=2.26, v_num=647]Epoch 29:  11%|█▏        | 250/2191 [03:29<27:00,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  11%|█▏        | 250/2191 [03:29<27:00,  1.20it/s, loss=2.21, v_num=647]Epoch 29:  12%|█▏        | 260/2191 [03:38<26:56,  1.19it/s, loss=2.21, v_num=647]Epoch 29:  12%|█▏        | 260/2191 [03:38<26:56,  1.19it/s, loss=2.23, v_num=647]Epoch 29:  12%|█▏        | 270/2191 [03:46<26:45,  1.20it/s, loss=2.23, v_num=647]Epoch 29:  12%|█▏        | 270/2191 [03:46<26:45,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  13%|█▎        | 280/2191 [03:54<26:34,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  13%|█▎        | 280/2191 [03:54<26:34,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  13%|█▎        | 290/2191 [04:02<26:25,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  13%|█▎        | 290/2191 [04:02<26:25,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  14%|█▎        | 300/2191 [04:09<26:08,  1.21it/s, loss=2.24, v_num=647]Epoch 29:  14%|█▎        | 300/2191 [04:09<26:08,  1.21it/s, loss=2.25, v_num=647]Epoch 29:  14%|█▍        | 310/2191 [04:17<25:57,  1.21it/s, loss=2.25, v_num=647]Epoch 29:  14%|█▍        | 310/2191 [04:17<25:57,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  15%|█▍        | 320/2191 [04:27<25:59,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  15%|█▍        | 320/2191 [04:27<25:59,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  15%|█▌        | 330/2191 [04:35<25:48,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  15%|█▌        | 330/2191 [04:35<25:48,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  16%|█▌        | 340/2191 [04:45<25:51,  1.19it/s, loss=2.26, v_num=647]Epoch 29:  16%|█▌        | 340/2191 [04:45<25:51,  1.19it/s, loss=2.32, v_num=647]Epoch 29:  16%|█▌        | 350/2191 [04:55<25:47,  1.19it/s, loss=2.32, v_num=647]Epoch 29:  16%|█▌        | 350/2191 [04:55<25:47,  1.19it/s, loss=2.3, v_num=647] Epoch 29:  16%|█▋        | 360/2191 [05:02<25:34,  1.19it/s, loss=2.3, v_num=647]Epoch 29:  16%|█▋        | 360/2191 [05:02<25:34,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  17%|█▋        | 370/2191 [05:11<25:29,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  17%|█▋        | 370/2191 [05:11<25:29,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  17%|█▋        | 380/2191 [05:19<25:17,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  17%|█▋        | 380/2191 [05:19<25:17,  1.19it/s, loss=2.25, v_num=647]Epoch 29:  18%|█▊        | 390/2191 [05:28<25:10,  1.19it/s, loss=2.25, v_num=647]Epoch 29:  18%|█▊        | 390/2191 [05:28<25:10,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  18%|█▊        | 400/2191 [05:37<25:06,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  18%|█▊        | 400/2191 [05:37<25:06,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  19%|█▊        | 410/2191 [05:45<24:56,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  19%|█▊        | 410/2191 [05:45<24:56,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  19%|█▉        | 420/2191 [05:53<24:47,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  19%|█▉        | 420/2191 [05:53<24:47,  1.19it/s, loss=2.26, v_num=647]Epoch 29:  20%|█▉        | 430/2191 [06:01<24:35,  1.19it/s, loss=2.26, v_num=647]Epoch 29:  20%|█▉        | 430/2191 [06:01<24:35,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  20%|██        | 440/2191 [06:12<24:37,  1.18it/s, loss=2.28, v_num=647]Epoch 29:  20%|██        | 440/2191 [06:12<24:37,  1.18it/s, loss=2.31, v_num=647]Epoch 29:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  21%|██        | 460/2191 [06:26<24:11,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  21%|██        | 460/2191 [06:26<24:11,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  21%|██▏       | 470/2191 [06:33<23:59,  1.20it/s, loss=2.31, v_num=647]Epoch 29:  21%|██▏       | 470/2191 [06:33<23:59,  1.20it/s, loss=2.3, v_num=647] Epoch 29:  22%|██▏       | 480/2191 [06:40<23:44,  1.20it/s, loss=2.3, v_num=647]Epoch 29:  22%|██▏       | 480/2191 [06:40<23:44,  1.20it/s, loss=2.3, v_num=647]Epoch 29:  22%|██▏       | 490/2191 [06:49<23:39,  1.20it/s, loss=2.3, v_num=647]Epoch 29:  22%|██▏       | 490/2191 [06:49<23:39,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  23%|██▎       | 500/2191 [06:58<23:31,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  23%|██▎       | 500/2191 [06:58<23:31,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  23%|██▎       | 510/2191 [07:06<23:24,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  23%|██▎       | 510/2191 [07:06<23:24,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  24%|██▎       | 520/2191 [07:13<23:11,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  24%|██▎       | 520/2191 [07:13<23:11,  1.20it/s, loss=2.23, v_num=647]Epoch 29:  24%|██▍       | 530/2191 [07:21<23:00,  1.20it/s, loss=2.23, v_num=647]Epoch 29:  24%|██▍       | 530/2191 [07:21<23:00,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  25%|██▍       | 540/2191 [07:29<22:52,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  25%|██▍       | 540/2191 [07:29<22:52,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  25%|██▌       | 550/2191 [07:37<22:42,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  25%|██▌       | 550/2191 [07:37<22:42,  1.20it/s, loss=2.23, v_num=647]Epoch 29:  26%|██▌       | 560/2191 [07:45<22:33,  1.20it/s, loss=2.23, v_num=647]Epoch 29:  26%|██▌       | 560/2191 [07:45<22:33,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  26%|██▌       | 570/2191 [07:53<22:23,  1.21it/s, loss=2.25, v_num=647]Epoch 29:  26%|██▌       | 570/2191 [07:53<22:23,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  26%|██▋       | 580/2191 [08:01<22:16,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  26%|██▋       | 580/2191 [08:01<22:16,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  27%|██▋       | 590/2191 [08:11<22:12,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  27%|██▋       | 590/2191 [08:11<22:12,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  27%|██▋       | 600/2191 [08:20<22:03,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  27%|██▋       | 600/2191 [08:20<22:03,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  28%|██▊       | 610/2191 [08:27<21:53,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  28%|██▊       | 610/2191 [08:27<21:53,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  28%|██▊       | 620/2191 [08:35<21:43,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  28%|██▊       | 620/2191 [08:35<21:43,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  29%|██▉       | 630/2191 [08:43<21:36,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  29%|██▉       | 630/2191 [08:43<21:36,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  29%|██▉       | 640/2191 [08:52<21:28,  1.20it/s, loss=2.25, v_num=647]Epoch 29:  29%|██▉       | 640/2191 [08:52<21:28,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  30%|██▉       | 650/2191 [09:01<21:20,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  30%|██▉       | 650/2191 [09:01<21:20,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  30%|███       | 660/2191 [09:08<21:11,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  30%|███       | 660/2191 [09:08<21:11,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  31%|███       | 670/2191 [09:17<21:03,  1.20it/s, loss=2.24, v_num=647]Epoch 29:  31%|███       | 670/2191 [09:17<21:03,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  31%|███       | 680/2191 [09:25<20:54,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  31%|███       | 680/2191 [09:25<20:54,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  31%|███▏      | 690/2191 [09:33<20:46,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  31%|███▏      | 690/2191 [09:33<20:46,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  32%|███▏      | 700/2191 [09:41<20:37,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  32%|███▏      | 700/2191 [09:41<20:37,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  32%|███▏      | 710/2191 [09:51<20:31,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  32%|███▏      | 710/2191 [09:51<20:31,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  33%|███▎      | 720/2191 [09:58<20:21,  1.20it/s, loss=2.26, v_num=647]Epoch 29:  33%|███▎      | 720/2191 [09:58<20:21,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  33%|███▎      | 730/2191 [10:07<20:13,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  33%|███▎      | 730/2191 [10:07<20:13,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  34%|███▍      | 740/2191 [10:15<20:06,  1.20it/s, loss=2.27, v_num=647]Epoch 29:  34%|███▍      | 740/2191 [10:15<20:06,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  34%|███▍      | 750/2191 [10:23<19:57,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  34%|███▍      | 750/2191 [10:23<19:57,  1.20it/s, loss=2.32, v_num=647]Epoch 29:  35%|███▍      | 760/2191 [10:37<19:58,  1.19it/s, loss=2.32, v_num=647]Epoch 29:  35%|███▍      | 760/2191 [10:37<19:58,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  35%|███▌      | 770/2191 [10:46<19:51,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  35%|███▌      | 770/2191 [10:46<19:51,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  36%|███▌      | 780/2191 [10:56<19:46,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  36%|███▌      | 780/2191 [10:56<19:46,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  36%|███▌      | 790/2191 [11:04<19:36,  1.19it/s, loss=2.28, v_num=647]Epoch 29:  36%|███▌      | 790/2191 [11:04<19:36,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  37%|███▋      | 800/2191 [11:11<19:25,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  37%|███▋      | 800/2191 [11:11<19:25,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  37%|███▋      | 810/2191 [11:19<19:16,  1.19it/s, loss=2.31, v_num=647]Epoch 29:  37%|███▋      | 810/2191 [11:19<19:16,  1.19it/s, loss=2.29, v_num=647]Epoch 29:  37%|███▋      | 820/2191 [11:26<19:06,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  37%|███▋      | 820/2191 [11:26<19:06,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  38%|███▊      | 830/2191 [11:33<18:55,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  38%|███▊      | 830/2191 [11:33<18:55,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  38%|███▊      | 840/2191 [11:40<18:45,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  38%|███▊      | 840/2191 [11:40<18:45,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  39%|███▉      | 850/2191 [11:49<18:38,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  39%|███▉      | 850/2191 [11:49<18:38,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  39%|███▉      | 860/2191 [11:56<18:27,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  39%|███▉      | 860/2191 [11:56<18:27,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  40%|███▉      | 870/2191 [12:05<18:20,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  40%|███▉      | 870/2191 [12:05<18:20,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  40%|████      | 880/2191 [12:13<18:10,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  40%|████      | 880/2191 [12:13<18:10,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  41%|████      | 890/2191 [12:20<18:01,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  41%|████      | 890/2191 [12:20<18:01,  1.20it/s, loss=2.31, v_num=647]Epoch 29:  41%|████      | 900/2191 [12:28<17:52,  1.20it/s, loss=2.31, v_num=647]Epoch 29:  41%|████      | 900/2191 [12:28<17:52,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  42%|████▏     | 910/2191 [12:36<17:43,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  42%|████▏     | 910/2191 [12:36<17:43,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  42%|████▏     | 920/2191 [12:44<17:35,  1.20it/s, loss=2.28, v_num=647]Epoch 29:  42%|████▏     | 920/2191 [12:44<17:35,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  42%|████▏     | 930/2191 [12:52<17:26,  1.20it/s, loss=2.29, v_num=647]Epoch 29:  42%|████▏     | 930/2191 [12:52<17:26,  1.20it/s, loss=2.3, v_num=647] Epoch 29:  43%|████▎     | 940/2191 [13:00<17:17,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  43%|████▎     | 940/2191 [13:00<17:17,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  43%|████▎     | 950/2191 [13:07<17:07,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  43%|████▎     | 950/2191 [13:07<17:07,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  44%|████▍     | 960/2191 [13:14<16:57,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  44%|████▍     | 960/2191 [13:14<16:57,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  44%|████▍     | 970/2191 [13:22<16:49,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  44%|████▍     | 970/2191 [13:22<16:49,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  45%|████▍     | 980/2191 [13:29<16:39,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  45%|████▍     | 980/2191 [13:29<16:39,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  45%|████▌     | 990/2191 [13:39<16:33,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  45%|████▌     | 990/2191 [13:39<16:33,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  46%|████▌     | 1000/2191 [13:48<16:25,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  46%|████▌     | 1000/2191 [13:48<16:25,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  46%|████▌     | 1010/2191 [13:56<16:17,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  46%|████▌     | 1010/2191 [13:56<16:17,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  47%|████▋     | 1020/2191 [14:04<16:08,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  47%|████▋     | 1020/2191 [14:04<16:08,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  47%|████▋     | 1030/2191 [14:13<16:01,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  47%|████▋     | 1030/2191 [14:13<16:01,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  47%|████▋     | 1040/2191 [14:21<15:52,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  47%|████▋     | 1040/2191 [14:21<15:52,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  48%|████▊     | 1050/2191 [14:28<15:43,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  48%|████▊     | 1050/2191 [14:28<15:43,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  48%|████▊     | 1060/2191 [14:36<15:34,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  48%|████▊     | 1060/2191 [14:36<15:34,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  49%|████▉     | 1070/2191 [14:44<15:25,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  49%|████▉     | 1070/2191 [14:44<15:25,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  49%|████▉     | 1080/2191 [14:55<15:19,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  49%|████▉     | 1080/2191 [14:55<15:19,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  50%|████▉     | 1090/2191 [15:02<15:10,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  50%|████▉     | 1090/2191 [15:02<15:10,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  50%|█████     | 1100/2191 [15:12<15:03,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  50%|█████     | 1100/2191 [15:12<15:03,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  51%|█████     | 1110/2191 [15:20<14:55,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  51%|█████     | 1110/2191 [15:20<14:55,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  51%|█████     | 1120/2191 [15:28<14:47,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  51%|█████     | 1120/2191 [15:28<14:47,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  52%|█████▏    | 1130/2191 [15:36<14:38,  1.21it/s, loss=2.26, v_num=647]Epoch 29:  52%|█████▏    | 1130/2191 [15:36<14:38,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  52%|█████▏    | 1140/2191 [15:43<14:29,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  52%|█████▏    | 1140/2191 [15:43<14:29,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  52%|█████▏    | 1150/2191 [15:52<14:21,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  52%|█████▏    | 1150/2191 [15:52<14:21,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  53%|█████▎    | 1160/2191 [15:59<14:12,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  53%|█████▎    | 1160/2191 [15:59<14:12,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  53%|█████▎    | 1170/2191 [16:06<14:03,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  53%|█████▎    | 1170/2191 [16:06<14:03,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  54%|█████▍    | 1180/2191 [16:14<13:54,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  54%|█████▍    | 1180/2191 [16:14<13:54,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  54%|█████▍    | 1190/2191 [16:21<13:44,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  54%|█████▍    | 1190/2191 [16:21<13:44,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  55%|█████▍    | 1200/2191 [16:30<13:37,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  55%|█████▍    | 1200/2191 [16:30<13:37,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  55%|█████▌    | 1210/2191 [16:40<13:30,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  55%|█████▌    | 1210/2191 [16:40<13:30,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  56%|█████▌    | 1220/2191 [16:48<13:22,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  56%|█████▌    | 1220/2191 [16:48<13:22,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  56%|█████▌    | 1230/2191 [16:56<13:13,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  56%|█████▌    | 1230/2191 [16:56<13:13,  1.21it/s, loss=2.34, v_num=647]Epoch 29:  57%|█████▋    | 1240/2191 [17:03<13:04,  1.21it/s, loss=2.34, v_num=647]Epoch 29:  57%|█████▋    | 1240/2191 [17:03<13:04,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  57%|█████▋    | 1250/2191 [17:13<12:57,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  57%|█████▋    | 1250/2191 [17:13<12:57,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  58%|█████▊    | 1260/2191 [17:21<12:48,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  58%|█████▊    | 1260/2191 [17:21<12:48,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  58%|█████▊    | 1270/2191 [17:29<12:40,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  58%|█████▊    | 1270/2191 [17:29<12:40,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  58%|█████▊    | 1280/2191 [17:36<12:31,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  58%|█████▊    | 1280/2191 [17:36<12:31,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  59%|█████▉    | 1290/2191 [17:45<12:23,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  59%|█████▉    | 1290/2191 [17:45<12:23,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  59%|█████▉    | 1300/2191 [17:53<12:15,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  59%|█████▉    | 1300/2191 [17:53<12:15,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  60%|█████▉    | 1310/2191 [18:02<12:07,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  60%|█████▉    | 1310/2191 [18:02<12:07,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  60%|██████    | 1320/2191 [18:10<11:59,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  60%|██████    | 1320/2191 [18:10<11:59,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  61%|██████    | 1330/2191 [18:18<11:50,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  61%|██████    | 1330/2191 [18:18<11:50,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  61%|██████    | 1340/2191 [18:26<11:42,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  61%|██████    | 1340/2191 [18:26<11:42,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  62%|██████▏   | 1350/2191 [18:34<11:33,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  62%|██████▏   | 1350/2191 [18:34<11:33,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  62%|██████▏   | 1360/2191 [18:43<11:25,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  62%|██████▏   | 1360/2191 [18:43<11:25,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  63%|██████▎   | 1370/2191 [18:50<11:17,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  63%|██████▎   | 1370/2191 [18:50<11:17,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  63%|██████▎   | 1380/2191 [18:59<11:09,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  63%|██████▎   | 1380/2191 [18:59<11:09,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  63%|██████▎   | 1390/2191 [19:07<11:01,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  63%|██████▎   | 1390/2191 [19:07<11:01,  1.21it/s, loss=2.35, v_num=647]Epoch 29:  64%|██████▍   | 1400/2191 [19:15<10:52,  1.21it/s, loss=2.35, v_num=647]Epoch 29:  64%|██████▍   | 1400/2191 [19:15<10:52,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  64%|██████▍   | 1410/2191 [19:22<10:43,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  64%|██████▍   | 1410/2191 [19:22<10:43,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  65%|██████▍   | 1420/2191 [19:31<10:35,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  65%|██████▍   | 1420/2191 [19:31<10:35,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  65%|██████▌   | 1430/2191 [19:38<10:26,  1.21it/s, loss=2.33, v_num=647]Epoch 29:  65%|██████▌   | 1430/2191 [19:38<10:26,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  66%|██████▌   | 1440/2191 [19:46<10:18,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  66%|██████▌   | 1440/2191 [19:46<10:18,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  66%|██████▌   | 1450/2191 [19:54<10:10,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  66%|██████▌   | 1450/2191 [19:54<10:10,  1.21it/s, loss=2.24, v_num=647]Epoch 29:  67%|██████▋   | 1460/2191 [20:02<10:01,  1.21it/s, loss=2.24, v_num=647]Epoch 29:  67%|██████▋   | 1460/2191 [20:02<10:01,  1.21it/s, loss=2.31, v_num=647]Epoch 29:  67%|██████▋   | 1470/2191 [20:10<09:53,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  67%|██████▋   | 1470/2191 [20:10<09:53,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  68%|██████▊   | 1480/2191 [20:17<09:44,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  68%|██████▊   | 1480/2191 [20:17<09:44,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  68%|██████▊   | 1490/2191 [20:26<09:36,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  68%|██████▊   | 1490/2191 [20:26<09:36,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  68%|██████▊   | 1500/2191 [20:34<09:28,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  68%|██████▊   | 1500/2191 [20:34<09:28,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  69%|██████▉   | 1510/2191 [20:44<09:20,  1.21it/s, loss=2.28, v_num=647]Epoch 29:  69%|██████▉   | 1510/2191 [20:44<09:20,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  69%|██████▉   | 1520/2191 [20:53<09:12,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  69%|██████▉   | 1520/2191 [20:53<09:12,  1.21it/s, loss=2.25, v_num=647]Epoch 29:  70%|██████▉   | 1530/2191 [21:01<09:04,  1.21it/s, loss=2.25, v_num=647]Epoch 29:  70%|██████▉   | 1530/2191 [21:01<09:04,  1.21it/s, loss=2.3, v_num=647] Epoch 29:  70%|███████   | 1540/2191 [21:09<08:56,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  70%|███████   | 1540/2191 [21:09<08:56,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  71%|███████   | 1550/2191 [21:16<08:47,  1.21it/s, loss=2.3, v_num=647]Epoch 29:  71%|███████   | 1550/2191 [21:16<08:47,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  71%|███████   | 1560/2191 [21:25<08:39,  1.21it/s, loss=2.27, v_num=647]Epoch 29:  71%|███████   | 1560/2191 [21:25<08:39,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  72%|███████▏  | 1570/2191 [21:33<08:31,  1.21it/s, loss=2.29, v_num=647]Epoch 29:  72%|███████▏  | 1570/2191 [21:33<08:31,  1.21it/s, loss=2.32, v_num=647]Epoch 29:  72%|███████▏  | 1580/2191 [21:39<08:22,  1.22it/s, loss=2.32, v_num=647]Epoch 29:  72%|███████▏  | 1580/2191 [21:39<08:22,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  73%|███████▎  | 1590/2191 [21:48<08:14,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  73%|███████▎  | 1590/2191 [21:48<08:14,  1.22it/s, loss=2.27, v_num=647]Epoch 29:  73%|███████▎  | 1600/2191 [21:56<08:05,  1.22it/s, loss=2.27, v_num=647]Epoch 29:  73%|███████▎  | 1600/2191 [21:56<08:05,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  73%|███████▎  | 1610/2191 [22:04<07:57,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  73%|███████▎  | 1610/2191 [22:04<07:57,  1.22it/s, loss=2.26, v_num=647]Epoch 29:  74%|███████▍  | 1620/2191 [22:13<07:49,  1.22it/s, loss=2.26, v_num=647]Epoch 29:  74%|███████▍  | 1620/2191 [22:13<07:49,  1.22it/s, loss=2.24, v_num=647]Epoch 29:  74%|███████▍  | 1630/2191 [22:21<07:41,  1.22it/s, loss=2.24, v_num=647]Epoch 29:  74%|███████▍  | 1630/2191 [22:21<07:41,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  75%|███████▍  | 1640/2191 [22:29<07:33,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  75%|███████▍  | 1640/2191 [22:29<07:33,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  75%|███████▌  | 1650/2191 [22:37<07:24,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  75%|███████▌  | 1650/2191 [22:37<07:24,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  76%|███████▌  | 1660/2191 [22:46<07:16,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  76%|███████▌  | 1660/2191 [22:46<07:16,  1.22it/s, loss=2.3, v_num=647] Epoch 29:  76%|███████▌  | 1670/2191 [22:53<07:08,  1.22it/s, loss=2.3, v_num=647]Epoch 29:  76%|███████▌  | 1670/2191 [22:53<07:08,  1.22it/s, loss=2.32, v_num=647]Epoch 29:  77%|███████▋  | 1680/2191 [23:00<06:59,  1.22it/s, loss=2.32, v_num=647]Epoch 29:  77%|███████▋  | 1680/2191 [23:00<06:59,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  77%|███████▋  | 1690/2191 [23:09<06:51,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  77%|███████▋  | 1690/2191 [23:09<06:51,  1.22it/s, loss=2.3, v_num=647] Epoch 29:  78%|███████▊  | 1700/2191 [23:17<06:43,  1.22it/s, loss=2.3, v_num=647]Epoch 29:  78%|███████▊  | 1700/2191 [23:17<06:43,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  78%|███████▊  | 1710/2191 [23:26<06:35,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  78%|███████▊  | 1710/2191 [23:26<06:35,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  79%|███████▊  | 1720/2191 [23:33<06:26,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  79%|███████▊  | 1720/2191 [23:33<06:26,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  79%|███████▉  | 1730/2191 [23:41<06:18,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  79%|███████▉  | 1730/2191 [23:41<06:18,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  79%|███████▉  | 1740/2191 [23:50<06:10,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  79%|███████▉  | 1740/2191 [23:50<06:10,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  80%|███████▉  | 1750/2191 [23:57<06:02,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  80%|███████▉  | 1750/2191 [23:57<06:02,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  80%|████████  | 1760/2191 [24:04<05:53,  1.22it/s, loss=2.28, v_num=647]Epoch 29:  80%|████████  | 1760/2191 [24:04<05:53,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  81%|████████  | 1770/2191 [24:12<05:45,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  81%|████████  | 1770/2191 [24:12<05:45,  1.22it/s, loss=2.3, v_num=647] Epoch 29:  81%|████████  | 1780/2191 [24:20<05:37,  1.22it/s, loss=2.3, v_num=647]Epoch 29:  81%|████████  | 1780/2191 [24:20<05:37,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  82%|████████▏ | 1790/2191 [24:29<05:28,  1.22it/s, loss=2.29, v_num=647]Epoch 29:  82%|████████▏ | 1790/2191 [24:29<05:28,  1.22it/s, loss=2.27, v_num=647]Epoch 29:  82%|████████▏ | 1800/2191 [24:38<05:20,  1.22it/s, loss=2.27, v_num=647]Epoch 29:  82%|████████▏ | 1800/2191 [24:38<05:20,  1.22it/s, loss=2.26, v_num=647]Epoch 29:  83%|████████▎ | 1810/2191 [24:46<05:12,  1.22it/s, loss=2.26, v_num=647]Epoch 29:  83%|████████▎ | 1810/2191 [24:46<05:12,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  83%|████████▎ | 1820/2191 [24:55<05:04,  1.22it/s, loss=2.31, v_num=647]Epoch 29:  83%|████████▎ | 1820/2191 [24:55<05:04,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  84%|████████▎ | 1830/2191 [25:03<04:56,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  84%|████████▎ | 1830/2191 [25:03<04:56,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  84%|████████▍ | 1840/2191 [25:08<04:47,  1.22it/s, loss=2.33, v_num=647]Epoch 29:  84%|████████▍ | 1840/2191 [25:08<04:47,  1.22it/s, loss=2.36, v_num=647]Epoch 29:  84%|████████▍ | 1850/2191 [25:12<04:38,  1.22it/s, loss=2.36, v_num=647]Epoch 29:  84%|████████▍ | 1850/2191 [25:12<04:38,  1.22it/s, loss=2.37, v_num=647]Epoch 29:  85%|████████▍ | 1860/2191 [25:16<04:29,  1.23it/s, loss=2.37, v_num=647]Epoch 29:  85%|████████▍ | 1860/2191 [25:16<04:29,  1.23it/s, loss=2.31, v_num=647]validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.9788314700126648
alidation_epoch_end
graph acc: 0.3514376996805112
valid accuracy: 0.9758614301681519
validation_epoch_end
graph acc: 0.4249201277955272
valid accuracy: 0.9752988815307617
lid accuracy: 0.9784539937973022
validation_epoch_end
graph acc: 0.329073482428115
valid accuracy: 0.977482795715332
validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9780876636505127
Epoch 29:  86%|████████▌ | 1880/2191 [25:20<04:11,  1.24it/s, loss=2.32, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/313 [00:01<00:50,  5.98it/s][AEpoch 29:  86%|████████▋ | 1890/2191 [25:22<04:02,  1.24it/s, loss=2.32, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:47,  6.23it/s][AEpoch 29:  87%|████████▋ | 1900/2191 [25:23<03:53,  1.25it/s, loss=2.32, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:36,  7.81it/s][AEpoch 29:  87%|████████▋ | 1910/2191 [25:24<03:44,  1.25it/s, loss=2.32, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:25, 10.60it/s][AEpoch 29:  88%|████████▊ | 1920/2191 [25:24<03:35,  1.26it/s, loss=2.32, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.39it/s][AEpoch 29:  88%|████████▊ | 1930/2191 [25:25<03:26,  1.27it/s, loss=2.32, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.88it/s][AEpoch 29:  89%|████████▊ | 1940/2191 [25:26<03:17,  1.27it/s, loss=2.32, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:23, 10.48it/s][AEpoch 29:  89%|████████▉ | 1950/2191 [25:27<03:08,  1.28it/s, loss=2.32, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.26it/s][AEpoch 29:  89%|████████▉ | 1960/2191 [25:28<03:00,  1.28it/s, loss=2.32, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.48it/s][AEpoch 29:  90%|████████▉ | 1970/2191 [25:29<02:51,  1.29it/s, loss=2.32, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.50it/s][AEpoch 29:  90%|█████████ | 1980/2191 [25:30<02:43,  1.29it/s, loss=2.32, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.97it/s][AEpoch 29:  91%|█████████ | 1990/2191 [25:31<02:34,  1.30it/s, loss=2.32, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:18, 10.20it/s][AEpoch 29:  91%|█████████▏| 2000/2191 [25:32<02:26,  1.31it/s, loss=2.32, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:18,  9.80it/s][AEpoch 29:  92%|█████████▏| 2010/2191 [25:33<02:18,  1.31it/s, loss=2.32, v_num=647]
Validating:  45%|████▍     | 140/313 [00:14<00:19,  8.99it/s][AEpoch 29:  92%|█████████▏| 2020/2191 [25:35<02:09,  1.32it/s, loss=2.32, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:16,  9.72it/s][AEpoch 29:  93%|█████████▎| 2030/2191 [25:35<02:01,  1.32it/s, loss=2.32, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:14, 10.38it/s][AEpoch 29:  93%|█████████▎| 2040/2191 [25:36<01:53,  1.33it/s, loss=2.32, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.95it/s][AEpoch 29:  94%|█████████▎| 2050/2191 [25:37<01:45,  1.33it/s, loss=2.32, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.44it/s][AEpoch 29:  94%|█████████▍| 2060/2191 [25:38<01:37,  1.34it/s, loss=2.32, v_num=647]
Validating:  61%|██████    | 190/313 [00:18<00:10, 12.13it/s][AEpoch 29:  94%|█████████▍| 2070/2191 [25:39<01:29,  1.35it/s, loss=2.32, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:19<00:09, 12.04it/s][AEpoch 29:  95%|█████████▍| 2080/2191 [25:39<01:22,  1.35it/s, loss=2.32, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:20<00:09, 11.11it/s][AEpoch 29:  95%|█████████▌| 2090/2191 [25:41<01:14,  1.36it/s, loss=2.32, v_num=647]
Validating:  70%|███████   | 220/313 [00:21<00:09, 10.18it/s][AEpoch 29:  96%|█████████▌| 2100/2191 [25:42<01:06,  1.36it/s, loss=2.32, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:22<00:07, 10.50it/s][AEpoch 29:  96%|█████████▋| 2110/2191 [25:43<00:59,  1.37it/s, loss=2.32, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.47it/s][AEpoch 29:  97%|█████████▋| 2120/2191 [25:44<00:51,  1.37it/s, loss=2.32, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:24<00:05, 10.67it/s][AEpoch 29:  97%|█████████▋| 2130/2191 [25:45<00:44,  1.38it/s, loss=2.32, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:25<00:05,  9.80it/s][AEpoch 29:  98%|█████████▊| 2140/2191 [25:46<00:36,  1.38it/s, loss=2.32, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 11.38it/s][AEpoch 29:  98%|█████████▊| 2150/2191 [25:46<00:29,  1.39it/s, loss=2.32, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.21it/s][AEpoch 29:  99%|█████████▊| 2160/2191 [25:47<00:22,  1.40it/s, loss=2.32, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.05it/s][AEpoch 29:  99%|█████████▉| 2170/2191 [25:48<00:14,  1.40it/s, loss=2.32, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:28<00:00, 13.12it/s][AEpoch 29:  99%|█████████▉| 2180/2191 [25:48<00:07,  1.41it/s, loss=2.32, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.62it/s][AEpoch 29: 100%|█████████▉| 2190/2191 [25:49<00:00,  1.41it/s, loss=2.32, v_num=647]validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.9789132475852966
Epoch 29: 100%|██████████| 2191/2191 [25:54<00:00,  1.41it/s, loss=2.29, v_num=647]
                                                             [AEpoch 29:   0%|          | 0/2191 [00:00<00:00, 13573.80it/s, loss=2.29, v_num=647]Epoch 30:   0%|          | 0/2191 [00:00<00:00, 3320.91it/s, loss=2.29, v_num=647] Epoch 30:   0%|          | 10/2191 [00:15<50:03,  1.38s/it, loss=2.29, v_num=647] Epoch 30:   0%|          | 10/2191 [00:15<50:03,  1.38s/it, loss=2.22, v_num=647]Epoch 30:   1%|          | 20/2191 [00:25<44:46,  1.24s/it, loss=2.22, v_num=647]Epoch 30:   1%|          | 20/2191 [00:25<44:47,  1.24s/it, loss=2.21, v_num=647]Epoch 30:   1%|▏         | 30/2191 [00:36<42:55,  1.19s/it, loss=2.21, v_num=647]Epoch 30:   1%|▏         | 30/2191 [00:36<42:55,  1.19s/it, loss=2.22, v_num=647]Epoch 30:   2%|▏         | 40/2191 [00:47<41:09,  1.15s/it, loss=2.22, v_num=647]Epoch 30:   2%|▏         | 40/2191 [00:47<41:09,  1.15s/it, loss=2.23, v_num=647]Epoch 30:   2%|▏         | 50/2191 [00:57<40:32,  1.14s/it, loss=2.23, v_num=647]Epoch 30:   2%|▏         | 50/2191 [00:57<40:32,  1.14s/it, loss=2.29, v_num=647]Epoch 30:   3%|▎         | 60/2191 [01:08<39:44,  1.12s/it, loss=2.29, v_num=647]Epoch 30:   3%|▎         | 60/2191 [01:08<39:44,  1.12s/it, loss=2.3, v_num=647] Epoch 30:   3%|▎         | 70/2191 [01:17<38:48,  1.10s/it, loss=2.3, v_num=647]Epoch 30:   3%|▎         | 70/2191 [01:17<38:48,  1.10s/it, loss=2.3, v_num=647]Epoch 30:   4%|▎         | 80/2191 [01:27<37:51,  1.08s/it, loss=2.3, v_num=647]Epoch 30:   4%|▎         | 80/2191 [01:27<37:51,  1.08s/it, loss=2.26, v_num=647]Epoch 30:   4%|▍         | 90/2191 [01:36<36:58,  1.06s/it, loss=2.26, v_num=647]Epoch 30:   4%|▍         | 90/2191 [01:36<36:58,  1.06s/it, loss=2.25, v_num=647]Epoch 30:   5%|▍         | 100/2191 [01:44<36:02,  1.03s/it, loss=2.25, v_num=647]Epoch 30:   5%|▍         | 100/2191 [01:44<36:02,  1.03s/it, loss=2.3, v_num=647] Epoch 30:   5%|▌         | 110/2191 [01:52<35:05,  1.01s/it, loss=2.3, v_num=647]Epoch 30:   5%|▌         | 110/2191 [01:52<35:05,  1.01s/it, loss=2.28, v_num=647]Epoch 30:   5%|▌         | 120/2191 [02:00<34:18,  1.01it/s, loss=2.28, v_num=647]Epoch 30:   5%|▌         | 120/2191 [02:00<34:18,  1.01it/s, loss=2.26, v_num=647]Epoch 30:   6%|▌         | 130/2191 [02:08<33:37,  1.02it/s, loss=2.26, v_num=647]Epoch 30:   6%|▌         | 130/2191 [02:08<33:37,  1.02it/s, loss=2.22, v_num=647]Epoch 30:   6%|▋         | 140/2191 [02:15<32:55,  1.04it/s, loss=2.22, v_num=647]Epoch 30:   6%|▋         | 140/2191 [02:15<32:55,  1.04it/s, loss=2.21, v_num=647]Epoch 30:   7%|▋         | 150/2191 [02:23<32:13,  1.06it/s, loss=2.21, v_num=647]Epoch 30:   7%|▋         | 150/2191 [02:23<32:13,  1.06it/s, loss=2.29, v_num=647]Epoch 30:   7%|▋         | 160/2191 [02:30<31:37,  1.07it/s, loss=2.29, v_num=647]Epoch 30:   7%|▋         | 160/2191 [02:30<31:37,  1.07it/s, loss=2.3, v_num=647] Epoch 30:   8%|▊         | 170/2191 [02:38<31:07,  1.08it/s, loss=2.3, v_num=647]Epoch 30:   8%|▊         | 170/2191 [02:38<31:07,  1.08it/s, loss=2.26, v_num=647]Epoch 30:   8%|▊         | 180/2191 [02:45<30:35,  1.10it/s, loss=2.26, v_num=647]Epoch 30:   8%|▊         | 180/2191 [02:45<30:35,  1.10it/s, loss=2.25, v_num=647]Epoch 30:   9%|▊         | 190/2191 [02:51<30:01,  1.11it/s, loss=2.25, v_num=647]Epoch 30:   9%|▊         | 190/2191 [02:51<30:01,  1.11it/s, loss=2.26, v_num=647]Epoch 30:   9%|▉         | 200/2191 [02:59<29:35,  1.12it/s, loss=2.26, v_num=647]Epoch 30:   9%|▉         | 200/2191 [02:59<29:35,  1.12it/s, loss=2.27, v_num=647]Epoch 30:  10%|▉         | 210/2191 [03:06<29:06,  1.13it/s, loss=2.27, v_num=647]Epoch 30:  10%|▉         | 210/2191 [03:06<29:06,  1.13it/s, loss=2.28, v_num=647]Epoch 30:  10%|█         | 220/2191 [03:14<28:54,  1.14it/s, loss=2.28, v_num=647]Epoch 30:  10%|█         | 220/2191 [03:14<28:54,  1.14it/s, loss=2.28, v_num=647]Epoch 30:  10%|█         | 230/2191 [03:22<28:37,  1.14it/s, loss=2.28, v_num=647]Epoch 30:  10%|█         | 230/2191 [03:22<28:37,  1.14it/s, loss=2.27, v_num=647]Epoch 30:  11%|█         | 240/2191 [03:29<28:15,  1.15it/s, loss=2.27, v_num=647]Epoch 30:  11%|█         | 240/2191 [03:29<28:15,  1.15it/s, loss=2.25, v_num=647]Epoch 30:  11%|█▏        | 250/2191 [03:37<28:05,  1.15it/s, loss=2.25, v_num=647]Epoch 30:  11%|█▏        | 250/2191 [03:37<28:05,  1.15it/s, loss=2.27, v_num=647]Epoch 30:  12%|█▏        | 260/2191 [03:45<27:49,  1.16it/s, loss=2.27, v_num=647]Epoch 30:  12%|█▏        | 260/2191 [03:45<27:49,  1.16it/s, loss=2.27, v_num=647]Epoch 30:  12%|█▏        | 270/2191 [03:53<27:32,  1.16it/s, loss=2.27, v_num=647]Epoch 30:  12%|█▏        | 270/2191 [03:53<27:32,  1.16it/s, loss=2.23, v_num=647]Epoch 30:  13%|█▎        | 280/2191 [04:00<27:18,  1.17it/s, loss=2.23, v_num=647]Epoch 30:  13%|█▎        | 280/2191 [04:00<27:18,  1.17it/s, loss=2.3, v_num=647] Epoch 30:  13%|█▎        | 290/2191 [04:08<27:01,  1.17it/s, loss=2.3, v_num=647]Epoch 30:  13%|█▎        | 290/2191 [04:08<27:01,  1.17it/s, loss=2.32, v_num=647]Epoch 30:  14%|█▎        | 300/2191 [04:15<26:47,  1.18it/s, loss=2.32, v_num=647]Epoch 30:  14%|█▎        | 300/2191 [04:15<26:47,  1.18it/s, loss=2.29, v_num=647]Epoch 30:  14%|█▍        | 310/2191 [04:25<26:42,  1.17it/s, loss=2.29, v_num=647]Epoch 30:  14%|█▍        | 310/2191 [04:25<26:42,  1.17it/s, loss=2.32, v_num=647]Epoch 30:  15%|█▍        | 320/2191 [04:33<26:31,  1.18it/s, loss=2.32, v_num=647]Epoch 30:  15%|█▍        | 320/2191 [04:33<26:31,  1.18it/s, loss=2.33, v_num=647]Epoch 30:  15%|█▌        | 330/2191 [04:41<26:20,  1.18it/s, loss=2.33, v_num=647]Epoch 30:  15%|█▌        | 330/2191 [04:41<26:20,  1.18it/s, loss=2.3, v_num=647] Epoch 30:  16%|█▌        | 340/2191 [04:49<26:10,  1.18it/s, loss=2.3, v_num=647]Epoch 30:  16%|█▌        | 340/2191 [04:49<26:10,  1.18it/s, loss=2.26, v_num=647]Epoch 30:  16%|█▌        | 350/2191 [04:56<25:55,  1.18it/s, loss=2.26, v_num=647]Epoch 30:  16%|█▌        | 350/2191 [04:56<25:55,  1.18it/s, loss=2.26, v_num=647]Epoch 30:  16%|█▋        | 360/2191 [05:04<25:43,  1.19it/s, loss=2.26, v_num=647]Epoch 30:  16%|█▋        | 360/2191 [05:04<25:43,  1.19it/s, loss=2.28, v_num=647]Epoch 30:  17%|█▋        | 370/2191 [05:12<25:33,  1.19it/s, loss=2.28, v_num=647]Epoch 30:  17%|█▋        | 370/2191 [05:12<25:33,  1.19it/s, loss=2.26, v_num=647]Epoch 30:  17%|█▋        | 380/2191 [05:19<25:19,  1.19it/s, loss=2.26, v_num=647]Epoch 30:  17%|█▋        | 380/2191 [05:19<25:19,  1.19it/s, loss=2.26, v_num=647]Epoch 30:  18%|█▊        | 390/2191 [05:26<25:05,  1.20it/s, loss=2.26, v_num=647]Epoch 30:  18%|█▊        | 390/2191 [05:26<25:05,  1.20it/s, loss=2.29, v_num=647]Epoch 30:  18%|█▊        | 400/2191 [05:34<24:51,  1.20it/s, loss=2.29, v_num=647]Epoch 30:  18%|█▊        | 400/2191 [05:34<24:51,  1.20it/s, loss=2.27, v_num=647]Epoch 30:  19%|█▊        | 410/2191 [05:42<24:42,  1.20it/s, loss=2.27, v_num=647]Epoch 30:  19%|█▊        | 410/2191 [05:42<24:42,  1.20it/s, loss=2.27, v_num=647]Epoch 30:  19%|█▉        | 420/2191 [05:49<24:31,  1.20it/s, loss=2.27, v_num=647]Epoch 30:  19%|█▉        | 420/2191 [05:49<24:31,  1.20it/s, loss=2.28, v_num=647]Epoch 30:  20%|█▉        | 430/2191 [05:57<24:22,  1.20it/s, loss=2.28, v_num=647]Epoch 30:  20%|█▉        | 430/2191 [05:57<24:22,  1.20it/s, loss=2.26, v_num=647]Epoch 30:  20%|██        | 440/2191 [06:06<24:15,  1.20it/s, loss=2.26, v_num=647]Epoch 30:  20%|██        | 440/2191 [06:06<24:15,  1.20it/s, loss=2.28, v_num=647]Epoch 30:  21%|██        | 450/2191 [06:13<24:02,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  21%|██        | 450/2191 [06:13<24:02,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  21%|██        | 460/2191 [06:22<23:54,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  21%|██        | 460/2191 [06:22<23:54,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  21%|██▏       | 470/2191 [06:29<23:45,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  21%|██▏       | 470/2191 [06:29<23:45,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  22%|██▏       | 480/2191 [06:38<23:36,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  22%|██▏       | 480/2191 [06:38<23:36,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  22%|██▏       | 490/2191 [06:46<23:28,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  22%|██▏       | 490/2191 [06:46<23:28,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  23%|██▎       | 500/2191 [06:54<23:18,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  23%|██▎       | 500/2191 [06:54<23:18,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  23%|██▎       | 510/2191 [07:01<23:05,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  23%|██▎       | 510/2191 [07:01<23:05,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  24%|██▎       | 520/2191 [07:08<22:55,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  24%|██▎       | 520/2191 [07:08<22:55,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  24%|██▍       | 530/2191 [07:17<22:49,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  24%|██▍       | 530/2191 [07:17<22:49,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  25%|██▍       | 540/2191 [07:25<22:39,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  25%|██▍       | 540/2191 [07:25<22:39,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  25%|██▌       | 550/2191 [07:32<22:28,  1.22it/s, loss=2.25, v_num=647]Epoch 30:  25%|██▌       | 550/2191 [07:32<22:28,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  26%|██▌       | 560/2191 [07:40<22:18,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  26%|██▌       | 560/2191 [07:40<22:18,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  26%|██▌       | 570/2191 [07:48<22:10,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  26%|██▌       | 570/2191 [07:48<22:10,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  26%|██▋       | 580/2191 [07:57<22:03,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  26%|██▋       | 580/2191 [07:57<22:03,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  27%|██▋       | 590/2191 [08:05<21:55,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  27%|██▋       | 590/2191 [08:05<21:55,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  27%|██▋       | 600/2191 [08:13<21:45,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  27%|██▋       | 600/2191 [08:13<21:45,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  28%|██▊       | 610/2191 [08:21<21:38,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  28%|██▊       | 610/2191 [08:21<21:38,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  28%|██▊       | 620/2191 [08:30<21:30,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  28%|██▊       | 620/2191 [08:30<21:30,  1.22it/s, loss=2.23, v_num=647]Epoch 30:  29%|██▉       | 630/2191 [08:38<21:22,  1.22it/s, loss=2.23, v_num=647]Epoch 30:  29%|██▉       | 630/2191 [08:38<21:22,  1.22it/s, loss=2.25, v_num=647]Epoch 30:  29%|██▉       | 640/2191 [08:47<21:16,  1.22it/s, loss=2.25, v_num=647]Epoch 30:  29%|██▉       | 640/2191 [08:47<21:16,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  30%|██▉       | 650/2191 [08:56<21:10,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  30%|██▉       | 650/2191 [08:56<21:10,  1.21it/s, loss=2.23, v_num=647]Epoch 30:  30%|███       | 660/2191 [09:04<21:01,  1.21it/s, loss=2.23, v_num=647]Epoch 30:  30%|███       | 660/2191 [09:04<21:01,  1.21it/s, loss=2.22, v_num=647]Epoch 30:  31%|███       | 670/2191 [09:14<20:56,  1.21it/s, loss=2.22, v_num=647]Epoch 30:  31%|███       | 670/2191 [09:14<20:56,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  31%|███▏      | 690/2191 [09:31<20:41,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  31%|███▏      | 690/2191 [09:31<20:41,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  32%|███▏      | 700/2191 [09:39<20:32,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  32%|███▏      | 700/2191 [09:39<20:32,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  32%|███▏      | 710/2191 [09:47<20:23,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  32%|███▏      | 710/2191 [09:47<20:23,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  33%|███▎      | 720/2191 [09:56<20:17,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  33%|███▎      | 720/2191 [09:56<20:17,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  33%|███▎      | 730/2191 [10:04<20:07,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  33%|███▎      | 730/2191 [10:04<20:07,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  34%|███▍      | 740/2191 [10:12<19:58,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  34%|███▍      | 740/2191 [10:12<19:58,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  34%|███▍      | 750/2191 [10:20<19:51,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  34%|███▍      | 750/2191 [10:20<19:51,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  35%|███▍      | 760/2191 [10:28<19:41,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  35%|███▍      | 760/2191 [10:28<19:41,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  35%|███▌      | 770/2191 [10:36<19:33,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  35%|███▌      | 770/2191 [10:36<19:33,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  36%|███▌      | 780/2191 [10:44<19:24,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  36%|███▌      | 780/2191 [10:44<19:25,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  36%|███▌      | 790/2191 [10:51<19:14,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  36%|███▌      | 790/2191 [10:51<19:14,  1.21it/s, loss=2.25, v_num=647]Epoch 30:  37%|███▋      | 800/2191 [10:59<19:04,  1.22it/s, loss=2.25, v_num=647]Epoch 30:  37%|███▋      | 800/2191 [10:59<19:04,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  37%|███▋      | 810/2191 [11:06<18:55,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  37%|███▋      | 810/2191 [11:06<18:55,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  37%|███▋      | 820/2191 [11:16<18:49,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  37%|███▋      | 820/2191 [11:16<18:49,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  38%|███▊      | 830/2191 [11:24<18:41,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  38%|███▊      | 830/2191 [11:24<18:41,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  38%|███▊      | 840/2191 [11:31<18:30,  1.22it/s, loss=2.24, v_num=647]Epoch 30:  38%|███▊      | 840/2191 [11:31<18:30,  1.22it/s, loss=2.23, v_num=647]Epoch 30:  39%|███▉      | 850/2191 [11:38<18:21,  1.22it/s, loss=2.23, v_num=647]Epoch 30:  39%|███▉      | 850/2191 [11:38<18:21,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  39%|███▉      | 860/2191 [11:46<18:11,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  39%|███▉      | 860/2191 [11:46<18:11,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  40%|███▉      | 870/2191 [11:52<18:01,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  40%|███▉      | 870/2191 [11:52<18:01,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  40%|████      | 880/2191 [11:59<17:50,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  40%|████      | 880/2191 [11:59<17:50,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  41%|████      | 890/2191 [12:08<17:43,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  41%|████      | 890/2191 [12:08<17:43,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  41%|████      | 900/2191 [12:16<17:35,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  41%|████      | 900/2191 [12:16<17:35,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  42%|████▏     | 910/2191 [12:23<17:25,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  42%|████▏     | 910/2191 [12:23<17:25,  1.22it/s, loss=2.3, v_num=647] Epoch 30:  42%|████▏     | 920/2191 [12:30<17:16,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  42%|████▏     | 920/2191 [12:30<17:16,  1.23it/s, loss=2.35, v_num=647]Epoch 30:  42%|████▏     | 930/2191 [12:38<17:07,  1.23it/s, loss=2.35, v_num=647]Epoch 30:  42%|████▏     | 930/2191 [12:38<17:07,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  43%|████▎     | 940/2191 [12:46<16:59,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  43%|████▎     | 940/2191 [12:46<16:59,  1.23it/s, loss=2.23, v_num=647]Epoch 30:  43%|████▎     | 950/2191 [12:54<16:50,  1.23it/s, loss=2.23, v_num=647]Epoch 30:  43%|████▎     | 950/2191 [12:54<16:50,  1.23it/s, loss=2.24, v_num=647]Epoch 30:  44%|████▍     | 960/2191 [13:02<16:41,  1.23it/s, loss=2.24, v_num=647]Epoch 30:  44%|████▍     | 960/2191 [13:02<16:41,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  44%|████▍     | 970/2191 [13:11<16:34,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  44%|████▍     | 970/2191 [13:11<16:34,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  45%|████▍     | 980/2191 [13:18<16:25,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  45%|████▍     | 980/2191 [13:18<16:25,  1.23it/s, loss=2.33, v_num=647]Epoch 30:  45%|████▌     | 990/2191 [13:25<16:16,  1.23it/s, loss=2.33, v_num=647]Epoch 30:  45%|████▌     | 990/2191 [13:25<16:16,  1.23it/s, loss=2.35, v_num=647]Epoch 30:  46%|████▌     | 1000/2191 [13:32<16:06,  1.23it/s, loss=2.35, v_num=647]Epoch 30:  46%|████▌     | 1000/2191 [13:32<16:06,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  46%|████▌     | 1010/2191 [13:40<15:58,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  46%|████▌     | 1010/2191 [13:40<15:58,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  47%|████▋     | 1020/2191 [13:48<15:49,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  47%|████▋     | 1020/2191 [13:48<15:49,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  47%|████▋     | 1030/2191 [13:55<15:41,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  47%|████▋     | 1030/2191 [13:55<15:41,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  47%|████▋     | 1040/2191 [14:03<15:32,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  47%|████▋     | 1040/2191 [14:03<15:32,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  48%|████▊     | 1050/2191 [14:11<15:23,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  48%|████▊     | 1050/2191 [14:11<15:23,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  48%|████▊     | 1060/2191 [14:19<15:15,  1.24it/s, loss=2.3, v_num=647]Epoch 30:  48%|████▊     | 1060/2191 [14:19<15:15,  1.24it/s, loss=2.31, v_num=647]Epoch 30:  49%|████▉     | 1070/2191 [14:27<15:08,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  49%|████▉     | 1070/2191 [14:27<15:08,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  49%|████▉     | 1080/2191 [14:36<15:00,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  49%|████▉     | 1080/2191 [14:36<15:00,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  50%|████▉     | 1090/2191 [14:45<14:53,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  50%|████▉     | 1090/2191 [14:45<14:53,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  50%|█████     | 1100/2191 [14:53<14:45,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  50%|█████     | 1100/2191 [14:53<14:45,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  51%|█████     | 1110/2191 [15:00<14:35,  1.23it/s, loss=2.31, v_num=647]Epoch 30:  51%|█████     | 1110/2191 [15:00<14:35,  1.23it/s, loss=2.33, v_num=647]Epoch 30:  51%|█████     | 1120/2191 [15:08<14:27,  1.23it/s, loss=2.33, v_num=647]Epoch 30:  51%|█████     | 1120/2191 [15:08<14:27,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  52%|█████▏    | 1130/2191 [15:16<14:19,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  52%|█████▏    | 1130/2191 [15:16<14:19,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  52%|█████▏    | 1140/2191 [15:24<14:11,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  52%|█████▏    | 1140/2191 [15:24<14:11,  1.23it/s, loss=2.24, v_num=647]Epoch 30:  52%|█████▏    | 1150/2191 [15:32<14:02,  1.23it/s, loss=2.24, v_num=647]Epoch 30:  52%|█████▏    | 1150/2191 [15:32<14:02,  1.23it/s, loss=2.24, v_num=647]Epoch 30:  53%|█████▎    | 1160/2191 [15:38<13:53,  1.24it/s, loss=2.24, v_num=647]Epoch 30:  53%|█████▎    | 1160/2191 [15:38<13:53,  1.24it/s, loss=2.22, v_num=647]Epoch 30:  53%|█████▎    | 1170/2191 [15:47<13:45,  1.24it/s, loss=2.22, v_num=647]Epoch 30:  53%|█████▎    | 1170/2191 [15:47<13:45,  1.24it/s, loss=2.23, v_num=647]Epoch 30:  54%|█████▍    | 1180/2191 [15:54<13:37,  1.24it/s, loss=2.23, v_num=647]Epoch 30:  54%|█████▍    | 1180/2191 [15:54<13:37,  1.24it/s, loss=2.25, v_num=647]Epoch 30:  54%|█████▍    | 1190/2191 [16:02<13:29,  1.24it/s, loss=2.25, v_num=647]Epoch 30:  54%|█████▍    | 1190/2191 [16:02<13:29,  1.24it/s, loss=2.27, v_num=647]Epoch 30:  54%|█████▍    | 1190/2191 [16:15<13:39,  1.22it/s, loss=2.27, v_num=647]Epoch 30:  55%|█████▍    | 1200/2191 [16:16<13:25,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  55%|█████▍    | 1200/2191 [16:16<13:25,  1.23it/s, loss=2.32, v_num=647]Epoch 30:  55%|█████▌    | 1210/2191 [16:24<13:17,  1.23it/s, loss=2.32, v_num=647]Epoch 30:  55%|█████▌    | 1210/2191 [16:24<13:17,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  56%|█████▌    | 1220/2191 [16:34<13:10,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  56%|█████▌    | 1220/2191 [16:34<13:10,  1.23it/s, loss=2.22, v_num=647]Epoch 30:  56%|█████▌    | 1230/2191 [16:41<13:02,  1.23it/s, loss=2.22, v_num=647]Epoch 30:  56%|█████▌    | 1230/2191 [16:41<13:02,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  57%|█████▋    | 1240/2191 [16:49<12:53,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  57%|█████▋    | 1240/2191 [16:49<12:53,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  57%|█████▋    | 1250/2191 [16:58<12:46,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  57%|█████▋    | 1250/2191 [16:58<12:46,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  58%|█████▊    | 1260/2191 [17:07<12:38,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  58%|█████▊    | 1260/2191 [17:07<12:38,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  58%|█████▊    | 1270/2191 [17:15<12:30,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  58%|█████▊    | 1270/2191 [17:15<12:30,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  58%|█████▊    | 1280/2191 [17:23<12:22,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  58%|█████▊    | 1280/2191 [17:23<12:22,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  59%|█████▉    | 1290/2191 [17:31<12:13,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  59%|█████▉    | 1290/2191 [17:31<12:13,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  59%|█████▉    | 1300/2191 [17:38<12:05,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  59%|█████▉    | 1300/2191 [17:38<12:05,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  60%|█████▉    | 1310/2191 [17:45<11:55,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  60%|█████▉    | 1310/2191 [17:45<11:55,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  60%|██████    | 1320/2191 [17:53<11:47,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  60%|██████    | 1320/2191 [17:53<11:47,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  61%|██████    | 1330/2191 [18:00<11:39,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  61%|██████    | 1330/2191 [18:00<11:39,  1.23it/s, loss=2.32, v_num=647]Epoch 30:  61%|██████    | 1340/2191 [18:09<11:31,  1.23it/s, loss=2.32, v_num=647]Epoch 30:  61%|██████    | 1340/2191 [18:09<11:31,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  62%|██████▏   | 1350/2191 [18:19<11:24,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  62%|██████▏   | 1350/2191 [18:19<11:24,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  62%|██████▏   | 1360/2191 [18:26<11:15,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  62%|██████▏   | 1360/2191 [18:26<11:15,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  63%|██████▎   | 1370/2191 [18:34<11:07,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  63%|██████▎   | 1370/2191 [18:34<11:07,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  63%|██████▎   | 1380/2191 [18:42<10:59,  1.23it/s, loss=2.27, v_num=647]Epoch 30:  63%|██████▎   | 1380/2191 [18:42<10:59,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  63%|██████▎   | 1390/2191 [18:53<10:52,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  63%|██████▎   | 1390/2191 [18:53<10:52,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  64%|██████▍   | 1400/2191 [19:01<10:44,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  64%|██████▍   | 1400/2191 [19:01<10:44,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  64%|██████▍   | 1410/2191 [19:10<10:37,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  64%|██████▍   | 1410/2191 [19:10<10:37,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  65%|██████▍   | 1420/2191 [19:18<10:28,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  65%|██████▍   | 1420/2191 [19:18<10:28,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  65%|██████▌   | 1430/2191 [19:26<10:20,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  65%|██████▌   | 1430/2191 [19:26<10:20,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  66%|██████▌   | 1440/2191 [19:34<10:12,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  66%|██████▌   | 1440/2191 [19:34<10:12,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  66%|██████▌   | 1450/2191 [19:43<10:04,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  66%|██████▌   | 1450/2191 [19:43<10:04,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  67%|██████▋   | 1460/2191 [19:52<09:56,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  67%|██████▋   | 1460/2191 [19:52<09:56,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  67%|██████▋   | 1470/2191 [19:59<09:48,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  67%|██████▋   | 1470/2191 [19:59<09:48,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  68%|██████▊   | 1480/2191 [20:07<09:39,  1.23it/s, loss=2.28, v_num=647]Epoch 30:  68%|██████▊   | 1480/2191 [20:07<09:39,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  68%|██████▊   | 1490/2191 [20:16<09:31,  1.23it/s, loss=2.25, v_num=647]Epoch 30:  68%|██████▊   | 1490/2191 [20:16<09:31,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  68%|██████▊   | 1500/2191 [20:23<09:23,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  68%|██████▊   | 1500/2191 [20:23<09:23,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  69%|██████▉   | 1510/2191 [20:31<09:15,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  69%|██████▉   | 1510/2191 [20:31<09:15,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  69%|██████▉   | 1520/2191 [20:39<09:07,  1.23it/s, loss=2.29, v_num=647]Epoch 30:  69%|██████▉   | 1520/2191 [20:39<09:07,  1.23it/s, loss=2.3, v_num=647] Epoch 30:  70%|██████▉   | 1530/2191 [20:48<08:59,  1.23it/s, loss=2.3, v_num=647]Epoch 30:  70%|██████▉   | 1530/2191 [20:48<08:59,  1.23it/s, loss=2.32, v_num=647]Epoch 30:  70%|██████▉   | 1530/2191 [21:05<09:06,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  70%|███████   | 1540/2191 [21:07<08:55,  1.22it/s, loss=2.32, v_num=647]Epoch 30:  70%|███████   | 1540/2191 [21:07<08:55,  1.22it/s, loss=2.3, v_num=647] Epoch 30:  71%|███████   | 1550/2191 [21:16<08:47,  1.22it/s, loss=2.3, v_num=647]Epoch 30:  71%|███████   | 1550/2191 [21:16<08:47,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  71%|███████   | 1560/2191 [21:25<08:39,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  71%|███████   | 1560/2191 [21:25<08:39,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  72%|███████▏  | 1570/2191 [21:34<08:31,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  72%|███████▏  | 1570/2191 [21:34<08:31,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  72%|███████▏  | 1580/2191 [21:42<08:23,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  72%|███████▏  | 1580/2191 [21:42<08:23,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  73%|███████▎  | 1590/2191 [21:50<08:14,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  73%|███████▎  | 1590/2191 [21:50<08:14,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  73%|███████▎  | 1600/2191 [21:57<08:06,  1.22it/s, loss=2.3, v_num=647]Epoch 30:  73%|███████▎  | 1600/2191 [21:57<08:06,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  73%|███████▎  | 1610/2191 [22:08<07:59,  1.21it/s, loss=2.31, v_num=647]Epoch 30:  73%|███████▎  | 1610/2191 [22:08<07:59,  1.21it/s, loss=2.31, v_num=647]Epoch 30:  74%|███████▍  | 1620/2191 [22:16<07:50,  1.21it/s, loss=2.31, v_num=647]Epoch 30:  74%|███████▍  | 1620/2191 [22:16<07:50,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  74%|███████▍  | 1630/2191 [22:24<07:42,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  74%|███████▍  | 1630/2191 [22:24<07:42,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  75%|███████▍  | 1640/2191 [22:32<07:34,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  75%|███████▍  | 1640/2191 [22:32<07:34,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  75%|███████▌  | 1650/2191 [22:40<07:25,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  75%|███████▌  | 1650/2191 [22:40<07:25,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  76%|███████▌  | 1660/2191 [22:47<07:17,  1.21it/s, loss=2.27, v_num=647]Epoch 30:  76%|███████▌  | 1660/2191 [22:47<07:17,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  76%|███████▌  | 1670/2191 [22:55<07:08,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  76%|███████▌  | 1670/2191 [22:55<07:08,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  77%|███████▋  | 1680/2191 [23:04<07:00,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  77%|███████▋  | 1680/2191 [23:04<07:00,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  77%|███████▋  | 1690/2191 [23:13<06:52,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  77%|███████▋  | 1690/2191 [23:13<06:52,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  78%|███████▊  | 1700/2191 [23:21<06:44,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  78%|███████▊  | 1700/2191 [23:21<06:44,  1.21it/s, loss=2.31, v_num=647]Epoch 30:  78%|███████▊  | 1710/2191 [23:29<06:36,  1.21it/s, loss=2.31, v_num=647]Epoch 30:  78%|███████▊  | 1710/2191 [23:29<06:36,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  79%|███████▊  | 1720/2191 [23:38<06:28,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  79%|███████▊  | 1720/2191 [23:38<06:28,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  79%|███████▉  | 1730/2191 [23:47<06:20,  1.21it/s, loss=2.26, v_num=647]Epoch 30:  79%|███████▉  | 1730/2191 [23:47<06:20,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  79%|███████▉  | 1740/2191 [23:56<06:11,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  79%|███████▉  | 1740/2191 [23:56<06:11,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  80%|███████▉  | 1750/2191 [24:03<06:03,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  80%|███████▉  | 1750/2191 [24:03<06:03,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  80%|████████  | 1760/2191 [24:11<05:55,  1.21it/s, loss=2.32, v_num=647]Epoch 30:  80%|████████  | 1760/2191 [24:11<05:55,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  81%|████████  | 1770/2191 [24:19<05:46,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  81%|████████  | 1770/2191 [24:19<05:46,  1.21it/s, loss=2.24, v_num=647]Epoch 30:  81%|████████  | 1780/2191 [24:25<05:38,  1.22it/s, loss=2.24, v_num=647]Epoch 30:  81%|████████  | 1780/2191 [24:25<05:38,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  82%|████████▏ | 1790/2191 [24:33<05:29,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  82%|████████▏ | 1790/2191 [24:33<05:29,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  82%|████████▏ | 1800/2191 [24:41<05:21,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  82%|████████▏ | 1800/2191 [24:41<05:21,  1.22it/s, loss=2.29, v_num=647]Epoch 30:  83%|████████▎ | 1810/2191 [24:50<05:13,  1.21it/s, loss=2.29, v_num=647]Epoch 30:  83%|████████▎ | 1810/2191 [24:50<05:13,  1.21it/s, loss=2.3, v_num=647] Epoch 30:  83%|████████▎ | 1820/2191 [24:59<05:05,  1.21it/s, loss=2.3, v_num=647]Epoch 30:  83%|████████▎ | 1820/2191 [24:59<05:05,  1.21it/s, loss=2.29, v_num=647]validation_epoch_end
graph acc: 0.3801916932907348
valid accuracy: 0.977937638759613
=2.29, v_num=647]Epoch 30:  84%|████████▎ | 1830/2191 [25:08<04:57,  1.21it/s, loss=2.28, v_num=647]Epoch 30:  84%|████████▍ | 1840/2191 [25:14<04:48,  1.22it/s, loss=2.28, v_num=647]Epoch 30:  84%|████████▍ | 1840/2191 [25:14<04:48,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  84%|████████▍ | 1850/2191 [25:19<04:40,  1.22it/s, loss=2.31, v_num=647]Epoch 30:  84%|████████▍ | 1850/2191 [25:19<04:40,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  85%|████████▍ | 1860/2191 [25:23<04:30,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  85%|████████▍ | 1860/2191 [25:23<04:30,  1.22it/s, loss=2.26, v_num=647]Epoch 30:  85%|████████▌ | 1870/2191 [25:25<04:21,  1.23it/s, loss=2.26, v_num=647]Epoch 30:  85%|████████▌ | 1870/2191 [25:25<04:21,  1.23it/s, loss=2.29, v_num=647]validation_epoch_end
graph acc: 0.41533546325878595
valid accuracy: 0.9786686897277832
validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9768466949462891
Epoch 30:  86%|████████▌ | 1880/2191 [25:27<04:12,  1.23it/s, loss=2.29, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3993610223642173
valid accuracy: 0.9793586134910583

Validating:   3%|▎         | 10/313 [00:01<00:48,  6.21it/s][AEpoch 30:  86%|████████▋ | 1890/2191 [25:28<04:03,  1.24it/s, loss=2.29, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:51,  5.66it/s][AEpoch 30:  87%|████████▋ | 1900/2191 [25:30<03:54,  1.24it/s, loss=2.29, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:42,  6.73it/s][AEpoch 30:  87%|████████▋ | 1910/2191 [25:32<03:45,  1.25it/s, loss=2.29, v_num=647]
Validating:  13%|█▎        | 40/313 [00:05<00:30,  8.95it/s][AEpoch 30:  88%|████████▊ | 1920/2191 [25:32<03:36,  1.25it/s, loss=2.29, v_num=647]
Validating:  16%|█▌        | 50/313 [00:06<00:27,  9.73it/s][AEpoch 30:  88%|████████▊ | 1930/2191 [25:33<03:27,  1.26it/s, loss=2.29, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.48it/s][AEpoch 30:  89%|████████▊ | 1940/2191 [25:34<03:18,  1.27it/s, loss=2.29, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:23, 10.47it/s][AEpoch 30:  89%|████████▉ | 1950/2191 [25:35<03:09,  1.27it/s, loss=2.29, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.22it/s][AEpoch 30:  89%|████████▉ | 1960/2191 [25:36<03:00,  1.28it/s, loss=2.29, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:22,  9.95it/s][AEpoch 30:  90%|████████▉ | 1970/2191 [25:37<02:52,  1.28it/s, loss=2.29, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.61it/s][AEpoch 30:  90%|█████████ | 1980/2191 [25:38<02:43,  1.29it/s, loss=2.29, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:19, 10.41it/s][AEpoch 30:  91%|█████████ | 1990/2191 [25:39<02:35,  1.29it/s, loss=2.29, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19,  9.79it/s][AEpoch 30:  91%|█████████▏| 2000/2191 [25:40<02:27,  1.30it/s, loss=2.29, v_num=647]
Validating:  42%|████▏     | 130/313 [00:14<00:20,  9.13it/s][AEpoch 30:  92%|█████████▏| 2010/2191 [25:41<02:18,  1.30it/s, loss=2.29, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:19,  8.90it/s][AEpoch 30:  92%|█████████▏| 2020/2191 [25:42<02:10,  1.31it/s, loss=2.29, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.11it/s][AEpoch 30:  93%|█████████▎| 2030/2191 [25:43<02:02,  1.32it/s, loss=2.29, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:15, 10.00it/s][AEpoch 30:  93%|█████████▎| 2040/2191 [25:44<01:54,  1.32it/s, loss=2.29, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:18<00:13, 10.45it/s][AEpoch 30:  94%|█████████▎| 2050/2191 [25:45<01:46,  1.33it/s, loss=2.29, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.88it/s][AEpoch 30:  94%|█████████▍| 2060/2191 [25:46<01:38,  1.33it/s, loss=2.29, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 12.03it/s][AEpoch 30:  94%|█████████▍| 2070/2191 [25:46<01:30,  1.34it/s, loss=2.29, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 12.16it/s][AEpoch 30:  95%|█████████▍| 2080/2191 [25:47<01:22,  1.34it/s, loss=2.29, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:10, 10.25it/s][AEpoch 30:  95%|█████████▌| 2090/2191 [25:48<01:14,  1.35it/s, loss=2.29, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.43it/s][AEpoch 30:  96%|█████████▌| 2100/2191 [25:50<01:07,  1.36it/s, loss=2.29, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.48it/s][AEpoch 30:  96%|█████████▋| 2110/2191 [25:50<00:59,  1.36it/s, loss=2.29, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.35it/s][AEpoch 30:  97%|█████████▋| 2120/2191 [25:51<00:51,  1.37it/s, loss=2.29, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06,  9.88it/s][AEpoch 30:  97%|█████████▋| 2130/2191 [25:53<00:44,  1.37it/s, loss=2.29, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05, 10.12it/s][AEpoch 30:  98%|█████████▊| 2140/2191 [25:53<00:37,  1.38it/s, loss=2.29, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:04, 10.69it/s][AEpoch 30:  98%|█████████▊| 2150/2191 [25:54<00:29,  1.38it/s, loss=2.29, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:28<00:02, 11.85it/s][AEpoch 30:  99%|█████████▊| 2160/2191 [25:55<00:22,  1.39it/s, loss=2.29, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 11.76it/s][AEpoch 30:  99%|█████████▉| 2170/2191 [25:56<00:15,  1.39it/s, loss=2.29, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.33it/s][AEpoch 30:  99%|█████████▉| 2180/2191 [25:56<00:07,  1.40it/s, loss=2.29, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 13.86it/s][AEpoch 30: 100%|█████████▉| 2190/2191 [25:57<00:00,  1.41it/s, loss=2.29, v_num=647]validation_epoch_end
graph acc: 0.4504792332268371
valid accuracy: 0.9788711071014404
Epoch 30: 100%|██████████| 2191/2191 [25:59<00:00,  1.41it/s, loss=2.29, v_num=647]
                                                             [AEpoch 30:   0%|          | 0/2191 [00:00<00:00, 12052.60it/s, loss=2.29, v_num=647]Epoch 31:   0%|          | 0/2191 [00:00<00:00, 2957.90it/s, loss=2.29, v_num=647] Epoch 31:   0%|          | 10/2191 [00:12<39:53,  1.10s/it, loss=2.29, v_num=647] Epoch 31:   0%|          | 10/2191 [00:12<39:53,  1.10s/it, loss=2.3, v_num=647] Epoch 31:   1%|          | 20/2191 [00:21<37:13,  1.03s/it, loss=2.3, v_num=647]Epoch 31:   1%|          | 20/2191 [00:21<37:13,  1.03s/it, loss=2.27, v_num=647]Epoch 31:   1%|▏         | 30/2191 [00:29<34:14,  1.05it/s, loss=2.27, v_num=647]Epoch 31:   1%|▏         | 30/2191 [00:29<34:14,  1.05it/s, loss=2.22, v_num=647]Epoch 31:   2%|▏         | 40/2191 [00:38<33:33,  1.07it/s, loss=2.22, v_num=647]Epoch 31:   2%|▏         | 40/2191 [00:38<33:33,  1.07it/s, loss=2.23, v_num=647]Epoch 31:   2%|▏         | 50/2191 [00:47<33:34,  1.06it/s, loss=2.23, v_num=647]Epoch 31:   2%|▏         | 50/2191 [00:47<33:34,  1.06it/s, loss=2.22, v_num=647]Epoch 31:   3%|▎         | 60/2191 [00:59<34:43,  1.02it/s, loss=2.22, v_num=647]Epoch 31:   3%|▎         | 60/2191 [00:59<34:43,  1.02it/s, loss=2.21, v_num=647]Epoch 31:   3%|▎         | 70/2191 [01:07<33:42,  1.05it/s, loss=2.21, v_num=647]Epoch 31:   3%|▎         | 70/2191 [01:07<33:42,  1.05it/s, loss=2.24, v_num=647]Epoch 31:   4%|▎         | 80/2191 [01:16<33:10,  1.06it/s, loss=2.24, v_num=647]Epoch 31:   4%|▎         | 80/2191 [01:16<33:10,  1.06it/s, loss=2.29, v_num=647]Epoch 31:   4%|▍         | 90/2191 [01:24<32:35,  1.07it/s, loss=2.29, v_num=647]Epoch 31:   4%|▍         | 90/2191 [01:24<32:36,  1.07it/s, loss=2.27, v_num=647]Epoch 31:   5%|▍         | 100/2191 [01:32<31:47,  1.10it/s, loss=2.27, v_num=647]Epoch 31:   5%|▍         | 100/2191 [01:32<31:47,  1.10it/s, loss=2.22, v_num=647]Epoch 31:   5%|▌         | 110/2191 [01:39<31:08,  1.11it/s, loss=2.22, v_num=647]Epoch 31:   5%|▌         | 110/2191 [01:39<31:08,  1.11it/s, loss=2.23, v_num=647]Epoch 31:   5%|▌         | 120/2191 [01:47<30:44,  1.12it/s, loss=2.23, v_num=647]Epoch 31:   5%|▌         | 120/2191 [01:47<30:44,  1.12it/s, loss=2.26, v_num=647]Epoch 31:   6%|▌         | 130/2191 [01:56<30:28,  1.13it/s, loss=2.26, v_num=647]Epoch 31:   6%|▌         | 130/2191 [01:56<30:28,  1.13it/s, loss=2.27, v_num=647]Epoch 31:   6%|▋         | 140/2191 [02:02<29:48,  1.15it/s, loss=2.27, v_num=647]Epoch 31:   6%|▋         | 140/2191 [02:02<29:48,  1.15it/s, loss=2.26, v_num=647]Epoch 31:   7%|▋         | 150/2191 [02:12<29:50,  1.14it/s, loss=2.26, v_num=647]Epoch 31:   7%|▋         | 150/2191 [02:12<29:50,  1.14it/s, loss=2.28, v_num=647]Epoch 31:   7%|▋         | 160/2191 [02:19<29:23,  1.15it/s, loss=2.28, v_num=647]Epoch 31:   7%|▋         | 160/2191 [02:19<29:23,  1.15it/s, loss=2.32, v_num=647]Epoch 31:   8%|▊         | 170/2191 [02:27<29:04,  1.16it/s, loss=2.32, v_num=647]Epoch 31:   8%|▊         | 170/2191 [02:27<29:04,  1.16it/s, loss=2.27, v_num=647]Epoch 31:   8%|▊         | 180/2191 [02:34<28:38,  1.17it/s, loss=2.27, v_num=647]Epoch 31:   8%|▊         | 180/2191 [02:34<28:38,  1.17it/s, loss=2.24, v_num=647]Epoch 31:   9%|▊         | 190/2191 [02:42<28:22,  1.18it/s, loss=2.24, v_num=647]Epoch 31:   9%|▊         | 190/2191 [02:42<28:22,  1.18it/s, loss=2.23, v_num=647]Epoch 31:   9%|▉         | 200/2191 [02:51<28:15,  1.17it/s, loss=2.23, v_num=647]Epoch 31:   9%|▉         | 200/2191 [02:51<28:15,  1.17it/s, loss=2.2, v_num=647] Epoch 31:  10%|▉         | 210/2191 [03:00<28:15,  1.17it/s, loss=2.2, v_num=647]Epoch 31:  10%|▉         | 210/2191 [03:00<28:15,  1.17it/s, loss=2.24, v_num=647]Epoch 31:  10%|█         | 220/2191 [03:08<27:57,  1.17it/s, loss=2.24, v_num=647]Epoch 31:  10%|█         | 220/2191 [03:08<27:57,  1.17it/s, loss=2.26, v_num=647]Epoch 31:  10%|█         | 230/2191 [03:15<27:38,  1.18it/s, loss=2.26, v_num=647]Epoch 31:  10%|█         | 230/2191 [03:15<27:38,  1.18it/s, loss=2.27, v_num=647]Epoch 31:  11%|█         | 240/2191 [03:23<27:24,  1.19it/s, loss=2.27, v_num=647]Epoch 31:  11%|█         | 240/2191 [03:23<27:24,  1.19it/s, loss=2.23, v_num=647]Epoch 31:  11%|█▏        | 250/2191 [03:30<27:07,  1.19it/s, loss=2.23, v_num=647]Epoch 31:  11%|█▏        | 250/2191 [03:30<27:07,  1.19it/s, loss=2.19, v_num=647]Epoch 31:  12%|█▏        | 260/2191 [03:39<27:03,  1.19it/s, loss=2.19, v_num=647]Epoch 31:  12%|█▏        | 260/2191 [03:39<27:03,  1.19it/s, loss=2.24, v_num=647]Epoch 31:  12%|█▏        | 270/2191 [03:48<27:02,  1.18it/s, loss=2.24, v_num=647]Epoch 31:  12%|█▏        | 270/2191 [03:48<27:02,  1.18it/s, loss=2.28, v_num=647]Epoch 31:  13%|█▎        | 280/2191 [03:58<26:59,  1.18it/s, loss=2.28, v_num=647]Epoch 31:  13%|█▎        | 280/2191 [03:58<26:59,  1.18it/s, loss=2.25, v_num=647]Epoch 31:  13%|█▎        | 290/2191 [04:05<26:44,  1.19it/s, loss=2.25, v_num=647]Epoch 31:  13%|█▎        | 290/2191 [04:05<26:44,  1.19it/s, loss=2.21, v_num=647]Epoch 31:  14%|█▎        | 300/2191 [04:12<26:25,  1.19it/s, loss=2.21, v_num=647]Epoch 31:  14%|█▎        | 300/2191 [04:12<26:25,  1.19it/s, loss=2.26, v_num=647]Epoch 31:  14%|█▍        | 310/2191 [04:20<26:13,  1.20it/s, loss=2.26, v_num=647]Epoch 31:  14%|█▍        | 310/2191 [04:20<26:13,  1.20it/s, loss=2.29, v_num=647]Epoch 31:  15%|█▍        | 320/2191 [04:27<25:59,  1.20it/s, loss=2.29, v_num=647]Epoch 31:  15%|█▍        | 320/2191 [04:27<25:59,  1.20it/s, loss=2.26, v_num=647]Epoch 31:  15%|█▌        | 330/2191 [04:34<25:44,  1.20it/s, loss=2.26, v_num=647]Epoch 31:  15%|█▌        | 330/2191 [04:34<25:44,  1.20it/s, loss=2.27, v_num=647]Epoch 31:  16%|█▌        | 340/2191 [04:41<25:26,  1.21it/s, loss=2.27, v_num=647]Epoch 31:  16%|█▌        | 340/2191 [04:41<25:26,  1.21it/s, loss=2.28, v_num=647]Epoch 31:  16%|█▌        | 350/2191 [04:47<25:08,  1.22it/s, loss=2.28, v_num=647]Epoch 31:  16%|█▌        | 350/2191 [04:47<25:08,  1.22it/s, loss=2.27, v_num=647]Epoch 31:  16%|█▋        | 360/2191 [04:54<24:55,  1.22it/s, loss=2.27, v_num=647]Epoch 31:  16%|█▋        | 360/2191 [04:54<24:55,  1.22it/s, loss=2.28, v_num=647]Epoch 31:  17%|█▋        | 370/2191 [05:03<24:47,  1.22it/s, loss=2.28, v_num=647]Epoch 31:  17%|█▋        | 370/2191 [05:03<24:47,  1.22it/s, loss=2.31, v_num=647]Epoch 31:  17%|█▋        | 380/2191 [05:11<24:41,  1.22it/s, loss=2.31, v_num=647]Epoch 31:  17%|█▋        | 380/2191 [05:11<24:41,  1.22it/s, loss=2.32, v_num=647]Epoch 31:  18%|█▊        | 390/2191 [05:18<24:27,  1.23it/s, loss=2.32, v_num=647]Epoch 31:  18%|█▊        | 390/2191 [05:18<24:27,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  18%|█▊        | 400/2191 [05:25<24:15,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  18%|█▊        | 400/2191 [05:25<24:15,  1.23it/s, loss=2.24, v_num=647]Epoch 31:  19%|█▊        | 410/2191 [05:35<24:14,  1.22it/s, loss=2.24, v_num=647]Epoch 31:  19%|█▊        | 410/2191 [05:35<24:14,  1.22it/s, loss=2.27, v_num=647]Epoch 31:  19%|█▉        | 420/2191 [05:43<24:06,  1.22it/s, loss=2.27, v_num=647]Epoch 31:  19%|█▉        | 420/2191 [05:43<24:06,  1.22it/s, loss=2.28, v_num=647]Epoch 31:  20%|█▉        | 430/2191 [05:52<24:00,  1.22it/s, loss=2.28, v_num=647]Epoch 31:  20%|█▉        | 430/2191 [05:52<24:00,  1.22it/s, loss=2.29, v_num=647]Epoch 31:  20%|██        | 440/2191 [06:02<23:57,  1.22it/s, loss=2.29, v_num=647]Epoch 31:  20%|██        | 440/2191 [06:02<23:57,  1.22it/s, loss=2.26, v_num=647]Epoch 31:  21%|██        | 450/2191 [06:12<23:56,  1.21it/s, loss=2.26, v_num=647]Epoch 31:  21%|██        | 450/2191 [06:12<23:56,  1.21it/s, loss=2.24, v_num=647]Epoch 31:  21%|██        | 460/2191 [06:19<23:45,  1.21it/s, loss=2.24, v_num=647]Epoch 31:  21%|██        | 460/2191 [06:19<23:45,  1.21it/s, loss=2.22, v_num=647]Epoch 31:  21%|██▏       | 470/2191 [06:27<23:34,  1.22it/s, loss=2.22, v_num=647]Epoch 31:  21%|██▏       | 470/2191 [06:27<23:34,  1.22it/s, loss=2.21, v_num=647]Epoch 31:  22%|██▏       | 480/2191 [06:34<23:22,  1.22it/s, loss=2.21, v_num=647]Epoch 31:  22%|██▏       | 480/2191 [06:34<23:22,  1.22it/s, loss=2.24, v_num=647]Epoch 31:  22%|██▏       | 490/2191 [06:41<23:11,  1.22it/s, loss=2.24, v_num=647]Epoch 31:  22%|██▏       | 490/2191 [06:41<23:11,  1.22it/s, loss=2.23, v_num=647]Epoch 31:  23%|██▎       | 500/2191 [06:49<23:00,  1.22it/s, loss=2.23, v_num=647]Epoch 31:  23%|██▎       | 500/2191 [06:49<23:00,  1.22it/s, loss=2.22, v_num=647]Epoch 31:  23%|██▎       | 510/2191 [06:55<22:48,  1.23it/s, loss=2.22, v_num=647]Epoch 31:  23%|██▎       | 510/2191 [06:55<22:48,  1.23it/s, loss=2.22, v_num=647]Epoch 31:  24%|██▎       | 520/2191 [07:04<22:39,  1.23it/s, loss=2.22, v_num=647]Epoch 31:  24%|██▎       | 520/2191 [07:04<22:39,  1.23it/s, loss=2.21, v_num=647]Epoch 31:  24%|██▍       | 530/2191 [07:12<22:32,  1.23it/s, loss=2.21, v_num=647]Epoch 31:  24%|██▍       | 530/2191 [07:12<22:32,  1.23it/s, loss=2.24, v_num=647]Epoch 31:  25%|██▍       | 540/2191 [07:20<22:23,  1.23it/s, loss=2.24, v_num=647]Epoch 31:  25%|██▍       | 540/2191 [07:20<22:23,  1.23it/s, loss=2.24, v_num=647]Epoch 31:  25%|██▌       | 550/2191 [07:27<22:13,  1.23it/s, loss=2.24, v_num=647]Epoch 31:  25%|██▌       | 550/2191 [07:27<22:13,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  26%|██▌       | 560/2191 [07:35<22:03,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  26%|██▌       | 560/2191 [07:35<22:03,  1.23it/s, loss=2.28, v_num=647]Epoch 31:  26%|██▌       | 570/2191 [07:43<21:56,  1.23it/s, loss=2.28, v_num=647]Epoch 31:  26%|██▌       | 570/2191 [07:43<21:56,  1.23it/s, loss=2.25, v_num=647]Epoch 31:  26%|██▋       | 580/2191 [07:53<21:52,  1.23it/s, loss=2.25, v_num=647]Epoch 31:  26%|██▋       | 580/2191 [07:53<21:52,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  27%|██▋       | 590/2191 [08:01<21:45,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  27%|██▋       | 590/2191 [08:01<21:45,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  27%|██▋       | 600/2191 [08:10<21:39,  1.22it/s, loss=2.27, v_num=647]Epoch 31:  27%|██▋       | 600/2191 [08:10<21:39,  1.22it/s, loss=2.23, v_num=647]Epoch 31:  28%|██▊       | 610/2191 [08:17<21:27,  1.23it/s, loss=2.23, v_num=647]Epoch 31:  28%|██▊       | 610/2191 [08:17<21:27,  1.23it/s, loss=2.22, v_num=647]Epoch 31:  28%|██▊       | 620/2191 [08:25<21:19,  1.23it/s, loss=2.22, v_num=647]Epoch 31:  28%|██▊       | 620/2191 [08:25<21:19,  1.23it/s, loss=2.3, v_num=647] Epoch 31:  29%|██▉       | 630/2191 [08:32<21:08,  1.23it/s, loss=2.3, v_num=647]Epoch 31:  29%|██▉       | 630/2191 [08:32<21:08,  1.23it/s, loss=2.32, v_num=647]Epoch 31:  29%|██▉       | 640/2191 [08:41<21:01,  1.23it/s, loss=2.32, v_num=647]Epoch 31:  29%|██▉       | 640/2191 [08:41<21:01,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  30%|██▉       | 650/2191 [08:49<20:53,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  30%|██▉       | 650/2191 [08:49<20:53,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  30%|███       | 660/2191 [08:58<20:47,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  30%|███       | 660/2191 [08:58<20:47,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  31%|███       | 670/2191 [09:07<20:40,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  31%|███       | 670/2191 [09:07<20:40,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  31%|███       | 680/2191 [09:14<20:29,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  31%|███       | 680/2191 [09:14<20:29,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  31%|███▏      | 690/2191 [09:23<20:23,  1.23it/s, loss=2.27, v_num=647]Epoch 31:  31%|███▏      | 690/2191 [09:23<20:23,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  32%|███▏      | 700/2191 [09:32<20:16,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  32%|███▏      | 700/2191 [09:32<20:16,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  32%|███▏      | 710/2191 [09:40<20:08,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  32%|███▏      | 710/2191 [09:40<20:08,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  33%|███▎      | 720/2191 [09:47<19:59,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  33%|███▎      | 720/2191 [09:47<19:59,  1.23it/s, loss=2.25, v_num=647]Epoch 31:  33%|███▎      | 730/2191 [09:55<19:49,  1.23it/s, loss=2.25, v_num=647]Epoch 31:  33%|███▎      | 730/2191 [09:55<19:49,  1.23it/s, loss=2.29, v_num=647]Epoch 31:  34%|███▍      | 740/2191 [10:02<19:39,  1.23it/s, loss=2.29, v_num=647]Epoch 31:  34%|███▍      | 740/2191 [10:02<19:39,  1.23it/s, loss=2.3, v_num=647] Epoch 31:  34%|███▍      | 750/2191 [10:09<19:30,  1.23it/s, loss=2.3, v_num=647]Epoch 31:  34%|███▍      | 750/2191 [10:09<19:30,  1.23it/s, loss=2.31, v_num=647]Epoch 31:  35%|███▍      | 760/2191 [10:16<19:19,  1.23it/s, loss=2.31, v_num=647]Epoch 31:  35%|███▍      | 760/2191 [10:16<19:19,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  35%|███▌      | 770/2191 [10:24<19:10,  1.23it/s, loss=2.26, v_num=647]Epoch 31:  35%|███▌      | 770/2191 [10:24<19:10,  1.23it/s, loss=2.21, v_num=647]Epoch 31:  36%|███▌      | 780/2191 [10:32<19:02,  1.24it/s, loss=2.21, v_num=647]Epoch 31:  36%|███▌      | 780/2191 [10:32<19:02,  1.24it/s, loss=2.21, v_num=647]Epoch 31:  36%|███▌      | 790/2191 [10:40<18:54,  1.23it/s, loss=2.21, v_num=647]Epoch 31:  36%|███▌      | 790/2191 [10:40<18:54,  1.23it/s, loss=2.2, v_num=647] Epoch 31:  37%|███▋      | 800/2191 [10:47<18:45,  1.24it/s, loss=2.2, v_num=647]Epoch 31:  37%|███▋      | 800/2191 [10:47<18:45,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  37%|███▋      | 810/2191 [10:55<18:35,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  37%|███▋      | 810/2191 [10:55<18:35,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  37%|███▋      | 820/2191 [11:01<18:25,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  37%|███▋      | 820/2191 [11:01<18:25,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  38%|███▊      | 830/2191 [11:09<18:16,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  38%|███▊      | 830/2191 [11:09<18:16,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  38%|███▊      | 840/2191 [11:16<18:07,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  38%|███▊      | 840/2191 [11:16<18:07,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  39%|███▉      | 850/2191 [11:24<17:58,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  39%|███▉      | 850/2191 [11:24<17:58,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  39%|███▉      | 860/2191 [11:33<17:51,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  39%|███▉      | 860/2191 [11:33<17:51,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  40%|███▉      | 870/2191 [11:40<17:43,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  40%|███▉      | 870/2191 [11:40<17:43,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  40%|████      | 880/2191 [11:49<17:35,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  40%|████      | 880/2191 [11:49<17:35,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  41%|████      | 890/2191 [11:56<17:26,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  41%|████      | 890/2191 [11:56<17:26,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  41%|████      | 900/2191 [12:05<17:19,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  41%|████      | 900/2191 [12:05<17:19,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  42%|████▏     | 910/2191 [12:12<17:10,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  42%|████▏     | 910/2191 [12:12<17:10,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  42%|████▏     | 920/2191 [12:20<17:02,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  42%|████▏     | 920/2191 [12:20<17:02,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  42%|████▏     | 930/2191 [12:29<16:54,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  42%|████▏     | 930/2191 [12:29<16:54,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  43%|████▎     | 940/2191 [12:37<16:46,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  43%|████▎     | 940/2191 [12:37<16:46,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  43%|████▎     | 950/2191 [12:44<16:38,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  43%|████▎     | 950/2191 [12:44<16:38,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  44%|████▍     | 960/2191 [12:52<16:29,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  44%|████▍     | 960/2191 [12:52<16:29,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  44%|████▍     | 970/2191 [13:01<16:22,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  44%|████▍     | 970/2191 [13:01<16:22,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  45%|████▍     | 980/2191 [13:11<16:16,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  45%|████▍     | 980/2191 [13:11<16:16,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  45%|████▌     | 990/2191 [13:19<16:09,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  45%|████▌     | 990/2191 [13:19<16:09,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  46%|████▌     | 1000/2191 [13:28<16:01,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  46%|████▌     | 1000/2191 [13:28<16:01,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  46%|████▌     | 1010/2191 [13:35<15:53,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  46%|████▌     | 1010/2191 [13:35<15:53,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  47%|████▋     | 1020/2191 [13:43<15:44,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  47%|████▋     | 1020/2191 [13:43<15:44,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  47%|████▋     | 1030/2191 [13:51<15:36,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  47%|████▋     | 1030/2191 [13:51<15:36,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  47%|████▋     | 1040/2191 [14:00<15:29,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  47%|████▋     | 1040/2191 [14:00<15:29,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  48%|████▊     | 1050/2191 [14:08<15:20,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  48%|████▊     | 1050/2191 [14:08<15:20,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  48%|████▊     | 1060/2191 [14:15<15:12,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  48%|████▊     | 1060/2191 [14:15<15:12,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  49%|████▉     | 1070/2191 [14:23<15:03,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  49%|████▉     | 1070/2191 [14:23<15:03,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  49%|████▉     | 1080/2191 [14:33<14:57,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  49%|████▉     | 1080/2191 [14:33<14:57,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  50%|████▉     | 1090/2191 [14:40<14:49,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  50%|████▉     | 1090/2191 [14:40<14:49,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  50%|█████     | 1100/2191 [14:48<14:40,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  50%|█████     | 1100/2191 [14:48<14:40,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  51%|█████     | 1110/2191 [14:55<14:31,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  51%|█████     | 1110/2191 [14:55<14:31,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  51%|█████     | 1120/2191 [15:02<14:22,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  51%|█████     | 1120/2191 [15:02<14:22,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  52%|█████▏    | 1130/2191 [15:11<14:14,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  52%|█████▏    | 1130/2191 [15:11<14:14,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  52%|█████▏    | 1140/2191 [15:18<14:06,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  52%|█████▏    | 1140/2191 [15:18<14:06,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  52%|█████▏    | 1150/2191 [15:27<13:58,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  52%|█████▏    | 1150/2191 [15:27<13:58,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  53%|█████▎    | 1160/2191 [15:34<13:50,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  53%|█████▎    | 1160/2191 [15:34<13:50,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  53%|█████▎    | 1170/2191 [15:43<13:42,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  53%|█████▎    | 1170/2191 [15:43<13:42,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  54%|█████▍    | 1180/2191 [15:51<13:34,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  54%|█████▍    | 1180/2191 [15:51<13:34,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  54%|█████▍    | 1190/2191 [15:59<13:26,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  54%|█████▍    | 1190/2191 [15:59<13:26,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  55%|█████▍    | 1200/2191 [16:07<13:18,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  55%|█████▍    | 1200/2191 [16:07<13:18,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  55%|█████▌    | 1210/2191 [16:17<13:11,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  55%|█████▌    | 1210/2191 [16:17<13:11,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  56%|█████▌    | 1220/2191 [16:23<13:02,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  56%|█████▌    | 1220/2191 [16:23<13:02,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  56%|█████▌    | 1230/2191 [16:30<12:53,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  56%|█████▌    | 1230/2191 [16:30<12:53,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  57%|█████▋    | 1240/2191 [16:37<12:44,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  57%|█████▋    | 1240/2191 [16:37<12:44,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  57%|█████▋    | 1250/2191 [16:45<12:36,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  57%|█████▋    | 1250/2191 [16:45<12:36,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  58%|█████▊    | 1260/2191 [16:53<12:28,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  58%|█████▊    | 1260/2191 [16:53<12:28,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  58%|█████▊    | 1270/2191 [17:03<12:21,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  58%|█████▊    | 1270/2191 [17:03<12:21,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  58%|█████▊    | 1280/2191 [17:12<12:14,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  58%|█████▊    | 1280/2191 [17:12<12:14,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  59%|█████▉    | 1290/2191 [17:20<12:05,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  59%|█████▉    | 1290/2191 [17:20<12:05,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  59%|█████▉    | 1300/2191 [17:28<11:58,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  59%|█████▉    | 1300/2191 [17:28<11:58,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  60%|█████▉    | 1310/2191 [17:36<11:49,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  60%|█████▉    | 1310/2191 [17:36<11:49,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  60%|██████    | 1320/2191 [17:47<11:43,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  60%|██████    | 1320/2191 [17:47<11:43,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  61%|██████    | 1330/2191 [17:55<11:35,  1.24it/s, loss=2.24, v_num=647]Epoch 31:  61%|██████    | 1330/2191 [17:55<11:35,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  61%|██████    | 1340/2191 [18:04<11:28,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  61%|██████    | 1340/2191 [18:04<11:28,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  62%|██████▏   | 1350/2191 [18:11<11:19,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  62%|██████▏   | 1350/2191 [18:11<11:19,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  62%|██████▏   | 1360/2191 [18:19<11:11,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  62%|██████▏   | 1360/2191 [18:19<11:11,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  63%|██████▎   | 1370/2191 [18:28<11:03,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  63%|██████▎   | 1370/2191 [18:28<11:03,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  63%|██████▎   | 1380/2191 [18:36<10:55,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  63%|██████▎   | 1380/2191 [18:36<10:55,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  63%|██████▎   | 1390/2191 [18:44<10:47,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  63%|██████▎   | 1390/2191 [18:44<10:47,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  64%|██████▍   | 1400/2191 [18:52<10:39,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  64%|██████▍   | 1400/2191 [18:52<10:39,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  64%|██████▍   | 1410/2191 [18:59<10:30,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  64%|██████▍   | 1410/2191 [18:59<10:30,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  65%|██████▍   | 1420/2191 [19:06<10:21,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  65%|██████▍   | 1420/2191 [19:06<10:21,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  65%|██████▌   | 1430/2191 [19:14<10:14,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  65%|██████▌   | 1430/2191 [19:14<10:14,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  66%|██████▌   | 1440/2191 [19:24<10:06,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  66%|██████▌   | 1440/2191 [19:24<10:06,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  66%|██████▌   | 1450/2191 [19:31<09:58,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  66%|██████▌   | 1450/2191 [19:31<09:58,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  67%|██████▋   | 1460/2191 [19:38<09:49,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  67%|██████▋   | 1460/2191 [19:38<09:49,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  67%|██████▋   | 1470/2191 [19:46<09:41,  1.24it/s, loss=2.23, v_num=647]Epoch 31:  67%|██████▋   | 1470/2191 [19:46<09:41,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  68%|██████▊   | 1480/2191 [19:53<09:33,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  68%|██████▊   | 1480/2191 [19:53<09:33,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  68%|██████▊   | 1490/2191 [20:02<09:25,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  68%|██████▊   | 1490/2191 [20:02<09:25,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  68%|██████▊   | 1500/2191 [20:10<09:17,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  68%|██████▊   | 1500/2191 [20:10<09:17,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  69%|██████▉   | 1510/2191 [20:18<09:09,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  69%|██████▉   | 1510/2191 [20:18<09:09,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  69%|██████▉   | 1520/2191 [20:26<09:00,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  69%|██████▉   | 1520/2191 [20:26<09:00,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  70%|██████▉   | 1530/2191 [20:33<08:52,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  70%|██████▉   | 1530/2191 [20:33<08:52,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  70%|███████   | 1540/2191 [20:40<08:44,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  70%|███████   | 1540/2191 [20:40<08:44,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  71%|███████   | 1550/2191 [20:48<08:35,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  71%|███████   | 1550/2191 [20:48<08:35,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  71%|███████   | 1560/2191 [20:57<08:28,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  71%|███████   | 1560/2191 [20:57<08:28,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  72%|███████▏  | 1570/2191 [21:04<08:19,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  72%|███████▏  | 1570/2191 [21:04<08:19,  1.24it/s, loss=2.33, v_num=647]Epoch 31:  72%|███████▏  | 1580/2191 [21:11<08:11,  1.24it/s, loss=2.33, v_num=647]Epoch 31:  72%|███████▏  | 1580/2191 [21:11<08:11,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  73%|███████▎  | 1590/2191 [21:19<08:03,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  73%|███████▎  | 1590/2191 [21:19<08:03,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  73%|███████▎  | 1600/2191 [21:27<07:55,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  73%|███████▎  | 1600/2191 [21:27<07:55,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  73%|███████▎  | 1610/2191 [21:34<07:46,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  73%|███████▎  | 1610/2191 [21:34<07:46,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  74%|███████▍  | 1620/2191 [21:42<07:38,  1.25it/s, loss=2.29, v_num=647]Epoch 31:  74%|███████▍  | 1620/2191 [21:42<07:38,  1.25it/s, loss=2.28, v_num=647]Epoch 31:  74%|███████▍  | 1630/2191 [21:50<07:30,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  74%|███████▍  | 1630/2191 [21:50<07:30,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  75%|███████▍  | 1640/2191 [21:59<07:23,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  75%|███████▍  | 1640/2191 [21:59<07:23,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  75%|███████▌  | 1650/2191 [22:07<07:14,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  75%|███████▌  | 1650/2191 [22:07<07:14,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  76%|███████▌  | 1660/2191 [22:15<07:06,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  76%|███████▌  | 1660/2191 [22:15<07:06,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  76%|███████▌  | 1670/2191 [22:22<06:58,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  76%|███████▌  | 1670/2191 [22:22<06:58,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  77%|███████▋  | 1680/2191 [22:32<06:51,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  77%|███████▋  | 1680/2191 [22:32<06:51,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  77%|███████▋  | 1690/2191 [22:41<06:43,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  77%|███████▋  | 1690/2191 [22:41<06:43,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  78%|███████▊  | 1700/2191 [22:48<06:35,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  78%|███████▊  | 1700/2191 [22:48<06:35,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  78%|███████▊  | 1710/2191 [22:55<06:26,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  78%|███████▊  | 1710/2191 [22:55<06:26,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  79%|███████▊  | 1720/2191 [23:04<06:18,  1.24it/s, loss=2.28, v_num=647]Epoch 31:  79%|███████▊  | 1720/2191 [23:04<06:18,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  79%|███████▉  | 1730/2191 [23:11<06:10,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  79%|███████▉  | 1730/2191 [23:11<06:10,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  79%|███████▉  | 1740/2191 [23:20<06:02,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  79%|███████▉  | 1740/2191 [23:20<06:02,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  80%|███████▉  | 1750/2191 [23:28<05:54,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  80%|███████▉  | 1750/2191 [23:28<05:54,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  80%|████████  | 1760/2191 [23:36<05:46,  1.24it/s, loss=2.3, v_num=647]Epoch 31:  80%|████████  | 1760/2191 [23:36<05:46,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  81%|████████  | 1770/2191 [23:45<05:38,  1.24it/s, loss=2.29, v_num=647]Epoch 31:  81%|████████  | 1770/2191 [23:45<05:38,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  81%|████████  | 1780/2191 [23:56<05:31,  1.24it/s, loss=2.32, v_num=647]Epoch 31:  81%|████████  | 1780/2191 [23:56<05:31,  1.24it/s, loss=2.34, v_num=647]Epoch 31:  82%|████████▏ | 1790/2191 [24:06<05:23,  1.24it/s, loss=2.34, v_num=647]Epoch 31:  82%|████████▏ | 1790/2191 [24:06<05:23,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  82%|████████▏ | 1800/2191 [24:13<05:15,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  82%|████████▏ | 1800/2191 [24:13<05:15,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  83%|████████▎ | 1810/2191 [24:21<05:07,  1.24it/s, loss=2.31, v_num=647]Epoch 31:  83%|████████▎ | 1810/2191 [24:21<05:07,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  83%|████████▎ | 1820/2191 [24:29<04:59,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  83%|████████▎ | 1820/2191 [24:29<04:59,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  84%|████████▎ | 1830/2191 [24:37<04:51,  1.24it/s, loss=2.26, v_num=647]Epoch 31:  84%|████████▎ | 1830/2191 [24:37<04:51,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  84%|████████▍ | 1840/2191 [24:44<04:43,  1.24it/s, loss=2.27, v_num=647]Epoch 31:  84%|████████▍ | 1840/2191 [24:44<04:43,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  84%|████████▍ | 1850/2191 [24:49<04:34,  1.24it/s, loss=2.25, v_num=647]Epoch 31:  84%|████████▍ | 1850/2191 [24:49<04:34,  1.24it/s, loss=2.3, v_num=647] Epoch 31:  85%|████████▍ | 1860/2191 [24:53<04:25,  1.25it/s, loss=2.3, v_num=647]Epoch 31:  85%|████████▍ | 1860/2191 [24:53<04:25,  1.25it/s, loss=2.3, v_num=647]validation_epoch_end
graph acc: 0.4057507987220447
valid accuracy: 0.9785470962524414
validation_epoch_end
graph acc: 0.38338658146964855
valid accuracy: 0.9751237034797668
validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.9775886535644531
accuracy: 0.9757441878318787
validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9787516593933105
validation_epoch_end
graph acc: 0.3706070287539936
valid accuracy: 0.9784128665924072
Epoch 31:  86%|████████▌ | 1880/2191 [24:57<04:07,  1.26it/s, loss=2.24, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/313 [00:01<00:51,  5.84it/s][AEpoch 31:  86%|████████▋ | 1890/2191 [24:58<03:58,  1.26it/s, loss=2.24, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:52,  5.61it/s][AEpoch 31:  87%|████████▋ | 1900/2191 [25:00<03:49,  1.27it/s, loss=2.24, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:36,  7.81it/s][AEpoch 31:  87%|████████▋ | 1910/2191 [25:01<03:40,  1.27it/s, loss=2.24, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:28,  9.42it/s][AEpoch 31:  88%|████████▊ | 1920/2191 [25:02<03:31,  1.28it/s, loss=2.24, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:24, 10.72it/s][AEpoch 31:  88%|████████▊ | 1930/2191 [25:02<03:23,  1.28it/s, loss=2.24, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.23it/s][AEpoch 31:  89%|████████▊ | 1940/2191 [25:03<03:14,  1.29it/s, loss=2.24, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:23, 10.23it/s][AEpoch 31:  89%|████████▉ | 1950/2191 [25:04<03:05,  1.30it/s, loss=2.24, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:23, 10.02it/s][AEpoch 31:  89%|████████▉ | 1960/2191 [25:05<02:57,  1.30it/s, loss=2.24, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:19, 11.19it/s][AEpoch 31:  90%|████████▉ | 1970/2191 [25:06<02:48,  1.31it/s, loss=2.24, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:20, 10.32it/s][AEpoch 31:  90%|█████████ | 1980/2191 [25:07<02:40,  1.31it/s, loss=2.24, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:21,  9.41it/s][AEpoch 31:  91%|█████████ | 1990/2191 [25:09<02:32,  1.32it/s, loss=2.24, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:18, 10.19it/s][AEpoch 31:  91%|█████████▏| 2000/2191 [25:09<02:24,  1.33it/s, loss=2.24, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:18,  9.86it/s][AEpoch 31:  92%|█████████▏| 2010/2191 [25:10<02:15,  1.33it/s, loss=2.24, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:20,  8.61it/s][AEpoch 31:  92%|█████████▏| 2020/2191 [25:12<02:07,  1.34it/s, loss=2.24, v_num=647]
Validating:  48%|████▊     | 150/313 [00:16<00:17,  9.06it/s][AEpoch 31:  93%|█████████▎| 2030/2191 [25:13<01:59,  1.34it/s, loss=2.24, v_num=647]
Validating:  51%|█████     | 160/313 [00:17<00:16,  9.44it/s][AEpoch 31:  93%|█████████▎| 2040/2191 [25:14<01:52,  1.35it/s, loss=2.24, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.91it/s][AEpoch 31:  94%|█████████▎| 2050/2191 [25:14<01:44,  1.35it/s, loss=2.24, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:13, 10.01it/s][AEpoch 31:  94%|█████████▍| 2060/2191 [25:16<01:36,  1.36it/s, loss=2.24, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:11, 11.07it/s][AEpoch 31:  94%|█████████▍| 2070/2191 [25:16<01:28,  1.37it/s, loss=2.24, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.48it/s][AEpoch 31:  95%|█████████▍| 2080/2191 [25:17<01:20,  1.37it/s, loss=2.24, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:09, 10.97it/s][AEpoch 31:  95%|█████████▌| 2090/2191 [25:18<01:13,  1.38it/s, loss=2.24, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:10,  9.20it/s][AEpoch 31:  96%|█████████▌| 2100/2191 [25:20<01:05,  1.38it/s, loss=2.24, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:08, 10.05it/s][AEpoch 31:  96%|█████████▋| 2110/2191 [25:20<00:58,  1.39it/s, loss=2.24, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.25it/s][AEpoch 31:  97%|█████████▋| 2120/2191 [25:21<00:50,  1.39it/s, loss=2.24, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.11it/s][AEpoch 31:  97%|█████████▋| 2130/2191 [25:22<00:43,  1.40it/s, loss=2.24, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05,  9.72it/s][AEpoch 31:  98%|█████████▊| 2140/2191 [25:23<00:36,  1.40it/s, loss=2.24, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:27<00:03, 11.06it/s][AEpoch 31:  98%|█████████▊| 2150/2191 [25:24<00:29,  1.41it/s, loss=2.24, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 13.17it/s][AEpoch 31:  99%|█████████▊| 2160/2191 [25:24<00:21,  1.42it/s, loss=2.24, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 11.58it/s][AEpoch 31:  99%|█████████▉| 2170/2191 [25:26<00:14,  1.42it/s, loss=2.24, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:01, 12.93it/s][AEpoch 31:  99%|█████████▉| 2180/2191 [25:26<00:07,  1.43it/s, loss=2.24, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:30<00:00, 12.95it/s][AEpoch 31: 100%|█████████▉| 2190/2191 [25:27<00:00,  1.43it/s, loss=2.24, v_num=647]validation_epoch_end
graph acc: 0.4185303514376997
valid accuracy: 0.9793341159820557
Epoch 31: 100%|██████████| 2191/2191 [25:29<00:00,  1.43it/s, loss=2.23, v_num=647]
                                                             [AEpoch 31:   0%|          | 0/2191 [00:00<00:00, 17476.27it/s, loss=2.23, v_num=647]Epoch 32:   0%|          | 0/2191 [00:00<00:00, 4190.11it/s, loss=2.23, v_num=647] Epoch 32:   0%|          | 10/2191 [00:12<41:04,  1.13s/it, loss=2.23, v_num=647] Epoch 32:   0%|          | 10/2191 [00:12<41:04,  1.13s/it, loss=2.28, v_num=647]Epoch 32:   1%|          | 20/2191 [00:22<38:00,  1.05s/it, loss=2.28, v_num=647]Epoch 32:   1%|          | 20/2191 [00:22<38:00,  1.05s/it, loss=2.28, v_num=647]Epoch 32:   1%|▏         | 30/2191 [00:30<35:47,  1.01it/s, loss=2.28, v_num=647]Epoch 32:   1%|▏         | 30/2191 [00:30<35:47,  1.01it/s, loss=2.23, v_num=647]Epoch 32:   2%|▏         | 40/2191 [00:39<34:27,  1.04it/s, loss=2.23, v_num=647]Epoch 32:   2%|▏         | 40/2191 [00:39<34:27,  1.04it/s, loss=2.27, v_num=647]Epoch 32:   2%|▏         | 50/2191 [00:48<33:45,  1.06it/s, loss=2.27, v_num=647]Epoch 32:   2%|▏         | 50/2191 [00:48<33:45,  1.06it/s, loss=2.3, v_num=647] Epoch 32:   3%|▎         | 60/2191 [00:56<33:09,  1.07it/s, loss=2.3, v_num=647]Epoch 32:   3%|▎         | 60/2191 [00:56<33:09,  1.07it/s, loss=2.24, v_num=647]Epoch 32:   3%|▎         | 70/2191 [01:06<33:06,  1.07it/s, loss=2.24, v_num=647]Epoch 32:   3%|▎         | 70/2191 [01:06<33:06,  1.07it/s, loss=2.22, v_num=647]Epoch 32:   4%|▎         | 80/2191 [01:18<34:11,  1.03it/s, loss=2.22, v_num=647]Epoch 32:   4%|▎         | 80/2191 [01:18<34:11,  1.03it/s, loss=2.26, v_num=647]Epoch 32:   4%|▍         | 90/2191 [01:26<33:25,  1.05it/s, loss=2.26, v_num=647]Epoch 32:   4%|▍         | 90/2191 [01:26<33:25,  1.05it/s, loss=2.22, v_num=647]Epoch 32:   5%|▍         | 100/2191 [01:34<32:40,  1.07it/s, loss=2.22, v_num=647]Epoch 32:   5%|▍         | 100/2191 [01:34<32:40,  1.07it/s, loss=2.16, v_num=647]Epoch 32:   5%|▌         | 110/2191 [01:42<31:57,  1.09it/s, loss=2.16, v_num=647]Epoch 32:   5%|▌         | 110/2191 [01:42<31:57,  1.09it/s, loss=2.21, v_num=647]Epoch 32:   5%|▌         | 120/2191 [01:50<31:31,  1.10it/s, loss=2.21, v_num=647]Epoch 32:   5%|▌         | 120/2191 [01:50<31:31,  1.10it/s, loss=2.25, v_num=647]Epoch 32:   6%|▌         | 130/2191 [01:58<30:57,  1.11it/s, loss=2.25, v_num=647]Epoch 32:   6%|▌         | 130/2191 [01:58<30:57,  1.11it/s, loss=2.28, v_num=647]Epoch 32:   6%|▋         | 140/2191 [02:05<30:23,  1.12it/s, loss=2.28, v_num=647]Epoch 32:   6%|▋         | 140/2191 [02:05<30:23,  1.12it/s, loss=2.27, v_num=647]Epoch 32:   7%|▋         | 150/2191 [02:13<30:04,  1.13it/s, loss=2.27, v_num=647]Epoch 32:   7%|▋         | 150/2191 [02:13<30:04,  1.13it/s, loss=2.26, v_num=647]Epoch 32:   7%|▋         | 160/2191 [02:23<30:05,  1.13it/s, loss=2.26, v_num=647]Epoch 32:   7%|▋         | 160/2191 [02:23<30:05,  1.13it/s, loss=2.24, v_num=647]Epoch 32:   8%|▊         | 170/2191 [02:30<29:38,  1.14it/s, loss=2.24, v_num=647]Epoch 32:   8%|▊         | 170/2191 [02:30<29:38,  1.14it/s, loss=2.23, v_num=647]Epoch 32:   8%|▊         | 180/2191 [02:37<29:14,  1.15it/s, loss=2.23, v_num=647]Epoch 32:   8%|▊         | 180/2191 [02:37<29:14,  1.15it/s, loss=2.26, v_num=647]Epoch 32:   9%|▊         | 190/2191 [02:46<29:08,  1.14it/s, loss=2.26, v_num=647]Epoch 32:   9%|▊         | 190/2191 [02:46<29:08,  1.14it/s, loss=2.26, v_num=647]Epoch 32:   9%|▉         | 200/2191 [02:55<29:00,  1.14it/s, loss=2.26, v_num=647]Epoch 32:   9%|▉         | 200/2191 [02:55<29:01,  1.14it/s, loss=2.29, v_num=647]Epoch 32:  10%|▉         | 210/2191 [03:02<28:37,  1.15it/s, loss=2.29, v_num=647]Epoch 32:  10%|▉         | 210/2191 [03:02<28:37,  1.15it/s, loss=2.27, v_num=647]Epoch 32:  10%|█         | 220/2191 [03:11<28:30,  1.15it/s, loss=2.27, v_num=647]Epoch 32:  10%|█         | 220/2191 [03:11<28:30,  1.15it/s, loss=2.22, v_num=647]Epoch 32:  10%|█         | 230/2191 [03:18<28:04,  1.16it/s, loss=2.22, v_num=647]Epoch 32:  10%|█         | 230/2191 [03:18<28:04,  1.16it/s, loss=2.25, v_num=647]Epoch 32:  11%|█         | 240/2191 [03:25<27:42,  1.17it/s, loss=2.25, v_num=647]Epoch 32:  11%|█         | 240/2191 [03:25<27:42,  1.17it/s, loss=2.24, v_num=647]Epoch 32:  11%|█▏        | 250/2191 [03:33<27:27,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  11%|█▏        | 250/2191 [03:33<27:27,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  12%|█▏        | 260/2191 [03:41<27:19,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  12%|█▏        | 260/2191 [03:41<27:19,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  12%|█▏        | 270/2191 [03:50<27:11,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  12%|█▏        | 270/2191 [03:50<27:11,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  13%|█▎        | 280/2191 [03:57<26:55,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  13%|█▎        | 280/2191 [03:57<26:55,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  13%|█▎        | 290/2191 [04:05<26:42,  1.19it/s, loss=2.22, v_num=647]Epoch 32:  13%|█▎        | 290/2191 [04:05<26:42,  1.19it/s, loss=2.22, v_num=647]Epoch 32:  14%|█▎        | 300/2191 [04:14<26:37,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  14%|█▎        | 300/2191 [04:14<26:37,  1.18it/s, loss=2.29, v_num=647]Epoch 32:  14%|█▍        | 310/2191 [04:22<26:29,  1.18it/s, loss=2.29, v_num=647]Epoch 32:  14%|█▍        | 310/2191 [04:22<26:29,  1.18it/s, loss=2.31, v_num=647]Epoch 32:  15%|█▍        | 320/2191 [04:31<26:24,  1.18it/s, loss=2.31, v_num=647]Epoch 32:  15%|█▍        | 320/2191 [04:31<26:24,  1.18it/s, loss=2.27, v_num=647]Epoch 32:  15%|█▌        | 330/2191 [04:40<26:16,  1.18it/s, loss=2.27, v_num=647]Epoch 32:  15%|█▌        | 330/2191 [04:40<26:16,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  16%|█▌        | 340/2191 [04:48<26:05,  1.18it/s, loss=2.24, v_num=647]Epoch 32:  16%|█▌        | 340/2191 [04:48<26:05,  1.18it/s, loss=2.27, v_num=647]Epoch 32:  16%|█▌        | 350/2191 [04:56<25:53,  1.19it/s, loss=2.27, v_num=647]Epoch 32:  16%|█▌        | 350/2191 [04:56<25:53,  1.19it/s, loss=2.29, v_num=647]Epoch 32:  16%|█▋        | 360/2191 [05:03<25:41,  1.19it/s, loss=2.29, v_num=647]Epoch 32:  16%|█▋        | 360/2191 [05:03<25:41,  1.19it/s, loss=2.28, v_num=647]Epoch 32:  17%|█▋        | 370/2191 [05:13<25:38,  1.18it/s, loss=2.28, v_num=647]Epoch 32:  17%|█▋        | 370/2191 [05:13<25:38,  1.18it/s, loss=2.29, v_num=647]Epoch 32:  17%|█▋        | 380/2191 [05:22<25:33,  1.18it/s, loss=2.29, v_num=647]Epoch 32:  17%|█▋        | 380/2191 [05:22<25:33,  1.18it/s, loss=2.26, v_num=647]Epoch 32:  18%|█▊        | 390/2191 [05:32<25:31,  1.18it/s, loss=2.26, v_num=647]Epoch 32:  18%|█▊        | 390/2191 [05:32<25:31,  1.18it/s, loss=2.21, v_num=647]Epoch 32:  18%|█▊        | 400/2191 [05:40<25:21,  1.18it/s, loss=2.21, v_num=647]Epoch 32:  18%|█▊        | 400/2191 [05:40<25:21,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  19%|█▊        | 410/2191 [05:47<25:07,  1.18it/s, loss=2.22, v_num=647]Epoch 32:  19%|█▊        | 410/2191 [05:47<25:07,  1.18it/s, loss=2.28, v_num=647]Epoch 32:  19%|█▉        | 420/2191 [05:55<24:53,  1.19it/s, loss=2.28, v_num=647]Epoch 32:  19%|█▉        | 420/2191 [05:55<24:53,  1.19it/s, loss=2.31, v_num=647]Epoch 32:  20%|█▉        | 430/2191 [06:03<24:44,  1.19it/s, loss=2.31, v_num=647]Epoch 32:  20%|█▉        | 430/2191 [06:03<24:44,  1.19it/s, loss=2.3, v_num=647] Epoch 32:  20%|██        | 440/2191 [06:11<24:34,  1.19it/s, loss=2.3, v_num=647]Epoch 32:  20%|██        | 440/2191 [06:11<24:34,  1.19it/s, loss=2.26, v_num=647]Epoch 32:  21%|██        | 450/2191 [06:18<24:19,  1.19it/s, loss=2.26, v_num=647]Epoch 32:  21%|██        | 450/2191 [06:18<24:19,  1.19it/s, loss=2.28, v_num=647]Epoch 32:  21%|██        | 460/2191 [06:25<24:08,  1.19it/s, loss=2.28, v_num=647]Epoch 32:  21%|██        | 460/2191 [06:25<24:08,  1.19it/s, loss=2.3, v_num=647] Epoch 32:  21%|██▏       | 470/2191 [06:32<23:55,  1.20it/s, loss=2.3, v_num=647]Epoch 32:  21%|██▏       | 470/2191 [06:32<23:55,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  22%|██▏       | 480/2191 [06:40<23:43,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  22%|██▏       | 480/2191 [06:40<23:43,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  22%|██▏       | 490/2191 [06:47<23:32,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  22%|██▏       | 490/2191 [06:47<23:32,  1.20it/s, loss=2.3, v_num=647] Epoch 32:  23%|██▎       | 500/2191 [06:56<23:24,  1.20it/s, loss=2.3, v_num=647]Epoch 32:  23%|██▎       | 500/2191 [06:56<23:24,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  23%|██▎       | 510/2191 [07:05<23:19,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  23%|██▎       | 510/2191 [07:05<23:19,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  24%|██▎       | 520/2191 [07:16<23:18,  1.19it/s, loss=2.22, v_num=647]Epoch 32:  24%|██▎       | 520/2191 [07:16<23:18,  1.19it/s, loss=2.26, v_num=647]Epoch 32:  24%|██▍       | 530/2191 [07:25<23:12,  1.19it/s, loss=2.26, v_num=647]Epoch 32:  24%|██▍       | 530/2191 [07:25<23:12,  1.19it/s, loss=2.31, v_num=647]Epoch 32:  25%|██▍       | 540/2191 [07:34<23:06,  1.19it/s, loss=2.31, v_num=647]Epoch 32:  25%|██▍       | 540/2191 [07:34<23:06,  1.19it/s, loss=2.25, v_num=647]Epoch 32:  25%|██▌       | 550/2191 [07:41<22:55,  1.19it/s, loss=2.25, v_num=647]Epoch 32:  25%|██▌       | 550/2191 [07:41<22:55,  1.19it/s, loss=2.2, v_num=647] Epoch 32:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.2, v_num=647]Epoch 32:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.19, v_num=647]Epoch 32:  26%|██▌       | 570/2191 [07:58<22:39,  1.19it/s, loss=2.19, v_num=647]Epoch 32:  26%|██▌       | 570/2191 [07:58<22:39,  1.19it/s, loss=2.22, v_num=647]Epoch 32:  26%|██▋       | 580/2191 [08:05<22:26,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  26%|██▋       | 580/2191 [08:05<22:26,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  27%|██▋       | 590/2191 [08:13<22:16,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  27%|██▋       | 590/2191 [08:13<22:16,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  27%|██▋       | 600/2191 [08:21<22:08,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  27%|██▋       | 600/2191 [08:21<22:08,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  28%|██▊       | 610/2191 [08:30<22:00,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  28%|██▊       | 610/2191 [08:30<22:00,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  28%|██▊       | 620/2191 [08:37<21:49,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  28%|██▊       | 620/2191 [08:37<21:49,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  29%|██▉       | 630/2191 [08:46<21:43,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  29%|██▉       | 630/2191 [08:46<21:43,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  29%|██▉       | 640/2191 [08:56<21:37,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  29%|██▉       | 640/2191 [08:56<21:37,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  30%|██▉       | 650/2191 [09:03<21:27,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  30%|██▉       | 650/2191 [09:03<21:27,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  30%|███       | 660/2191 [09:11<21:17,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  31%|███       | 670/2191 [09:19<21:08,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  31%|███       | 670/2191 [09:19<21:08,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  31%|███       | 680/2191 [09:26<20:58,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  31%|███       | 680/2191 [09:26<20:58,  1.20it/s, loss=2.3, v_num=647] Epoch 32:  31%|███▏      | 690/2191 [09:35<20:49,  1.20it/s, loss=2.3, v_num=647]Epoch 32:  31%|███▏      | 690/2191 [09:35<20:49,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  32%|███▏      | 700/2191 [09:43<20:40,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  32%|███▏      | 700/2191 [09:43<20:40,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  32%|███▏      | 710/2191 [09:51<20:31,  1.20it/s, loss=2.22, v_num=647]Epoch 32:  32%|███▏      | 710/2191 [09:51<20:31,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  33%|███▎      | 720/2191 [09:58<20:21,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  33%|███▎      | 720/2191 [09:58<20:21,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  33%|███▎      | 730/2191 [10:10<20:19,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  33%|███▎      | 730/2191 [10:10<20:19,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  34%|███▍      | 740/2191 [10:18<20:11,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  34%|███▍      | 740/2191 [10:18<20:11,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  34%|███▍      | 750/2191 [10:28<20:05,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  34%|███▍      | 750/2191 [10:28<20:05,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  35%|███▍      | 760/2191 [10:37<19:59,  1.19it/s, loss=2.28, v_num=647]Epoch 32:  35%|███▍      | 760/2191 [10:37<19:59,  1.19it/s, loss=2.29, v_num=647]Epoch 32:  35%|███▌      | 770/2191 [10:45<19:49,  1.19it/s, loss=2.29, v_num=647]Epoch 32:  35%|███▌      | 770/2191 [10:45<19:49,  1.19it/s, loss=2.26, v_num=647]Epoch 32:  36%|███▌      | 780/2191 [10:52<19:39,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  36%|███▌      | 780/2191 [10:52<19:39,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  36%|███▌      | 790/2191 [11:00<19:29,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  36%|███▌      | 790/2191 [11:00<19:29,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  37%|███▋      | 800/2191 [11:08<19:20,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  37%|███▋      | 800/2191 [11:08<19:20,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  37%|███▋      | 810/2191 [11:16<19:11,  1.20it/s, loss=2.28, v_num=647]Epoch 32:  37%|███▋      | 810/2191 [11:16<19:11,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  37%|███▋      | 820/2191 [11:23<19:01,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  37%|███▋      | 820/2191 [11:23<19:01,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  38%|███▊      | 830/2191 [11:32<18:53,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  38%|███▊      | 830/2191 [11:32<18:53,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  38%|███▊      | 840/2191 [11:40<18:45,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  38%|███▊      | 840/2191 [11:40<18:45,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  39%|███▉      | 850/2191 [11:48<18:37,  1.20it/s, loss=2.27, v_num=647]Epoch 32:  39%|███▉      | 850/2191 [11:48<18:37,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  39%|███▉      | 860/2191 [11:56<18:28,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  39%|███▉      | 860/2191 [11:56<18:28,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  40%|███▉      | 870/2191 [12:04<18:19,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  40%|███▉      | 870/2191 [12:04<18:19,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  40%|████      | 880/2191 [12:12<18:10,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  40%|████      | 880/2191 [12:12<18:10,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  41%|████      | 890/2191 [12:21<18:03,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  41%|████      | 890/2191 [12:21<18:03,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  41%|████      | 900/2191 [12:29<17:54,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  41%|████      | 900/2191 [12:29<17:54,  1.20it/s, loss=2.24, v_num=647]Epoch 32:  42%|████▏     | 910/2191 [12:38<17:47,  1.20it/s, loss=2.24, v_num=647]Epoch 32:  42%|████▏     | 910/2191 [12:38<17:47,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  42%|████▏     | 920/2191 [12:48<17:40,  1.20it/s, loss=2.23, v_num=647]Epoch 32:  42%|████▏     | 920/2191 [12:48<17:40,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  42%|████▏     | 930/2191 [12:56<17:31,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  42%|████▏     | 930/2191 [12:56<17:31,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  43%|████▎     | 940/2191 [13:05<17:23,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  43%|████▎     | 940/2191 [13:05<17:23,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  43%|████▎     | 950/2191 [13:12<17:13,  1.20it/s, loss=2.25, v_num=647]Epoch 32:  43%|████▎     | 950/2191 [13:12<17:13,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  44%|████▍     | 960/2191 [13:19<17:04,  1.20it/s, loss=2.29, v_num=647]Epoch 32:  44%|████▍     | 960/2191 [13:19<17:04,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  44%|████▍     | 970/2191 [13:27<16:55,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  44%|████▍     | 970/2191 [13:27<16:55,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  45%|████▍     | 980/2191 [13:34<16:45,  1.20it/s, loss=2.31, v_num=647]Epoch 32:  45%|████▍     | 980/2191 [13:34<16:45,  1.20it/s, loss=2.3, v_num=647] Epoch 32:  45%|████▌     | 990/2191 [13:42<16:36,  1.20it/s, loss=2.3, v_num=647]Epoch 32:  45%|████▌     | 990/2191 [13:42<16:36,  1.20it/s, loss=2.26, v_num=647]Epoch 32:  46%|████▌     | 1000/2191 [13:49<16:27,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  46%|████▌     | 1000/2191 [13:49<16:27,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  46%|████▌     | 1010/2191 [13:57<16:17,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  46%|████▌     | 1010/2191 [13:57<16:17,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  47%|████▋     | 1020/2191 [14:05<16:09,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  47%|████▋     | 1020/2191 [14:05<16:09,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  47%|████▋     | 1030/2191 [14:13<16:01,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  47%|████▋     | 1030/2191 [14:13<16:01,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  47%|████▋     | 1040/2191 [14:23<15:54,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  47%|████▋     | 1040/2191 [14:23<15:54,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  48%|████▊     | 1050/2191 [14:30<15:45,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  48%|████▊     | 1050/2191 [14:30<15:45,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  48%|████▊     | 1060/2191 [14:38<15:36,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  48%|████▊     | 1060/2191 [14:38<15:36,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  49%|████▉     | 1070/2191 [14:47<15:29,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  49%|████▉     | 1070/2191 [14:47<15:29,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  49%|████▉     | 1080/2191 [14:55<15:20,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  49%|████▉     | 1080/2191 [14:55<15:20,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  50%|████▉     | 1090/2191 [15:03<15:11,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  50%|████▉     | 1090/2191 [15:03<15:11,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  50%|█████     | 1100/2191 [15:12<15:04,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  50%|█████     | 1100/2191 [15:12<15:04,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  51%|█████     | 1110/2191 [15:21<14:56,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  51%|█████     | 1110/2191 [15:21<14:56,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  51%|█████     | 1120/2191 [15:29<14:47,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  51%|█████     | 1120/2191 [15:29<14:47,  1.21it/s, loss=2.19, v_num=647]Epoch 32:  52%|█████▏    | 1130/2191 [15:37<14:39,  1.21it/s, loss=2.19, v_num=647]Epoch 32:  52%|█████▏    | 1130/2191 [15:37<14:39,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  52%|█████▏    | 1140/2191 [15:46<14:31,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  52%|█████▏    | 1140/2191 [15:46<14:31,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  52%|█████▏    | 1150/2191 [15:54<14:22,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  52%|█████▏    | 1150/2191 [15:54<14:22,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  53%|█████▎    | 1160/2191 [16:02<14:14,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  53%|█████▎    | 1160/2191 [16:02<14:14,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  53%|█████▎    | 1170/2191 [16:09<14:05,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  53%|█████▎    | 1170/2191 [16:09<14:05,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  54%|█████▍    | 1180/2191 [16:18<13:57,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  54%|█████▍    | 1180/2191 [16:18<13:57,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  54%|█████▍    | 1190/2191 [16:25<13:48,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  54%|█████▍    | 1190/2191 [16:25<13:48,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  55%|█████▍    | 1200/2191 [16:33<13:39,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  55%|█████▍    | 1200/2191 [16:33<13:39,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  55%|█████▌    | 1210/2191 [16:41<13:31,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  55%|█████▌    | 1210/2191 [16:41<13:31,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  56%|█████▌    | 1220/2191 [16:49<13:22,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  56%|█████▌    | 1220/2191 [16:49<13:22,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  56%|█████▌    | 1230/2191 [16:57<13:14,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  56%|█████▌    | 1230/2191 [16:57<13:14,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  57%|█████▋    | 1240/2191 [17:06<13:06,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  57%|█████▋    | 1240/2191 [17:06<13:06,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  57%|█████▋    | 1250/2191 [17:14<12:58,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  57%|█████▋    | 1250/2191 [17:14<12:58,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  58%|█████▊    | 1260/2191 [17:21<12:48,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  58%|█████▊    | 1260/2191 [17:21<12:48,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  58%|█████▊    | 1270/2191 [17:29<12:40,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  58%|█████▊    | 1270/2191 [17:29<12:40,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  58%|█████▊    | 1280/2191 [17:38<12:32,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  58%|█████▊    | 1280/2191 [17:38<12:32,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  59%|█████▉    | 1290/2191 [17:46<12:24,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  59%|█████▉    | 1290/2191 [17:46<12:24,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  59%|█████▉    | 1300/2191 [17:56<12:17,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  59%|█████▉    | 1300/2191 [17:56<12:17,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  60%|█████▉    | 1310/2191 [18:04<12:08,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  60%|█████▉    | 1310/2191 [18:04<12:08,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  60%|██████    | 1320/2191 [18:12<12:00,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  60%|██████    | 1320/2191 [18:12<12:00,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  61%|██████    | 1330/2191 [18:20<11:51,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  61%|██████    | 1330/2191 [18:20<11:51,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  61%|██████    | 1340/2191 [18:27<11:42,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  61%|██████    | 1340/2191 [18:27<11:42,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  62%|██████▏   | 1350/2191 [18:34<11:33,  1.21it/s, loss=2.23, v_num=647]Epoch 32:  62%|██████▏   | 1350/2191 [18:34<11:33,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  62%|██████▏   | 1360/2191 [18:43<11:26,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  62%|██████▏   | 1360/2191 [18:43<11:26,  1.21it/s, loss=2.28, v_num=647]Epoch 32:  63%|██████▎   | 1370/2191 [18:50<11:17,  1.21it/s, loss=2.28, v_num=647]Epoch 32:  63%|██████▎   | 1370/2191 [18:50<11:17,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  63%|██████▎   | 1380/2191 [18:59<11:09,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  63%|██████▎   | 1380/2191 [18:59<11:09,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  63%|██████▎   | 1390/2191 [19:07<11:01,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  63%|██████▎   | 1390/2191 [19:07<11:01,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  64%|██████▍   | 1400/2191 [19:16<10:52,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  64%|██████▍   | 1400/2191 [19:16<10:52,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  64%|██████▍   | 1410/2191 [19:24<10:44,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  64%|██████▍   | 1410/2191 [19:24<10:44,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  65%|██████▍   | 1420/2191 [19:31<10:35,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  65%|██████▍   | 1420/2191 [19:31<10:35,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  65%|██████▌   | 1430/2191 [19:38<10:26,  1.21it/s, loss=2.27, v_num=647]Epoch 32:  65%|██████▌   | 1430/2191 [19:38<10:26,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  66%|██████▌   | 1440/2191 [19:46<10:18,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  66%|██████▌   | 1440/2191 [19:46<10:18,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  66%|██████▌   | 1450/2191 [19:55<10:10,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  66%|██████▌   | 1450/2191 [19:55<10:10,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  67%|██████▋   | 1460/2191 [20:05<10:02,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  67%|██████▋   | 1460/2191 [20:05<10:02,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  67%|██████▋   | 1470/2191 [20:14<09:55,  1.21it/s, loss=2.26, v_num=647]Epoch 32:  67%|██████▋   | 1470/2191 [20:14<09:55,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  68%|██████▊   | 1480/2191 [20:21<09:46,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  68%|██████▊   | 1480/2191 [20:21<09:46,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  68%|██████▊   | 1490/2191 [20:29<09:37,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  68%|██████▊   | 1490/2191 [20:29<09:37,  1.21it/s, loss=2.28, v_num=647]Epoch 32:  68%|██████▊   | 1500/2191 [20:37<09:29,  1.21it/s, loss=2.28, v_num=647]Epoch 32:  68%|██████▊   | 1500/2191 [20:37<09:29,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  69%|██████▉   | 1510/2191 [20:47<09:22,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  69%|██████▉   | 1510/2191 [20:47<09:22,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  69%|██████▉   | 1520/2191 [20:55<09:13,  1.21it/s, loss=2.21, v_num=647]Epoch 32:  69%|██████▉   | 1520/2191 [20:55<09:13,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  70%|██████▉   | 1530/2191 [21:03<09:05,  1.21it/s, loss=2.24, v_num=647]Epoch 32:  70%|██████▉   | 1530/2191 [21:03<09:05,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  70%|███████   | 1540/2191 [21:09<08:56,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  70%|███████   | 1540/2191 [21:09<08:56,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  71%|███████   | 1550/2191 [21:20<08:49,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  71%|███████   | 1550/2191 [21:20<08:49,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  71%|███████   | 1560/2191 [21:27<08:40,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  71%|███████   | 1560/2191 [21:27<08:40,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  72%|███████▏  | 1570/2191 [21:36<08:32,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  72%|███████▏  | 1570/2191 [21:36<08:32,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  72%|███████▏  | 1580/2191 [21:45<08:24,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  72%|███████▏  | 1580/2191 [21:45<08:24,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  73%|███████▎  | 1590/2191 [21:53<08:16,  1.21it/s, loss=2.29, v_num=647]Epoch 32:  73%|███████▎  | 1590/2191 [21:53<08:16,  1.21it/s, loss=2.3, v_num=647] Epoch 32:  73%|███████▎  | 1600/2191 [22:01<08:07,  1.21it/s, loss=2.3, v_num=647]Epoch 32:  73%|███████▎  | 1600/2191 [22:01<08:07,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  73%|███████▎  | 1610/2191 [22:08<07:59,  1.21it/s, loss=2.25, v_num=647]Epoch 32:  73%|███████▎  | 1610/2191 [22:08<07:59,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  74%|███████▍  | 1620/2191 [22:17<07:51,  1.21it/s, loss=2.22, v_num=647]Epoch 32:  74%|███████▍  | 1620/2191 [22:17<07:51,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  74%|███████▍  | 1630/2191 [22:24<07:42,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  74%|███████▍  | 1630/2191 [22:24<07:42,  1.21it/s, loss=2.34, v_num=647]Epoch 32:  75%|███████▍  | 1640/2191 [22:33<07:34,  1.21it/s, loss=2.34, v_num=647]Epoch 32:  75%|███████▍  | 1640/2191 [22:33<07:34,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  75%|███████▌  | 1650/2191 [22:40<07:25,  1.21it/s, loss=2.31, v_num=647]Epoch 32:  75%|███████▌  | 1650/2191 [22:40<07:25,  1.21it/s, loss=2.35, v_num=647]Epoch 32:  76%|███████▌  | 1660/2191 [22:47<07:17,  1.21it/s, loss=2.35, v_num=647]Epoch 32:  76%|███████▌  | 1660/2191 [22:47<07:17,  1.21it/s, loss=2.34, v_num=647]Epoch 32:  76%|███████▌  | 1670/2191 [22:54<07:08,  1.22it/s, loss=2.34, v_num=647]Epoch 32:  76%|███████▌  | 1670/2191 [22:54<07:08,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  77%|███████▋  | 1680/2191 [23:02<07:00,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  77%|███████▋  | 1680/2191 [23:02<07:00,  1.22it/s, loss=2.26, v_num=647]Epoch 32:  77%|███████▋  | 1690/2191 [23:08<06:51,  1.22it/s, loss=2.26, v_num=647]Epoch 32:  77%|███████▋  | 1690/2191 [23:08<06:51,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  78%|███████▊  | 1700/2191 [23:17<06:43,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  78%|███████▊  | 1700/2191 [23:17<06:43,  1.22it/s, loss=2.31, v_num=647]Epoch 32:  78%|███████▊  | 1710/2191 [23:25<06:34,  1.22it/s, loss=2.31, v_num=647]Epoch 32:  78%|███████▊  | 1710/2191 [23:25<06:34,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  79%|███████▊  | 1720/2191 [23:32<06:26,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  79%|███████▊  | 1720/2191 [23:32<06:26,  1.22it/s, loss=2.24, v_num=647]Epoch 32:  79%|███████▉  | 1730/2191 [23:39<06:18,  1.22it/s, loss=2.24, v_num=647]Epoch 32:  79%|███████▉  | 1730/2191 [23:39<06:18,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  79%|███████▉  | 1740/2191 [23:45<06:09,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  79%|███████▉  | 1740/2191 [23:45<06:09,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  80%|███████▉  | 1750/2191 [23:53<06:01,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  80%|███████▉  | 1750/2191 [23:53<06:01,  1.22it/s, loss=2.3, v_num=647] Epoch 32:  80%|████████  | 1760/2191 [24:00<05:52,  1.22it/s, loss=2.3, v_num=647]Epoch 32:  80%|████████  | 1760/2191 [24:00<05:52,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  81%|████████  | 1770/2191 [24:08<05:44,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  81%|████████  | 1770/2191 [24:08<05:44,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  81%|████████  | 1780/2191 [24:16<05:36,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  81%|████████  | 1780/2191 [24:16<05:36,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  82%|████████▏ | 1790/2191 [24:22<05:27,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  82%|████████▏ | 1790/2191 [24:22<05:27,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  82%|████████▏ | 1800/2191 [24:31<05:19,  1.22it/s, loss=2.25, v_num=647]Epoch 32:  82%|████████▏ | 1800/2191 [24:31<05:19,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  83%|████████▎ | 1810/2191 [24:39<05:11,  1.22it/s, loss=2.29, v_num=647]Epoch 32:  83%|████████▎ | 1810/2191 [24:39<05:11,  1.22it/s, loss=2.3, v_num=647] Epoch 32:  83%|████████▎ | 1820/2191 [24:47<05:03,  1.22it/s, loss=2.3, v_num=647]Epoch 32:  83%|████████▎ | 1820/2191 [24:47<05:03,  1.22it/s, loss=2.31, v_num=647]Epoch 32:  84%|████████▎ | 1830/2191 [24:55<04:54,  1.22it/s, loss=2.31, v_num=647]Epoch 32:  84%|████████▎ | 1830/2191 [24:55<04:54,  1.22it/s, loss=2.27, v_num=647]Epoch 32:  84%|████████▍ | 1840/2191 [25:02<04:46,  1.23it/s, loss=2.27, v_num=647]Epoch 32:  84%|████████▍ | 1840/2191 [25:02<04:46,  1.23it/s, loss=2.22, v_num=647]Epoch 32:  84%|████████▍ | 1850/2191 [25:09<04:38,  1.23it/s, loss=2.22, v_num=647]Epoch 32:  84%|████████▍ | 1850/2191 [25:09<04:38,  1.23it/s, loss=2.23, v_num=647]Epoch 32:  85%|████████▍ | 1860/2191 [25:12<04:29,  1.23it/s, loss=2.23, v_num=647]Epoch 32:  85%|████████▍ | 1860/2191 [25:12<04:29,  1.23it/s, loss=2.25, v_num=647]validation_epoch_end
graph acc: 0.38977635782747605
valid accuracy: 0.979115903377533
validation_epoch_end
graph acc: 0.4281150159744409
valid accuracy: 0.9768701195716858
validation_epoch_end
graph acc: 0.3610223642172524
valid accuracy: 0.9768725037574768
1 [25:17<04:10,  1.24it/s, loss=2.25, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3929712460063898
valid accuracy: 0.979111909866333
validation_epoch_end
graph acc: 0.3769968051118211
valid accuracy: 0.9771011471748352
validation_epoch_end
graph acc: 0.4504792332268371
valid accuracy: 0.9800382256507874

Validating:   3%|▎         | 10/313 [00:01<00:50,  6.05it/s][AEpoch 32:  86%|████████▋ | 1890/2191 [25:18<04:01,  1.24it/s, loss=2.25, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:49,  5.97it/s][AEpoch 32:  87%|████████▋ | 1900/2191 [25:20<03:52,  1.25it/s, loss=2.25, v_num=647]
Validating:  10%|▉         | 30/313 [00:04<00:36,  7.67it/s][AEpoch 32:  87%|████████▋ | 1910/2191 [25:21<03:43,  1.26it/s, loss=2.25, v_num=647]
Validating:  13%|█▎        | 40/313 [00:05<00:30,  9.04it/s][AEpoch 32:  88%|████████▊ | 1920/2191 [25:22<03:34,  1.26it/s, loss=2.25, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:25, 10.38it/s][AEpoch 32:  88%|████████▊ | 1930/2191 [25:22<03:25,  1.27it/s, loss=2.25, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:23, 10.91it/s][AEpoch 32:  89%|████████▊ | 1940/2191 [25:23<03:17,  1.27it/s, loss=2.25, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:25,  9.51it/s][AEpoch 32:  89%|████████▉ | 1950/2191 [25:25<03:08,  1.28it/s, loss=2.25, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:22, 10.47it/s][AEpoch 32:  89%|████████▉ | 1960/2191 [25:25<02:59,  1.29it/s, loss=2.25, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.23it/s][AEpoch 32:  90%|████████▉ | 1970/2191 [25:26<02:51,  1.29it/s, loss=2.25, v_num=647]
Validating:  32%|███▏      | 100/313 [00:10<00:19, 10.72it/s][AEpoch 32:  90%|█████████ | 1980/2191 [25:27<02:42,  1.30it/s, loss=2.25, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.89it/s][AEpoch 32:  91%|█████████ | 1990/2191 [25:28<02:34,  1.30it/s, loss=2.25, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:19, 10.06it/s][AEpoch 32:  91%|█████████▏| 2000/2191 [25:29<02:26,  1.31it/s, loss=2.25, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.52it/s][AEpoch 32:  92%|█████████▏| 2010/2191 [25:31<02:17,  1.31it/s, loss=2.25, v_num=647]
Validating:  45%|████▍     | 140/313 [00:15<00:19,  8.71it/s][AEpoch 32:  92%|█████████▏| 2020/2191 [25:32<02:09,  1.32it/s, loss=2.25, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:16,  9.61it/s][AEpoch 32:  93%|█████████▎| 2030/2191 [25:33<02:01,  1.32it/s, loss=2.25, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:15,  9.96it/s][AEpoch 32:  93%|█████████▎| 2040/2191 [25:34<01:53,  1.33it/s, loss=2.25, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.76it/s][AEpoch 32:  94%|█████████▎| 2050/2191 [25:34<01:45,  1.34it/s, loss=2.25, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:12, 10.96it/s][AEpoch 32:  94%|█████████▍| 2060/2191 [25:35<01:37,  1.34it/s, loss=2.25, v_num=647]
Validating:  61%|██████    | 190/313 [00:19<00:10, 12.03it/s][AEpoch 32:  94%|█████████▍| 2070/2191 [25:36<01:29,  1.35it/s, loss=2.25, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:20<00:09, 11.60it/s][AEpoch 32:  95%|█████████▍| 2080/2191 [25:37<01:22,  1.35it/s, loss=2.25, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:21<00:10, 10.00it/s][AEpoch 32:  95%|█████████▌| 2090/2191 [25:38<01:14,  1.36it/s, loss=2.25, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:10,  9.14it/s][AEpoch 32:  96%|█████████▌| 2100/2191 [25:39<01:06,  1.36it/s, loss=2.25, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:07, 10.49it/s][AEpoch 32:  96%|█████████▋| 2110/2191 [25:40<00:59,  1.37it/s, loss=2.25, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07, 10.03it/s][AEpoch 32:  97%|█████████▋| 2120/2191 [25:41<00:51,  1.38it/s, loss=2.25, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:05, 10.78it/s][AEpoch 32:  97%|█████████▋| 2130/2191 [25:42<00:44,  1.38it/s, loss=2.25, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05, 10.28it/s][AEpoch 32:  98%|█████████▊| 2140/2191 [25:43<00:36,  1.39it/s, loss=2.25, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 11.42it/s][AEpoch 32:  98%|█████████▊| 2150/2191 [25:44<00:29,  1.39it/s, loss=2.25, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.30it/s][AEpoch 32:  99%|█████████▊| 2160/2191 [25:44<00:22,  1.40it/s, loss=2.25, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 11.56it/s][AEpoch 32:  99%|█████████▉| 2170/2191 [25:45<00:14,  1.40it/s, loss=2.25, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:29<00:00, 13.19it/s][AEpoch 32:  99%|█████████▉| 2180/2191 [25:46<00:07,  1.41it/s, loss=2.25, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.94it/s][AEpoch 32: 100%|█████████▉| 2190/2191 [25:46<00:00,  1.42it/s, loss=2.25, v_num=647]validation_epoch_end
graph acc: 0.41214057507987223
valid accuracy: 0.9791657328605652
Epoch 32: 100%|██████████| 2191/2191 [25:49<00:00,  1.42it/s, loss=2.28, v_num=647]
                                                             [AEpoch 32:   0%|          | 0/2191 [00:00<00:00, 11949.58it/s, loss=2.28, v_num=647]Epoch 33:   0%|          | 0/2191 [00:00<00:00, 2870.84it/s, loss=2.28, v_num=647] Epoch 33:   0%|          | 10/2191 [00:12<42:41,  1.17s/it, loss=2.28, v_num=647] Epoch 33:   0%|          | 10/2191 [00:12<42:41,  1.17s/it, loss=2.25, v_num=647]Epoch 33:   1%|          | 20/2191 [00:21<36:44,  1.02s/it, loss=2.25, v_num=647]Epoch 33:   1%|          | 20/2191 [00:21<36:44,  1.02s/it, loss=2.19, v_num=647]Epoch 33:   1%|▏         | 30/2191 [00:29<33:45,  1.07it/s, loss=2.19, v_num=647]Epoch 33:   1%|▏         | 30/2191 [00:29<33:45,  1.07it/s, loss=2.21, v_num=647]Epoch 33:   2%|▏         | 40/2191 [00:38<33:19,  1.08it/s, loss=2.21, v_num=647]Epoch 33:   2%|▏         | 40/2191 [00:38<33:19,  1.08it/s, loss=2.26, v_num=647]Epoch 33:   2%|▏         | 50/2191 [00:45<31:57,  1.12it/s, loss=2.26, v_num=647]Epoch 33:   2%|▏         | 50/2191 [00:45<31:58,  1.12it/s, loss=2.24, v_num=647]Epoch 33:   3%|▎         | 60/2191 [00:53<31:01,  1.14it/s, loss=2.24, v_num=647]Epoch 33:   3%|▎         | 60/2191 [00:53<31:01,  1.14it/s, loss=2.24, v_num=647]Epoch 33:   3%|▎         | 70/2191 [01:01<30:33,  1.16it/s, loss=2.24, v_num=647]Epoch 33:   3%|▎         | 70/2191 [01:01<30:33,  1.16it/s, loss=2.22, v_num=647]Epoch 33:   4%|▎         | 80/2191 [01:10<30:32,  1.15it/s, loss=2.22, v_num=647]Epoch 33:   4%|▎         | 80/2191 [01:10<30:32,  1.15it/s, loss=2.18, v_num=647]Epoch 33:   4%|▍         | 90/2191 [01:18<30:14,  1.16it/s, loss=2.18, v_num=647]Epoch 33:   4%|▍         | 90/2191 [01:18<30:14,  1.16it/s, loss=2.24, v_num=647]Epoch 33:   5%|▍         | 100/2191 [01:26<29:44,  1.17it/s, loss=2.24, v_num=647]Epoch 33:   5%|▍         | 100/2191 [01:26<29:44,  1.17it/s, loss=2.27, v_num=647]Epoch 33:   5%|▌         | 110/2191 [01:34<29:31,  1.17it/s, loss=2.27, v_num=647]Epoch 33:   5%|▌         | 110/2191 [01:34<29:31,  1.17it/s, loss=2.21, v_num=647]Epoch 33:   5%|▌         | 120/2191 [01:42<29:10,  1.18it/s, loss=2.21, v_num=647]Epoch 33:   5%|▌         | 120/2191 [01:42<29:10,  1.18it/s, loss=2.21, v_num=647]Epoch 33:   6%|▌         | 130/2191 [01:50<29:04,  1.18it/s, loss=2.21, v_num=647]Epoch 33:   6%|▌         | 130/2191 [01:50<29:04,  1.18it/s, loss=2.26, v_num=647]Epoch 33:   6%|▋         | 140/2191 [01:59<29:02,  1.18it/s, loss=2.26, v_num=647]Epoch 33:   6%|▋         | 140/2191 [01:59<29:02,  1.18it/s, loss=2.25, v_num=647]Epoch 33:   7%|▋         | 150/2191 [02:09<29:06,  1.17it/s, loss=2.25, v_num=647]Epoch 33:   7%|▋         | 150/2191 [02:09<29:06,  1.17it/s, loss=2.24, v_num=647]Epoch 33:   7%|▋         | 160/2191 [02:16<28:42,  1.18it/s, loss=2.24, v_num=647]Epoch 33:   7%|▋         | 160/2191 [02:16<28:42,  1.18it/s, loss=2.25, v_num=647]Epoch 33:   8%|▊         | 170/2191 [02:26<28:51,  1.17it/s, loss=2.25, v_num=647]Epoch 33:   8%|▊         | 170/2191 [02:26<28:51,  1.17it/s, loss=2.26, v_num=647]Epoch 33:   8%|▊         | 180/2191 [02:35<28:45,  1.17it/s, loss=2.26, v_num=647]Epoch 33:   8%|▊         | 180/2191 [02:35<28:45,  1.17it/s, loss=2.25, v_num=647]Epoch 33:   9%|▊         | 190/2191 [02:42<28:24,  1.17it/s, loss=2.25, v_num=647]Epoch 33:   9%|▊         | 190/2191 [02:42<28:24,  1.17it/s, loss=2.23, v_num=647]Epoch 33:   9%|▉         | 200/2191 [02:52<28:29,  1.16it/s, loss=2.23, v_num=647]Epoch 33:   9%|▉         | 200/2191 [02:52<28:29,  1.16it/s, loss=2.24, v_num=647]Epoch 33:  10%|▉         | 210/2191 [02:59<28:08,  1.17it/s, loss=2.24, v_num=647]Epoch 33:  10%|▉         | 210/2191 [02:59<28:08,  1.17it/s, loss=2.23, v_num=647]Epoch 33:  10%|█         | 220/2191 [03:08<27:59,  1.17it/s, loss=2.23, v_num=647]Epoch 33:  10%|█         | 220/2191 [03:08<27:59,  1.17it/s, loss=2.24, v_num=647]Epoch 33:  10%|█         | 230/2191 [03:17<27:53,  1.17it/s, loss=2.24, v_num=647]Epoch 33:  10%|█         | 230/2191 [03:17<27:53,  1.17it/s, loss=2.25, v_num=647]Epoch 33:  11%|█         | 240/2191 [03:26<27:50,  1.17it/s, loss=2.25, v_num=647]Epoch 33:  11%|█         | 240/2191 [03:26<27:50,  1.17it/s, loss=2.23, v_num=647]Epoch 33:  11%|█▏        | 250/2191 [03:34<27:38,  1.17it/s, loss=2.23, v_num=647]Epoch 33:  11%|█▏        | 250/2191 [03:34<27:38,  1.17it/s, loss=2.27, v_num=647]Epoch 33:  12%|█▏        | 260/2191 [03:42<27:26,  1.17it/s, loss=2.27, v_num=647]Epoch 33:  12%|█▏        | 260/2191 [03:42<27:26,  1.17it/s, loss=2.27, v_num=647]Epoch 33:  12%|█▏        | 270/2191 [03:51<27:17,  1.17it/s, loss=2.27, v_num=647]Epoch 33:  12%|█▏        | 270/2191 [03:51<27:17,  1.17it/s, loss=2.24, v_num=647]Epoch 33:  13%|█▎        | 280/2191 [04:00<27:18,  1.17it/s, loss=2.24, v_num=647]Epoch 33:  13%|█▎        | 280/2191 [04:00<27:18,  1.17it/s, loss=2.21, v_num=647]Epoch 33:  13%|█▎        | 290/2191 [04:09<27:08,  1.17it/s, loss=2.21, v_num=647]Epoch 33:  13%|█▎        | 290/2191 [04:09<27:08,  1.17it/s, loss=2.2, v_num=647] Epoch 33:  14%|█▎        | 300/2191 [04:17<26:54,  1.17it/s, loss=2.2, v_num=647]Epoch 33:  14%|█▎        | 300/2191 [04:17<26:54,  1.17it/s, loss=2.25, v_num=647]Epoch 33:  14%|█▍        | 310/2191 [04:24<26:41,  1.17it/s, loss=2.25, v_num=647]Epoch 33:  14%|█▍        | 310/2191 [04:24<26:41,  1.17it/s, loss=2.27, v_num=647]Epoch 33:  15%|█▍        | 320/2191 [04:32<26:28,  1.18it/s, loss=2.27, v_num=647]Epoch 33:  15%|█▍        | 320/2191 [04:32<26:28,  1.18it/s, loss=2.21, v_num=647]Epoch 33:  15%|█▌        | 330/2191 [04:39<26:09,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  15%|█▌        | 330/2191 [04:39<26:09,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  16%|█▌        | 340/2191 [04:47<25:58,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  16%|█▌        | 340/2191 [04:47<25:58,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  16%|█▌        | 350/2191 [04:56<25:55,  1.18it/s, loss=2.26, v_num=647]Epoch 33:  16%|█▌        | 350/2191 [04:56<25:55,  1.18it/s, loss=2.26, v_num=647]Epoch 33:  16%|█▋        | 360/2191 [05:04<25:42,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  16%|█▋        | 360/2191 [05:04<25:42,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  17%|█▋        | 370/2191 [05:12<25:35,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  17%|█▋        | 370/2191 [05:12<25:35,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  17%|█▋        | 380/2191 [05:21<25:29,  1.18it/s, loss=2.25, v_num=647]Epoch 33:  17%|█▋        | 380/2191 [05:21<25:29,  1.18it/s, loss=2.27, v_num=647]Epoch 33:  18%|█▊        | 390/2191 [05:28<25:14,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  18%|█▊        | 390/2191 [05:28<25:14,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  18%|█▊        | 400/2191 [05:36<25:03,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  18%|█▊        | 400/2191 [05:36<25:03,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  19%|█▊        | 410/2191 [05:44<24:51,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  19%|█▊        | 410/2191 [05:44<24:51,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  19%|█▉        | 420/2191 [05:52<24:43,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  19%|█▉        | 420/2191 [05:52<24:43,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  20%|█▉        | 430/2191 [05:59<24:28,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  20%|█▉        | 430/2191 [05:59<24:29,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  20%|██        | 440/2191 [06:07<24:18,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  20%|██        | 440/2191 [06:07<24:18,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  21%|██        | 450/2191 [06:15<24:08,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  21%|██        | 450/2191 [06:15<24:08,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  21%|██        | 460/2191 [06:24<24:05,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  21%|██        | 460/2191 [06:24<24:05,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  21%|██▏       | 470/2191 [06:33<23:56,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  21%|██▏       | 470/2191 [06:33<23:56,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  22%|██▏       | 480/2191 [06:42<23:50,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  22%|██▏       | 480/2191 [06:42<23:50,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  22%|██▏       | 490/2191 [06:51<23:45,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  22%|██▏       | 490/2191 [06:51<23:45,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  23%|██▎       | 500/2191 [07:01<23:41,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  23%|██▎       | 500/2191 [07:01<23:41,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  23%|██▎       | 510/2191 [07:08<23:31,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  23%|██▎       | 510/2191 [07:08<23:31,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  24%|██▎       | 520/2191 [07:16<23:18,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  24%|██▎       | 520/2191 [07:16<23:18,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  24%|██▍       | 530/2191 [07:25<23:14,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  24%|██▍       | 530/2191 [07:25<23:14,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  25%|██▍       | 540/2191 [07:33<23:03,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  25%|██▍       | 540/2191 [07:33<23:03,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  25%|██▌       | 550/2191 [07:41<22:54,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  25%|██▌       | 550/2191 [07:41<22:54,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  26%|██▌       | 560/2191 [07:50<22:47,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  26%|██▌       | 570/2191 [07:57<22:34,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  26%|██▌       | 570/2191 [07:57<22:34,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  26%|██▋       | 580/2191 [08:06<22:27,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  26%|██▋       | 580/2191 [08:06<22:27,  1.20it/s, loss=2.22, v_num=647]Epoch 33:  27%|██▋       | 590/2191 [08:14<22:18,  1.20it/s, loss=2.22, v_num=647]Epoch 33:  27%|██▋       | 590/2191 [08:14<22:18,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  27%|██▋       | 600/2191 [08:23<22:12,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  27%|██▋       | 600/2191 [08:23<22:12,  1.19it/s, loss=2.32, v_num=647]Epoch 33:  28%|██▊       | 610/2191 [08:31<22:03,  1.19it/s, loss=2.32, v_num=647]Epoch 33:  28%|██▊       | 610/2191 [08:31<22:03,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  28%|██▊       | 620/2191 [08:39<21:55,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  28%|██▊       | 620/2191 [08:39<21:55,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  29%|██▉       | 630/2191 [08:48<21:47,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  29%|██▉       | 630/2191 [08:48<21:47,  1.19it/s, loss=2.28, v_num=647]Epoch 33:  29%|██▉       | 640/2191 [08:57<21:40,  1.19it/s, loss=2.28, v_num=647]Epoch 33:  29%|██▉       | 640/2191 [08:57<21:40,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  30%|██▉       | 650/2191 [09:05<21:31,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  30%|██▉       | 650/2191 [09:05<21:31,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  30%|███       | 660/2191 [09:14<21:24,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  30%|███       | 660/2191 [09:14<21:24,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  31%|███       | 670/2191 [09:23<21:16,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  31%|███       | 670/2191 [09:23<21:16,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  31%|███       | 680/2191 [09:30<21:06,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  31%|███       | 680/2191 [09:30<21:06,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  31%|███▏      | 690/2191 [09:39<20:59,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  31%|███▏      | 690/2191 [09:39<20:59,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  32%|███▏      | 700/2191 [09:49<20:54,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  32%|███▏      | 700/2191 [09:49<20:54,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  32%|███▏      | 710/2191 [09:56<20:43,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  32%|███▏      | 710/2191 [09:56<20:43,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  33%|███▎      | 720/2191 [10:05<20:35,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  33%|███▎      | 720/2191 [10:05<20:35,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  33%|███▎      | 730/2191 [10:13<20:26,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  33%|███▎      | 730/2191 [10:13<20:26,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  34%|███▍      | 740/2191 [10:22<20:18,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  34%|███▍      | 740/2191 [10:22<20:18,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  34%|███▍      | 750/2191 [10:30<20:10,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  34%|███▍      | 750/2191 [10:30<20:10,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  35%|███▍      | 760/2191 [10:38<20:01,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  35%|███▍      | 760/2191 [10:38<20:01,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  35%|███▌      | 770/2191 [10:47<19:52,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  35%|███▌      | 770/2191 [10:47<19:52,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  36%|███▌      | 780/2191 [10:54<19:42,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  36%|███▌      | 780/2191 [10:54<19:42,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  36%|███▌      | 790/2191 [11:01<19:32,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  36%|███▌      | 790/2191 [11:01<19:32,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  37%|███▋      | 800/2191 [11:09<19:23,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  37%|███▋      | 800/2191 [11:09<19:23,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  37%|███▋      | 810/2191 [11:17<19:14,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  37%|███▋      | 810/2191 [11:17<19:14,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  37%|███▋      | 820/2191 [11:27<19:07,  1.19it/s, loss=2.32, v_num=647]Epoch 33:  37%|███▋      | 820/2191 [11:27<19:07,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  38%|███▊      | 830/2191 [11:35<18:58,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  38%|███▊      | 830/2191 [11:35<18:58,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  38%|███▊      | 840/2191 [11:45<18:52,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  38%|███▊      | 840/2191 [11:45<18:52,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  39%|███▉      | 850/2191 [11:54<18:45,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  39%|███▉      | 850/2191 [11:54<18:45,  1.19it/s, loss=2.2, v_num=647] Epoch 33:  39%|███▉      | 860/2191 [12:03<18:38,  1.19it/s, loss=2.2, v_num=647]Epoch 33:  39%|███▉      | 860/2191 [12:03<18:38,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  40%|███▉      | 870/2191 [12:11<18:29,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  40%|███▉      | 870/2191 [12:11<18:29,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  40%|████      | 880/2191 [12:20<18:21,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  40%|████      | 880/2191 [12:20<18:21,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  41%|████      | 890/2191 [12:27<18:12,  1.19it/s, loss=2.29, v_num=647]Epoch 33:  41%|████      | 890/2191 [12:27<18:12,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  41%|████      | 900/2191 [12:35<18:02,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  41%|████      | 900/2191 [12:35<18:02,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  42%|████▏     | 910/2191 [12:44<17:54,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  42%|████▏     | 910/2191 [12:44<17:54,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  42%|████▏     | 920/2191 [12:52<17:45,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  42%|████▏     | 920/2191 [12:52<17:45,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  42%|████▏     | 930/2191 [13:01<17:38,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  42%|████▏     | 930/2191 [13:01<17:38,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  43%|████▎     | 940/2191 [13:09<17:29,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  43%|████▎     | 940/2191 [13:09<17:29,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  43%|████▎     | 950/2191 [13:16<17:19,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  43%|████▎     | 950/2191 [13:16<17:19,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  44%|████▍     | 960/2191 [13:25<17:12,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  44%|████▍     | 960/2191 [13:25<17:12,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  44%|████▍     | 970/2191 [13:32<17:01,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  44%|████▍     | 970/2191 [13:32<17:01,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  45%|████▍     | 980/2191 [13:40<16:53,  1.19it/s, loss=2.21, v_num=647]Epoch 33:  45%|████▍     | 980/2191 [13:40<16:53,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  45%|████▌     | 990/2191 [13:48<16:43,  1.20it/s, loss=2.22, v_num=647]Epoch 33:  45%|████▌     | 990/2191 [13:48<16:43,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  46%|████▌     | 1000/2191 [13:56<16:35,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  46%|████▌     | 1000/2191 [13:56<16:35,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  46%|████▌     | 1010/2191 [14:06<16:29,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  46%|████▌     | 1010/2191 [14:06<16:29,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  47%|████▋     | 1020/2191 [14:15<16:20,  1.19it/s, loss=2.22, v_num=647]Epoch 33:  47%|████▋     | 1020/2191 [14:15<16:20,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  47%|████▋     | 1030/2191 [14:23<16:11,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  47%|████▋     | 1030/2191 [14:23<16:11,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  47%|████▋     | 1040/2191 [14:30<16:02,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  47%|████▋     | 1040/2191 [14:30<16:02,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  48%|████▊     | 1050/2191 [14:38<15:53,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  48%|████▊     | 1050/2191 [14:38<15:53,  1.20it/s, loss=2.3, v_num=647] Epoch 33:  48%|████▊     | 1060/2191 [14:47<15:46,  1.20it/s, loss=2.3, v_num=647]Epoch 33:  48%|████▊     | 1060/2191 [14:47<15:46,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  49%|████▉     | 1070/2191 [14:57<15:38,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  49%|████▉     | 1070/2191 [14:57<15:38,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  49%|████▉     | 1080/2191 [15:05<15:30,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  49%|████▉     | 1080/2191 [15:05<15:30,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  50%|████▉     | 1090/2191 [15:14<15:22,  1.19it/s, loss=2.23, v_num=647]Epoch 33:  50%|████▉     | 1090/2191 [15:14<15:22,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  50%|█████     | 1100/2191 [15:23<15:15,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  50%|█████     | 1100/2191 [15:23<15:15,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  51%|█████     | 1110/2191 [15:31<15:06,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  51%|█████     | 1110/2191 [15:31<15:06,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  51%|█████     | 1120/2191 [15:39<14:57,  1.19it/s, loss=2.24, v_num=647]Epoch 33:  51%|█████     | 1120/2191 [15:39<14:57,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  52%|█████▏    | 1130/2191 [15:47<14:49,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  52%|█████▏    | 1130/2191 [15:47<14:49,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  52%|█████▏    | 1140/2191 [15:55<14:39,  1.19it/s, loss=2.26, v_num=647]Epoch 33:  52%|█████▏    | 1140/2191 [15:55<14:39,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  52%|█████▏    | 1150/2191 [16:03<14:31,  1.19it/s, loss=2.25, v_num=647]Epoch 33:  52%|█████▏    | 1150/2191 [16:03<14:31,  1.19it/s, loss=2.27, v_num=647]Epoch 33:  53%|█████▎    | 1160/2191 [16:11<14:22,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  53%|█████▎    | 1160/2191 [16:11<14:22,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  53%|█████▎    | 1170/2191 [16:18<14:13,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  53%|█████▎    | 1170/2191 [16:18<14:13,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  54%|█████▍    | 1180/2191 [16:25<14:03,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  54%|█████▍    | 1180/2191 [16:25<14:03,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  54%|█████▍    | 1190/2191 [16:33<13:54,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  54%|█████▍    | 1190/2191 [16:33<13:54,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  55%|█████▍    | 1200/2191 [16:41<13:46,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  55%|█████▍    | 1200/2191 [16:41<13:46,  1.20it/s, loss=2.34, v_num=647]Epoch 33:  55%|█████▌    | 1210/2191 [16:50<13:38,  1.20it/s, loss=2.34, v_num=647]Epoch 33:  55%|█████▌    | 1210/2191 [16:50<13:38,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  56%|█████▌    | 1220/2191 [16:58<13:30,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  56%|█████▌    | 1220/2191 [16:58<13:30,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  56%|█████▌    | 1230/2191 [17:06<13:21,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  56%|█████▌    | 1230/2191 [17:06<13:21,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  57%|█████▋    | 1240/2191 [17:14<13:12,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  57%|█████▋    | 1240/2191 [17:14<13:12,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  57%|█████▋    | 1250/2191 [17:22<13:04,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  57%|█████▋    | 1250/2191 [17:22<13:04,  1.20it/s, loss=2.22, v_num=647]Epoch 33:  58%|█████▊    | 1260/2191 [17:31<12:56,  1.20it/s, loss=2.22, v_num=647]Epoch 33:  58%|█████▊    | 1260/2191 [17:31<12:56,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  58%|█████▊    | 1270/2191 [17:39<12:47,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  58%|█████▊    | 1270/2191 [17:39<12:47,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  58%|█████▊    | 1280/2191 [17:47<12:38,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  58%|█████▊    | 1280/2191 [17:47<12:38,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  59%|█████▉    | 1290/2191 [17:55<12:30,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  59%|█████▉    | 1290/2191 [17:55<12:30,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  59%|█████▉    | 1300/2191 [18:03<12:21,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  59%|█████▉    | 1300/2191 [18:03<12:21,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  60%|█████▉    | 1310/2191 [18:11<12:13,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  60%|█████▉    | 1310/2191 [18:11<12:13,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  60%|██████    | 1320/2191 [18:21<12:05,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  60%|██████    | 1320/2191 [18:21<12:05,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  61%|██████    | 1330/2191 [18:30<11:58,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  61%|██████    | 1330/2191 [18:30<11:58,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  61%|██████    | 1340/2191 [18:37<11:49,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  61%|██████    | 1340/2191 [18:37<11:49,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  62%|██████▏   | 1350/2191 [18:47<11:41,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  62%|██████▏   | 1350/2191 [18:47<11:41,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  62%|██████▏   | 1360/2191 [18:55<11:33,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  62%|██████▏   | 1360/2191 [18:55<11:33,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  63%|██████▎   | 1370/2191 [19:04<11:25,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  63%|██████▎   | 1370/2191 [19:04<11:25,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  63%|██████▎   | 1380/2191 [19:12<11:16,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  63%|██████▎   | 1380/2191 [19:12<11:16,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  63%|██████▎   | 1390/2191 [19:21<11:08,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  63%|██████▎   | 1390/2191 [19:21<11:08,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  64%|██████▍   | 1400/2191 [19:32<11:01,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  64%|██████▍   | 1400/2191 [19:32<11:01,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  64%|██████▍   | 1410/2191 [19:40<10:53,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  64%|██████▍   | 1410/2191 [19:40<10:53,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  65%|██████▍   | 1420/2191 [19:47<10:44,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  65%|██████▍   | 1420/2191 [19:47<10:44,  1.20it/s, loss=2.21, v_num=647]Epoch 33:  65%|██████▌   | 1430/2191 [19:55<10:35,  1.20it/s, loss=2.21, v_num=647]Epoch 33:  65%|██████▌   | 1430/2191 [19:55<10:35,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  66%|██████▌   | 1440/2191 [20:05<10:28,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  66%|██████▌   | 1440/2191 [20:05<10:28,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  66%|██████▌   | 1450/2191 [20:11<10:18,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  66%|██████▌   | 1450/2191 [20:11<10:18,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  67%|██████▋   | 1460/2191 [20:20<10:10,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  67%|██████▋   | 1460/2191 [20:20<10:10,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  67%|██████▋   | 1470/2191 [20:28<10:02,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  67%|██████▋   | 1470/2191 [20:28<10:02,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  68%|██████▊   | 1480/2191 [20:37<09:54,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  68%|██████▊   | 1480/2191 [20:37<09:54,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  68%|██████▊   | 1490/2191 [20:46<09:45,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  68%|██████▊   | 1490/2191 [20:46<09:45,  1.20it/s, loss=2.3, v_num=647] Epoch 33:  68%|██████▊   | 1500/2191 [20:54<09:37,  1.20it/s, loss=2.3, v_num=647]Epoch 33:  68%|██████▊   | 1500/2191 [20:54<09:37,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  69%|██████▉   | 1510/2191 [21:02<09:28,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  69%|██████▉   | 1510/2191 [21:02<09:28,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  69%|██████▉   | 1520/2191 [21:10<09:20,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  69%|██████▉   | 1520/2191 [21:10<09:20,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  70%|██████▉   | 1530/2191 [21:19<09:12,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  70%|██████▉   | 1530/2191 [21:19<09:12,  1.20it/s, loss=2.2, v_num=647] Epoch 33:  70%|███████   | 1540/2191 [21:29<09:04,  1.20it/s, loss=2.2, v_num=647]Epoch 33:  70%|███████   | 1540/2191 [21:29<09:04,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  71%|███████   | 1550/2191 [21:36<08:55,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  71%|███████   | 1550/2191 [21:36<08:55,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  71%|███████   | 1560/2191 [21:44<08:47,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  71%|███████   | 1560/2191 [21:44<08:47,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  72%|███████▏  | 1570/2191 [21:54<08:39,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  72%|███████▏  | 1570/2191 [21:54<08:39,  1.20it/s, loss=2.34, v_num=647]Epoch 33:  72%|███████▏  | 1580/2191 [22:03<08:31,  1.19it/s, loss=2.34, v_num=647]Epoch 33:  72%|███████▏  | 1580/2191 [22:03<08:31,  1.19it/s, loss=2.28, v_num=647]Epoch 33:  73%|███████▎  | 1590/2191 [22:11<08:22,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  73%|███████▎  | 1590/2191 [22:11<08:22,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  73%|███████▎  | 1600/2191 [22:19<08:14,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  73%|███████▎  | 1600/2191 [22:19<08:14,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  73%|███████▎  | 1610/2191 [22:27<08:05,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  73%|███████▎  | 1610/2191 [22:27<08:05,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  74%|███████▍  | 1620/2191 [22:35<07:57,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  74%|███████▍  | 1620/2191 [22:35<07:57,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  74%|███████▍  | 1630/2191 [22:42<07:48,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  74%|███████▍  | 1630/2191 [22:42<07:48,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  75%|███████▍  | 1640/2191 [22:50<07:40,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  75%|███████▍  | 1640/2191 [22:50<07:40,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  75%|███████▌  | 1650/2191 [22:59<07:31,  1.20it/s, loss=2.31, v_num=647]Epoch 33:  75%|███████▌  | 1650/2191 [22:59<07:31,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  76%|███████▌  | 1660/2191 [23:07<07:23,  1.20it/s, loss=2.29, v_num=647]Epoch 33:  76%|███████▌  | 1660/2191 [23:07<07:23,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  76%|███████▌  | 1670/2191 [23:15<07:15,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  76%|███████▌  | 1670/2191 [23:15<07:15,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  77%|███████▋  | 1680/2191 [23:22<07:06,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  77%|███████▋  | 1680/2191 [23:22<07:06,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  77%|███████▋  | 1690/2191 [23:31<06:58,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  77%|███████▋  | 1690/2191 [23:31<06:58,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  78%|███████▊  | 1700/2191 [23:38<06:49,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  78%|███████▊  | 1700/2191 [23:38<06:49,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  78%|███████▊  | 1710/2191 [23:45<06:40,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  78%|███████▊  | 1710/2191 [23:45<06:40,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  79%|███████▊  | 1720/2191 [23:52<06:32,  1.20it/s, loss=2.23, v_num=647]Epoch 33:  79%|███████▊  | 1720/2191 [23:52<06:32,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  79%|███████▉  | 1730/2191 [24:00<06:23,  1.20it/s, loss=2.27, v_num=647]Epoch 33:  79%|███████▉  | 1730/2191 [24:00<06:23,  1.20it/s, loss=2.3, v_num=647] Epoch 33:  79%|███████▉  | 1740/2191 [24:09<06:15,  1.20it/s, loss=2.3, v_num=647]Epoch 33:  79%|███████▉  | 1740/2191 [24:09<06:15,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  80%|███████▉  | 1750/2191 [24:16<06:06,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  80%|███████▉  | 1750/2191 [24:16<06:06,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  80%|████████  | 1760/2191 [24:23<05:58,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  80%|████████  | 1760/2191 [24:23<05:58,  1.20it/s, loss=2.3, v_num=647] Epoch 33:  81%|████████  | 1770/2191 [24:31<05:49,  1.20it/s, loss=2.3, v_num=647]Epoch 33:  81%|████████  | 1770/2191 [24:31<05:49,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  81%|████████  | 1780/2191 [24:41<05:41,  1.20it/s, loss=2.32, v_num=647]Epoch 33:  81%|████████  | 1780/2191 [24:41<05:41,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  82%|████████▏ | 1790/2191 [24:48<05:33,  1.20it/s, loss=2.28, v_num=647]Epoch 33:  82%|████████▏ | 1790/2191 [24:48<05:33,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  82%|████████▏ | 1800/2191 [24:57<05:25,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  82%|████████▏ | 1800/2191 [24:57<05:25,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  83%|████████▎ | 1810/2191 [25:05<05:16,  1.20it/s, loss=2.24, v_num=647]Epoch 33:  83%|████████▎ | 1810/2191 [25:05<05:16,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  83%|████████▎ | 1820/2191 [25:13<05:08,  1.20it/s, loss=2.26, v_num=647]Epoch 33:  83%|████████▎ | 1820/2191 [25:13<05:08,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  84%|████████▎ | 1830/2191 [25:20<04:59,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  84%|████████▎ | 1830/2191 [25:20<04:59,  1.20it/s, loss=2.25, v_num=647]Epoch 33:  84%|████████▍ | 1840/2191 [25:27<04:51,  1.21it/s, loss=2.25, v_num=647]Epoch 33:  84%|████████▍ | 1840/2191 [25:27<04:51,  1.21it/s, loss=2.28, v_num=647]Epoch 33:  84%|████████▍ | 1850/2191 [25:30<04:41,  1.21it/s, loss=2.28, v_num=647]Epoch 33:  84%|████████▍ | 1850/2191 [25:30<04:41,  1.21it/s, loss=2.29, v_num=647]validation_epoch_end
graph acc: 0.3801916932907348
valid accuracy: 0.9782626628875732
alidation_epoch_end
graph acc: 0.43769968051118213
valid accuracy: 0.9764375686645508
validation_epoch_end
graph acc: 0.3738019169329074
valid accuracy: 0.9761337637901306
1 [25:36<04:23,  1.22it/s, loss=2.29, v_num=647]Epoch 33:  85%|████████▌ | 1870/2191 [25:36<04:23,  1.22it/s, loss=2.24, v_num=647]validation_epoch_end
graph acc: 0.40894568690095845
valid accuracy: 0.9800164699554443
Epoch 33:  86%|████████▌ | 1880/2191 [25:38<04:14,  1.22it/s, loss=2.24, v_num=647]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/313 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.3610223642172524
valid accuracy: 0.975616991519928
validation_epoch_end
graph acc: 0.43130990415335463
valid accuracy: 0.9792081713676453

Validating:   3%|▎         | 10/313 [00:01<00:44,  6.80it/s][AEpoch 33:  86%|████████▋ | 1890/2191 [25:39<04:05,  1.23it/s, loss=2.24, v_num=647]
Validating:   6%|▋         | 20/313 [00:03<00:47,  6.16it/s][AEpoch 33:  87%|████████▋ | 1900/2191 [25:41<03:55,  1.23it/s, loss=2.24, v_num=647]
Validating:  10%|▉         | 30/313 [00:03<00:32,  8.80it/s][AEpoch 33:  87%|████████▋ | 1910/2191 [25:42<03:46,  1.24it/s, loss=2.24, v_num=647]
Validating:  13%|█▎        | 40/313 [00:04<00:27,  9.96it/s][AEpoch 33:  88%|████████▊ | 1920/2191 [25:43<03:37,  1.24it/s, loss=2.24, v_num=647]
Validating:  16%|█▌        | 50/313 [00:05<00:24, 10.72it/s][AEpoch 33:  88%|████████▊ | 1930/2191 [25:43<03:28,  1.25it/s, loss=2.24, v_num=647]
Validating:  19%|█▉        | 60/313 [00:06<00:24, 10.44it/s][AEpoch 33:  89%|████████▊ | 1940/2191 [25:44<03:19,  1.26it/s, loss=2.24, v_num=647]
Validating:  22%|██▏       | 70/313 [00:07<00:21, 11.23it/s][AEpoch 33:  89%|████████▉ | 1950/2191 [25:45<03:10,  1.26it/s, loss=2.24, v_num=647]
Validating:  26%|██▌       | 80/313 [00:08<00:20, 11.15it/s][AEpoch 33:  89%|████████▉ | 1960/2191 [25:46<03:02,  1.27it/s, loss=2.24, v_num=647]
Validating:  29%|██▉       | 90/313 [00:09<00:21, 10.43it/s][AEpoch 33:  90%|████████▉ | 1970/2191 [25:47<02:53,  1.27it/s, loss=2.24, v_num=647]
Validating:  32%|███▏      | 100/313 [00:09<00:18, 11.49it/s][AEpoch 33:  90%|█████████ | 1980/2191 [25:48<02:44,  1.28it/s, loss=2.24, v_num=647]
Validating:  35%|███▌      | 110/313 [00:11<00:20,  9.94it/s][AEpoch 33:  91%|█████████ | 1990/2191 [25:49<02:36,  1.28it/s, loss=2.24, v_num=647]
Validating:  38%|███▊      | 120/313 [00:12<00:18, 10.22it/s][AEpoch 33:  91%|█████████▏| 2000/2191 [25:50<02:27,  1.29it/s, loss=2.24, v_num=647]
Validating:  42%|████▏     | 130/313 [00:13<00:19,  9.29it/s][AEpoch 33:  92%|█████████▏| 2010/2191 [25:51<02:19,  1.30it/s, loss=2.24, v_num=647]
Validating:  45%|████▍     | 140/313 [00:14<00:20,  8.41it/s][AEpoch 33:  92%|█████████▏| 2020/2191 [25:53<02:11,  1.30it/s, loss=2.24, v_num=647]
Validating:  48%|████▊     | 150/313 [00:15<00:18,  9.02it/s][AEpoch 33:  93%|█████████▎| 2030/2191 [25:54<02:03,  1.31it/s, loss=2.24, v_num=647]
Validating:  51%|█████     | 160/313 [00:16<00:16,  9.48it/s][AEpoch 33:  93%|█████████▎| 2040/2191 [25:55<01:55,  1.31it/s, loss=2.24, v_num=647]
Validating:  54%|█████▍    | 170/313 [00:17<00:13, 10.79it/s][AEpoch 33:  94%|█████████▎| 2050/2191 [25:55<01:46,  1.32it/s, loss=2.24, v_num=647]
Validating:  58%|█████▊    | 180/313 [00:18<00:13, 10.07it/s][AEpoch 33:  94%|█████████▍| 2060/2191 [25:56<01:38,  1.32it/s, loss=2.24, v_num=647]
Validating:  61%|██████    | 190/313 [00:18<00:10, 11.58it/s][AEpoch 33:  94%|█████████▍| 2070/2191 [25:57<01:30,  1.33it/s, loss=2.24, v_num=647]
Validating:  64%|██████▍   | 200/313 [00:19<00:10, 11.24it/s][AEpoch 33:  95%|█████████▍| 2080/2191 [25:58<01:23,  1.34it/s, loss=2.24, v_num=647]
Validating:  67%|██████▋   | 210/313 [00:20<00:09, 10.84it/s][AEpoch 33:  95%|█████████▌| 2090/2191 [25:59<01:15,  1.34it/s, loss=2.24, v_num=647]
Validating:  70%|███████   | 220/313 [00:22<00:09,  9.36it/s][AEpoch 33:  96%|█████████▌| 2100/2191 [26:00<01:07,  1.35it/s, loss=2.24, v_num=647]
Validating:  73%|███████▎  | 230/313 [00:23<00:08, 10.07it/s][AEpoch 33:  96%|█████████▋| 2110/2191 [26:01<00:59,  1.35it/s, loss=2.24, v_num=647]
Validating:  77%|███████▋  | 240/313 [00:24<00:07,  9.99it/s][AEpoch 33:  97%|█████████▋| 2120/2191 [26:02<00:52,  1.36it/s, loss=2.24, v_num=647]
Validating:  80%|███████▉  | 250/313 [00:25<00:06, 10.17it/s][AEpoch 33:  97%|█████████▋| 2130/2191 [26:03<00:44,  1.36it/s, loss=2.24, v_num=647]
Validating:  83%|████████▎ | 260/313 [00:26<00:05, 10.06it/s][AEpoch 33:  98%|█████████▊| 2140/2191 [26:04<00:37,  1.37it/s, loss=2.24, v_num=647]
Validating:  86%|████████▋ | 270/313 [00:26<00:03, 11.28it/s][AEpoch 33:  98%|█████████▊| 2150/2191 [26:05<00:29,  1.37it/s, loss=2.24, v_num=647]
Validating:  89%|████████▉ | 280/313 [00:27<00:02, 12.66it/s][AEpoch 33:  99%|█████████▊| 2160/2191 [26:05<00:22,  1.38it/s, loss=2.24, v_num=647]
Validating:  93%|█████████▎| 290/313 [00:28<00:01, 12.59it/s][AEpoch 33:  99%|█████████▉| 2170/2191 [26:06<00:15,  1.39it/s, loss=2.24, v_num=647]
Validating:  96%|█████████▌| 300/313 [00:28<00:00, 13.57it/s][AEpoch 33:  99%|█████████▉| 2180/2191 [26:07<00:07,  1.39it/s, loss=2.24, v_num=647]
Validating:  99%|█████████▉| 310/313 [00:29<00:00, 13.80it/s][AEpoch 33: 100%|█████████▉| 2190/2191 [26:07<00:00,  1.40it/s, loss=2.24, v_num=647]validation_epoch_end
graph acc: 0.43450479233226835
valid accuracy: 0.9787448644638062
Epoch 33: 100%|██████████| 2191/2191 [26:10<00:00,  1.40it/s, loss=2.21, v_num=647]
                                                             [AEpoch 33:   0%|          | 0/2191 [00:00<00:00, 9822.73it/s, loss=2.21, v_num=647] Epoch 34:   0%|          | 0/2191 [00:00<00:00, 2304.56it/s, loss=2.21, v_num=647]Epoch 34:   0%|          | 10/2191 [00:11<39:25,  1.08s/it, loss=2.21, v_num=647] Epoch 34:   0%|          | 10/2191 [00:11<39:25,  1.08s/it, loss=2.23, v_num=647]Epoch 34:   1%|          | 20/2191 [00:20<34:51,  1.04it/s, loss=2.23, v_num=647]Epoch 34:   1%|          | 20/2191 [00:20<34:51,  1.04it/s, loss=2.2, v_num=647] Epoch 34:   1%|▏         | 30/2191 [00:30<35:16,  1.02it/s, loss=2.2, v_num=647]Epoch 34:   1%|▏         | 30/2191 [00:30<35:16,  1.02it/s, loss=2.16, v_num=647]Epoch 34:   2%|▏         | 40/2191 [00:41<36:17,  1.01s/it, loss=2.16, v_num=647]Epoch 34:   2%|▏         | 40/2191 [00:41<36:17,  1.01s/it, loss=2.17, v_num=647]Epoch 34:   2%|▏         | 50/2191 [00:49<34:45,  1.03it/s, loss=2.17, v_num=647]Epoch 34:   2%|▏         | 50/2191 [00:49<34:45,  1.03it/s, loss=2.21, v_num=647]Epoch 34:   3%|▎         | 60/2191 [00:58<33:57,  1.05it/s, loss=2.21, v_num=647]Epoch 34:   3%|▎         | 60/2191 [00:58<33:57,  1.05it/s, loss=2.26, v_num=647]Epoch 34:   3%|▎         | 70/2191 [01:07<33:35,  1.05it/s, loss=2.26, v_num=647]Epoch 34:   3%|▎         | 70/2191 [01:07<33:35,  1.05it/s, loss=2.28, v_num=647]Epoch 34:   4%|▎         | 80/2191 [01:17<33:43,  1.04it/s, loss=2.28, v_num=647]Epoch 34:   4%|▎         | 80/2191 [01:17<33:43,  1.04it/s, loss=2.25, v_num=647]Epoch 34:   4%|▍         | 90/2191 [01:25<32:54,  1.06it/s, loss=2.25, v_num=647]Epoch 34:   4%|▍         | 90/2191 [01:25<32:54,  1.06it/s, loss=2.24, v_num=647]Epoch 34:   5%|▍         | 100/2191 [01:33<32:09,  1.08it/s, loss=2.24, v_num=647]Epoch 34:   5%|▍         | 100/2191 [01:33<32:09,  1.08it/s, loss=2.25, v_num=647]Epoch 34:   5%|▌         | 110/2191 [01:41<31:41,  1.09it/s, loss=2.25, v_num=647]Epoch 34:   5%|▌         | 110/2191 [01:41<31:41,  1.09it/s, loss=2.26, v_num=647]Epoch 34:   5%|▌         | 120/2191 [01:49<31:18,  1.10it/s, loss=2.26, v_num=647]Epoch 34:   5%|▌         | 120/2191 [01:49<31:18,  1.10it/s, loss=2.22, v_num=647]Epoch 34:   6%|▌         | 130/2191 [01:58<31:11,  1.10it/s, loss=2.22, v_num=647]Epoch 34:   6%|▌         | 130/2191 [01:58<31:11,  1.10it/s, loss=2.22, v_num=647]Epoch 34:   6%|▋         | 140/2191 [02:06<30:45,  1.11it/s, loss=2.22, v_num=647]Epoch 34:   6%|▋         | 140/2191 [02:06<30:45,  1.11it/s, loss=2.23, v_num=647]Epoch 34:   7%|▋         | 150/2191 [02:14<30:22,  1.12it/s, loss=2.23, v_num=647]Epoch 34:   7%|▋         | 150/2191 [02:14<30:22,  1.12it/s, loss=2.18, v_num=647]Epoch 34:   7%|▋         | 160/2191 [02:22<30:02,  1.13it/s, loss=2.18, v_num=647]Epoch 34:   7%|▋         | 160/2191 [02:22<30:02,  1.13it/s, loss=2.18, v_num=647]Epoch 34:   8%|▊         | 170/2191 [02:31<29:48,  1.13it/s, loss=2.18, v_num=647]Epoch 34:   8%|▊         | 170/2191 [02:31<29:48,  1.13it/s, loss=2.18, v_num=647]Epoch 34:   8%|▊         | 180/2191 [02:40<29:43,  1.13it/s, loss=2.18, v_num=647]Epoch 34:   8%|▊         | 180/2191 [02:40<29:43,  1.13it/s, loss=2.19, v_num=647]Epoch 34:   9%|▊         | 190/2191 [02:48<29:23,  1.13it/s, loss=2.19, v_num=647]Epoch 34:   9%|▊         | 190/2191 [02:48<29:23,  1.13it/s, loss=2.23, v_num=647]Epoch 34:   9%|▉         | 200/2191 [02:55<28:55,  1.15it/s, loss=2.23, v_num=647]Epoch 34:   9%|▉         | 200/2191 [02:55<28:55,  1.15it/s, loss=2.26, v_num=647]Epoch 34:  10%|▉         | 210/2191 [03:04<28:51,  1.14it/s, loss=2.26, v_num=647]Epoch 34:  10%|▉         | 210/2191 [03:04<28:51,  1.14it/s, loss=2.24, v_num=647]Epoch 34:  10%|█         | 220/2191 [03:12<28:36,  1.15it/s, loss=2.24, v_num=647]Epoch 34:  10%|█         | 220/2191 [03:12<28:36,  1.15it/s, loss=2.25, v_num=647]Epoch 34:  10%|█         | 230/2191 [03:21<28:28,  1.15it/s, loss=2.25, v_num=647]Epoch 34:  10%|█         | 230/2191 [03:21<28:28,  1.15it/s, loss=2.27, v_num=647]Epoch 34:  11%|█         | 240/2191 [03:29<28:16,  1.15it/s, loss=2.27, v_num=647]Epoch 34:  11%|█         | 240/2191 [03:29<28:16,  1.15it/s, loss=2.23, v_num=647]Epoch 34:  11%|█▏        | 250/2191 [03:35<27:49,  1.16it/s, loss=2.23, v_num=647]Epoch 34:  11%|█▏        | 250/2191 [03:35<27:49,  1.16it/s, loss=2.23, v_num=647]Epoch 34:  12%|█▏        | 260/2191 [03:45<27:45,  1.16it/s, loss=2.23, v_num=647]Epoch 34:  12%|█▏        | 260/2191 [03:45<27:45,  1.16it/s, loss=2.24, v_num=647]Epoch 34:  12%|█▏        | 270/2191 [03:55<27:50,  1.15it/s, loss=2.24, v_num=647]Epoch 34:  12%|█▏        | 270/2191 [03:55<27:50,  1.15it/s, loss=2.23, v_num=647]Epoch 34:  13%|█▎        | 280/2191 [04:03<27:34,  1.15it/s, loss=2.23, v_num=647]Epoch 34:  13%|█▎        | 280/2191 [04:03<27:34,  1.15it/s, loss=2.24, v_num=647]Epoch 34:  13%|█▎        | 290/2191 [04:11<27:26,  1.15it/s, loss=2.24, v_num=647]Epoch 34:  13%|█▎        | 290/2191 [04:11<27:26,  1.15it/s, loss=2.24, v_num=647]Epoch 34:  14%|█▎        | 300/2191 [04:20<27:16,  1.16it/s, loss=2.24, v_num=647]Epoch 34:  14%|█▎        | 300/2191 [04:20<27:16,  1.16it/s, loss=2.23, v_num=647]Epoch 34:  14%|█▍        | 310/2191 [04:29<27:07,  1.16it/s, loss=2.23, v_num=647]Epoch 34:  14%|█▍        | 310/2191 [04:29<27:07,  1.16it/s, loss=2.25, v_num=647]Epoch 34:  15%|█▍        | 320/2191 [04:38<27:00,  1.15it/s, loss=2.25, v_num=647]Epoch 34:  15%|█▍        | 320/2191 [04:38<27:00,  1.15it/s, loss=2.25, v_num=647]Epoch 34:  15%|█▌        | 330/2191 [04:44<26:41,  1.16it/s, loss=2.25, v_num=647]Epoch 34:  15%|█▌        | 330/2191 [04:44<26:41,  1.16it/s, loss=2.27, v_num=647]Epoch 34:  16%|█▌        | 340/2191 [04:52<26:28,  1.17it/s, loss=2.27, v_num=647]Epoch 34:  16%|█▌        | 340/2191 [04:52<26:28,  1.17it/s, loss=2.28, v_num=647]Epoch 34:  16%|█▌        | 350/2191 [05:00<26:16,  1.17it/s, loss=2.28, v_num=647]Epoch 34:  16%|█▌        | 350/2191 [05:00<26:17,  1.17it/s, loss=2.24, v_num=647]Epoch 34:  16%|█▋        | 360/2191 [05:08<26:02,  1.17it/s, loss=2.24, v_num=647]Epoch 34:  16%|█▋        | 360/2191 [05:08<26:02,  1.17it/s, loss=2.24, v_num=647]Epoch 34:  17%|█▋        | 370/2191 [05:15<25:46,  1.18it/s, loss=2.24, v_num=647]Epoch 34:  17%|█▋        | 370/2191 [05:15<25:46,  1.18it/s, loss=2.26, v_num=647]Epoch 34:  17%|█▋        | 380/2191 [05:23<25:36,  1.18it/s, loss=2.26, v_num=647]Epoch 34:  17%|█▋        | 380/2191 [05:23<25:36,  1.18it/s, loss=2.26, v_num=647]Epoch 34:  18%|█▊        | 390/2191 [05:33<25:36,  1.17it/s, loss=2.26, v_num=647]Epoch 34:  18%|█▊        | 390/2191 [05:33<25:36,  1.17it/s, loss=2.23, v_num=647]Epoch 34:  18%|█▊        | 400/2191 [05:40<25:22,  1.18it/s, loss=2.23, v_num=647]Epoch 34:  18%|█▊        | 400/2191 [05:40<25:22,  1.18it/s, loss=2.23, v_num=647]Epoch 34:  19%|█▊        | 410/2191 [05:48<25:08,  1.18it/s, loss=2.23, v_num=647]Epoch 34:  19%|█▊        | 410/2191 [05:48<25:08,  1.18it/s, loss=2.25, v_num=647]Epoch 34:  19%|█▉        | 420/2191 [05:55<24:56,  1.18it/s, loss=2.25, v_num=647]Epoch 34:  19%|█▉        | 420/2191 [05:55<24:56,  1.18it/s, loss=2.26, v_num=647]Epoch 34:  20%|█▉        | 430/2191 [06:02<24:40,  1.19it/s, loss=2.26, v_num=647]Epoch 34:  20%|█▉        | 430/2191 [06:02<24:40,  1.19it/s, loss=2.25, v_num=647]Epoch 34:  20%|██        | 440/2191 [06:10<24:30,  1.19it/s, loss=2.25, v_num=647]Epoch 34:  20%|██        | 440/2191 [06:10<24:30,  1.19it/s, loss=2.23, v_num=647]Epoch 34:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.23, v_num=647]Epoch 34:  21%|██        | 450/2191 [06:19<24:25,  1.19it/s, loss=2.26, v_num=647]Epoch 34:  21%|██        | 460/2191 [06:27<24:15,  1.19it/s, loss=2.26, v_num=647]Epoch 34:  21%|██        | 460/2191 [06:27<24:15,  1.19it/s, loss=2.24, v_num=647]Epoch 34:  21%|██▏       | 470/2191 [06:34<24:02,  1.19it/s, loss=2.24, v_num=647]Epoch 34:  21%|██▏       | 470/2191 [06:34<24:02,  1.19it/s, loss=2.22, v_num=647]Epoch 34:  22%|██▏       | 480/2191 [06:43<23:54,  1.19it/s, loss=2.22, v_num=647]Epoch 34:  22%|██▏       | 480/2191 [06:43<23:54,  1.19it/s, loss=2.23, v_num=647]Epoch 34:  22%|██▏       | 490/2191 [06:50<23:42,  1.20it/s, loss=2.23, v_num=647]Epoch 34:  22%|██▏       | 490/2191 [06:50<23:42,  1.20it/s, loss=2.26, v_num=647]Epoch 34:  23%|██▎       | 500/2191 [06:59<23:35,  1.19it/s, loss=2.26, v_num=647]Epoch 34:  23%|██▎       | 500/2191 [06:59<23:35,  1.19it/s, loss=2.27, v_num=647]Epoch 34:  23%|██▎       | 510/2191 [07:06<23:23,  1.20it/s, loss=2.27, v_num=647]Epoch 34:  23%|██▎       | 510/2191 [07:06<23:23,  1.20it/s, loss=2.27, v_num=647]Epoch 34:  24%|██▎       | 520/2191 [07:14<23:14,  1.20it/s, loss=2.27, v_num=647]Epoch 34:  24%|██▎       | 520/2191 [07:14<23:14,  1.20it/s, loss=2.26, v_num=647]Epoch 34:  24%|██▍       | 530/2191 [07:24<23:09,  1.20it/s, loss=2.26, v_num=647]Epoch 34:  24%|██▍       | 530/2191 [07:24<23:09,  1.20it/s, loss=2.25, v_num=647]Epoch 34:  25%|██▍       | 540/2191 [07:31<22:57,  1.20it/s, loss=2.25, v_num=647]Epoch 34:  25%|██▍       | 540/2191 [07:31<22:57,  1.20it/s, loss=2.25, v_num=647]Epoch 34:  25%|██▌       | 550/2191 [07:38<22:45,  1.20it/s, loss=2.25, v_num=647]Epoch 34:  25%|██▌       | 550/2191 [07:38<22:45,  1.20it/s, loss=2.23, v_num=647]Epoch 34:  26%|██▌       | 560/2191 [07:47<22:38,  1.20it/s, loss=2.23, v_num=647]Epoch 34:  26%|██▌       | 560/2191 [07:47<22:38,  1.20it/s, loss=2.22, v_num=647]Epoch 34:  26%|██▌       | 570/2191 [07:54<22:26,  1.20it/s, loss=2.22, v_num=647]Epoch 34:  26%|██▌       | 570/2191 [07:54<22:26,  1.20it/s, loss=2.24, v_num=647]Epoch 34:  26%|██▋       | 580/2191 [08:01<22:14,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  26%|██▋       | 580/2191 [08:01<22:14,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  27%|██▋       | 590/2191 [08:08<22:04,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  27%|██▋       | 590/2191 [08:08<22:04,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  27%|██▋       | 600/2191 [08:16<21:54,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  27%|██▋       | 600/2191 [08:16<21:54,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  28%|██▊       | 610/2191 [08:25<21:47,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  28%|██▊       | 610/2191 [08:25<21:47,  1.21it/s, loss=2.21, v_num=647]Epoch 34:  28%|██▊       | 620/2191 [08:33<21:38,  1.21it/s, loss=2.21, v_num=647]Epoch 34:  28%|██▊       | 620/2191 [08:33<21:38,  1.21it/s, loss=2.21, v_num=647]Epoch 34:  29%|██▉       | 630/2191 [08:41<21:29,  1.21it/s, loss=2.21, v_num=647]Epoch 34:  29%|██▉       | 630/2191 [08:41<21:29,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  29%|██▉       | 640/2191 [08:48<21:19,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  29%|██▉       | 640/2191 [08:48<21:19,  1.21it/s, loss=2.29, v_num=647]Epoch 34:  30%|██▉       | 650/2191 [08:57<21:11,  1.21it/s, loss=2.29, v_num=647]Epoch 34:  30%|██▉       | 650/2191 [08:57<21:11,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  30%|███       | 660/2191 [09:06<21:04,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  30%|███       | 660/2191 [09:06<21:04,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  31%|███       | 670/2191 [09:14<20:55,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  31%|███       | 670/2191 [09:14<20:55,  1.21it/s, loss=2.3, v_num=647] Epoch 34:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.3, v_num=647]Epoch 34:  31%|███       | 680/2191 [09:22<20:48,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  31%|███▏      | 690/2191 [09:31<20:41,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  31%|███▏      | 690/2191 [09:31<20:41,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  32%|███▏      | 700/2191 [09:38<20:31,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  32%|███▏      | 700/2191 [09:38<20:31,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  32%|███▏      | 710/2191 [09:46<20:21,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  32%|███▏      | 710/2191 [09:46<20:21,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  33%|███▎      | 720/2191 [09:54<20:12,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  33%|███▎      | 720/2191 [09:54<20:12,  1.21it/s, loss=2.27, v_num=647]Epoch 34:  33%|███▎      | 730/2191 [10:03<20:06,  1.21it/s, loss=2.27, v_num=647]Epoch 34:  33%|███▎      | 730/2191 [10:03<20:06,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  34%|███▍      | 740/2191 [10:11<19:57,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  34%|███▍      | 740/2191 [10:11<19:57,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  34%|███▍      | 750/2191 [10:19<19:49,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  34%|███▍      | 750/2191 [10:19<19:49,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  35%|███▍      | 760/2191 [10:27<19:40,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  35%|███▍      | 760/2191 [10:27<19:40,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  35%|███▌      | 770/2191 [10:35<19:32,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  35%|███▌      | 770/2191 [10:35<19:32,  1.21it/s, loss=2.22, v_num=647]Epoch 34:  36%|███▌      | 780/2191 [10:44<19:24,  1.21it/s, loss=2.22, v_num=647]Epoch 34:  36%|███▌      | 780/2191 [10:44<19:24,  1.21it/s, loss=2.22, v_num=647]Epoch 34:  36%|███▌      | 790/2191 [10:52<19:16,  1.21it/s, loss=2.22, v_num=647]Epoch 34:  36%|███▌      | 790/2191 [10:52<19:16,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  37%|███▋      | 800/2191 [11:00<19:06,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  37%|███▋      | 800/2191 [11:00<19:06,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  37%|███▋      | 810/2191 [11:08<18:57,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  37%|███▋      | 810/2191 [11:08<18:57,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  37%|███▋      | 820/2191 [11:18<18:53,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  37%|███▋      | 820/2191 [11:18<18:53,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  38%|███▊      | 830/2191 [11:25<18:43,  1.21it/s, loss=2.28, v_num=647]Epoch 34:  38%|███▊      | 830/2191 [11:25<18:43,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  38%|███▊      | 840/2191 [11:34<18:36,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  38%|███▊      | 840/2191 [11:34<18:36,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  39%|███▉      | 850/2191 [11:41<18:25,  1.21it/s, loss=2.24, v_num=647]Epoch 34:  39%|███▉      | 850/2191 [11:41<18:25,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  39%|███▉      | 860/2191 [11:49<18:16,  1.21it/s, loss=2.23, v_num=647]Epoch 34:  39%|███▉      | 860/2191 [11:49<18:16,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  40%|███▉      | 870/2191 [11:56<18:06,  1.22it/s, loss=2.29, v_num=647]Epoch 34:  40%|████      | 880/2191 [12:04<17:58,  1.22it/s, loss=2.29, v_num=647]Epoch 34:  40%|████      | 880/2191 [12:04<17:58,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  41%|████      | 890/2191 [12:12<17:48,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  41%|████      | 890/2191 [12:12<17:48,  1.22it/s, loss=2.24, v_num=647]Epoch 34:  41%|████      | 900/2191 [12:20<17:41,  1.22it/s, loss=2.24, v_num=647]Epoch 34:  41%|████      | 900/2191 [12:20<17:41,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  42%|████▏     | 910/2191 [12:27<17:31,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  42%|████▏     | 910/2191 [12:27<17:31,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  42%|████▏     | 920/2191 [12:35<17:22,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  42%|████▏     | 920/2191 [12:35<17:22,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  42%|████▏     | 930/2191 [12:44<17:15,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  42%|████▏     | 930/2191 [12:44<17:15,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  43%|████▎     | 940/2191 [12:54<17:09,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  43%|████▎     | 940/2191 [12:54<17:09,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  43%|████▎     | 950/2191 [13:01<16:59,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  43%|████▎     | 950/2191 [13:01<16:59,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  44%|████▍     | 960/2191 [13:09<16:50,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  44%|████▍     | 960/2191 [13:09<16:50,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  44%|████▍     | 970/2191 [13:17<16:42,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  44%|████▍     | 970/2191 [13:17<16:42,  1.22it/s, loss=2.28, v_num=647]Epoch 34:  45%|████▍     | 980/2191 [13:25<16:34,  1.22it/s, loss=2.28, v_num=647]Epoch 34:  45%|████▍     | 980/2191 [13:25<16:34,  1.22it/s, loss=2.29, v_num=647]Epoch 34:  45%|████▌     | 990/2191 [13:33<16:26,  1.22it/s, loss=2.29, v_num=647]Epoch 34:  45%|████▌     | 990/2191 [13:33<16:26,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  46%|████▌     | 1000/2191 [13:43<16:19,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  46%|████▌     | 1000/2191 [13:43<16:19,  1.22it/s, loss=2.26, v_num=647]Epoch 34:  46%|████▌     | 1010/2191 [13:53<16:13,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  46%|████▌     | 1010/2191 [13:53<16:13,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  47%|████▋     | 1020/2191 [14:01<16:05,  1.21it/s, loss=2.25, v_num=647]Epoch 34:  47%|████▋     | 1020/2191 [14:01<16:05,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  47%|████▋     | 1030/2191 [14:08<15:55,  1.21it/s, loss=2.26, v_num=647]Epoch 34:  47%|████▋     | 1030/2191 [14:08<15:55,  1.21it/s, loss=2.3, v_num=647] Epoch 34:  47%|████▋     | 1040/2191 [14:16<15:47,  1.22it/s, loss=2.3, v_num=647]Epoch 34:  47%|████▋     | 1040/2191 [14:16<15:47,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  48%|████▊     | 1050/2191 [14:24<15:38,  1.22it/s, loss=2.27, v_num=647]Epoch 34:  48%|████▊     | 1050/2191 [14:24<15:38,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  48%|████▊     | 1060/2191 [14:32<15:29,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  48%|████▊     | 1060/2191 [14:32<15:29,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  49%|████▉     | 1070/2191 [14:40<15:21,  1.22it/s, loss=2.25, v_num=647]Epoch 34:  49%|████▉     | 1070/2191 [14:40<15:21,  1.22it/s, loss=2.3, v_num=647] Epoch 34:  49%|████▉     | 1080/2191 [14:47<15:11,  1.22it/s, loss=2.3, v_num=647]Epoch 34:  49%|████▉     | 1080/2191 [14:47<15:11,  1.22it/s, loss=2.32, v_num=647]Epoch 34:  50%|████▉     | 1090/2191 [14:55<15:03,  1.22it/s, loss=2.32, v_num=647]Epoch 34:  50%|████▉     | 1090/2191 [14:55<15:03,  1.22it/s, loss=2.28, v_num=647]Epoch 34:  50%|█████     | 1100/2191 [15:02<14:54,  1.22it/s, loss=2.28, v_num=647]Epoch 34:  50%|█████     | 1100/2191 [15:02<14:54,  1.22it/s, loss=2.28, v_num=647]Epoch 34:  50%|█████     | 1100/2191 [15:15<15:07,  1.20it/s, loss=2.28, v_num=647]attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=9, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f70dc806210>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
Converting SMILES strings into graphs...
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=9, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa13090d710>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
Converting SMILES strings into graphs...
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa150caf170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=0, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f86f8740750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
Converting SMILES strings into graphs...
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f8718ae7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]                                                              Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=0, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1d6ef98950>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
Converting SMILES strings into graphs...
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1d8ea78170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9935483932495117
                                                              len(train_dataloader) 3000
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/8005 [00:00<00:00, 44620.26it/s]Epoch 35:   0%|          | 0/8005 [00:00<00:01, 7667.83it/s] Epoch 35:   0%|          | 10/8005 [00:26<5:23:42,  2.43s/it]Epoch 35:   0%|          | 10/8005 [00:26<5:23:43,  2.43s/it, loss=0.0566, v_num=649]Epoch 35:   0%|          | 20/8005 [00:41<4:24:38,  1.99s/it, loss=0.0566, v_num=649]Epoch 35:   0%|          | 20/8005 [00:41<4:24:39,  1.99s/it, loss=0.0542, v_num=649]Epoch 35:   0%|          | 30/8005 [00:55<3:59:02,  1.80s/it, loss=0.0542, v_num=649]Epoch 35:   0%|          | 30/8005 [00:55<3:59:02,  1.80s/it, loss=0.053, v_num=649] Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa6b82c88d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa6fabf6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa72820dcd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa7725d3170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4dc1d35b10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4e0c97b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f41f2b62f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f423cf2e170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f604616d6d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6090d4c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcc417d2c10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcc84118170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f11bd0875d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f11fe19a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f11b8b7ef10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f120321e170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f14e4ebd750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f152faf8170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6bbfef9e50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6c0ab37170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa55d43ef50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa59e707170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f9a47376e10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f9a91a15170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f157dc15910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f15c8681170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4906e1e450>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f494bca7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f99a17fe2d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f99e2ac9170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9615384936332703
len(train_dataloader) 3000
ntion_drovalidation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
                                                              len(train_dataloader) 3000
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/334 [00:00<00:00, 27060.03it/s]Epoch 35:   0%|          | 0/334 [00:00<00:00, 5706.54it/s] s=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f5079e99f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f50c4265170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f57c9960690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f5813dbb170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ff37dd0d350>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ff3c80cb170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2493fed590>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f24d50f2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f43aa7c3e90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f43f4a0f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9818181395530701
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9826086759567261
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
aloader) 3000
ataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9736842513084412
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9784946441650391
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa8ec1706d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa92e8c6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]pout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f5bcaa39ad0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f5c14a9d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7efc689ab350>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7efcb2bb3170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f3d7e7375d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f3dc8937170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcc0ff297d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcc59d1c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa241d2e8d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa28c79a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9826086759567261
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9818181395530701
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
aloader) 3000
ch_end
graph acc: 0.5
valid accuracy: 0.9863945245742798
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
.9945946335792542
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcdfcb5f110>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fce46f32170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]pout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2034be8450>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f207efaa170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9735449552536011
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
t_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f69fef5c910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6a493be170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0bc1437690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0c0c070170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f683b37ce50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6885e09170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ef85e20a950>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ef8a8c78170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9818181395530701
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9826086759567261
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
aloader) 3000
c: 0.5
valid accuracy: 0.9863945245742798
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9696969985961914
len(train_dataloader) 3000
.9887640476226807
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd719305990>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd763d71170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f27f7f73b10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2842bd1170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f9900358b10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f994a6ca170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f689c29d690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f68e6ed7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fe3197f0e90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe3639e0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f743b234810>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7485c9f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6526c81210>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f657104f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdb6e750d10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fdbb89df170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb114c70890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb15f6e0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f38938e6690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f38de357170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fbeadd1ef50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fbef7f90170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f5a99f70e50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f5ae49e6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f48cdbee550>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f49179e2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f145d339890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f149e60c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd667005350>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd6ac05f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]pout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd7852a8d50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd7cfc7c170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1fb606b290>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f20000d7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f7f0f582550>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7f59f49170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7efaedb69f90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7efb37d5e170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f7ce4261a50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7d2eccf170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.981566846370697
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9941520690917969
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9787233471870422
len(train_dataloader) 3000
c: 0.5
valid accuracy: 0.9948186278343201
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9696969985961914
len(train_dataloader) 3000
.9945946335792542
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f7bc874a190>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7c12b18170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6149e2acd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f618c5b4170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9736842513084412
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
e', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd4656c1cd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd4a7e35170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb2d484bf50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb31f2bf170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
pout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f19d8a7a690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1a234e4170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc7b97efc90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc80425b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
                                                              len(train_dataloader) 3000
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9826086759567261
len(train_dataloader) 3000
Training:   0%|          | 0/334 [00:00<00:00, 15534.46it/s]Epoch 35:   0%|          | 0/334 [00:00<00:00, 3495.25it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.981566846370697
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9818181395530701
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f8597f95ed0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f85e2a16170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4f8b15a610>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4fcffc6170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fd94a1f01d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fd994404170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ff5c0418e90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ff60aea2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f673e769b50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f67891ea170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0fd63ffe90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1020e7d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4a88078750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4ad2af2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7feb7831a910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7febc25b8170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcd22a96e50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcd653e8170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1c373d41d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1c79b61170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=2, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=12, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fe25ff2d2d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe2a4dc5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9405405521392822
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
ining:   0%|          | 0/334 [00:00<00:00, 17623.13it/s]Epoch 35:   0%|          | 0/334 [00:00<00:00, 3469.23it/s] Epoch 35:   3%|▎         | 10/334 [00:53<26:27,  4.90s/it] Epoch 35:   3%|▎         | 10/334 [00:53<26:27,  4.90s/it, loss=0.69, v_num=656]Epoch 35:   6%|▌         | 20/334 [01:21<20:24,  3.90s/it, loss=0.69, v_num=656]Epoch 35:   6%|▌         | 20/334 [01:21<20:24,  3.90s/it, loss=0.655, v_num=656]Epoch 35:   9%|▉         | 30/334 [01:49<17:50,  3.52s/it, loss=0.655, v_num=656]Epoch 35:   9%|▉         | 30/334 [01:49<17:50,  3.52s/it, loss=0.609, v_num=656]Epoch 35:  12%|█▏        | 40/334 [02:20<16:48,  3.43s/it, loss=0.609, v_num=656]Epoch 35:  12%|█▏        | 40/334 [02:20<16:48,  3.43s/it, loss=0.62, v_num=656] Epoch 35:  15%|█▍        | 50/334 [02:52<16:01,  3.39s/it, loss=0.62, v_num=656]Epoch 35:  15%|█▍        | 50/334 [02:52<16:01,  3.39s/it, loss=0.621, v_num=656]Epoch 35:  18%|█▊        | 60/334 [03:22<15:10,  3.32s/it, loss=0.621, v_num=656]Epoch 35:  18%|█▊        | 60/334 [03:22<15:10,  3.32s/it, loss=0.588, v_num=656]Epoch 35:  21%|██        | 70/334 [03:57<14:42,  3.34s/it, loss=0.588, v_num=656]Epoch 35:  21%|██        | 70/334 [03:57<14:42,  3.34s/it, loss=0.589, v_num=656]Epoch 35:  24%|██▍       | 80/334 [04:22<13:42,  3.24s/it, loss=0.589, v_num=656]Epoch 35:  24%|██▍       | 80/334 [04:22<13:42,  3.24s/it, loss=0.604, v_num=656]Epoch 35:  27%|██▋       | 90/334 [04:52<13:02,  3.21s/it, loss=0.604, v_num=656]Epoch 35:  27%|██▋       | 90/334 [04:52<13:02,  3.21s/it, loss=0.604, v_num=656]Epoch 35:  30%|██▉       | 100/334 [05:23<12:29,  3.20s/it, loss=0.604, v_num=656]Epoch 35:  30%|██▉       | 100/334 [05:23<12:29,  3.20s/it, loss=0.609, v_num=656]Epoch 35:  33%|███▎      | 110/334 [05:54<11:55,  3.19s/it, loss=0.609, v_num=656]Epoch 35:  33%|███▎      | 110/334 [05:54<11:55,  3.19s/it, loss=0.608, v_num=656]Epoch 35:  36%|███▌      | 120/334 [06:01<10:39,  2.99s/it, loss=0.608, v_num=656]Epoch 35:  36%|███▌      | 120/334 [06:01<10:39,  2.99s/it, loss=0.6, v_num=656]  Epoch 35:  39%|███▉      | 130/334 [06:03<09:26,  2.78s/it, loss=0.6, v_num=656]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/209 [00:00<?, ?it/s][A
Validating:   5%|▍         | 10/209 [00:04<01:23,  2.39it/s][AEpoch 35:  42%|████▏     | 140/334 [06:08<08:26,  2.61s/it, loss=0.6, v_num=656]
Validating:  10%|▉         | 20/209 [00:04<00:38,  4.95it/s][AEpoch 35:  45%|████▍     | 150/334 [06:08<07:29,  2.44s/it, loss=0.6, v_num=656]
Validating:  14%|█▍        | 30/209 [00:06<00:32,  5.45it/s][AEpoch 35:  48%|████▊     | 160/334 [06:10<06:40,  2.30s/it, loss=0.6, v_num=656]
Validating:  19%|█▉        | 40/209 [00:08<00:32,  5.18it/s][AEpoch 35:  51%|█████     | 170/334 [06:12<05:57,  2.18s/it, loss=0.6, v_num=656]
Validating:  24%|██▍       | 50/209 [00:08<00:22,  6.92it/s][AEpoch 35:  54%|█████▍    | 180/334 [06:12<05:17,  2.06s/it, loss=0.6, v_num=656]
Validating:  29%|██▊       | 60/209 [00:11<00:25,  5.91it/s][AEpoch 35:  57%|█████▋    | 190/334 [06:15<04:42,  1.96s/it, loss=0.6, v_num=656]
Validating:  33%|███▎      | 70/209 [00:14<00:29,  4.71it/s][AEpoch 35:  60%|█████▉    | 200/334 [06:18<04:12,  1.88s/it, loss=0.6, v_num=656]
Validating:  38%|███▊      | 80/209 [00:15<00:23,  5.51it/s][AEpoch 35:  63%|██████▎   | 210/334 [06:19<03:42,  1.80s/it, loss=0.6, v_num=656]
Validating:  43%|████▎     | 90/209 [00:17<00:21,  5.57it/s][AEpoch 35:  66%|██████▌   | 220/334 [06:21<03:16,  1.72s/it, loss=0.6, v_num=656]
Validating:  48%|████▊     | 100/209 [00:18<00:17,  6.17it/s][AEpoch 35:  69%|██████▉   | 230/334 [06:22<02:52,  1.65s/it, loss=0.6, v_num=656]
Validating:  53%|█████▎    | 110/209 [00:19<00:14,  6.71it/s][AEpoch 35:  72%|███████▏  | 240/334 [06:23<02:29,  1.59s/it, loss=0.6, v_num=656]
Validating:  57%|█████▋    | 120/209 [00:20<00:11,  7.64it/s][AEpoch 35:  75%|███████▍  | 250/334 [06:24<02:08,  1.53s/it, loss=0.6, v_num=656]
Validating:  62%|██████▏   | 130/209 [00:20<00:08,  9.28it/s][AEpoch 35:  78%|███████▊  | 260/334 [06:24<01:49,  1.47s/it, loss=0.6, v_num=656]
Validating:  67%|██████▋   | 140/209 [00:22<00:08,  8.03it/s][AEpoch 35:  81%|████████  | 270/334 [06:26<01:31,  1.43s/it, loss=0.6, v_num=656]
Validating:  72%|███████▏  | 150/209 [00:25<00:09,  6.03it/s][AEpoch 35:  84%|████████▍ | 280/334 [06:29<01:14,  1.38s/it, loss=0.6, v_num=656]
Validating:  77%|███████▋  | 160/209 [00:26<00:08,  5.86it/s][AEpoch 35:  87%|████████▋ | 290/334 [06:30<00:59,  1.34s/it, loss=0.6, v_num=656]
Validating:  81%|████████▏ | 170/209 [00:28<00:06,  6.03it/s][AEpoch 35:  90%|████████▉ | 300/334 [06:32<00:44,  1.30s/it, loss=0.6, v_num=656]
Validating:  86%|████████▌ | 180/209 [00:30<00:04,  5.95it/s][AEpoch 35:  93%|█████████▎| 310/334 [06:34<00:30,  1.27s/it, loss=0.6, v_num=656]
Validating:  91%|█████████ | 190/209 [00:31<00:02,  6.83it/s][AEpoch 35:  96%|█████████▌| 320/334 [06:35<00:17,  1.23s/it, loss=0.6, v_num=656]
Validating:  96%|█████████▌| 200/209 [00:31<00:01,  8.00it/s][AEpoch 35:  99%|█████████▉| 330/334 [06:35<00:04,  1.20s/it, loss=0.6, v_num=656]
Validating: 100%|██████████| 209/209 [00:32<00:00,  9.21it/s][Avalidation_epoch_end
graph acc: 0.3827751196172249
valid accuracy: 0.9796129465103149
Epoch 35: 100%|██████████| 334/334 [06:36<00:00,  1.18s/it, loss=0.59, v_num=656]
                                                             [AEpoch 35:   0%|          | 0/334 [00:00<00:00, 10979.85it/s, loss=0.59, v_num=656]Epoch 36:   0%|          | 0/334 [00:00<00:00, 2372.34it/s, loss=0.59, v_num=656] Epoch 36:   0%|          | 0/334 [00:19<1:49:51, 19.73s/it, loss=0.59, v_num=656]Epoch 36:   3%|▎         | 10/334 [00:43<21:25,  3.97s/it, loss=0.59, v_num=656] Epoch 36:   3%|▎         | 10/334 [00:43<21:25,  3.97s/it, loss=0.552, v_num=656]Epoch 36:   6%|▌         | 20/334 [01:12<18:09,  3.47s/it, loss=0.552, v_num=656]Epoch 36:   6%|▌         | 20/334 [01:12<18:09,  3.47s/it, loss=0.553, v_num=656]Epoch 36:   9%|▉         | 30/334 [01:40<16:26,  3.25s/it, loss=0.553, v_num=656]Epoch 36:   9%|▉         | 30/334 [01:40<16:26,  3.25s/it, loss=0.547, v_num=656]Epoch 36:  12%|█▏        | 40/334 [02:09<15:27,  3.16s/it, loss=0.547, v_num=656]Epoch 36:  12%|█▏        | 40/334 [02:09<15:27,  3.16s/it, loss=0.537, v_num=656]Epoch 36:  15%|█▍        | 50/334 [02:35<14:25,  3.05s/it, loss=0.537, v_num=656]Epoch 36:  15%|█▍        | 50/334 [02:35<14:25,  3.05s/it, loss=0.538, v_num=656]Epoch 36:  18%|█▊        | 60/334 [03:18<14:53,  3.26s/it, loss=0.538, v_num=656]Epoch 36:  18%|█▊        | 60/334 [03:18<14:53,  3.26s/it, loss=0.554, v_num=656]Epoch 36:  21%|██        | 70/334 [03:48<14:11,  3.23s/it, loss=0.554, v_num=656]Epoch 36:  21%|██        | 70/334 [03:48<14:11,  3.23s/it, loss=0.589, v_num=656]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcad1430750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcb1beca170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ef97bfa0f50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ef9c6976170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1f54e8ff50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1f9f11a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f3c6d78cbd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f3cb7a25170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6a81fc1750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6acca02170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f7c9ea1d8d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f7ce95ca170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6a29a26c50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6a744c3170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fe2b982cad0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe2f912d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9784946441650391
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9735449552536011
len(train_dataloader) 3000
h_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
| 0/334 [00:00<00:00, 6043.67it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9887640476226807
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9945946335792542
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0e3f242690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0e8033b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f94a1a5c390>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f94ebec0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4050f8ea50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f409bbcf170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_drovalidation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
                                                              len(train_dataloader) 3000
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/334 [00:00<00:00, 17549.39it/s]Epoch 35:   0%|          | 0/334 [00:00<00:00, 3206.65it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9615384936332703
len(train_dataloader) 3000
n=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa73522c190>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa77f5e9170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f183f6df610>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f18807e1170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1a1b205c10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1a5c309170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f24f54658d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f253fed2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6ae8e6a890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6b338de170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9405405521392822
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
taloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9887640476226807
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9735449552536011
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9948186278343201
len(train_dataloader) 3000
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f676df16f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f67b7f83170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=24, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f40478a4a50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4092311170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f92dd388290>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f931e655170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f66775f3910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f66c222d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9860139489173889
len(train_dataloader) 3000
ntion_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fca46b86850>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fca915f4170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f76eadb2c50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f773577d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f9a86aa3290>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f9ac6656170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_drovalidation_epoch_end
graph acc: 0.5
valid accuracy: 0.9868420958518982
                                                              len(train_dataloader) 3000
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/501 [00:00<00:00, 22671.91it/s]Epoch 35:   0%|          | 0/501 [00:00<00:00, 5468.45it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9894737005233765
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9791666865348816
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f31c2be35d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f320c9cc170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdfb77942d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe00221f170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=16, num_processes=1, num_sanity_val_steps=2, num_workers=8, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc84677cf10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc8911f2170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9800000190734863
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9948453307151794
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9704142212867737
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9702127575874329
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9627659320831299
len(train_dataloader) 3000
Epoch 35:   2%|▏         | 10/501 [00:50<37:48,  4.62s/it] Epoch 35:   2%|▏         | 10/501 [00:50<37:50,  4.62s/it, loss=0.921, v_num=660]Epoch 35:   4%|▍         | 20/501 [01:11<27:15,  3.40s/it, loss=0.921, v_num=660]Epoch 35:   4%|▍         | 20/501 [01:11<27:15,  3.40s/it, loss=0.893, v_num=660]Epoch 35:   6%|▌         | 30/501 [01:32<23:20,  2.97s/it, loss=0.893, v_num=660]Epoch 35:   6%|▌         | 30/501 [01:32<23:20,  2.97s/it, loss=0.831, v_num=660]Epoch 35:   8%|▊         | 40/501 [01:51<20:50,  2.71s/it, loss=0.831, v_num=660]Epoch 35:   8%|▊         | 40/501 [01:51<20:50,  2.71s/it, loss=0.821, v_num=660]Epoch 35:  10%|▉         | 50/501 [02:10<19:12,  2.56s/it, loss=0.821, v_num=660]Epoch 35:  10%|▉         | 50/501 [02:10<19:12,  2.56s/it, loss=0.841, v_num=660]Epoch 35:  12%|█▏        | 60/501 [02:29<17:57,  2.44s/it, loss=0.841, v_num=660]Epoch 35:  12%|█▏        | 60/501 [02:29<17:57,  2.44s/it, loss=0.837, v_num=660]Epoch 35:  14%|█▍        | 70/501 [02:48<16:59,  2.37s/it, loss=0.837, v_num=660]Epoch 35:  14%|█▍        | 70/501 [02:48<16:59,  2.37s/it, loss=0.819, v_num=660]Epoch 35:  16%|█▌        | 80/501 [03:06<16:08,  2.30s/it, loss=0.819, v_num=660]Epoch 35:  16%|█▌        | 80/501 [03:06<16:08,  2.30s/it, loss=0.792, v_num=660]Epoch 35:  18%|█▊        | 90/501 [03:25<15:28,  2.26s/it, loss=0.792, v_num=660]Epoch 35:  18%|█▊        | 90/501 [03:25<15:28,  2.26s/it, loss=0.779, v_num=660]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f11acf2c910>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f11f79ab170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fe73e7d7b90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fe788a72170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1d5f997f90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1da4809170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb1202e0850>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb16ad70170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 1.0
valid accuracy: 0.9999999403953552
len(train_dataloader) 3000
ntion_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcb50348710>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcb9144d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f335a02f510>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f339eec0170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fcb94f5bb10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fcbd605d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f1eb9306690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f1f03d74170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6b60eb9f50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f6bab924170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=15, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2760627a90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f279fee5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9885057210922241
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9878048300743103
len(train_dataloader) 3000
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9921569228172302
len(train_dataloader) 3000
3799744
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9928571581840515
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9826589226722717
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.949999988079071
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9466667175292969
len(train_dataloader) 3000
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9753085970878601
len(train_dataloader) 3000
Epoch 35:   2%|▏         | 10/534 [00:28<22:58,  2.63s/it] Epoch 35:   2%|▏         | 10/534 [00:28<22:58,  2.63s/it, loss=0.863, v_num=661]Epoch 35:   4%|▎         | 20/534 [00:54<22:13,  2.59s/it, loss=0.863, v_num=661]Epoch 35:   4%|▎         | 20/534 [00:54<22:13,  2.59s/it, loss=0.845, v_num=661]Epoch 35:   6%|▌         | 30/534 [01:10<19:03,  2.27s/it, loss=0.845, v_num=661]Epoch 35:   6%|▌         | 30/534 [01:10<19:03,  2.27s/it, loss=0.796, v_num=661]Epoch 35:   7%|▋         | 40/534 [01:22<16:37,  2.02s/it, loss=0.796, v_num=661]Epoch 35:   7%|▋         | 40/534 [01:22<16:37,  2.02s/it, loss=0.768, v_num=661]Epoch 35:   9%|▉         | 50/534 [01:38<15:37,  1.94s/it, loss=0.768, v_num=661]Epoch 35:   9%|▉         | 50/534 [01:38<15:37,  1.94s/it, loss=0.76, v_num=661] Epoch 35:  11%|█         | 60/534 [01:49<14:13,  1.80s/it, loss=0.76, v_num=661]Epoch 35:  11%|█         | 60/534 [01:49<14:13,  1.80s/it, loss=0.781, v_num=661]Epoch 35:  13%|█▎        | 70/534 [02:02<13:23,  1.73s/it, loss=0.781, v_num=661]Epoch 35:  13%|█▎        | 70/534 [02:02<13:23,  1.73s/it, loss=0.78, v_num=661] Epoch 35:  15%|█▍        | 80/534 [02:15<12:36,  1.67s/it, loss=0.78, v_num=661]Epoch 35:  15%|█▍        | 80/534 [02:15<12:36,  1.67s/it, loss=0.746, v_num=661]Epoch 35:  17%|█▋        | 90/534 [02:27<12:00,  1.62s/it, loss=0.746, v_num=661]Epoch 35:  17%|█▋        | 90/534 [02:27<12:00,  1.62s/it, loss=0.738, v_num=661]Epoch 35:  19%|█▊        | 100/534 [02:41<11:33,  1.60s/it, loss=0.738, v_num=661]Epoch 35:  19%|█▊        | 100/534 [02:41<11:33,  1.60s/it, loss=0.718, v_num=661]Epoch 35:  21%|██        | 110/534 [02:56<11:14,  1.59s/it, loss=0.718, v_num=661]Epoch 35:  21%|██        | 110/534 [02:56<11:14,  1.59s/it, loss=0.717, v_num=661]Epoch 35:  22%|██▏       | 120/534 [03:10<10:50,  1.57s/it, loss=0.717, v_num=661]Epoch 35:  22%|██▏       | 120/534 [03:10<10:50,  1.57s/it, loss=0.712, v_num=661]Epoch 35:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.712, v_num=661]Epoch 35:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.707, v_num=661]Epoch 35:  26%|██▌       | 140/534 [03:42<10:21,  1.58s/it, loss=0.707, v_num=661]Epoch 35:  26%|██▌       | 140/534 [03:42<10:21,  1.58s/it, loss=0.725, v_num=661]Epoch 35:  28%|██▊       | 150/534 [03:54<09:55,  1.55s/it, loss=0.725, v_num=661]Epoch 35:  28%|██▊       | 150/534 [03:54<09:55,  1.55s/it, loss=0.733, v_num=661]Epoch 35:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.733, v_num=661]Epoch 35:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.725, v_num=661]Epoch 35:  32%|███▏      | 170/534 [04:18<09:09,  1.51s/it, loss=0.725, v_num=661]Epoch 35:  32%|███▏      | 170/534 [04:18<09:09,  1.51s/it, loss=0.702, v_num=661]Epoch 35:  34%|███▎      | 180/534 [04:31<08:50,  1.50s/it, loss=0.702, v_num=661]Epoch 35:  34%|███▎      | 180/534 [04:31<08:50,  1.50s/it, loss=0.697, v_num=661]Epoch 35:  36%|███▌      | 190/534 [04:42<08:27,  1.48s/it, loss=0.697, v_num=661]Epoch 35:  36%|███▌      | 190/534 [04:42<08:27,  1.48s/it, loss=0.703, v_num=661]Epoch 35:  37%|███▋      | 200/534 [04:45<07:54,  1.42s/it, loss=0.703, v_num=661]Epoch 35:  37%|███▋      | 200/534 [04:45<07:54,  1.42s/it, loss=0.705, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/334 [00:01<00:55,  5.81it/s][AEpoch 35:  41%|████      | 220/534 [04:47<06:47,  1.30s/it, loss=0.705, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:38,  8.08it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:30,  9.88it/s][AEpoch 35:  45%|████▍     | 240/534 [04:48<05:52,  1.20s/it, loss=0.705, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.89it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28,  9.90it/s][AEpoch 35:  49%|████▊     | 260/534 [04:50<05:05,  1.11s/it, loss=0.705, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.21it/s][A
Validating:  21%|██        | 70/334 [00:08<00:32,  8.04it/s][AEpoch 35:  52%|█████▏    | 280/534 [04:53<04:25,  1.04s/it, loss=0.705, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.43it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24, 10.04it/s][AEpoch 35:  56%|█████▌    | 300/534 [04:55<03:49,  1.02it/s, loss=0.705, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.73it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.80it/s][AEpoch 35:  60%|█████▉    | 320/534 [04:56<03:17,  1.08it/s, loss=0.705, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.68it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:20, 10.18it/s][AEpoch 35:  64%|██████▎   | 340/534 [04:58<02:49,  1.14it/s, loss=0.705, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.59it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.27it/s][AEpoch 35:  67%|██████▋   | 360/534 [04:59<02:24,  1.20it/s, loss=0.705, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 11.77it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.19it/s][AEpoch 35:  71%|███████   | 380/534 [05:01<02:02,  1.26it/s, loss=0.705, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.35it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.69it/s][AEpoch 35:  75%|███████▍  | 400/534 [05:03<01:41,  1.32it/s, loss=0.705, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 11.72it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 11.69it/s][AEpoch 35:  79%|███████▊  | 420/534 [05:05<01:22,  1.38it/s, loss=0.705, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.95it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.54it/s][AEpoch 35:  82%|████████▏ | 440/534 [05:06<01:05,  1.44it/s, loss=0.705, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.25it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:06, 12.17it/s][AEpoch 35:  86%|████████▌ | 460/534 [05:08<00:49,  1.49it/s, loss=0.705, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.20it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 12.94it/s][AEpoch 35:  90%|████████▉ | 480/534 [05:09<00:34,  1.55it/s, loss=0.705, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.08it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 13.28it/s][AEpoch 35:  94%|█████████▎| 500/534 [05:11<00:21,  1.61it/s, loss=0.705, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.39it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.49it/s][AEpoch 35:  97%|█████████▋| 520/534 [05:13<00:08,  1.66it/s, loss=0.705, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.68it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.28it/s][Avalidation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9797157049179077
Epoch 35: 100%|██████████| 534/534 [05:22<00:00,  1.66it/s, loss=0.705, v_num=661]
                                                             [AEpoch 35:   0%|          | 0/534 [00:00<00:00, 12826.62it/s, loss=0.705, v_num=661]Epoch 36:   0%|          | 0/534 [00:00<00:00, 2832.08it/s, loss=0.705, v_num=661] Epoch 36:   0%|          | 0/534 [00:14<2:07:04, 14.28s/it, loss=0.705, v_num=661]Epoch 36:   2%|▏         | 10/534 [00:23<18:18,  2.10s/it, loss=0.705, v_num=661] Epoch 36:   2%|▏         | 10/534 [00:23<18:18,  2.10s/it, loss=0.663, v_num=661]Epoch 36:   4%|▎         | 20/534 [00:37<15:23,  1.80s/it, loss=0.663, v_num=661]Epoch 36:   4%|▎         | 20/534 [00:37<15:24,  1.80s/it, loss=0.629, v_num=661]Epoch 36:   6%|▌         | 30/534 [00:52<14:20,  1.71s/it, loss=0.629, v_num=661]Epoch 36:   6%|▌         | 30/534 [00:52<14:20,  1.71s/it, loss=0.634, v_num=661]Epoch 36:   7%|▋         | 40/534 [01:07<13:30,  1.64s/it, loss=0.634, v_num=661]Epoch 36:   7%|▋         | 40/534 [01:07<13:30,  1.64s/it, loss=0.646, v_num=661]Epoch 36:   9%|▉         | 50/534 [01:22<12:59,  1.61s/it, loss=0.646, v_num=661]Epoch 36:   9%|▉         | 50/534 [01:22<12:59,  1.61s/it, loss=0.63, v_num=661] Epoch 36:  11%|█         | 60/534 [01:36<12:26,  1.58s/it, loss=0.63, v_num=661]Epoch 36:  11%|█         | 60/534 [01:36<12:26,  1.58s/it, loss=0.626, v_num=661]Epoch 36:  13%|█▎        | 70/534 [01:52<12:13,  1.58s/it, loss=0.626, v_num=661]Epoch 36:  13%|█▎        | 70/534 [01:52<12:13,  1.58s/it, loss=0.647, v_num=661]Epoch 36:  15%|█▍        | 80/534 [02:11<12:18,  1.63s/it, loss=0.647, v_num=661]Epoch 36:  15%|█▍        | 80/534 [02:11<12:18,  1.63s/it, loss=0.645, v_num=661]Epoch 36:  17%|█▋        | 90/534 [02:24<11:45,  1.59s/it, loss=0.645, v_num=661]Epoch 36:  17%|█▋        | 90/534 [02:24<11:45,  1.59s/it, loss=0.664, v_num=661]Epoch 36:  19%|█▊        | 100/534 [02:38<11:22,  1.57s/it, loss=0.664, v_num=661]Epoch 36:  19%|█▊        | 100/534 [02:38<11:22,  1.57s/it, loss=0.653, v_num=661]Epoch 36:  21%|██        | 110/534 [02:54<11:06,  1.57s/it, loss=0.653, v_num=661]Epoch 36:  21%|██        | 110/534 [02:54<11:06,  1.57s/it, loss=0.664, v_num=661]Epoch 36:  22%|██▏       | 120/534 [03:05<10:35,  1.54s/it, loss=0.664, v_num=661]Epoch 36:  22%|██▏       | 120/534 [03:05<10:35,  1.54s/it, loss=0.67, v_num=661] Epoch 36:  24%|██▍       | 130/534 [03:26<10:38,  1.58s/it, loss=0.67, v_num=661]Epoch 36:  24%|██▍       | 130/534 [03:26<10:38,  1.58s/it, loss=0.64, v_num=661]Epoch 36:  26%|██▌       | 140/534 [03:42<10:22,  1.58s/it, loss=0.64, v_num=661]Epoch 36:  26%|██▌       | 140/534 [03:42<10:22,  1.58s/it, loss=0.658, v_num=661]Epoch 36:  28%|██▊       | 150/534 [03:56<10:01,  1.57s/it, loss=0.658, v_num=661]Epoch 36:  28%|██▊       | 150/534 [03:56<10:01,  1.57s/it, loss=0.658, v_num=661]Epoch 36:  30%|██▉       | 160/534 [04:09<09:39,  1.55s/it, loss=0.658, v_num=661]Epoch 36:  30%|██▉       | 160/534 [04:09<09:39,  1.55s/it, loss=0.632, v_num=661]Epoch 36:  32%|███▏      | 170/534 [04:22<09:19,  1.54s/it, loss=0.632, v_num=661]Epoch 36:  32%|███▏      | 170/534 [04:22<09:19,  1.54s/it, loss=0.628, v_num=661]Epoch 36:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.628, v_num=661]Epoch 36:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.63, v_num=661] validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9812920689582825
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.978666365146637
validation_epoch_end
graph acc: 0.4221556886227545
valid accuracy: 0.9760451912879944
validation_epoch_end
graph acc: 0.4041916167664671
valid accuracy: 0.9798759818077087
15, v_num=661]Epoch 36:  37%|███▋      | 200/534 [04:44<07:52,  1.41s/it, loss=0.606, v_num=661]validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.981117308139801
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9797493815422058
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.980328381061554
validation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9787314534187317
validation_epoch_end
graph acc: 0.4101796407185629
valid accuracy: 0.9787282943725586

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/334 [00:01<01:03,  5.07it/s][AEpoch 36:  41%|████      | 220/534 [04:46<06:46,  1.29s/it, loss=0.606, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:43,  7.26it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  9.16it/s][AEpoch 36:  45%|████▍     | 240/534 [04:47<05:51,  1.19s/it, loss=0.606, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.16it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.23it/s][AEpoch 36:  49%|████▊     | 260/534 [04:49<05:04,  1.11s/it, loss=0.606, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.53it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.98it/s][AEpoch 36:  52%|█████▏    | 280/534 [04:52<04:24,  1.04s/it, loss=0.606, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.76it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.67it/s][AEpoch 36:  56%|█████▌    | 300/534 [04:53<03:48,  1.03it/s, loss=0.606, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:20, 11.57it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.65it/s][AEpoch 36:  60%|█████▉    | 320/534 [04:55<03:16,  1.09it/s, loss=0.606, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.09it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:20,  9.76it/s][AEpoch 36:  64%|██████▎   | 340/534 [04:57<02:49,  1.15it/s, loss=0.606, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:17, 11.35it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 13.10it/s][AEpoch 36:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.606, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.55it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.23it/s][AEpoch 36:  71%|███████   | 380/534 [05:00<02:01,  1.27it/s, loss=0.606, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.05it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.62it/s][AEpoch 36:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.606, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.59it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:09, 12.78it/s][AEpoch 36:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.606, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.26it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.82it/s][AEpoch 36:  82%|████████▏ | 440/534 [05:04<01:04,  1.45it/s, loss=0.606, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.64it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.70it/s][AEpoch 36:  86%|████████▌ | 460/534 [05:06<00:49,  1.50it/s, loss=0.606, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.27it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.12it/s][AEpoch 36:  90%|████████▉ | 480/534 [05:08<00:34,  1.56it/s, loss=0.606, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.73it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 13.01it/s][AEpoch 36:  94%|█████████▎| 500/534 [05:09<00:21,  1.62it/s, loss=0.606, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.58it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 13.30it/s][AEpoch 36:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.606, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.77it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 12.49it/s][Avalidation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9805142879486084
Epoch 36: 100%|██████████| 534/534 [05:20<00:00,  1.67it/s, loss=0.606, v_num=661]
                                                             [AEpoch 36:   0%|          | 0/534 [00:00<00:00, 7913.78it/s, loss=0.606, v_num=661]Epoch 37:   0%|          | 0/534 [00:00<00:00, 2385.84it/s, loss=0.606, v_num=661]Epoch 37:   0%|          | 0/534 [00:13<1:58:39, 13.33s/it, loss=0.606, v_num=661]Epoch 37:   2%|▏         | 10/534 [00:24<19:07,  2.19s/it, loss=0.606, v_num=661] Epoch 37:   2%|▏         | 10/534 [00:24<19:07,  2.19s/it, loss=0.617, v_num=661]Epoch 37:   4%|▎         | 20/534 [00:43<17:38,  2.06s/it, loss=0.617, v_num=661]Epoch 37:   4%|▎         | 20/534 [00:43<17:38,  2.06s/it, loss=0.615, v_num=661]Epoch 37:   6%|▌         | 30/534 [00:57<15:33,  1.85s/it, loss=0.615, v_num=661]Epoch 37:   6%|▌         | 30/534 [00:57<15:33,  1.85s/it, loss=0.614, v_num=661]Epoch 37:   7%|▋         | 40/534 [01:13<14:46,  1.80s/it, loss=0.614, v_num=661]Epoch 37:   7%|▋         | 40/534 [01:13<14:46,  1.80s/it, loss=0.6, v_num=661]  Epoch 37:   9%|▉         | 50/534 [01:27<13:50,  1.72s/it, loss=0.6, v_num=661]Epoch 37:   9%|▉         | 50/534 [01:27<13:50,  1.72s/it, loss=0.567, v_num=661]Epoch 37:  11%|█         | 60/534 [01:40<13:02,  1.65s/it, loss=0.567, v_num=661]Epoch 37:  11%|█         | 60/534 [01:40<13:02,  1.65s/it, loss=0.573, v_num=661]Epoch 37:  13%|█▎        | 70/534 [01:52<12:15,  1.59s/it, loss=0.573, v_num=661]Epoch 37:  13%|█▎        | 70/534 [01:52<12:15,  1.59s/it, loss=0.586, v_num=661]Epoch 37:  15%|█▍        | 80/534 [02:08<11:58,  1.58s/it, loss=0.586, v_num=661]Epoch 37:  15%|█▍        | 80/534 [02:08<11:58,  1.58s/it, loss=0.593, v_num=661]Epoch 37:  17%|█▋        | 90/534 [02:27<11:57,  1.62s/it, loss=0.593, v_num=661]Epoch 37:  17%|█▋        | 90/534 [02:27<11:57,  1.62s/it, loss=0.605, v_num=661]Epoch 37:  19%|█▊        | 100/534 [02:40<11:31,  1.59s/it, loss=0.605, v_num=661]Epoch 37:  19%|█▊        | 100/534 [02:40<11:31,  1.59s/it, loss=0.602, v_num=661]Epoch 37:  21%|██        | 110/534 [02:53<11:01,  1.56s/it, loss=0.602, v_num=661]Epoch 37:  21%|██        | 110/534 [02:53<11:01,  1.56s/it, loss=0.58, v_num=661] Epoch 37:  22%|██▏       | 120/534 [03:05<10:34,  1.53s/it, loss=0.58, v_num=661]Epoch 37:  22%|██▏       | 120/534 [03:05<10:34,  1.53s/it, loss=0.562, v_num=661]Epoch 37:  24%|██▍       | 130/534 [03:18<10:13,  1.52s/it, loss=0.562, v_num=661]Epoch 37:  24%|██▍       | 130/534 [03:18<10:13,  1.52s/it, loss=0.547, v_num=661]Epoch 37:  26%|██▌       | 140/534 [03:35<10:03,  1.53s/it, loss=0.547, v_num=661]Epoch 37:  26%|██▌       | 140/534 [03:35<10:03,  1.53s/it, loss=0.556, v_num=661]Epoch 37:  28%|██▊       | 150/534 [03:50<09:45,  1.52s/it, loss=0.556, v_num=661]Epoch 37:  28%|██▊       | 150/534 [03:50<09:45,  1.53s/it, loss=0.579, v_num=661]Epoch 37:  30%|██▉       | 160/534 [04:02<09:24,  1.51s/it, loss=0.579, v_num=661]Epoch 37:  30%|██▉       | 160/534 [04:02<09:24,  1.51s/it, loss=0.588, v_num=661]Epoch 37:  32%|███▏      | 170/534 [04:27<09:29,  1.56s/it, loss=0.588, v_num=661]Epoch 37:  32%|███▏      | 170/534 [04:27<09:29,  1.56s/it, loss=0.579, v_num=661]Epoch 37:  34%|███▎      | 180/534 [04:37<09:01,  1.53s/it, loss=0.579, v_num=661]Epoch 37:  34%|███▎      | 180/534 [04:37<09:01,  1.53s/it, loss=0.573, v_num=661]Epoch 37:  36%|███▌      | 190/534 [04:41<08:26,  1.47s/it, loss=0.573, v_num=661]Epoch 37:  36%|███▌      | 190/534 [04:41<08:26,  1.47s/it, loss=0.595, v_num=661]validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9777837991714478
validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9795069098472595
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9816406965255737
validation_epoch_end
graph acc: 0.40718562874251496
valid accuracy: 0.979595959186554
3148078918
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9800031185150146

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9810346364974976
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9816768765449524
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9797136783599854

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.69it/s][AEpoch 37:  41%|████      | 220/534 [04:45<06:45,  1.29s/it, loss=0.602, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:43,  7.18it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:30,  9.93it/s][AEpoch 37:  45%|████▍     | 240/534 [04:47<05:50,  1.19s/it, loss=0.602, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.66it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.38it/s][AEpoch 37:  49%|████▊     | 260/534 [04:49<05:03,  1.11s/it, loss=0.602, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.09it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.85it/s][AEpoch 37:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.602, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:24, 10.42it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 11.00it/s][AEpoch 37:  56%|█████▌    | 300/534 [04:53<03:47,  1.03it/s, loss=0.602, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:23,  9.77it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.89it/s][AEpoch 37:  60%|█████▉    | 320/534 [04:55<03:16,  1.09it/s, loss=0.602, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.79it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:20,  9.91it/s][AEpoch 37:  64%|██████▎   | 340/534 [04:57<02:49,  1.15it/s, loss=0.602, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:16, 11.65it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 13.12it/s][AEpoch 37:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.602, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.21it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.89it/s][AEpoch 37:  71%|███████   | 380/534 [04:59<02:01,  1.27it/s, loss=0.602, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.15it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.38it/s][AEpoch 37:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.602, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 13.01it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.08it/s][AEpoch 37:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.602, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.29it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 14.06it/s][AEpoch 37:  82%|████████▏ | 440/534 [05:04<01:04,  1.45it/s, loss=0.602, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.49it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.33it/s][AEpoch 37:  86%|████████▌ | 460/534 [05:06<00:49,  1.50it/s, loss=0.602, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 11.93it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.76it/s][AEpoch 37:  90%|████████▉ | 480/534 [05:07<00:34,  1.56it/s, loss=0.602, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.87it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.72it/s][AEpoch 37:  94%|█████████▎| 500/534 [05:09<00:21,  1.62it/s, loss=0.602, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.96it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.77it/s][AEpoch 37:  97%|█████████▋| 520/534 [05:11<00:08,  1.68it/s, loss=0.602, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 15.68it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.10it/s][Avalidation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9803546071052551
Epoch 37: 100%|██████████| 534/534 [05:20<00:00,  1.67it/s, loss=0.602, v_num=661]
                                                             [AEpoch 37:   0%|          | 0/534 [00:00<00:00, 8208.03it/s, loss=0.602, v_num=661]Epoch 38:   0%|          | 0/534 [00:00<00:00, 1909.97it/s, loss=0.602, v_num=661]Epoch 38:   0%|          | 0/534 [00:12<1:48:25, 12.18s/it, loss=0.602, v_num=661]Epoch 38:   2%|▏         | 10/534 [00:26<21:25,  2.45s/it, loss=0.602, v_num=661] Epoch 38:   2%|▏         | 10/534 [00:26<21:25,  2.45s/it, loss=0.571, v_num=661]Epoch 38:   4%|▎         | 20/534 [00:43<17:43,  2.07s/it, loss=0.571, v_num=661]Epoch 38:   4%|▎         | 20/534 [00:43<17:43,  2.07s/it, loss=0.553, v_num=661]Epoch 38:   6%|▌         | 30/534 [00:58<15:52,  1.89s/it, loss=0.553, v_num=661]Epoch 38:   6%|▌         | 30/534 [00:58<15:52,  1.89s/it, loss=0.545, v_num=661]Epoch 38:   7%|▋         | 40/534 [01:12<14:33,  1.77s/it, loss=0.545, v_num=661]Epoch 38:   7%|▋         | 40/534 [01:12<14:33,  1.77s/it, loss=0.54, v_num=661] Epoch 38:   9%|▉         | 50/534 [01:29<14:12,  1.76s/it, loss=0.54, v_num=661]Epoch 38:   9%|▉         | 50/534 [01:29<14:12,  1.76s/it, loss=0.555, v_num=661]Epoch 38:  11%|█         | 60/534 [01:44<13:35,  1.72s/it, loss=0.555, v_num=661]Epoch 38:  11%|█         | 60/534 [01:44<13:35,  1.72s/it, loss=0.552, v_num=661]Epoch 38:  13%|█▎        | 70/534 [02:02<13:19,  1.72s/it, loss=0.552, v_num=661]Epoch 38:  13%|█▎        | 70/534 [02:02<13:19,  1.72s/it, loss=0.548, v_num=661]Epoch 38:  15%|█▍        | 80/534 [02:17<12:52,  1.70s/it, loss=0.548, v_num=661]Epoch 38:  15%|█▍        | 80/534 [02:17<12:52,  1.70s/it, loss=0.541, v_num=661]Epoch 38:  17%|█▋        | 90/534 [02:33<12:27,  1.68s/it, loss=0.541, v_num=661]Epoch 38:  17%|█▋        | 90/534 [02:33<12:27,  1.68s/it, loss=0.556, v_num=661]Epoch 38:  19%|█▊        | 100/534 [02:48<12:04,  1.67s/it, loss=0.556, v_num=661]Epoch 38:  19%|█▊        | 100/534 [02:48<12:04,  1.67s/it, loss=0.578, v_num=661]Epoch 38:  21%|██        | 110/534 [03:04<11:46,  1.67s/it, loss=0.578, v_num=661]Epoch 38:  21%|██        | 110/534 [03:04<11:46,  1.67s/it, loss=0.564, v_num=661]Epoch 38:  22%|██▏       | 120/534 [03:17<11:16,  1.63s/it, loss=0.564, v_num=661]Epoch 38:  22%|██▏       | 120/534 [03:17<11:16,  1.63s/it, loss=0.542, v_num=661]Epoch 38:  24%|██▍       | 130/534 [03:32<10:54,  1.62s/it, loss=0.542, v_num=661]Epoch 38:  24%|██▍       | 130/534 [03:32<10:54,  1.62s/it, loss=0.547, v_num=661]Epoch 38:  26%|██▌       | 140/534 [03:50<10:43,  1.63s/it, loss=0.547, v_num=661]Epoch 38:  26%|██▌       | 140/534 [03:50<10:43,  1.63s/it, loss=0.554, v_num=661]Epoch 38:  28%|██▊       | 150/534 [04:01<10:15,  1.60s/it, loss=0.554, v_num=661]Epoch 38:  28%|██▊       | 150/534 [04:01<10:15,  1.60s/it, loss=0.554, v_num=661]Epoch 38:  30%|██▉       | 160/534 [04:11<09:44,  1.56s/it, loss=0.554, v_num=661]Epoch 38:  30%|██▉       | 160/534 [04:11<09:44,  1.56s/it, loss=0.56, v_num=661] Epoch 38:  30%|██▉       | 160/534 [04:22<10:09,  1.63s/it, loss=0.56, v_num=661]Epoch 38:  32%|███▏      | 170/534 [04:24<09:23,  1.55s/it, loss=0.56, v_num=661]Epoch 38:  32%|███▏      | 170/534 [04:24<09:23,  1.55s/it, loss=0.555, v_num=661]Epoch 38:  34%|███▎      | 180/534 [04:37<09:03,  1.53s/it, loss=0.555, v_num=661]Epoch 38:  34%|███▎      | 180/534 [04:37<09:03,  1.53s/it, loss=0.555, v_num=661]validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.981679379940033
validation_epoch_end
graph acc: 0.45209580838323354
valid accuracy: 0.9778611063957214
validation_epoch_end
graph acc: 0.40718562874251496
valid accuracy: 0.980076014995575
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9799872040748596
s=0.542, v_num=661]validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9808356165885925
validation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9810026288032532
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.98023521900177

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9813238978385925
validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9802047610282898
validation_epoch_end
graph acc: 0.41916167664670656
valid accuracy: 0.9796813130378723

Validating:   3%|▎         | 10/334 [00:01<01:00,  5.37it/s][AEpoch 38:  41%|████      | 220/534 [04:50<06:52,  1.31s/it, loss=0.542, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:44,  7.09it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:35,  8.65it/s][AEpoch 38:  45%|████▍     | 240/534 [04:52<05:56,  1.21s/it, loss=0.542, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.91it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.05it/s][AEpoch 38:  49%|████▊     | 260/534 [04:54<05:08,  1.13s/it, loss=0.542, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:26, 10.46it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.98it/s][AEpoch 38:  52%|█████▏    | 280/534 [04:56<04:28,  1.06s/it, loss=0.542, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:26,  9.41it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.49it/s][AEpoch 38:  56%|█████▌    | 300/534 [04:58<03:51,  1.01it/s, loss=0.542, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 11.09it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.70it/s][AEpoch 38:  60%|█████▉    | 320/534 [05:00<03:20,  1.07it/s, loss=0.542, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.68it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:19, 10.33it/s][AEpoch 38:  64%|██████▎   | 340/534 [05:01<02:51,  1.13it/s, loss=0.542, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.69it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.30it/s][AEpoch 38:  67%|██████▋   | 360/534 [05:03<02:26,  1.19it/s, loss=0.542, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.25it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.25it/s][AEpoch 38:  71%|███████   | 380/534 [05:05<02:03,  1.25it/s, loss=0.542, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.42it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:11, 12.19it/s][AEpoch 38:  75%|███████▍  | 400/534 [05:06<01:42,  1.31it/s, loss=0.542, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.79it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.33it/s][AEpoch 38:  79%|███████▊  | 420/534 [05:08<01:23,  1.36it/s, loss=0.542, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.92it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.41it/s][AEpoch 38:  82%|████████▏ | 440/534 [05:09<01:06,  1.42it/s, loss=0.542, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.31it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.58it/s][AEpoch 38:  86%|████████▌ | 460/534 [05:11<00:50,  1.48it/s, loss=0.542, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.14it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.79it/s][AEpoch 38:  90%|████████▉ | 480/534 [05:13<00:35,  1.54it/s, loss=0.542, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.82it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.60it/s][AEpoch 38:  94%|█████████▎| 500/534 [05:14<00:21,  1.59it/s, loss=0.542, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.79it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:02, 11.74it/s][AEpoch 38:  97%|█████████▋| 520/534 [05:16<00:08,  1.65it/s, loss=0.542, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.61it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.34it/s][Avalidation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9806740283966064
Epoch 38: 100%|██████████| 534/534 [05:24<00:00,  1.65it/s, loss=0.542, v_num=661]
                                                             [AEpoch 38:   0%|          | 0/534 [00:00<00:00, 11155.06it/s, loss=0.542, v_num=661]Epoch 39:   0%|          | 0/534 [00:00<00:00, 2674.94it/s, loss=0.542, v_num=661] Epoch 39:   0%|          | 0/534 [00:17<2:31:47, 17.06s/it, loss=0.542, v_num=661]Epoch 39:   2%|▏         | 10/534 [00:24<19:30,  2.23s/it, loss=0.542, v_num=661] Epoch 39:   2%|▏         | 10/534 [00:24<19:30,  2.23s/it, loss=0.539, v_num=661]Epoch 39:   4%|▎         | 20/534 [00:40<16:35,  1.94s/it, loss=0.539, v_num=661]Epoch 39:   4%|▎         | 20/534 [00:40<16:35,  1.94s/it, loss=0.526, v_num=661]Epoch 39:   6%|▌         | 30/534 [00:53<14:31,  1.73s/it, loss=0.526, v_num=661]Epoch 39:   6%|▌         | 30/534 [00:53<14:31,  1.73s/it, loss=0.534, v_num=661]Epoch 39:   7%|▋         | 40/534 [01:10<14:11,  1.72s/it, loss=0.534, v_num=661]Epoch 39:   7%|▋         | 40/534 [01:10<14:11,  1.72s/it, loss=0.549, v_num=661]Epoch 39:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.549, v_num=661]Epoch 39:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.532, v_num=661]Epoch 39:  11%|█         | 60/534 [01:34<12:16,  1.55s/it, loss=0.532, v_num=661]Epoch 39:  11%|█         | 60/534 [01:34<12:16,  1.55s/it, loss=0.516, v_num=661]Epoch 39:  13%|█▎        | 70/534 [01:49<11:52,  1.54s/it, loss=0.516, v_num=661]Epoch 39:  13%|█▎        | 70/534 [01:49<11:52,  1.54s/it, loss=0.521, v_num=661]Epoch 39:  15%|█▍        | 80/534 [02:06<11:50,  1.57s/it, loss=0.521, v_num=661]Epoch 39:  15%|█▍        | 80/534 [02:06<11:50,  1.57s/it, loss=0.52, v_num=661] Epoch 39:  17%|█▋        | 90/534 [02:25<11:48,  1.60s/it, loss=0.52, v_num=661]Epoch 39:  17%|█▋        | 90/534 [02:25<11:48,  1.60s/it, loss=0.53, v_num=661]Epoch 39:  19%|█▊        | 100/534 [02:39<11:24,  1.58s/it, loss=0.53, v_num=661]Epoch 39:  19%|█▊        | 100/534 [02:39<11:24,  1.58s/it, loss=0.54, v_num=661]Epoch 39:  21%|██        | 110/534 [02:51<10:55,  1.55s/it, loss=0.54, v_num=661]Epoch 39:  21%|██        | 110/534 [02:51<10:55,  1.55s/it, loss=0.541, v_num=661]Epoch 39:  22%|██▏       | 120/534 [03:06<10:36,  1.54s/it, loss=0.541, v_num=661]Epoch 39:  22%|██▏       | 120/534 [03:06<10:36,  1.54s/it, loss=0.533, v_num=661]Epoch 39:  24%|██▍       | 130/534 [03:21<10:21,  1.54s/it, loss=0.533, v_num=661]Epoch 39:  24%|██▍       | 130/534 [03:21<10:21,  1.54s/it, loss=0.526, v_num=661]Epoch 39:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.526, v_num=661]Epoch 39:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.535, v_num=661]Epoch 39:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.535, v_num=661]Epoch 39:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.522, v_num=661]Epoch 39:  30%|██▉       | 160/534 [04:08<09:36,  1.54s/it, loss=0.522, v_num=661]Epoch 39:  30%|██▉       | 160/534 [04:08<09:36,  1.54s/it, loss=0.515, v_num=661]Epoch 39:  32%|███▏      | 170/534 [04:18<09:09,  1.51s/it, loss=0.515, v_num=661]Epoch 39:  32%|███▏      | 170/534 [04:18<09:09,  1.51s/it, loss=0.525, v_num=661]Epoch 39:  34%|███▎      | 180/534 [04:33<08:54,  1.51s/it, loss=0.525, v_num=661]Epoch 39:  34%|███▎      | 180/534 [04:33<08:54,  1.51s/it, loss=0.512, v_num=661]validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9805075526237488
validation_epoch_end
graph acc: 0.41916167664670656
valid accuracy: 0.9776293039321899
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9822991490364075
validation_epoch_end
graph acc: 0.4311377245508982
valid accuracy: 0.9801960587501526
0.522, v_num=661]validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9817370772361755
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9804672598838806
validation_epoch_end
graph acc: 0.45209580838323354
valid accuracy: 0.9818354845046997
validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9814951419830322

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.980863094329834
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.980809211730957

Validating:   3%|▎         | 10/334 [00:01<00:58,  5.52it/s][AEpoch 39:  41%|████      | 220/534 [04:48<06:49,  1.30s/it, loss=0.522, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:41,  7.52it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:35,  8.61it/s][AEpoch 39:  45%|████▍     | 240/534 [04:50<05:54,  1.20s/it, loss=0.522, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.02it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.20it/s][AEpoch 39:  49%|████▊     | 260/534 [04:51<05:06,  1.12s/it, loss=0.522, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:22, 11.93it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.71it/s][AEpoch 39:  52%|█████▏    | 280/534 [04:54<04:26,  1.05s/it, loss=0.522, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.68it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.63it/s][AEpoch 39:  56%|█████▌    | 300/534 [04:55<03:50,  1.02it/s, loss=0.522, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.96it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.93it/s][AEpoch 39:  60%|█████▉    | 320/534 [04:57<03:18,  1.08it/s, loss=0.522, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 11.91it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.61it/s][AEpoch 39:  64%|██████▎   | 340/534 [04:59<02:50,  1.14it/s, loss=0.522, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.89it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.49it/s][AEpoch 39:  67%|██████▋   | 360/534 [05:01<02:25,  1.20it/s, loss=0.522, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.25it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:15, 10.83it/s][AEpoch 39:  71%|███████   | 380/534 [05:03<02:02,  1.26it/s, loss=0.522, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.69it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.52it/s][AEpoch 39:  75%|███████▍  | 400/534 [05:04<01:41,  1.32it/s, loss=0.522, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.30it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.63it/s][AEpoch 39:  79%|███████▊  | 420/534 [05:06<01:22,  1.37it/s, loss=0.522, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.44it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.26it/s][AEpoch 39:  82%|████████▏ | 440/534 [05:07<01:05,  1.43it/s, loss=0.522, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 13.88it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.29it/s][AEpoch 39:  86%|████████▌ | 460/534 [05:09<00:49,  1.49it/s, loss=0.522, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.85it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.07it/s][AEpoch 39:  90%|████████▉ | 480/534 [05:11<00:34,  1.55it/s, loss=0.522, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.32it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.63it/s][AEpoch 39:  94%|█████████▎| 500/534 [05:12<00:21,  1.60it/s, loss=0.522, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.48it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.61it/s][AEpoch 39:  97%|█████████▋| 520/534 [05:14<00:08,  1.66it/s, loss=0.522, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.50it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.74it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9809535145759583
Epoch 39: 100%|██████████| 534/534 [05:20<00:00,  1.67it/s, loss=0.522, v_num=661]
                                                             [AEpoch 39:   0%|          | 0/534 [00:00<00:00, 13231.24it/s, loss=0.522, v_num=661]Epoch 40:   0%|          | 0/534 [00:00<00:00, 2519.10it/s, loss=0.522, v_num=661] Epoch 40:   0%|          | 0/534 [00:15<2:16:54, 15.38s/it, loss=0.522, v_num=661]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9780929088592529
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9807077050209045
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9824928045272827
validation_epoch_end
graph acc: 0.41916167664670656
valid accuracy: 0.979915976524353
<?, ?it/s][Avalidation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9814951419830322
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9817958474159241
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9812408685684204
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9811114072799683
validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9808249473571777

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.69it/s][AEpoch 40:  41%|████      | 220/534 [04:46<06:47,  1.30s/it, loss=0.499, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:39,  7.87it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  8.96it/s][AEpoch 40:  45%|████▍     | 240/534 [04:48<05:52,  1.20s/it, loss=0.499, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.85it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.41it/s][AEpoch 40:  49%|████▊     | 260/534 [04:50<05:04,  1.11s/it, loss=0.499, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:26, 10.46it/s][A
Validating:  21%|██        | 70/334 [00:08<00:32,  8.01it/s][AEpoch 40:  52%|█████▏    | 280/534 [04:53<04:25,  1.04s/it, loss=0.499, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25,  9.99it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.70it/s][AEpoch 40:  56%|█████▌    | 300/534 [04:54<03:48,  1.02it/s, loss=0.499, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.92it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.48it/s][AEpoch 40:  60%|█████▉    | 320/534 [04:56<03:17,  1.08it/s, loss=0.499, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.24it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.40it/s][AEpoch 40:  64%|██████▎   | 340/534 [04:58<02:49,  1.14it/s, loss=0.499, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:17, 11.36it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:13, 13.35it/s][AEpoch 40:  67%|██████▋   | 360/534 [04:59<02:24,  1.21it/s, loss=0.499, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.53it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.64it/s][AEpoch 40:  71%|███████   | 380/534 [05:01<02:01,  1.26it/s, loss=0.499, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.11it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.94it/s][AEpoch 40:  75%|███████▍  | 400/534 [05:03<01:41,  1.32it/s, loss=0.499, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.58it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.36it/s][AEpoch 40:  79%|███████▊  | 420/534 [05:04<01:22,  1.38it/s, loss=0.499, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.84it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.33it/s][AEpoch 40:  82%|████████▏ | 440/534 [05:05<01:05,  1.44it/s, loss=0.499, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.75it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.50it/s][AEpoch 40:  86%|████████▌ | 460/534 [05:07<00:49,  1.50it/s, loss=0.499, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 11.74it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.44it/s][AEpoch 40:  90%|████████▉ | 480/534 [05:09<00:34,  1.56it/s, loss=0.499, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.48it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.67it/s][AEpoch 40:  94%|█████████▎| 500/534 [05:10<00:21,  1.61it/s, loss=0.499, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.54it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:02, 11.81it/s][AEpoch 40:  97%|█████████▋| 520/534 [05:12<00:08,  1.67it/s, loss=0.499, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:01, 13.84it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.66it/s][A
Validating: 100%|██████████| 334/334 [00:28<00:00, 15.10it/s][Avalidation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9809535145759583
Epoch 40: 100%|██████████| 534/534 [05:20<00:00,  1.67it/s, loss=0.499, v_num=661]
                                                             [AEpoch 40:   0%|          | 0/534 [00:00<00:00, 10155.70it/s, loss=0.499, v_num=661]Epoch 41:   0%|          | 0/534 [00:00<00:00, 2626.36it/s, loss=0.499, v_num=661] Epoch 41:   0%|          | 0/534 [00:14<2:07:18, 14.30s/it, loss=0.499, v_num=661]Epoch 41:   2%|▏         | 10/534 [00:23<18:31,  2.12s/it, loss=0.499, v_num=661] Epoch 41:   2%|▏         | 10/534 [00:23<18:31,  2.12s/it, loss=0.489, v_num=661]Epoch 41:   4%|▎         | 20/534 [00:40<16:25,  1.92s/it, loss=0.489, v_num=661]Epoch 41:   4%|▎         | 20/534 [00:40<16:25,  1.92s/it, loss=0.454, v_num=661]Epoch 41:   6%|▌         | 30/534 [01:00<16:25,  1.96s/it, loss=0.454, v_num=661]Epoch 41:   6%|▌         | 30/534 [01:00<16:25,  1.96s/it, loss=0.465, v_num=661]Epoch 41:   7%|▋         | 40/534 [01:15<15:04,  1.83s/it, loss=0.465, v_num=661]Epoch 41:   7%|▋         | 40/534 [01:15<15:04,  1.83s/it, loss=0.476, v_num=661]Epoch 41:   9%|▉         | 50/534 [01:31<14:26,  1.79s/it, loss=0.476, v_num=661]Epoch 41:   9%|▉         | 50/534 [01:31<14:26,  1.79s/it, loss=0.47, v_num=661] Epoch 41:  11%|█         | 60/534 [01:47<13:55,  1.76s/it, loss=0.47, v_num=661]Epoch 41:  11%|█         | 60/534 [01:47<13:55,  1.76s/it, loss=0.482, v_num=661]Epoch 41:  13%|█▎        | 70/534 [02:01<13:16,  1.72s/it, loss=0.482, v_num=661]Epoch 41:  13%|█▎        | 70/534 [02:01<13:16,  1.72s/it, loss=0.491, v_num=661]Epoch 41:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.491, v_num=661]Epoch 41:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.478, v_num=661]Epoch 41:  17%|█▋        | 90/534 [02:30<12:15,  1.66s/it, loss=0.478, v_num=661]Epoch 41:  17%|█▋        | 90/534 [02:30<12:15,  1.66s/it, loss=0.467, v_num=661]Epoch 41:  19%|█▊        | 100/534 [02:45<11:49,  1.64s/it, loss=0.467, v_num=661]Epoch 41:  19%|█▊        | 100/534 [02:45<11:49,  1.64s/it, loss=0.471, v_num=661]Epoch 41:  21%|██        | 110/534 [02:57<11:17,  1.60s/it, loss=0.471, v_num=661]Epoch 41:  21%|██        | 110/534 [02:57<11:17,  1.60s/it, loss=0.478, v_num=661]Epoch 41:  22%|██▏       | 120/534 [03:11<10:54,  1.58s/it, loss=0.478, v_num=661]Epoch 41:  22%|██▏       | 120/534 [03:11<10:54,  1.58s/it, loss=0.493, v_num=661]Epoch 41:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.493, v_num=661]Epoch 41:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.501, v_num=661]Epoch 41:  26%|██▌       | 140/534 [03:41<10:19,  1.57s/it, loss=0.501, v_num=661]Epoch 41:  26%|██▌       | 140/534 [03:41<10:19,  1.57s/it, loss=0.497, v_num=661]Epoch 41:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.497, v_num=661]Epoch 41:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.488, v_num=661]Epoch 41:  30%|██▉       | 160/534 [04:09<09:40,  1.55s/it, loss=0.488, v_num=661]Epoch 41:  30%|██▉       | 160/534 [04:09<09:40,  1.55s/it, loss=0.476, v_num=661]Epoch 41:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.476, v_num=661]Epoch 41:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.478, v_num=661]Epoch 41:  34%|███▎      | 180/534 [04:32<08:53,  1.51s/it, loss=0.478, v_num=661]Epoch 41:  34%|███▎      | 180/534 [04:32<08:53,  1.51s/it, loss=0.482, v_num=661]validation_epoch_end
graph acc: 0.4251497005988024
valid accuracy: 0.9783633947372437
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9807476997375488
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9807561635971069
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9823378324508667
s=0.475, v_num=661]validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9824650287628174
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.9819148182868958
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9810861349105835
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9825634360313416

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9809980988502502
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.9811299443244934

Validating:   3%|▎         | 10/334 [00:01<01:00,  5.33it/s][AEpoch 41:  41%|████      | 220/534 [04:46<06:47,  1.30s/it, loss=0.475, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.37it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:35,  8.61it/s][AEpoch 41:  45%|████▍     | 240/534 [04:48<05:52,  1.20s/it, loss=0.475, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.74it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.05it/s][AEpoch 41:  49%|████▊     | 260/534 [04:50<05:04,  1.11s/it, loss=0.475, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.87it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.54it/s][AEpoch 41:  52%|█████▏    | 280/534 [04:53<04:25,  1.04s/it, loss=0.475, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.22it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24,  9.93it/s][AEpoch 41:  56%|█████▌    | 300/534 [04:54<03:49,  1.02it/s, loss=0.475, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 11.04it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.62it/s][AEpoch 41:  60%|█████▉    | 320/534 [04:56<03:17,  1.08it/s, loss=0.475, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.03it/s][A
Validating:  39%|███▉      | 130/334 [00:14<00:24,  8.42it/s][AEpoch 41:  64%|██████▎   | 340/534 [04:58<02:50,  1.14it/s, loss=0.475, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.52it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:14, 12.38it/s][AEpoch 41:  67%|██████▋   | 360/534 [04:59<02:24,  1.20it/s, loss=0.475, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:15, 10.91it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.94it/s][AEpoch 41:  71%|███████   | 380/534 [05:01<02:01,  1.26it/s, loss=0.475, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.95it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:11, 12.66it/s][AEpoch 41:  75%|███████▍  | 400/534 [05:03<01:41,  1.32it/s, loss=0.475, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.35it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.16it/s][AEpoch 41:  79%|███████▊  | 420/534 [05:04<01:22,  1.38it/s, loss=0.475, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 12.01it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.60it/s][AEpoch 41:  82%|████████▏ | 440/534 [05:06<01:05,  1.44it/s, loss=0.475, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.27it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.65it/s][AEpoch 41:  86%|████████▌ | 460/534 [05:08<00:49,  1.50it/s, loss=0.475, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:05, 12.48it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.11it/s][AEpoch 41:  90%|████████▉ | 480/534 [05:09<00:34,  1.55it/s, loss=0.475, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.72it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 13.39it/s][AEpoch 41:  94%|█████████▎| 500/534 [05:11<00:21,  1.61it/s, loss=0.475, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.37it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.39it/s][AEpoch 41:  97%|█████████▋| 520/534 [05:12<00:08,  1.67it/s, loss=0.475, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.85it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.87it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9814726114273071
Epoch 41: 100%|██████████| 534/534 [05:19<00:00,  1.67it/s, loss=0.475, v_num=661]
                                                             [AEpoch 41:   0%|          | 0/534 [00:00<00:00, 5675.65it/s, loss=0.475, v_num=661]Epoch 42:   0%|          | 0/534 [00:00<00:00, 1971.01it/s, loss=0.475, v_num=661]Epoch 42:   0%|          | 0/534 [00:14<2:04:56, 14.04s/it, loss=0.475, v_num=661]Epoch 42:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.475, v_num=661] Epoch 42:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.461, v_num=661]Epoch 42:   4%|▎         | 20/534 [00:44<18:10,  2.12s/it, loss=0.461, v_num=661]Epoch 42:   4%|▎         | 20/534 [00:44<18:10,  2.12s/it, loss=0.434, v_num=661]Epoch 42:   6%|▌         | 30/534 [00:57<15:29,  1.84s/it, loss=0.434, v_num=661]Epoch 42:   6%|▌         | 30/534 [00:57<15:29,  1.84s/it, loss=0.445, v_num=661]Epoch 42:   7%|▋         | 40/534 [01:12<14:29,  1.76s/it, loss=0.445, v_num=661]Epoch 42:   7%|▋         | 40/534 [01:12<14:29,  1.76s/it, loss=0.471, v_num=661]Epoch 42:   9%|▉         | 50/534 [01:29<14:05,  1.75s/it, loss=0.471, v_num=661]Epoch 42:   9%|▉         | 50/534 [01:29<14:05,  1.75s/it, loss=0.456, v_num=661]Epoch 42:  11%|█         | 60/534 [01:43<13:24,  1.70s/it, loss=0.456, v_num=661]Epoch 42:  11%|█         | 60/534 [01:43<13:24,  1.70s/it, loss=0.443, v_num=661]Epoch 42:  13%|█▎        | 70/534 [01:55<12:36,  1.63s/it, loss=0.443, v_num=661]Epoch 42:  13%|█▎        | 70/534 [01:55<12:36,  1.63s/it, loss=0.467, v_num=661]Epoch 42:  15%|█▍        | 80/534 [02:12<12:24,  1.64s/it, loss=0.467, v_num=661]Epoch 42:  15%|█▍        | 80/534 [02:12<12:24,  1.64s/it, loss=0.479, v_num=661]Epoch 42:  17%|█▋        | 90/534 [02:29<12:09,  1.64s/it, loss=0.479, v_num=661]Epoch 42:  17%|█▋        | 90/534 [02:29<12:09,  1.64s/it, loss=0.467, v_num=661]Epoch 42:  19%|█▊        | 100/534 [02:41<11:35,  1.60s/it, loss=0.467, v_num=661]Epoch 42:  19%|█▊        | 100/534 [02:41<11:35,  1.60s/it, loss=0.459, v_num=661]Epoch 42:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.459, v_num=661]Epoch 42:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.463, v_num=661]Epoch 42:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.463, v_num=661]Epoch 42:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.452, v_num=661]Epoch 42:  24%|██▍       | 130/534 [03:29<10:45,  1.60s/it, loss=0.452, v_num=661]Epoch 42:  24%|██▍       | 130/534 [03:29<10:45,  1.60s/it, loss=0.448, v_num=661]Epoch 42:  26%|██▌       | 140/534 [03:45<10:29,  1.60s/it, loss=0.448, v_num=661]Epoch 42:  26%|██▌       | 140/534 [03:45<10:29,  1.60s/it, loss=0.469, v_num=661]Epoch 42:  28%|██▊       | 150/534 [04:00<10:11,  1.59s/it, loss=0.469, v_num=661]Epoch 42:  28%|██▊       | 150/534 [04:00<10:11,  1.59s/it, loss=0.462, v_num=661]Epoch 42:  30%|██▉       | 160/534 [04:09<09:40,  1.55s/it, loss=0.462, v_num=661]Epoch 42:  30%|██▉       | 160/534 [04:09<09:40,  1.55s/it, loss=0.439, v_num=661]Epoch 42:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.439, v_num=661]Epoch 42:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.46, v_num=661] Epoch 42:  34%|███▎      | 180/534 [04:30<08:48,  1.49s/it, loss=0.46, v_num=661]Epoch 42:  34%|███▎      | 180/534 [04:30<08:48,  1.49s/it, loss=0.472, v_num=661]validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9810279011726379
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9822991490364075
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9780929088592529
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9813162684440613
0.461, v_num=661]validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9821916222572327
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9816890954971313

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9811248183250427
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9818751811981201
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9808469414710999
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.98074871301651

Validating:   3%|▎         | 10/334 [00:01<00:55,  5.84it/s][AEpoch 42:  41%|████      | 220/534 [04:47<06:48,  1.30s/it, loss=0.461, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:39,  8.02it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:32,  9.39it/s][AEpoch 42:  45%|████▍     | 240/534 [04:49<05:53,  1.20s/it, loss=0.461, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.01it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:26, 10.54it/s][AEpoch 42:  49%|████▊     | 260/534 [04:51<05:05,  1.12s/it, loss=0.461, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:23, 11.69it/s][A
Validating:  21%|██        | 70/334 [00:08<00:32,  8.05it/s][AEpoch 42:  52%|█████▏    | 280/534 [04:53<04:25,  1.05s/it, loss=0.461, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.62it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:20, 11.64it/s][AEpoch 42:  56%|█████▌    | 300/534 [04:55<03:49,  1.02it/s, loss=0.461, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.35it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.15it/s][AEpoch 42:  60%|█████▉    | 320/534 [04:56<03:17,  1.08it/s, loss=0.461, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.63it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.00it/s][AEpoch 42:  64%|██████▎   | 340/534 [04:59<02:50,  1.14it/s, loss=0.461, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:17, 11.30it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.92it/s][AEpoch 42:  67%|██████▋   | 360/534 [05:00<02:24,  1.20it/s, loss=0.461, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 11.70it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.08it/s][AEpoch 42:  71%|███████   | 380/534 [05:02<02:02,  1.26it/s, loss=0.461, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.97it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.45it/s][AEpoch 42:  75%|███████▍  | 400/534 [05:03<01:41,  1.32it/s, loss=0.461, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.84it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:09, 12.54it/s][AEpoch 42:  79%|███████▊  | 420/534 [05:05<01:22,  1.38it/s, loss=0.461, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.18it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.74it/s][AEpoch 42:  82%|████████▏ | 440/534 [05:06<01:05,  1.44it/s, loss=0.461, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.38it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.90it/s][AEpoch 42:  86%|████████▌ | 460/534 [05:08<00:49,  1.49it/s, loss=0.461, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.57it/s][A
Validating:  81%|████████  | 270/334 [00:23<00:05, 12.74it/s][AEpoch 42:  90%|████████▉ | 480/534 [05:09<00:34,  1.55it/s, loss=0.461, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:24<00:04, 11.78it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 13.20it/s][AEpoch 42:  94%|█████████▎| 500/534 [05:11<00:21,  1.61it/s, loss=0.461, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:25<00:02, 16.71it/s][A
Validating:  93%|█████████▎| 310/334 [00:26<00:01, 13.04it/s][AEpoch 42:  97%|█████████▋| 520/534 [05:12<00:08,  1.67it/s, loss=0.461, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 15.00it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.24it/s][Avalidation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9815924167633057
Epoch 42: 100%|██████████| 534/534 [05:22<00:00,  1.66it/s, loss=0.461, v_num=661]
                                                             [AEpoch 42:   0%|          | 0/534 [00:00<00:00, 12122.27it/s, loss=0.461, v_num=661]Epoch 43:   0%|          | 0/534 [00:00<00:00, 3102.30it/s, loss=0.461, v_num=661] Epoch 43:   0%|          | 0/534 [00:10<1:36:29, 10.84s/it, loss=0.461, v_num=661]Epoch 43:   2%|▏         | 10/534 [00:22<17:56,  2.05s/it, loss=0.461, v_num=661] Epoch 43:   2%|▏         | 10/534 [00:22<17:56,  2.05s/it, loss=0.433, v_num=661]Epoch 43:   4%|▎         | 20/534 [00:43<17:41,  2.07s/it, loss=0.433, v_num=661]Epoch 43:   4%|▎         | 20/534 [00:43<17:41,  2.07s/it, loss=0.432, v_num=661]Epoch 43:   6%|▌         | 30/534 [00:55<15:08,  1.80s/it, loss=0.432, v_num=661]Epoch 43:   6%|▌         | 30/534 [00:55<15:08,  1.80s/it, loss=0.45, v_num=661] Epoch 43:   7%|▋         | 40/534 [01:11<14:15,  1.73s/it, loss=0.45, v_num=661]Epoch 43:   7%|▋         | 40/534 [01:11<14:15,  1.73s/it, loss=0.448, v_num=661]Epoch 43:   9%|▉         | 50/534 [01:27<13:52,  1.72s/it, loss=0.448, v_num=661]Epoch 43:   9%|▉         | 50/534 [01:27<13:52,  1.72s/it, loss=0.433, v_num=661]Epoch 43:  11%|█         | 60/534 [01:40<13:00,  1.65s/it, loss=0.433, v_num=661]Epoch 43:  11%|█         | 60/534 [01:40<13:00,  1.65s/it, loss=0.417, v_num=661]Epoch 43:  13%|█▎        | 70/534 [01:54<12:28,  1.61s/it, loss=0.417, v_num=661]Epoch 43:  13%|█▎        | 70/534 [01:54<12:28,  1.61s/it, loss=0.411, v_num=661]Epoch 43:  15%|█▍        | 80/534 [02:09<12:07,  1.60s/it, loss=0.411, v_num=661]Epoch 43:  15%|█▍        | 80/534 [02:09<12:07,  1.60s/it, loss=0.413, v_num=661]Epoch 43:  17%|█▋        | 90/534 [02:25<11:49,  1.60s/it, loss=0.413, v_num=661]Epoch 43:  17%|█▋        | 90/534 [02:25<11:49,  1.60s/it, loss=0.438, v_num=661]Epoch 43:  19%|█▊        | 100/534 [02:37<11:18,  1.56s/it, loss=0.438, v_num=661]Epoch 43:  19%|█▊        | 100/534 [02:37<11:18,  1.56s/it, loss=0.452, v_num=661]Epoch 43:  21%|██        | 110/534 [02:52<10:59,  1.55s/it, loss=0.452, v_num=661]Epoch 43:  21%|██        | 110/534 [02:52<10:59,  1.55s/it, loss=0.447, v_num=661]Epoch 43:  22%|██▏       | 120/534 [03:06<10:37,  1.54s/it, loss=0.447, v_num=661]Epoch 43:  22%|██▏       | 120/534 [03:06<10:37,  1.54s/it, loss=0.454, v_num=661]Epoch 43:  24%|██▍       | 130/534 [03:20<10:18,  1.53s/it, loss=0.454, v_num=661]Epoch 43:  24%|██▍       | 130/534 [03:20<10:18,  1.53s/it, loss=0.452, v_num=661]Epoch 43:  26%|██▌       | 140/534 [03:46<10:33,  1.61s/it, loss=0.452, v_num=661]Epoch 43:  26%|██▌       | 140/534 [03:46<10:33,  1.61s/it, loss=0.45, v_num=661] Epoch 43:  28%|██▊       | 150/534 [03:58<10:07,  1.58s/it, loss=0.45, v_num=661]Epoch 43:  28%|██▊       | 150/534 [03:58<10:07,  1.58s/it, loss=0.444, v_num=661]Epoch 43:  30%|██▉       | 160/534 [04:12<09:46,  1.57s/it, loss=0.444, v_num=661]Epoch 43:  30%|██▉       | 160/534 [04:12<09:46,  1.57s/it, loss=0.442, v_num=661]Epoch 43:  32%|███▏      | 170/534 [04:20<09:14,  1.52s/it, loss=0.442, v_num=661]Epoch 43:  32%|███▏      | 170/534 [04:20<09:14,  1.52s/it, loss=0.444, v_num=661]Epoch 43:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.444, v_num=661]Epoch 43:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.441, v_num=661]validation_epoch_end
graph acc: 0.4251497005988024
valid accuracy: 0.9779383540153503
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9822216629981995
validation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9811562299728394
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9804275035858154
46, v_num=661]Epoch 43:  37%|███▋      | 200/534 [04:43<07:50,  1.41s/it, loss=0.44, v_num=661] validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9824700951576233
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9826874136924744
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9814342260360718
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9812247157096863

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.9804818630218506

Validating:   3%|▎         | 10/334 [00:01<00:59,  5.47it/s][AEpoch 43:  41%|████      | 220/534 [04:44<06:44,  1.29s/it, loss=0.44, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:38,  8.06it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  8.94it/s][AEpoch 43:  45%|████▍     | 240/534 [04:46<05:49,  1.19s/it, loss=0.44, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.04it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:30,  9.35it/s][AEpoch 43:  49%|████▊     | 260/534 [04:48<05:03,  1.11s/it, loss=0.44, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.59it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.56it/s][AEpoch 43:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.44, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.29it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24,  9.90it/s][AEpoch 43:  56%|█████▌    | 300/534 [04:53<03:47,  1.03it/s, loss=0.44, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:24,  9.70it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.35it/s][AEpoch 43:  60%|█████▉    | 320/534 [04:54<03:16,  1.09it/s, loss=0.44, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.42it/s][A
Validating:  39%|███▉      | 130/334 [00:14<00:22,  9.12it/s][AEpoch 43:  64%|██████▎   | 340/534 [04:57<02:49,  1.15it/s, loss=0.44, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.95it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:14, 12.53it/s][AEpoch 43:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.44, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:15, 11.30it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.64it/s][AEpoch 43:  71%|███████   | 380/534 [05:00<02:01,  1.27it/s, loss=0.44, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:14, 10.63it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:13, 10.76it/s][AEpoch 43:  75%|███████▍  | 400/534 [05:02<01:40,  1.33it/s, loss=0.44, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 12.17it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.01it/s][AEpoch 43:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.44, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 11.56it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.31it/s][AEpoch 43:  82%|████████▏ | 440/534 [05:04<01:05,  1.45it/s, loss=0.44, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.19it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.58it/s][AEpoch 43:  86%|████████▌ | 460/534 [05:07<00:49,  1.50it/s, loss=0.44, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.93it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:05, 12.06it/s][AEpoch 43:  90%|████████▉ | 480/534 [05:08<00:34,  1.56it/s, loss=0.44, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:05, 10.49it/s][A
Validating:  87%|████████▋ | 290/334 [00:27<00:03, 12.33it/s][AEpoch 43:  94%|█████████▎| 500/534 [05:10<00:21,  1.62it/s, loss=0.44, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 13.69it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.75it/s][AEpoch 43:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.44, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:29<00:00, 14.04it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.98it/s][Avalidation_epoch_end
graph acc: 0.45209580838323354
valid accuracy: 0.9815125465393066
Epoch 43: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.44, v_num=661]
                                                             [AEpoch 43:   0%|          | 0/534 [00:00<00:00, 9383.23it/s, loss=0.44, v_num=661]Epoch 44:   0%|          | 0/534 [00:00<00:00, 2432.89it/s, loss=0.44, v_num=661]Epoch 44:   0%|          | 0/534 [00:11<1:42:01, 11.46s/it, loss=0.44, v_num=661]Epoch 44:   2%|▏         | 10/534 [00:38<30:37,  3.51s/it, loss=0.44, v_num=661] Epoch 44:   2%|▏         | 10/534 [00:38<30:37,  3.51s/it, loss=0.425, v_num=661]Epoch 44:   4%|▎         | 20/534 [00:51<20:53,  2.44s/it, loss=0.425, v_num=661]Epoch 44:   4%|▎         | 20/534 [00:51<20:53,  2.44s/it, loss=0.429, v_num=661]Epoch 44:   6%|▌         | 30/534 [01:04<17:29,  2.08s/it, loss=0.429, v_num=661]Epoch 44:   6%|▌         | 30/534 [01:04<17:29,  2.08s/it, loss=0.429, v_num=661]Epoch 44:   7%|▋         | 40/534 [01:16<15:27,  1.88s/it, loss=0.429, v_num=661]Epoch 44:   7%|▋         | 40/534 [01:16<15:27,  1.88s/it, loss=0.406, v_num=661]Epoch 44:   9%|▉         | 50/534 [01:30<14:20,  1.78s/it, loss=0.406, v_num=661]Epoch 44:   9%|▉         | 50/534 [01:30<14:20,  1.78s/it, loss=0.401, v_num=661]Epoch 44:  11%|█         | 60/534 [01:45<13:41,  1.73s/it, loss=0.401, v_num=661]Epoch 44:  11%|█         | 60/534 [01:45<13:41,  1.73s/it, loss=0.43, v_num=661] Epoch 44:  13%|█▎        | 70/534 [01:57<12:50,  1.66s/it, loss=0.43, v_num=661]Epoch 44:  13%|█▎        | 70/534 [01:57<12:50,  1.66s/it, loss=0.435, v_num=661]Epoch 44:  15%|█▍        | 80/534 [02:09<12:08,  1.60s/it, loss=0.435, v_num=661]Epoch 44:  15%|█▍        | 80/534 [02:09<12:08,  1.60s/it, loss=0.41, v_num=661] Epoch 44:  17%|█▋        | 90/534 [02:22<11:37,  1.57s/it, loss=0.41, v_num=661]Epoch 44:  17%|█▋        | 90/534 [02:22<11:37,  1.57s/it, loss=0.404, v_num=661]Epoch 44:  19%|█▊        | 100/534 [02:38<11:20,  1.57s/it, loss=0.404, v_num=661]Epoch 44:  19%|█▊        | 100/534 [02:38<11:20,  1.57s/it, loss=0.426, v_num=661]Epoch 44:  21%|██        | 110/534 [02:52<10:58,  1.55s/it, loss=0.426, v_num=661]Epoch 44:  21%|██        | 110/534 [02:52<10:58,  1.55s/it, loss=0.435, v_num=661]Epoch 44:  22%|██▏       | 120/534 [03:06<10:38,  1.54s/it, loss=0.435, v_num=661]Epoch 44:  22%|██▏       | 120/534 [03:06<10:38,  1.54s/it, loss=0.424, v_num=661]Epoch 44:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.424, v_num=661]Epoch 44:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.423, v_num=661]Epoch 44:  26%|██▌       | 140/534 [03:40<10:15,  1.56s/it, loss=0.423, v_num=661]Epoch 44:  26%|██▌       | 140/534 [03:40<10:15,  1.56s/it, loss=0.424, v_num=661]Epoch 44:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.424, v_num=661]Epoch 44:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.43, v_num=661] Epoch 44:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.43, v_num=661]Epoch 44:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.435, v_num=661]Epoch 44:  32%|███▏      | 170/534 [04:19<09:11,  1.52s/it, loss=0.435, v_num=661]Epoch 44:  32%|███▏      | 170/534 [04:19<09:11,  1.52s/it, loss=0.433, v_num=661]Epoch 44:  34%|███▎      | 180/534 [04:30<08:48,  1.49s/it, loss=0.433, v_num=661]Epoch 44:  34%|███▎      | 180/534 [04:30<08:48,  1.49s/it, loss=0.432, v_num=661]validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9788656830787659
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.982686460018158
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9805476069450378
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.981276273727417
ss=0.418, v_num=661]validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9824650287628174
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9815502762794495
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.982646107673645

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9812625050544739
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9810917973518372
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.982866644859314

Validating:   3%|▎         | 10/334 [00:01<00:58,  5.57it/s][AEpoch 44:  41%|████      | 220/534 [04:46<06:46,  1.29s/it, loss=0.418, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:38,  8.14it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.93it/s][AEpoch 44:  45%|████▍     | 240/534 [04:47<05:51,  1.19s/it, loss=0.418, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:31,  9.23it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.23it/s][AEpoch 44:  49%|████▊     | 260/534 [04:49<05:04,  1.11s/it, loss=0.418, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.54it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.84it/s][AEpoch 44:  52%|█████▏    | 280/534 [04:52<04:24,  1.04s/it, loss=0.418, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25,  9.92it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.99it/s][AEpoch 44:  56%|█████▌    | 300/534 [04:53<03:48,  1.02it/s, loss=0.418, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.74it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.36it/s][AEpoch 44:  60%|█████▉    | 320/534 [04:55<03:16,  1.09it/s, loss=0.418, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:15, 13.41it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.53it/s][AEpoch 44:  64%|██████▎   | 340/534 [04:57<02:49,  1.15it/s, loss=0.418, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:17, 11.10it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.46it/s][AEpoch 44:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.418, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.26it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.17it/s][AEpoch 44:  71%|███████   | 380/534 [05:00<02:01,  1.27it/s, loss=0.418, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.40it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:11, 12.13it/s][AEpoch 44:  75%|███████▍  | 400/534 [05:02<01:41,  1.33it/s, loss=0.418, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.21it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.39it/s][AEpoch 44:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.418, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:08, 12.82it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.76it/s][AEpoch 44:  82%|████████▏ | 440/534 [05:05<01:05,  1.44it/s, loss=0.418, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.82it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:06, 12.26it/s][AEpoch 44:  86%|████████▌ | 460/534 [05:07<00:49,  1.50it/s, loss=0.418, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.74it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.56it/s][AEpoch 44:  90%|████████▉ | 480/534 [05:08<00:34,  1.56it/s, loss=0.418, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:05, 10.29it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.76it/s][AEpoch 44:  94%|█████████▎| 500/534 [05:10<00:21,  1.61it/s, loss=0.418, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.33it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.02it/s][AEpoch 44:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.418, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.92it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.54it/s][Avalidation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9813128709793091
Epoch 44: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.418, v_num=661]
                                                             [AEpoch 44:   0%|          | 0/534 [00:00<00:00, 13231.24it/s, loss=0.418, v_num=661]Epoch 45:   0%|          | 0/534 [00:00<00:00, 3175.10it/s, loss=0.418, v_num=661] Epoch 45:   0%|          | 0/534 [00:12<1:47:16, 12.05s/it, loss=0.418, v_num=661]Epoch 45:   2%|▏         | 10/534 [00:24<19:17,  2.21s/it, loss=0.418, v_num=661] Epoch 45:   2%|▏         | 10/534 [00:24<19:17,  2.21s/it, loss=0.403, v_num=661]Epoch 45:   4%|▎         | 20/534 [00:37<15:24,  1.80s/it, loss=0.403, v_num=661]Epoch 45:   4%|▎         | 20/534 [00:37<15:24,  1.80s/it, loss=0.391, v_num=661]Epoch 45:   6%|▌         | 30/534 [00:52<14:10,  1.69s/it, loss=0.391, v_num=661]Epoch 45:   6%|▌         | 30/534 [00:52<14:10,  1.69s/it, loss=0.395, v_num=661]Epoch 45:   7%|▋         | 40/534 [01:09<14:00,  1.70s/it, loss=0.395, v_num=661]Epoch 45:   7%|▋         | 40/534 [01:09<14:00,  1.70s/it, loss=0.407, v_num=661]Epoch 45:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.407, v_num=661]Epoch 45:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.41, v_num=661] Epoch 45:  11%|█         | 60/534 [01:41<13:09,  1.67s/it, loss=0.41, v_num=661]Epoch 45:  11%|█         | 60/534 [01:41<13:09,  1.67s/it, loss=0.425, v_num=661]Epoch 45:  13%|█▎        | 70/534 [01:57<12:44,  1.65s/it, loss=0.425, v_num=661]Epoch 45:  13%|█▎        | 70/534 [01:57<12:44,  1.65s/it, loss=0.418, v_num=661]Epoch 45:  15%|█▍        | 80/534 [02:08<11:59,  1.59s/it, loss=0.418, v_num=661]Epoch 45:  15%|█▍        | 80/534 [02:08<11:59,  1.59s/it, loss=0.406, v_num=661]Epoch 45:  17%|█▋        | 90/534 [02:22<11:37,  1.57s/it, loss=0.406, v_num=661]Epoch 45:  17%|█▋        | 90/534 [02:22<11:37,  1.57s/it, loss=0.404, v_num=661]Epoch 45:  19%|█▊        | 100/534 [02:39<11:23,  1.58s/it, loss=0.404, v_num=661]Epoch 45:  19%|█▊        | 100/534 [02:39<11:23,  1.58s/it, loss=0.405, v_num=661]Epoch 45:  21%|██        | 110/534 [02:55<11:09,  1.58s/it, loss=0.405, v_num=661]Epoch 45:  21%|██        | 110/534 [02:55<11:09,  1.58s/it, loss=0.413, v_num=661]Epoch 45:  22%|██▏       | 120/534 [03:20<11:25,  1.66s/it, loss=0.413, v_num=661]Epoch 45:  22%|██▏       | 120/534 [03:20<11:25,  1.66s/it, loss=0.419, v_num=661]Epoch 45:  24%|██▍       | 130/534 [03:34<11:00,  1.64s/it, loss=0.419, v_num=661]Epoch 45:  24%|██▍       | 130/534 [03:34<11:00,  1.64s/it, loss=0.419, v_num=661]Epoch 45:  26%|██▌       | 140/534 [03:46<10:32,  1.60s/it, loss=0.419, v_num=661]Epoch 45:  26%|██▌       | 140/534 [03:46<10:32,  1.60s/it, loss=0.417, v_num=661]Epoch 45:  28%|██▊       | 150/534 [03:59<10:08,  1.59s/it, loss=0.417, v_num=661]Epoch 45:  28%|██▊       | 150/534 [03:59<10:08,  1.59s/it, loss=0.424, v_num=661]Epoch 45:  30%|██▉       | 160/534 [04:10<09:41,  1.55s/it, loss=0.424, v_num=661]Epoch 45:  30%|██▉       | 160/534 [04:10<09:41,  1.55s/it, loss=0.425, v_num=661]Epoch 45:  32%|███▏      | 170/534 [04:22<09:18,  1.54s/it, loss=0.425, v_num=661]Epoch 45:  32%|███▏      | 170/534 [04:22<09:18,  1.54s/it, loss=0.418, v_num=661]Epoch 45:  34%|███▎      | 180/534 [04:29<08:46,  1.49s/it, loss=0.418, v_num=661]Epoch 45:  34%|███▎      | 180/534 [04:29<08:46,  1.49s/it, loss=0.41, v_num=661] validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9788656830787659
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.982686460018158
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9810362458229065
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9804275035858154
0.415, v_num=661]validation_epoch_end
graph acc: 0.4311377245508982
valid accuracy: 0.9824303984642029

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9818831086158752
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9824808239936829
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9816024899482727
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9809393286705017
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9818597435951233

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.76it/s][AEpoch 45:  41%|████      | 220/534 [04:40<06:38,  1.27s/it, loss=0.415, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:41,  7.53it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:29, 10.36it/s][AEpoch 45:  45%|████▍     | 240/534 [04:42<05:44,  1.17s/it, loss=0.415, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.08it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.35it/s][AEpoch 45:  49%|████▊     | 260/534 [04:44<04:58,  1.09s/it, loss=0.415, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:22, 12.17it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.75it/s][AEpoch 45:  52%|█████▏    | 280/534 [04:47<04:19,  1.02s/it, loss=0.415, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.46it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.44it/s][AEpoch 45:  56%|█████▌    | 300/534 [04:48<03:44,  1.04it/s, loss=0.415, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 11.14it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.72it/s][AEpoch 45:  60%|█████▉    | 320/534 [04:50<03:13,  1.11it/s, loss=0.415, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:18, 11.80it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  8.97it/s][AEpoch 45:  64%|██████▎   | 340/534 [04:52<02:46,  1.17it/s, loss=0.415, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.97it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:13, 13.33it/s][AEpoch 45:  67%|██████▋   | 360/534 [04:53<02:21,  1.23it/s, loss=0.415, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 11.62it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.75it/s][AEpoch 45:  71%|███████   | 380/534 [04:55<01:59,  1.29it/s, loss=0.415, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.49it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.51it/s][AEpoch 45:  75%|███████▍  | 400/534 [04:57<01:39,  1.35it/s, loss=0.415, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.47it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.16it/s][AEpoch 45:  79%|███████▊  | 420/534 [04:58<01:20,  1.41it/s, loss=0.415, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:08, 12.76it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 14.06it/s][AEpoch 45:  82%|████████▏ | 440/534 [04:59<01:03,  1.47it/s, loss=0.415, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 15.00it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.63it/s][AEpoch 45:  86%|████████▌ | 460/534 [05:01<00:48,  1.53it/s, loss=0.415, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 11.73it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.07it/s][AEpoch 45:  90%|████████▉ | 480/534 [05:03<00:34,  1.59it/s, loss=0.415, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.13it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.90it/s][AEpoch 45:  94%|█████████▎| 500/534 [05:04<00:20,  1.64it/s, loss=0.415, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.20it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.61it/s][AEpoch 45:  97%|█████████▋| 520/534 [05:06<00:08,  1.70it/s, loss=0.415, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.39it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.64it/s][A
Validating: 100%|██████████| 334/334 [00:28<00:00, 14.41it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9808736443519592
Epoch 45: 100%|██████████| 534/534 [05:13<00:00,  1.71it/s, loss=0.415, v_num=661]
                                                             [AEpoch 45:   0%|          | 0/534 [00:00<00:00, 13706.88it/s, loss=0.415, v_num=661]Epoch 46:   0%|          | 0/534 [00:00<00:00, 3266.59it/s, loss=0.415, v_num=661] Epoch 46:   0%|          | 0/534 [00:18<2:40:50, 18.07s/it, loss=0.415, v_num=661]Epoch 46:   2%|▏         | 10/534 [00:24<19:18,  2.21s/it, loss=0.415, v_num=661] Epoch 46:   2%|▏         | 10/534 [00:24<19:18,  2.21s/it, loss=0.402, v_num=661]Epoch 46:   4%|▎         | 20/534 [00:43<17:41,  2.07s/it, loss=0.402, v_num=661]Epoch 46:   4%|▎         | 20/534 [00:43<17:41,  2.07s/it, loss=0.392, v_num=661]Epoch 46:   6%|▌         | 30/534 [00:56<15:21,  1.83s/it, loss=0.392, v_num=661]Epoch 46:   6%|▌         | 30/534 [00:56<15:21,  1.83s/it, loss=0.395, v_num=661]Epoch 46:   7%|▋         | 40/534 [01:09<13:57,  1.69s/it, loss=0.395, v_num=661]Epoch 46:   7%|▋         | 40/534 [01:09<13:57,  1.69s/it, loss=0.391, v_num=661]Epoch 46:   9%|▉         | 50/534 [01:23<13:14,  1.64s/it, loss=0.391, v_num=661]Epoch 46:   9%|▉         | 50/534 [01:23<13:14,  1.64s/it, loss=0.392, v_num=661]Epoch 46:  11%|█         | 60/534 [01:38<12:46,  1.62s/it, loss=0.392, v_num=661]Epoch 46:  11%|█         | 60/534 [01:38<12:46,  1.62s/it, loss=0.389, v_num=661]Epoch 46:  13%|█▎        | 70/534 [01:52<12:16,  1.59s/it, loss=0.389, v_num=661]Epoch 46:  13%|█▎        | 70/534 [01:52<12:16,  1.59s/it, loss=0.391, v_num=661]Epoch 46:  15%|█▍        | 80/534 [02:06<11:51,  1.57s/it, loss=0.391, v_num=661]Epoch 46:  15%|█▍        | 80/534 [02:06<11:51,  1.57s/it, loss=0.388, v_num=661]Epoch 46:  17%|█▋        | 90/534 [02:28<12:03,  1.63s/it, loss=0.388, v_num=661]Epoch 46:  17%|█▋        | 90/534 [02:28<12:03,  1.63s/it, loss=0.387, v_num=661]Epoch 46:  19%|█▊        | 100/534 [02:41<11:36,  1.60s/it, loss=0.387, v_num=661]Epoch 46:  19%|█▊        | 100/534 [02:41<11:36,  1.60s/it, loss=0.385, v_num=661]Epoch 46:  21%|██        | 110/534 [02:59<11:25,  1.62s/it, loss=0.385, v_num=661]Epoch 46:  21%|██        | 110/534 [02:59<11:25,  1.62s/it, loss=0.384, v_num=661]Epoch 46:  22%|██▏       | 120/534 [03:10<10:53,  1.58s/it, loss=0.384, v_num=661]Epoch 46:  22%|██▏       | 120/534 [03:10<10:53,  1.58s/it, loss=0.393, v_num=661]Epoch 46:  24%|██▍       | 130/534 [03:25<10:33,  1.57s/it, loss=0.393, v_num=661]Epoch 46:  24%|██▍       | 130/534 [03:25<10:33,  1.57s/it, loss=0.395, v_num=661]Epoch 46:  26%|██▌       | 140/534 [03:43<10:24,  1.59s/it, loss=0.395, v_num=661]Epoch 46:  26%|██▌       | 140/534 [03:43<10:24,  1.59s/it, loss=0.399, v_num=661]Epoch 46:  28%|██▊       | 150/534 [03:55<10:00,  1.56s/it, loss=0.399, v_num=661]Epoch 46:  28%|██▊       | 150/534 [03:55<10:00,  1.56s/it, loss=0.407, v_num=661]Epoch 46:  30%|██▉       | 160/534 [04:10<09:41,  1.56s/it, loss=0.407, v_num=661]Epoch 46:  30%|██▉       | 160/534 [04:10<09:41,  1.56s/it, loss=0.406, v_num=661]Epoch 46:  32%|███▏      | 170/534 [04:22<09:19,  1.54s/it, loss=0.406, v_num=661]Epoch 46:  32%|███▏      | 170/534 [04:22<09:19,  1.54s/it, loss=0.389, v_num=661]Epoch 46:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.389, v_num=661]Epoch 46:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.391, v_num=661]validation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9824540615081787
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.981956422328949
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9787883758544922
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9803874492645264
s=0.403, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9816663265228271
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9820382595062256
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9813758730888367
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.982589066028595
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9812824130058289
validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9831418991088867

Validating:   3%|▎         | 10/334 [00:01<01:02,  5.21it/s][AEpoch 46:  41%|████      | 220/534 [04:44<06:44,  1.29s/it, loss=0.403, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:41,  7.58it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:31,  9.59it/s][AEpoch 46:  45%|████▍     | 240/534 [04:46<05:49,  1.19s/it, loss=0.403, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.03it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.46it/s][AEpoch 46:  49%|████▊     | 260/534 [04:48<05:02,  1.10s/it, loss=0.403, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.66it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.84it/s][AEpoch 46:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.403, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.19it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.66it/s][AEpoch 46:  56%|█████▌    | 300/534 [04:52<03:47,  1.03it/s, loss=0.403, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.18it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.21it/s][AEpoch 46:  60%|█████▉    | 320/534 [04:54<03:16,  1.09it/s, loss=0.403, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.74it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:19, 10.39it/s][AEpoch 46:  64%|██████▎   | 340/534 [04:56<02:48,  1.15it/s, loss=0.403, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:16, 11.67it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.76it/s][AEpoch 46:  67%|██████▋   | 360/534 [04:57<02:23,  1.21it/s, loss=0.403, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:13, 12.45it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.35it/s][AEpoch 46:  71%|███████   | 380/534 [04:59<02:00,  1.27it/s, loss=0.403, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.95it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.45it/s][AEpoch 46:  75%|███████▍  | 400/534 [05:00<01:40,  1.33it/s, loss=0.403, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.44it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.19it/s][AEpoch 46:  79%|███████▊  | 420/534 [05:02<01:21,  1.39it/s, loss=0.403, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.36it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.55it/s][AEpoch 46:  82%|████████▏ | 440/534 [05:03<01:04,  1.45it/s, loss=0.403, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.54it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.43it/s][AEpoch 46:  86%|████████▌ | 460/534 [05:05<00:49,  1.51it/s, loss=0.403, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.28it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.48it/s][AEpoch 46:  90%|████████▉ | 480/534 [05:07<00:34,  1.57it/s, loss=0.403, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.88it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.91it/s][AEpoch 46:  94%|█████████▎| 500/534 [05:08<00:20,  1.62it/s, loss=0.403, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.05it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.50it/s][AEpoch 46:  97%|█████████▋| 520/534 [05:10<00:08,  1.68it/s, loss=0.403, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.26it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 12.72it/s][Avalidation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9811931252479553
Epoch 46: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.403, v_num=661]
                                                             [AEpoch 46:   0%|          | 0/534 [00:00<00:00, 15827.56it/s, loss=0.403, v_num=661]Epoch 47:   0%|          | 0/534 [00:00<00:00, 3628.29it/s, loss=0.403, v_num=661] Epoch 47:   0%|          | 0/534 [00:19<2:51:29, 19.27s/it, loss=0.403, v_num=661]Epoch 47:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.403, v_num=661] Epoch 47:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.392, v_num=661]Epoch 47:   4%|▎         | 20/534 [00:41<16:51,  1.97s/it, loss=0.392, v_num=661]Epoch 47:   4%|▎         | 20/534 [00:41<16:52,  1.97s/it, loss=0.38, v_num=661] Epoch 47:   6%|▌         | 30/534 [00:58<15:57,  1.90s/it, loss=0.38, v_num=661]Epoch 47:   6%|▌         | 30/534 [00:58<15:57,  1.90s/it, loss=0.38, v_num=661]Epoch 47:   7%|▋         | 40/534 [01:12<14:34,  1.77s/it, loss=0.38, v_num=661]Epoch 47:   7%|▋         | 40/534 [01:12<14:34,  1.77s/it, loss=0.38, v_num=661]Epoch 47:   9%|▉         | 50/534 [01:32<14:40,  1.82s/it, loss=0.38, v_num=661]Epoch 47:   9%|▉         | 50/534 [01:32<14:40,  1.82s/it, loss=0.378, v_num=661]Epoch 47:  11%|█         | 60/534 [01:43<13:27,  1.70s/it, loss=0.378, v_num=661]Epoch 47:  11%|█         | 60/534 [01:43<13:27,  1.70s/it, loss=0.383, v_num=661]Epoch 47:  13%|█▎        | 70/534 [02:01<13:14,  1.71s/it, loss=0.383, v_num=661]Epoch 47:  13%|█▎        | 70/534 [02:01<13:14,  1.71s/it, loss=0.382, v_num=661]Epoch 47:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.382, v_num=661]Epoch 47:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.38, v_num=661] Epoch 47:  17%|█▋        | 90/534 [02:27<11:58,  1.62s/it, loss=0.38, v_num=661]Epoch 47:  17%|█▋        | 90/534 [02:27<11:58,  1.62s/it, loss=0.385, v_num=661]Epoch 47:  19%|█▊        | 100/534 [02:44<11:45,  1.62s/it, loss=0.385, v_num=661]Epoch 47:  19%|█▊        | 100/534 [02:44<11:45,  1.62s/it, loss=0.372, v_num=661]Epoch 47:  21%|██        | 110/534 [02:56<11:15,  1.59s/it, loss=0.372, v_num=661]Epoch 47:  21%|██        | 110/534 [02:56<11:15,  1.59s/it, loss=0.373, v_num=661]Epoch 47:  22%|██▏       | 120/534 [03:20<11:24,  1.65s/it, loss=0.373, v_num=661]Epoch 47:  22%|██▏       | 120/534 [03:20<11:24,  1.65s/it, loss=0.377, v_num=661]Epoch 47:  24%|██▍       | 130/534 [03:31<10:51,  1.61s/it, loss=0.377, v_num=661]Epoch 47:  24%|██▍       | 130/534 [03:31<10:51,  1.61s/it, loss=0.375, v_num=661]Epoch 47:  26%|██▌       | 140/534 [03:44<10:28,  1.60s/it, loss=0.375, v_num=661]Epoch 47:  26%|██▌       | 140/534 [03:44<10:28,  1.60s/it, loss=0.38, v_num=661] Epoch 47:  28%|██▊       | 150/534 [03:57<10:03,  1.57s/it, loss=0.38, v_num=661]Epoch 47:  28%|██▊       | 150/534 [03:57<10:03,  1.57s/it, loss=0.386, v_num=661]Epoch 47:  30%|██▉       | 160/534 [04:11<09:45,  1.56s/it, loss=0.386, v_num=661]Epoch 47:  30%|██▉       | 160/534 [04:11<09:45,  1.56s/it, loss=0.384, v_num=661]Epoch 47:  32%|███▏      | 170/534 [04:25<09:25,  1.55s/it, loss=0.384, v_num=661]Epoch 47:  32%|███▏      | 170/534 [04:25<09:25,  1.55s/it, loss=0.382, v_num=661]Epoch 47:  34%|███▎      | 180/534 [04:39<09:05,  1.54s/it, loss=0.382, v_num=661]Epoch 47:  34%|███▎      | 180/534 [04:39<09:05,  1.54s/it, loss=0.385, v_num=661]validation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9782474637031555
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9810678958892822
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9814763069152832
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.982763946056366
88, v_num=661]Epoch 47:  37%|███▋      | 200/534 [04:46<07:56,  1.43s/it, loss=0.39, v_num=661] validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9820144176483154
validation_epoch_end
graph acc: 0.5089820359281437
valid accuracy: 0.9832658767700195
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9818831086158752
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.9812824130058289

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.982589066028595

Validating:   3%|▎         | 10/334 [00:01<00:59,  5.48it/s][AEpoch 47:  41%|████      | 220/534 [04:48<06:49,  1.30s/it, loss=0.39, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.35it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:29, 10.42it/s][AEpoch 47:  45%|████▍     | 240/534 [04:49<05:53,  1.20s/it, loss=0.39, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.01it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:26, 10.79it/s][AEpoch 47:  49%|████▊     | 260/534 [04:51<05:06,  1.12s/it, loss=0.39, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.43it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.99it/s][AEpoch 47:  52%|█████▏    | 280/534 [04:54<04:26,  1.05s/it, loss=0.39, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.68it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.27it/s][AEpoch 47:  56%|█████▌    | 300/534 [04:56<03:50,  1.02it/s, loss=0.39, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.66it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.08it/s][AEpoch 47:  60%|█████▉    | 320/534 [04:57<03:18,  1.08it/s, loss=0.39, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 11.89it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:19, 10.35it/s][AEpoch 47:  64%|██████▎   | 340/534 [04:59<02:50,  1.14it/s, loss=0.39, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:16, 11.58it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.83it/s][AEpoch 47:  67%|██████▋   | 360/534 [05:00<02:25,  1.20it/s, loss=0.39, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.38it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.66it/s][AEpoch 47:  71%|███████   | 380/534 [05:02<02:02,  1.26it/s, loss=0.39, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:15, 10.05it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.52it/s][AEpoch 47:  75%|███████▍  | 400/534 [05:04<01:41,  1.32it/s, loss=0.39, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.25it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.08it/s][AEpoch 47:  79%|███████▊  | 420/534 [05:06<01:22,  1.37it/s, loss=0.39, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.00it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 14.08it/s][AEpoch 47:  82%|████████▏ | 440/534 [05:07<01:05,  1.43it/s, loss=0.39, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.88it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:06, 12.13it/s][AEpoch 47:  86%|████████▌ | 460/534 [05:09<00:49,  1.49it/s, loss=0.39, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.25it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.63it/s][AEpoch 47:  90%|████████▉ | 480/534 [05:10<00:34,  1.55it/s, loss=0.39, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.95it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 12.80it/s][AEpoch 47:  94%|█████████▎| 500/534 [05:12<00:21,  1.60it/s, loss=0.39, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.33it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.43it/s][AEpoch 47:  97%|█████████▋| 520/534 [05:14<00:08,  1.66it/s, loss=0.39, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.42it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.76it/s][Avalidation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9814726114273071
Epoch 47: 100%|██████████| 534/534 [05:20<00:00,  1.67it/s, loss=0.39, v_num=661]
                                                             [AEpoch 47:   0%|          | 0/534 [00:00<00:00, 13981.01it/s, loss=0.39, v_num=661]Epoch 48:   0%|          | 0/534 [00:00<00:00, 3336.76it/s, loss=0.39, v_num=661] Epoch 48:   0%|          | 0/534 [00:17<2:37:43, 17.72s/it, loss=0.39, v_num=661]Epoch 48:   2%|▏         | 10/534 [00:23<18:42,  2.14s/it, loss=0.39, v_num=661] Epoch 48:   2%|▏         | 10/534 [00:23<18:42,  2.14s/it, loss=0.377, v_num=661]Epoch 48:   4%|▎         | 20/534 [00:37<15:24,  1.80s/it, loss=0.377, v_num=661]Epoch 48:   4%|▎         | 20/534 [00:37<15:24,  1.80s/it, loss=0.362, v_num=661]Epoch 48:   6%|▌         | 30/534 [00:59<16:03,  1.91s/it, loss=0.362, v_num=661]Epoch 48:   6%|▌         | 30/534 [00:59<16:03,  1.91s/it, loss=0.368, v_num=661]Epoch 48:   7%|▋         | 40/534 [01:12<14:36,  1.77s/it, loss=0.368, v_num=661]Epoch 48:   7%|▋         | 40/534 [01:12<14:36,  1.77s/it, loss=0.373, v_num=661]Epoch 48:   9%|▉         | 50/534 [01:24<13:26,  1.67s/it, loss=0.373, v_num=661]Epoch 48:   9%|▉         | 50/534 [01:24<13:26,  1.67s/it, loss=0.375, v_num=661]Epoch 48:  11%|█         | 60/534 [01:38<12:47,  1.62s/it, loss=0.375, v_num=661]Epoch 48:  11%|█         | 60/534 [01:38<12:47,  1.62s/it, loss=0.378, v_num=661]Epoch 48:  13%|█▎        | 70/534 [01:56<12:38,  1.63s/it, loss=0.378, v_num=661]Epoch 48:  13%|█▎        | 70/534 [01:56<12:38,  1.64s/it, loss=0.378, v_num=661]Epoch 48:  15%|█▍        | 80/534 [02:18<12:54,  1.71s/it, loss=0.378, v_num=661]Epoch 48:  15%|█▍        | 80/534 [02:18<12:54,  1.71s/it, loss=0.368, v_num=661]Epoch 48:  17%|█▋        | 90/534 [02:31<12:19,  1.66s/it, loss=0.368, v_num=661]Epoch 48:  17%|█▋        | 90/534 [02:31<12:19,  1.66s/it, loss=0.367, v_num=661]Epoch 48:  19%|█▊        | 100/534 [02:47<11:57,  1.65s/it, loss=0.367, v_num=661]Epoch 48:  19%|█▊        | 100/534 [02:47<11:57,  1.65s/it, loss=0.374, v_num=661]Epoch 48:  21%|██        | 110/534 [02:58<11:22,  1.61s/it, loss=0.374, v_num=661]Epoch 48:  21%|██        | 110/534 [02:58<11:22,  1.61s/it, loss=0.371, v_num=661]Epoch 48:  22%|██▏       | 120/534 [03:10<10:52,  1.58s/it, loss=0.371, v_num=661]Epoch 48:  22%|██▏       | 120/534 [03:10<10:52,  1.58s/it, loss=0.378, v_num=661]Epoch 48:  24%|██▍       | 130/534 [03:22<10:23,  1.54s/it, loss=0.378, v_num=661]Epoch 48:  24%|██▍       | 130/534 [03:22<10:23,  1.54s/it, loss=0.377, v_num=661]Epoch 48:  26%|██▌       | 140/534 [03:36<10:03,  1.53s/it, loss=0.377, v_num=661]Epoch 48:  26%|██▌       | 140/534 [03:36<10:03,  1.53s/it, loss=0.371, v_num=661]Epoch 48:  28%|██▊       | 150/534 [03:49<09:44,  1.52s/it, loss=0.371, v_num=661]Epoch 48:  28%|██▊       | 150/534 [03:49<09:44,  1.52s/it, loss=0.37, v_num=661] Epoch 48:  30%|██▉       | 160/534 [04:06<09:32,  1.53s/it, loss=0.37, v_num=661]Epoch 48:  30%|██▉       | 160/534 [04:06<09:32,  1.53s/it, loss=0.374, v_num=661]Epoch 48:  32%|███▏      | 170/534 [04:20<09:15,  1.53s/it, loss=0.374, v_num=661]Epoch 48:  32%|███▏      | 170/534 [04:20<09:15,  1.53s/it, loss=0.371, v_num=661]Epoch 48:  34%|███▎      | 180/534 [04:28<08:45,  1.48s/it, loss=0.371, v_num=661]Epoch 48:  34%|███▎      | 180/534 [04:28<08:45,  1.48s/it, loss=0.368, v_num=661]validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9814363121986389
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9810678958892822
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9787497520446777
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9830737709999084
0.366, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5119760479041916
valid accuracy: 0.9823874235153198
validation_epoch_end
graph acc: 0.5119760479041916
valid accuracy: 0.9832658767700195
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9815874099731445
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9827080368995667
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9813758730888367
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9812408685684204

Validating:   3%|▎         | 10/334 [00:01<00:59,  5.43it/s][AEpoch 48:  41%|████      | 220/534 [04:39<06:37,  1.27s/it, loss=0.366, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.36it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.87it/s][AEpoch 48:  45%|████▍     | 240/534 [04:41<05:43,  1.17s/it, loss=0.366, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.78it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.35it/s][AEpoch 48:  49%|████▊     | 260/534 [04:43<04:57,  1.09s/it, loss=0.366, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.05it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.82it/s][AEpoch 48:  52%|█████▏    | 280/534 [04:46<04:18,  1.02s/it, loss=0.366, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25,  9.87it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.70it/s][AEpoch 48:  56%|█████▌    | 300/534 [04:47<03:43,  1.05it/s, loss=0.366, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:23, 10.15it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.22it/s][AEpoch 48:  60%|█████▉    | 320/534 [04:49<03:12,  1.11it/s, loss=0.366, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.53it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.37it/s][AEpoch 48:  64%|██████▎   | 340/534 [04:51<02:45,  1.17it/s, loss=0.366, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.85it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.70it/s][AEpoch 48:  67%|██████▋   | 360/534 [04:52<02:21,  1.23it/s, loss=0.366, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.21it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.71it/s][AEpoch 48:  71%|███████   | 380/534 [04:54<01:59,  1.29it/s, loss=0.366, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.11it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.88it/s][AEpoch 48:  75%|███████▍  | 400/534 [04:56<01:38,  1.35it/s, loss=0.366, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.70it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 11.83it/s][AEpoch 48:  79%|███████▊  | 420/534 [04:57<01:20,  1.41it/s, loss=0.366, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.25it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.44it/s][AEpoch 48:  82%|████████▏ | 440/534 [04:59<01:03,  1.47it/s, loss=0.366, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:07, 13.15it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.90it/s][AEpoch 48:  86%|████████▌ | 460/534 [05:01<00:48,  1.53it/s, loss=0.366, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.29it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.18it/s][AEpoch 48:  90%|████████▉ | 480/534 [05:02<00:33,  1.59it/s, loss=0.366, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.82it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.61it/s][AEpoch 48:  94%|█████████▎| 500/534 [05:04<00:20,  1.65it/s, loss=0.366, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.27it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:02, 11.45it/s][AEpoch 48:  97%|█████████▋| 520/534 [05:06<00:08,  1.70it/s, loss=0.366, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.21it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.96it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9813927412033081
Epoch 48: 100%|██████████| 534/534 [05:14<00:00,  1.70it/s, loss=0.366, v_num=661]
                                                             [AEpoch 48:   0%|          | 0/534 [00:00<00:00, 11125.47it/s, loss=0.366, v_num=661]Epoch 49:   0%|          | 0/534 [00:00<00:00, 2601.93it/s, loss=0.366, v_num=661] Epoch 49:   0%|          | 0/534 [00:12<1:54:26, 12.86s/it, loss=0.366, v_num=661]Epoch 49:   2%|▏         | 10/534 [00:26<21:15,  2.43s/it, loss=0.366, v_num=661] Epoch 49:   2%|▏         | 10/534 [00:26<21:15,  2.44s/it, loss=0.362, v_num=661]Epoch 49:   4%|▎         | 20/534 [00:44<18:04,  2.11s/it, loss=0.362, v_num=661]Epoch 49:   4%|▎         | 20/534 [00:44<18:04,  2.11s/it, loss=0.354, v_num=661]Epoch 49:   6%|▌         | 30/534 [00:58<15:55,  1.90s/it, loss=0.354, v_num=661]Epoch 49:   6%|▌         | 30/534 [00:58<15:55,  1.90s/it, loss=0.354, v_num=661]Epoch 49:   7%|▋         | 40/534 [01:11<14:22,  1.75s/it, loss=0.354, v_num=661]Epoch 49:   7%|▋         | 40/534 [01:11<14:22,  1.75s/it, loss=0.352, v_num=661]Epoch 49:   9%|▉         | 50/534 [01:28<13:59,  1.74s/it, loss=0.352, v_num=661]Epoch 49:   9%|▉         | 50/534 [01:28<13:59,  1.74s/it, loss=0.352, v_num=661]Epoch 49:  11%|█         | 60/534 [01:43<13:23,  1.69s/it, loss=0.352, v_num=661]Epoch 49:  11%|█         | 60/534 [01:43<13:23,  1.69s/it, loss=0.352, v_num=661]Epoch 49:  13%|█▎        | 70/534 [01:56<12:38,  1.63s/it, loss=0.352, v_num=661]Epoch 49:  13%|█▎        | 70/534 [01:56<12:38,  1.63s/it, loss=0.362, v_num=661]Epoch 49:  15%|█▍        | 80/534 [02:11<12:15,  1.62s/it, loss=0.362, v_num=661]Epoch 49:  15%|█▍        | 80/534 [02:11<12:15,  1.62s/it, loss=0.357, v_num=661]Epoch 49:  17%|█▋        | 90/534 [02:24<11:46,  1.59s/it, loss=0.357, v_num=661]Epoch 49:  17%|█▋        | 90/534 [02:24<11:46,  1.59s/it, loss=0.359, v_num=661]Epoch 49:  19%|█▊        | 100/534 [02:38<11:21,  1.57s/it, loss=0.359, v_num=661]Epoch 49:  19%|█▊        | 100/534 [02:38<11:21,  1.57s/it, loss=0.364, v_num=661]Epoch 49:  21%|██        | 110/534 [02:53<11:03,  1.57s/it, loss=0.364, v_num=661]Epoch 49:  21%|██        | 110/534 [02:53<11:03,  1.57s/it, loss=0.351, v_num=661]Epoch 49:  22%|██▏       | 120/534 [03:12<10:58,  1.59s/it, loss=0.351, v_num=661]Epoch 49:  22%|██▏       | 120/534 [03:12<10:58,  1.59s/it, loss=0.359, v_num=661]Epoch 49:  24%|██▍       | 130/534 [03:27<10:38,  1.58s/it, loss=0.359, v_num=661]Epoch 49:  24%|██▍       | 130/534 [03:27<10:38,  1.58s/it, loss=0.365, v_num=661]Epoch 49:  26%|██▌       | 140/534 [03:40<10:16,  1.56s/it, loss=0.365, v_num=661]Epoch 49:  26%|██▌       | 140/534 [03:40<10:16,  1.56s/it, loss=0.359, v_num=661]Epoch 49:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.359, v_num=661]Epoch 49:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.359, v_num=661]Epoch 49:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.359, v_num=661]Epoch 49:  30%|██▉       | 160/534 [04:07<09:35,  1.54s/it, loss=0.355, v_num=661]Epoch 49:  32%|███▏      | 170/534 [04:16<09:05,  1.50s/it, loss=0.355, v_num=661]Epoch 49:  32%|███▏      | 170/534 [04:16<09:05,  1.50s/it, loss=0.359, v_num=661]Epoch 49:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.359, v_num=661]Epoch 49:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.371, v_num=661]Epoch 49:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.371, v_num=661]Epoch 49:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.367, v_num=661]validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.981508195400238
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9829576015472412
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9787111282348633
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9819964170455933
60654258728
validation_epoch_end
graph acc: 0.5239520958083832
valid accuracy: 0.983472466468811
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9827476739883423
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9815889596939087
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9811680912971497
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9816780686378479

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.78it/s][AEpoch 49:  41%|████      | 220/534 [04:44<06:44,  1.29s/it, loss=0.37, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:40,  7.70it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.83it/s][AEpoch 49:  45%|████▍     | 240/534 [04:46<05:49,  1.19s/it, loss=0.37, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.75it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.16it/s][AEpoch 49:  49%|████▊     | 260/534 [04:48<05:02,  1.11s/it, loss=0.37, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:26, 10.21it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.54it/s][AEpoch 49:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.37, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.37it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24,  9.87it/s][AEpoch 49:  56%|█████▌    | 300/534 [04:52<03:47,  1.03it/s, loss=0.37, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 11.09it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.30it/s][AEpoch 49:  60%|█████▉    | 320/534 [04:54<03:16,  1.09it/s, loss=0.37, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.95it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:18, 10.87it/s][AEpoch 49:  64%|██████▎   | 340/534 [04:56<02:48,  1.15it/s, loss=0.37, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.06it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.66it/s][AEpoch 49:  67%|██████▋   | 360/534 [04:57<02:23,  1.21it/s, loss=0.37, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:13, 12.48it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.03it/s][AEpoch 49:  71%|███████   | 380/534 [04:59<02:01,  1.27it/s, loss=0.37, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:15, 10.14it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.33it/s][AEpoch 49:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.37, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 11.91it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.67it/s][AEpoch 49:  79%|███████▊  | 420/534 [05:02<01:22,  1.39it/s, loss=0.37, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.64it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.61it/s][AEpoch 49:  82%|████████▏ | 440/534 [05:04<01:04,  1.45it/s, loss=0.37, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:07, 13.15it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.32it/s][AEpoch 49:  86%|████████▌ | 460/534 [05:06<00:49,  1.50it/s, loss=0.37, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:05, 12.38it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.66it/s][AEpoch 49:  90%|████████▉ | 480/534 [05:07<00:34,  1.56it/s, loss=0.37, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:05, 10.75it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.54it/s][AEpoch 49:  94%|█████████▎| 500/534 [05:09<00:21,  1.62it/s, loss=0.37, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.40it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:02, 11.90it/s][AEpoch 49:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.37, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.93it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.75it/s][Avalidation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9815924167633057
Epoch 49: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.37, v_num=661]
                                                             [AEpoch 49:   0%|          | 0/534 [00:00<00:00, 5607.36it/s, loss=0.37, v_num=661]Epoch 50:   0%|          | 0/534 [00:00<00:00, 1820.44it/s, loss=0.37, v_num=661]Epoch 50:   0%|          | 0/534 [00:13<1:58:10, 13.28s/it, loss=0.37, v_num=661]Epoch 50:   2%|▏         | 10/534 [00:25<20:15,  2.32s/it, loss=0.37, v_num=661] Epoch 50:   2%|▏         | 10/534 [00:25<20:15,  2.32s/it, loss=0.357, v_num=661]Epoch 50:   4%|▎         | 20/534 [00:40<16:39,  1.94s/it, loss=0.357, v_num=661]Epoch 50:   4%|▎         | 20/534 [00:40<16:39,  1.94s/it, loss=0.332, v_num=661]Epoch 50:   6%|▌         | 30/534 [00:54<14:46,  1.76s/it, loss=0.332, v_num=661]Epoch 50:   6%|▌         | 30/534 [00:54<14:46,  1.76s/it, loss=0.341, v_num=661]Epoch 50:   7%|▋         | 40/534 [01:14<14:59,  1.82s/it, loss=0.341, v_num=661]Epoch 50:   7%|▋         | 40/534 [01:14<14:59,  1.82s/it, loss=0.352, v_num=661]Epoch 50:   9%|▉         | 50/534 [01:27<13:47,  1.71s/it, loss=0.352, v_num=661]Epoch 50:   9%|▉         | 50/534 [01:27<13:47,  1.71s/it, loss=0.351, v_num=661]Epoch 50:  11%|█         | 60/534 [01:41<13:06,  1.66s/it, loss=0.351, v_num=661]Epoch 50:  11%|█         | 60/534 [01:41<13:06,  1.66s/it, loss=0.345, v_num=661]Epoch 50:  13%|█▎        | 70/534 [01:55<12:34,  1.63s/it, loss=0.345, v_num=661]Epoch 50:  13%|█▎        | 70/534 [01:55<12:34,  1.63s/it, loss=0.348, v_num=661]Epoch 50:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.348, v_num=661]Epoch 50:  15%|█▍        | 80/534 [02:15<12:37,  1.67s/it, loss=0.348, v_num=661]Epoch 50:  17%|█▋        | 90/534 [02:26<11:54,  1.61s/it, loss=0.348, v_num=661]Epoch 50:  17%|█▋        | 90/534 [02:26<11:54,  1.61s/it, loss=0.346, v_num=661]Epoch 50:  19%|█▊        | 100/534 [02:41<11:32,  1.60s/it, loss=0.346, v_num=661]Epoch 50:  19%|█▊        | 100/534 [02:41<11:32,  1.60s/it, loss=0.358, v_num=661]Epoch 50:  21%|██        | 110/534 [02:54<11:07,  1.57s/it, loss=0.358, v_num=661]Epoch 50:  21%|██        | 110/534 [02:54<11:07,  1.57s/it, loss=0.346, v_num=661]Epoch 50:  22%|██▏       | 120/534 [03:08<10:43,  1.56s/it, loss=0.346, v_num=661]Epoch 50:  22%|██▏       | 120/534 [03:08<10:43,  1.56s/it, loss=0.355, v_num=661]Epoch 50:  24%|██▍       | 130/534 [03:21<10:21,  1.54s/it, loss=0.355, v_num=661]Epoch 50:  24%|██▍       | 130/534 [03:21<10:21,  1.54s/it, loss=0.367, v_num=661]Epoch 50:  26%|██▌       | 140/534 [03:36<10:04,  1.54s/it, loss=0.367, v_num=661]Epoch 50:  26%|██▌       | 140/534 [03:36<10:04,  1.54s/it, loss=0.345, v_num=661]Epoch 50:  28%|██▊       | 150/534 [03:50<09:45,  1.52s/it, loss=0.345, v_num=661]Epoch 50:  28%|██▊       | 150/534 [03:50<09:45,  1.52s/it, loss=0.336, v_num=661]Epoch 50:  30%|██▉       | 160/534 [04:04<09:28,  1.52s/it, loss=0.336, v_num=661]Epoch 50:  30%|██▉       | 160/534 [04:04<09:28,  1.52s/it, loss=0.331, v_num=661]Epoch 50:  32%|███▏      | 170/534 [04:13<09:00,  1.48s/it, loss=0.331, v_num=661]Epoch 50:  32%|███▏      | 170/534 [04:13<09:00,  1.48s/it, loss=0.34, v_num=661] Epoch 50:  32%|███▏      | 170/534 [04:33<09:41,  1.60s/it, loss=0.34, v_num=661]Epoch 50:  34%|███▎      | 180/534 [04:34<08:57,  1.52s/it, loss=0.34, v_num=661]Epoch 50:  34%|███▎      | 180/534 [04:34<08:57,  1.52s/it, loss=0.359, v_num=661]validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9807077050209045
validation_epoch_end
graph acc: 0.45209580838323354
valid accuracy: 0.9815563559532166
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.979638397693634
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.982570230960846
s=0.346, v_num=661]validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9815727472305298
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9820531010627747
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9832236170768738
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9831832647323608
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9813205599784851
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9817536473274231

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/334 [00:02<01:05,  4.98it/s][AEpoch 50:  41%|████      | 220/534 [04:54<06:58,  1.33s/it, loss=0.346, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:40,  7.70it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  9.16it/s][AEpoch 50:  45%|████▍     | 240/534 [04:56<06:01,  1.23s/it, loss=0.346, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.09it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:26, 10.56it/s][AEpoch 50:  49%|████▊     | 260/534 [04:58<05:13,  1.14s/it, loss=0.346, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:22, 12.45it/s][A
Validating:  21%|██        | 70/334 [00:08<00:36,  7.27it/s][AEpoch 50:  52%|█████▏    | 280/534 [05:01<04:32,  1.07s/it, loss=0.346, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.21it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24,  9.85it/s][AEpoch 50:  56%|█████▌    | 300/534 [05:02<03:55,  1.01s/it, loss=0.346, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:24,  9.46it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.65it/s][AEpoch 50:  60%|█████▉    | 320/534 [05:04<03:22,  1.05it/s, loss=0.346, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.77it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.37it/s][AEpoch 50:  64%|██████▎   | 340/534 [05:06<02:54,  1.11it/s, loss=0.346, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.03it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.52it/s][AEpoch 50:  67%|██████▋   | 360/534 [05:07<02:28,  1.17it/s, loss=0.346, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:15, 11.44it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.11it/s][AEpoch 50:  71%|███████   | 380/534 [05:09<02:05,  1.23it/s, loss=0.346, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:15, 10.03it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.48it/s][AEpoch 50:  75%|███████▍  | 400/534 [05:11<01:44,  1.29it/s, loss=0.346, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 12.10it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.54it/s][AEpoch 50:  79%|███████▊  | 420/534 [05:13<01:24,  1.34it/s, loss=0.346, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 11.75it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.20it/s][AEpoch 50:  82%|████████▏ | 440/534 [05:14<01:07,  1.40it/s, loss=0.346, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.04it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.69it/s][AEpoch 50:  86%|████████▌ | 460/534 [05:16<00:50,  1.46it/s, loss=0.346, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:05, 12.81it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 12.82it/s][AEpoch 50:  90%|████████▉ | 480/534 [05:17<00:35,  1.51it/s, loss=0.346, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 10.96it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.39it/s][AEpoch 50:  94%|█████████▎| 500/534 [05:19<00:21,  1.57it/s, loss=0.346, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.71it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:02, 11.25it/s][AEpoch 50:  97%|█████████▋| 520/534 [05:21<00:08,  1.62it/s, loss=0.346, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.27it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.89it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9817122220993042
Epoch 50: 100%|██████████| 534/534 [05:28<00:00,  1.63it/s, loss=0.346, v_num=661]
                                                             [AEpoch 50:   0%|          | 0/534 [00:00<00:00, 9279.43it/s, loss=0.346, v_num=661]Epoch 51:   0%|          | 0/534 [00:00<00:00, 2458.56it/s, loss=0.346, v_num=661]Epoch 51:   0%|          | 0/534 [00:13<2:01:23, 13.64s/it, loss=0.346, v_num=661]Epoch 51:   2%|▏         | 10/534 [00:23<18:56,  2.17s/it, loss=0.346, v_num=661] Epoch 51:   2%|▏         | 10/534 [00:23<18:56,  2.17s/it, loss=0.336, v_num=661]Epoch 51:   4%|▎         | 20/534 [00:36<14:59,  1.75s/it, loss=0.336, v_num=661]Epoch 51:   4%|▎         | 20/534 [00:36<15:00,  1.75s/it, loss=0.322, v_num=661]Epoch 51:   6%|▌         | 30/534 [00:55<14:56,  1.78s/it, loss=0.322, v_num=661]Epoch 51:   6%|▌         | 30/534 [00:55<14:56,  1.78s/it, loss=0.326, v_num=661]Epoch 51:   7%|▋         | 40/534 [01:08<13:45,  1.67s/it, loss=0.326, v_num=661]Epoch 51:   7%|▋         | 40/534 [01:08<13:45,  1.67s/it, loss=0.342, v_num=661]Epoch 51:   9%|▉         | 50/534 [01:21<12:49,  1.59s/it, loss=0.342, v_num=661]Epoch 51:   9%|▉         | 50/534 [01:21<12:49,  1.59s/it, loss=0.336, v_num=661]Epoch 51:  11%|█         | 60/534 [01:38<12:44,  1.61s/it, loss=0.336, v_num=661]Epoch 51:  11%|█         | 60/534 [01:38<12:44,  1.61s/it, loss=0.339, v_num=661]Epoch 51:  13%|█▎        | 70/534 [01:50<12:00,  1.55s/it, loss=0.339, v_num=661]Epoch 51:  13%|█▎        | 70/534 [01:50<12:00,  1.55s/it, loss=0.347, v_num=661]Epoch 51:  15%|█▍        | 80/534 [02:06<11:46,  1.56s/it, loss=0.347, v_num=661]Epoch 51:  15%|█▍        | 80/534 [02:06<11:46,  1.56s/it, loss=0.342, v_num=661]Epoch 51:  17%|█▋        | 90/534 [02:18<11:14,  1.52s/it, loss=0.342, v_num=661]Epoch 51:  17%|█▋        | 90/534 [02:18<11:14,  1.52s/it, loss=0.355, v_num=661]Epoch 51:  19%|█▊        | 100/534 [02:42<11:37,  1.61s/it, loss=0.355, v_num=661]Epoch 51:  19%|█▊        | 100/534 [02:42<11:37,  1.61s/it, loss=0.355, v_num=661]Epoch 51:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.355, v_num=661]Epoch 51:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.342, v_num=661]Epoch 51:  22%|██▏       | 120/534 [03:10<10:51,  1.57s/it, loss=0.342, v_num=661]Epoch 51:  22%|██▏       | 120/534 [03:10<10:51,  1.57s/it, loss=0.349, v_num=661]Epoch 51:  24%|██▍       | 130/534 [03:25<10:35,  1.57s/it, loss=0.349, v_num=661]Epoch 51:  24%|██▍       | 130/534 [03:25<10:35,  1.57s/it, loss=0.348, v_num=661]Epoch 51:  26%|██▌       | 140/534 [03:39<10:14,  1.56s/it, loss=0.348, v_num=661]Epoch 51:  26%|██▌       | 140/534 [03:39<10:14,  1.56s/it, loss=0.331, v_num=661]Epoch 51:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.331, v_num=661]Epoch 51:  28%|██▊       | 150/534 [03:56<10:00,  1.56s/it, loss=0.318, v_num=661]Epoch 51:  30%|██▉       | 160/534 [04:08<09:37,  1.54s/it, loss=0.318, v_num=661]Epoch 51:  30%|██▉       | 160/534 [04:08<09:37,  1.54s/it, loss=0.326, v_num=661]Epoch 51:  32%|███▏      | 170/534 [04:19<09:12,  1.52s/it, loss=0.326, v_num=661]Epoch 51:  32%|███▏      | 170/534 [04:19<09:12,  1.52s/it, loss=0.342, v_num=661]Epoch 51:  34%|███▎      | 180/534 [04:31<08:51,  1.50s/it, loss=0.342, v_num=661]Epoch 51:  34%|███▎      | 180/534 [04:31<08:51,  1.50s/it, loss=0.334, v_num=661]validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9816763401031494
validation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9824540615081787
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.979445219039917
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9807476997375488
40s/it, loss=0.331, v_num=661]Epoch 51:  37%|███▋      | 200/534 [04:42<07:49,  1.40s/it, loss=0.343, v_num=661]validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.981921911239624

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9825493693351746
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9813205599784851
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9817536473274231
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9815115928649902

Validating:   3%|▎         | 10/334 [00:01<00:52,  6.16it/s][AEpoch 51:  41%|████      | 220/534 [04:43<06:43,  1.29s/it, loss=0.343, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:40,  7.76it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.77it/s][AEpoch 51:  45%|████▍     | 240/534 [04:46<05:48,  1.19s/it, loss=0.343, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  8.92it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.32it/s][AEpoch 51:  49%|████▊     | 260/534 [04:47<05:02,  1.10s/it, loss=0.343, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:22, 12.24it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.43it/s][AEpoch 51:  52%|█████▏    | 280/534 [04:50<04:22,  1.03s/it, loss=0.343, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:28,  9.05it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:25,  9.52it/s][AEpoch 51:  56%|█████▌    | 300/534 [04:52<03:47,  1.03it/s, loss=0.343, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:23, 10.06it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.05it/s][AEpoch 51:  60%|█████▉    | 320/534 [04:53<03:15,  1.09it/s, loss=0.343, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.46it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.50it/s][AEpoch 51:  64%|██████▎   | 340/534 [04:56<02:48,  1.15it/s, loss=0.343, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.53it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:15, 12.25it/s][AEpoch 51:  67%|██████▋   | 360/534 [04:57<02:23,  1.21it/s, loss=0.343, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.15it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.85it/s][AEpoch 51:  71%|███████   | 380/534 [04:59<02:00,  1.27it/s, loss=0.343, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.62it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:11, 12.29it/s][AEpoch 51:  75%|███████▍  | 400/534 [05:00<01:40,  1.33it/s, loss=0.343, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.22it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.77it/s][AEpoch 51:  79%|███████▊  | 420/534 [05:02<01:21,  1.39it/s, loss=0.343, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.94it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.57it/s][AEpoch 51:  82%|████████▏ | 440/534 [05:03<01:04,  1.45it/s, loss=0.343, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:07, 12.69it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.86it/s][AEpoch 51:  86%|████████▌ | 460/534 [05:05<00:49,  1.51it/s, loss=0.343, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.56it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.39it/s][AEpoch 51:  90%|████████▉ | 480/534 [05:07<00:34,  1.56it/s, loss=0.343, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 11.24it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.75it/s][AEpoch 51:  94%|█████████▎| 500/534 [05:08<00:20,  1.62it/s, loss=0.343, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.73it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.40it/s][AEpoch 51:  97%|█████████▋| 520/534 [05:10<00:08,  1.68it/s, loss=0.343, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.02it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.57it/s][Avalidation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9811931252479553
Epoch 51: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.343, v_num=661]
                                                             [AEpoch 51:   0%|          | 0/534 [00:00<00:00, 13189.64it/s, loss=0.343, v_num=661]Epoch 52:   0%|          | 0/534 [00:00<00:00, 3006.67it/s, loss=0.343, v_num=661] Epoch 52:   0%|          | 0/534 [00:14<2:12:35, 14.90s/it, loss=0.343, v_num=661]Epoch 52:   2%|▏         | 10/534 [00:30<24:18,  2.78s/it, loss=0.343, v_num=661] Epoch 52:   2%|▏         | 10/534 [00:30<24:18,  2.78s/it, loss=0.33, v_num=661] Epoch 52:   4%|▎         | 20/534 [00:42<17:20,  2.02s/it, loss=0.33, v_num=661]Epoch 52:   4%|▎         | 20/534 [00:42<17:20,  2.02s/it, loss=0.313, v_num=661]Epoch 52:   6%|▌         | 30/534 [00:53<14:33,  1.73s/it, loss=0.313, v_num=661]Epoch 52:   6%|▌         | 30/534 [00:53<14:33,  1.73s/it, loss=0.319, v_num=661]Epoch 52:   7%|▋         | 40/534 [01:11<14:24,  1.75s/it, loss=0.319, v_num=661]Epoch 52:   7%|▋         | 40/534 [01:11<14:24,  1.75s/it, loss=0.331, v_num=661]Epoch 52:   9%|▉         | 50/534 [01:27<13:50,  1.71s/it, loss=0.331, v_num=661]Epoch 52:   9%|▉         | 50/534 [01:27<13:50,  1.71s/it, loss=0.336, v_num=661]Epoch 52:  11%|█         | 60/534 [01:42<13:16,  1.68s/it, loss=0.336, v_num=661]Epoch 52:  11%|█         | 60/534 [01:42<13:16,  1.68s/it, loss=0.331, v_num=661]Epoch 52:  13%|█▎        | 70/534 [01:57<12:48,  1.66s/it, loss=0.331, v_num=661]Epoch 52:  13%|█▎        | 70/534 [01:57<12:48,  1.66s/it, loss=0.331, v_num=661]Epoch 52:  15%|█▍        | 80/534 [02:10<12:11,  1.61s/it, loss=0.331, v_num=661]Epoch 52:  15%|█▍        | 80/534 [02:10<12:11,  1.61s/it, loss=0.341, v_num=661]Epoch 52:  17%|█▋        | 90/534 [02:29<12:09,  1.64s/it, loss=0.341, v_num=661]Epoch 52:  17%|█▋        | 90/534 [02:29<12:09,  1.64s/it, loss=0.334, v_num=661]Epoch 52:  19%|█▊        | 100/534 [02:48<12:04,  1.67s/it, loss=0.334, v_num=661]Epoch 52:  19%|█▊        | 100/534 [02:48<12:04,  1.67s/it, loss=0.336, v_num=661]Epoch 52:  21%|██        | 110/534 [03:01<11:34,  1.64s/it, loss=0.336, v_num=661]Epoch 52:  21%|██        | 110/534 [03:01<11:34,  1.64s/it, loss=0.342, v_num=661]Epoch 52:  22%|██▏       | 120/534 [03:16<11:11,  1.62s/it, loss=0.342, v_num=661]Epoch 52:  22%|██▏       | 120/534 [03:16<11:11,  1.62s/it, loss=0.332, v_num=661]Epoch 52:  24%|██▍       | 130/534 [03:31<10:51,  1.61s/it, loss=0.332, v_num=661]Epoch 52:  24%|██▍       | 130/534 [03:31<10:51,  1.61s/it, loss=0.325, v_num=661]Epoch 52:  26%|██▌       | 140/534 [03:45<10:31,  1.60s/it, loss=0.325, v_num=661]Epoch 52:  26%|██▌       | 140/534 [03:45<10:31,  1.60s/it, loss=0.327, v_num=661]Epoch 52:  28%|██▊       | 150/534 [04:00<10:11,  1.59s/it, loss=0.327, v_num=661]Epoch 52:  28%|██▊       | 150/534 [04:00<10:11,  1.59s/it, loss=0.328, v_num=661]Epoch 52:  30%|██▉       | 160/534 [04:14<09:50,  1.58s/it, loss=0.328, v_num=661]Epoch 52:  30%|██▉       | 160/534 [04:14<09:50,  1.58s/it, loss=0.324, v_num=661]Epoch 52:  32%|███▏      | 170/534 [04:26<09:26,  1.56s/it, loss=0.324, v_num=661]Epoch 52:  32%|███▏      | 170/534 [04:26<09:26,  1.56s/it, loss=0.341, v_num=661]Epoch 52:  34%|███▎      | 180/534 [04:34<08:56,  1.51s/it, loss=0.341, v_num=661]Epoch 52:  34%|███▎      | 180/534 [04:34<08:56,  1.51s/it, loss=0.345, v_num=661]Epoch 52:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.345, v_num=661]Epoch 52:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.332, v_num=661]validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9799475073814392
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9825315475463867
validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9809077978134155
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9818763732910156
uracy: 0.9829063415527344
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.9810917973518372
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9823580384254456

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s]validation_epoch_end
graph acc: 0.5149700598802395
valid accuracy: 0.9820770621299744
[Avalidation_epoch_end
graph acc: 0.5
valid accuracy: 0.9834311604499817

Validating:   3%|▎         | 10/334 [00:02<01:05,  4.95it/s][AEpoch 52:  41%|████      | 220/534 [04:42<06:41,  1.28s/it, loss=0.331, v_num=661]
Validating:   6%|▌         | 20/334 [00:03<00:47,  6.68it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:36,  8.40it/s][AEpoch 52:  45%|████▍     | 240/534 [04:44<05:47,  1.18s/it, loss=0.331, v_num=661]
Validating:  12%|█▏        | 40/334 [00:05<00:35,  8.22it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.11it/s][AEpoch 52:  49%|████▊     | 260/534 [04:46<05:00,  1.10s/it, loss=0.331, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:25, 10.91it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.38it/s][AEpoch 52:  52%|█████▏    | 280/534 [04:49<04:21,  1.03s/it, loss=0.331, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.29it/s][A
Validating:  27%|██▋       | 90/334 [00:10<00:23, 10.17it/s][AEpoch 52:  56%|█████▌    | 300/534 [04:50<03:46,  1.03it/s, loss=0.331, v_num=661]
Validating:  30%|██▉       | 100/334 [00:11<00:22, 10.39it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.82it/s][AEpoch 52:  60%|█████▉    | 320/534 [04:52<03:15,  1.10it/s, loss=0.331, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:19, 11.15it/s][A
Validating:  39%|███▉      | 130/334 [00:14<00:24,  8.23it/s][AEpoch 52:  64%|██████▎   | 340/534 [04:55<02:48,  1.15it/s, loss=0.331, v_num=661]
Validating:  42%|████▏     | 140/334 [00:15<00:19,  9.91it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:15, 11.70it/s][AEpoch 52:  67%|██████▋   | 360/534 [04:56<02:22,  1.22it/s, loss=0.331, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:16, 10.38it/s][A
Validating:  51%|█████     | 170/334 [00:17<00:14, 11.43it/s][AEpoch 52:  71%|███████   | 380/534 [04:58<02:00,  1.28it/s, loss=0.331, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:15,  9.95it/s][A
Validating:  57%|█████▋    | 190/334 [00:19<00:13, 10.98it/s][AEpoch 52:  75%|███████▍  | 400/534 [05:00<01:40,  1.34it/s, loss=0.331, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:20<00:11, 12.04it/s][A
Validating:  63%|██████▎   | 210/334 [00:21<00:10, 11.38it/s][AEpoch 52:  79%|███████▊  | 420/534 [05:01<01:21,  1.39it/s, loss=0.331, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:22<00:09, 11.62it/s][A
Validating:  69%|██████▉   | 230/334 [00:22<00:08, 12.40it/s][AEpoch 52:  82%|████████▏ | 440/534 [05:03<01:04,  1.45it/s, loss=0.331, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:23<00:06, 13.85it/s][A
Validating:  75%|███████▍  | 250/334 [00:24<00:06, 12.31it/s][AEpoch 52:  86%|████████▌ | 460/534 [05:05<00:48,  1.51it/s, loss=0.331, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:25<00:06, 12.32it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:04, 12.87it/s][AEpoch 52:  90%|████████▉ | 480/534 [05:06<00:34,  1.57it/s, loss=0.331, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 11.23it/s][A
Validating:  87%|████████▋ | 290/334 [00:27<00:03, 13.21it/s][AEpoch 52:  94%|█████████▎| 500/534 [05:08<00:20,  1.63it/s, loss=0.331, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 15.17it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.49it/s][AEpoch 52:  97%|█████████▋| 520/534 [05:09<00:08,  1.68it/s, loss=0.331, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:29<00:01, 13.64it/s][A
Validating:  99%|█████████▉| 330/334 [00:30<00:00, 13.83it/s][Avalidation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.981991708278656
Epoch 52: 100%|██████████| 534/534 [05:16<00:00,  1.69it/s, loss=0.331, v_num=661]
                                                             [AEpoch 52:   0%|          | 0/534 [00:00<00:00, 11554.56it/s, loss=0.331, v_num=661]Epoch 53:   0%|          | 0/534 [00:00<00:00, 2646.25it/s, loss=0.331, v_num=661] Epoch 53:   0%|          | 0/534 [00:17<2:36:52, 17.63s/it, loss=0.331, v_num=661]Epoch 53:   2%|▏         | 10/534 [00:22<18:00,  2.06s/it, loss=0.331, v_num=661] Epoch 53:   2%|▏         | 10/534 [00:22<18:00,  2.06s/it, loss=0.321, v_num=661]Epoch 53:   4%|▎         | 20/534 [00:38<15:52,  1.85s/it, loss=0.321, v_num=661]Epoch 53:   4%|▎         | 20/534 [00:38<15:52,  1.85s/it, loss=0.316, v_num=661]Epoch 53:   6%|▌         | 30/534 [00:54<14:41,  1.75s/it, loss=0.316, v_num=661]Epoch 53:   6%|▌         | 30/534 [00:54<14:42,  1.75s/it, loss=0.323, v_num=661]Epoch 53:   7%|▋         | 40/534 [01:10<14:14,  1.73s/it, loss=0.323, v_num=661]Epoch 53:   7%|▋         | 40/534 [01:10<14:14,  1.73s/it, loss=0.325, v_num=661]Epoch 53:   9%|▉         | 50/534 [01:28<14:02,  1.74s/it, loss=0.325, v_num=661]Epoch 53:   9%|▉         | 50/534 [01:28<14:02,  1.74s/it, loss=0.323, v_num=661]Epoch 53:  11%|█         | 60/534 [01:41<13:07,  1.66s/it, loss=0.323, v_num=661]Epoch 53:  11%|█         | 60/534 [01:41<13:07,  1.66s/it, loss=0.323, v_num=661]Epoch 53:  13%|█▎        | 70/534 [01:55<12:35,  1.63s/it, loss=0.323, v_num=661]Epoch 53:  13%|█▎        | 70/534 [01:55<12:35,  1.63s/it, loss=0.327, v_num=661]Epoch 53:  15%|█▍        | 80/534 [02:09<12:03,  1.59s/it, loss=0.327, v_num=661]Epoch 53:  15%|█▍        | 80/534 [02:09<12:03,  1.59s/it, loss=0.32, v_num=661] Epoch 53:  17%|█▋        | 90/534 [02:26<11:53,  1.61s/it, loss=0.32, v_num=661]Epoch 53:  17%|█▋        | 90/534 [02:26<11:53,  1.61s/it, loss=0.325, v_num=661]Epoch 53:  19%|█▊        | 100/534 [02:39<11:24,  1.58s/it, loss=0.325, v_num=661]Epoch 53:  19%|█▊        | 100/534 [02:39<11:24,  1.58s/it, loss=0.325, v_num=661]Epoch 53:  21%|██        | 110/534 [02:51<10:56,  1.55s/it, loss=0.325, v_num=661]Epoch 53:  21%|██        | 110/534 [02:51<10:56,  1.55s/it, loss=0.314, v_num=661]Epoch 53:  22%|██▏       | 120/534 [03:07<10:43,  1.55s/it, loss=0.314, v_num=661]Epoch 53:  22%|██▏       | 120/534 [03:07<10:43,  1.55s/it, loss=0.308, v_num=661]Epoch 53:  24%|██▍       | 130/534 [03:19<10:15,  1.52s/it, loss=0.308, v_num=661]Epoch 53:  24%|██▍       | 130/534 [03:19<10:15,  1.52s/it, loss=0.303, v_num=661]Epoch 53:  26%|██▌       | 140/534 [03:31<09:50,  1.50s/it, loss=0.303, v_num=661]Epoch 53:  26%|██▌       | 140/534 [03:31<09:50,  1.50s/it, loss=0.308, v_num=661]Epoch 53:  28%|██▊       | 150/534 [03:43<09:27,  1.48s/it, loss=0.308, v_num=661]Epoch 53:  28%|██▊       | 150/534 [03:43<09:27,  1.48s/it, loss=0.313, v_num=661]Epoch 53:  30%|██▉       | 160/534 [03:56<09:10,  1.47s/it, loss=0.313, v_num=661]Epoch 53:  30%|██▉       | 160/534 [03:56<09:10,  1.47s/it, loss=0.321, v_num=661]Epoch 53:  32%|███▏      | 170/534 [04:13<08:59,  1.48s/it, loss=0.321, v_num=661]Epoch 53:  32%|███▏      | 170/534 [04:13<08:59,  1.48s/it, loss=0.329, v_num=661]Epoch 53:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.329, v_num=661]Epoch 53:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.322, v_num=661]validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9831125140190125
validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9807877540588379
validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.981796383857727
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9798702001571655
s=0.33, v_num=661] validation_epoch_end
graph acc: 0.5119760479041916
valid accuracy: 0.9821934700012207
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9819047451019287
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9827476739883423

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9836377501487732
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9815889596939087
validation_epoch_end
graph acc: 0.44011976047904194
valid accuracy: 0.98074871301651

Validating:   3%|▎         | 10/334 [00:01<00:50,  6.37it/s][AEpoch 53:  41%|████      | 220/534 [04:40<06:38,  1.27s/it, loss=0.33, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:38,  8.19it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:29, 10.20it/s][AEpoch 53:  45%|████▍     | 240/534 [04:41<05:44,  1.17s/it, loss=0.33, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:34,  8.50it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:29,  9.56it/s][AEpoch 53:  49%|████▊     | 260/534 [04:44<04:58,  1.09s/it, loss=0.33, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.24it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.58it/s][AEpoch 53:  52%|█████▏    | 280/534 [04:47<04:19,  1.02s/it, loss=0.33, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.23it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:25,  9.70it/s][AEpoch 53:  56%|█████▌    | 300/534 [04:48<03:44,  1.04it/s, loss=0.33, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.91it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.93it/s][AEpoch 53:  60%|█████▉    | 320/534 [04:50<03:13,  1.11it/s, loss=0.33, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.71it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.69it/s][AEpoch 53:  64%|██████▎   | 340/534 [04:52<02:46,  1.17it/s, loss=0.33, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.89it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.52it/s][AEpoch 53:  67%|██████▋   | 360/534 [04:53<02:21,  1.23it/s, loss=0.33, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 11.61it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:15, 10.30it/s][AEpoch 53:  71%|███████   | 380/534 [04:55<01:59,  1.29it/s, loss=0.33, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:15, 10.04it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.34it/s][AEpoch 53:  75%|███████▍  | 400/534 [04:57<01:39,  1.35it/s, loss=0.33, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.46it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.00it/s][AEpoch 53:  79%|███████▊  | 420/534 [04:58<01:20,  1.41it/s, loss=0.33, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.39it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.93it/s][AEpoch 53:  82%|████████▏ | 440/534 [05:00<01:04,  1.47it/s, loss=0.33, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 13.74it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.20it/s][AEpoch 53:  86%|████████▌ | 460/534 [05:02<00:48,  1.53it/s, loss=0.33, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.85it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.49it/s][AEpoch 53:  90%|████████▉ | 480/534 [05:03<00:34,  1.58it/s, loss=0.33, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.34it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.77it/s][AEpoch 53:  94%|█████████▎| 500/534 [05:05<00:20,  1.64it/s, loss=0.33, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.64it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.52it/s][AEpoch 53:  97%|█████████▋| 520/534 [05:06<00:08,  1.70it/s, loss=0.33, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.26it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.12it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.981911838054657
Epoch 53: 100%|██████████| 534/534 [05:14<00:00,  1.70it/s, loss=0.33, v_num=661]
                                                             [AEpoch 53:   0%|          | 0/534 [00:00<00:00, 10330.80it/s, loss=0.33, v_num=661]Epoch 54:   0%|          | 0/534 [00:00<00:00, 2562.19it/s, loss=0.33, v_num=661] Epoch 54:   0%|          | 0/534 [00:12<1:47:50, 12.12s/it, loss=0.33, v_num=661]Epoch 54:   2%|▏         | 10/534 [00:23<18:40,  2.14s/it, loss=0.33, v_num=661] Epoch 54:   2%|▏         | 10/534 [00:23<18:40,  2.14s/it, loss=0.322, v_num=661]Epoch 54:   4%|▎         | 20/534 [00:42<17:31,  2.05s/it, loss=0.322, v_num=661]Epoch 54:   4%|▎         | 20/534 [00:42<17:31,  2.05s/it, loss=0.309, v_num=661]Epoch 54:   6%|▌         | 30/534 [00:55<14:54,  1.77s/it, loss=0.309, v_num=661]Epoch 54:   6%|▌         | 30/534 [00:55<14:54,  1.77s/it, loss=0.316, v_num=661]Epoch 54:   7%|▋         | 40/534 [01:07<13:31,  1.64s/it, loss=0.316, v_num=661]Epoch 54:   7%|▋         | 40/534 [01:07<13:31,  1.64s/it, loss=0.309, v_num=661]Epoch 54:   9%|▉         | 50/534 [01:20<12:42,  1.58s/it, loss=0.309, v_num=661]Epoch 54:   9%|▉         | 50/534 [01:20<12:42,  1.58s/it, loss=0.306, v_num=661]Epoch 54:  11%|█         | 60/534 [01:35<12:19,  1.56s/it, loss=0.306, v_num=661]Epoch 54:  11%|█         | 60/534 [01:35<12:19,  1.56s/it, loss=0.304, v_num=661]Epoch 54:  13%|█▎        | 70/534 [01:50<12:01,  1.56s/it, loss=0.304, v_num=661]Epoch 54:  13%|█▎        | 70/534 [01:50<12:01,  1.56s/it, loss=0.296, v_num=661]Epoch 54:  15%|█▍        | 80/534 [02:04<11:37,  1.54s/it, loss=0.296, v_num=661]Epoch 54:  15%|█▍        | 80/534 [02:04<11:37,  1.54s/it, loss=0.304, v_num=661]Epoch 54:  17%|█▋        | 90/534 [02:18<11:17,  1.53s/it, loss=0.304, v_num=661]Epoch 54:  17%|█▋        | 90/534 [02:18<11:17,  1.53s/it, loss=0.31, v_num=661] Epoch 54:  19%|█▊        | 100/534 [02:34<11:03,  1.53s/it, loss=0.31, v_num=661]Epoch 54:  19%|█▊        | 100/534 [02:34<11:03,  1.53s/it, loss=0.311, v_num=661]Epoch 54:  21%|██        | 110/534 [02:46<10:37,  1.50s/it, loss=0.311, v_num=661]Epoch 54:  21%|██        | 110/534 [02:46<10:37,  1.50s/it, loss=0.308, v_num=661]Epoch 54:  22%|██▏       | 120/534 [03:11<10:54,  1.58s/it, loss=0.308, v_num=661]Epoch 54:  22%|██▏       | 120/534 [03:11<10:54,  1.58s/it, loss=0.314, v_num=661]Epoch 54:  24%|██▍       | 130/534 [03:24<10:31,  1.56s/it, loss=0.314, v_num=661]Epoch 54:  24%|██▍       | 130/534 [03:24<10:31,  1.56s/it, loss=0.323, v_num=661]Epoch 54:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.323, v_num=661]Epoch 54:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.325, v_num=661]Epoch 54:  28%|██▊       | 150/534 [03:50<09:45,  1.53s/it, loss=0.325, v_num=661]Epoch 54:  28%|██▊       | 150/534 [03:50<09:45,  1.53s/it, loss=0.308, v_num=661]Epoch 54:  30%|██▉       | 160/534 [03:59<09:17,  1.49s/it, loss=0.308, v_num=661]Epoch 54:  30%|██▉       | 160/534 [03:59<09:17,  1.49s/it, loss=0.302, v_num=661]Epoch 54:  30%|██▉       | 160/534 [04:12<09:45,  1.57s/it, loss=0.302, v_num=661]Epoch 54:  32%|███▏      | 170/534 [04:13<09:00,  1.49s/it, loss=0.302, v_num=661]Epoch 54:  32%|███▏      | 170/534 [04:13<09:00,  1.49s/it, loss=0.306, v_num=661]Epoch 54:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.306, v_num=661]Epoch 54:  34%|███▎      | 180/534 [04:25<08:39,  1.47s/it, loss=0.309, v_num=661]validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9793292880058289
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9822764992713928
validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.981268048286438
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9834610819816589
=0.327, v_num=661]validation_epoch_end
graph acc: 0.5239520958083832
valid accuracy: 0.9834311604499817

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5269461077844312
valid accuracy: 0.9830081462860107
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9830649495124817
validation_epoch_end
graph acc: 0.4341317365269461
valid accuracy: 0.9810537099838257
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9822069406509399
validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9826332926750183

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.72it/s][AEpoch 54:  41%|████      | 220/534 [04:42<06:40,  1.28s/it, loss=0.327, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:40,  7.68it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:31,  9.70it/s][AEpoch 54:  45%|████▍     | 240/534 [04:43<05:46,  1.18s/it, loss=0.327, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.10it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:25, 11.07it/s][AEpoch 54:  49%|████▊     | 260/534 [04:45<04:59,  1.09s/it, loss=0.327, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:22, 12.45it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.47it/s][AEpoch 54:  52%|█████▏    | 280/534 [04:48<04:20,  1.03s/it, loss=0.327, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.08it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:25,  9.54it/s][AEpoch 54:  56%|█████▌    | 300/534 [04:50<03:45,  1.04it/s, loss=0.327, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:24,  9.65it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.49it/s][AEpoch 54:  60%|█████▉    | 320/534 [04:51<03:14,  1.10it/s, loss=0.327, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 11.98it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.11it/s][AEpoch 54:  64%|██████▎   | 340/534 [04:54<02:47,  1.16it/s, loss=0.327, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.84it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.35it/s][AEpoch 54:  67%|██████▋   | 360/534 [04:55<02:22,  1.22it/s, loss=0.327, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.41it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:12, 12.64it/s][AEpoch 54:  71%|███████   | 380/534 [04:56<01:59,  1.28it/s, loss=0.327, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.78it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.87it/s][AEpoch 54:  75%|███████▍  | 400/534 [04:58<01:39,  1.34it/s, loss=0.327, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 11.93it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.01it/s][AEpoch 54:  79%|███████▊  | 420/534 [05:00<01:21,  1.40it/s, loss=0.327, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.12it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.76it/s][AEpoch 54:  82%|████████▏ | 440/534 [05:01<01:04,  1.46it/s, loss=0.327, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 13.67it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.60it/s][AEpoch 54:  86%|████████▌ | 460/534 [05:03<00:48,  1.52it/s, loss=0.327, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.49it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.78it/s][AEpoch 54:  90%|████████▉ | 480/534 [05:04<00:34,  1.58it/s, loss=0.327, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 10.98it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.52it/s][AEpoch 54:  94%|█████████▎| 500/534 [05:06<00:20,  1.63it/s, loss=0.327, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.23it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.73it/s][AEpoch 54:  97%|█████████▋| 520/534 [05:08<00:08,  1.69it/s, loss=0.327, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.39it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.54it/s][Avalidation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9813927412033081
Epoch 54: 100%|██████████| 534/534 [05:15<00:00,  1.69it/s, loss=0.327, v_num=661]
                                                             [AEpoch 54:   0%|          | 0/534 [00:00<00:00, 12052.60it/s, loss=0.327, v_num=661]Epoch 55:   0%|          | 0/534 [00:00<00:00, 2935.13it/s, loss=0.327, v_num=661] Epoch 55:   0%|          | 0/534 [00:15<2:19:22, 15.66s/it, loss=0.327, v_num=661]Epoch 55:   2%|▏         | 10/534 [00:20<16:38,  1.91s/it, loss=0.327, v_num=661] Epoch 55:   2%|▏         | 10/534 [00:20<16:38,  1.91s/it, loss=0.307, v_num=661]Epoch 55:   4%|▎         | 20/534 [00:39<16:14,  1.90s/it, loss=0.307, v_num=661]Epoch 55:   4%|▎         | 20/534 [00:39<16:14,  1.90s/it, loss=0.301, v_num=661]Epoch 55:   6%|▌         | 30/534 [00:53<14:26,  1.72s/it, loss=0.301, v_num=661]Epoch 55:   6%|▌         | 30/534 [00:53<14:26,  1.72s/it, loss=0.304, v_num=661]Epoch 55:   7%|▋         | 40/534 [01:07<13:37,  1.66s/it, loss=0.304, v_num=661]Epoch 55:   7%|▋         | 40/534 [01:07<13:37,  1.66s/it, loss=0.305, v_num=661]Epoch 55:   9%|▉         | 50/534 [01:20<12:46,  1.58s/it, loss=0.305, v_num=661]Epoch 55:   9%|▉         | 50/534 [01:20<12:46,  1.58s/it, loss=0.297, v_num=661]Epoch 55:  11%|█         | 60/534 [01:34<12:14,  1.55s/it, loss=0.297, v_num=661]Epoch 55:  11%|█         | 60/534 [01:34<12:14,  1.55s/it, loss=0.296, v_num=661]Epoch 55:  13%|█▎        | 70/534 [01:48<11:51,  1.53s/it, loss=0.296, v_num=661]Epoch 55:  13%|█▎        | 70/534 [01:48<11:51,  1.53s/it, loss=0.303, v_num=661]Epoch 55:  15%|█▍        | 80/534 [02:02<11:26,  1.51s/it, loss=0.303, v_num=661]Epoch 55:  15%|█▍        | 80/534 [02:02<11:26,  1.51s/it, loss=0.299, v_num=661]Epoch 55:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.299, v_num=661]Epoch 55:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.294, v_num=661]Epoch 55:  19%|█▊        | 100/534 [02:30<10:47,  1.49s/it, loss=0.294, v_num=661]Epoch 55:  19%|█▊        | 100/534 [02:30<10:47,  1.49s/it, loss=0.299, v_num=661]Epoch 55:  21%|██        | 110/534 [02:46<10:34,  1.50s/it, loss=0.299, v_num=661]Epoch 55:  21%|██        | 110/534 [02:46<10:34,  1.50s/it, loss=0.301, v_num=661]Epoch 55:  22%|██▏       | 120/534 [03:00<10:17,  1.49s/it, loss=0.301, v_num=661]Epoch 55:  22%|██▏       | 120/534 [03:00<10:17,  1.49s/it, loss=0.3, v_num=661]  Epoch 55:  24%|██▍       | 130/534 [03:11<09:51,  1.47s/it, loss=0.3, v_num=661]Epoch 55:  24%|██▍       | 130/534 [03:11<09:51,  1.47s/it, loss=0.308, v_num=661]Epoch 55:  26%|██▌       | 140/534 [03:23<09:29,  1.45s/it, loss=0.308, v_num=661]Epoch 55:  26%|██▌       | 140/534 [03:23<09:29,  1.45s/it, loss=0.317, v_num=661]Epoch 55:  28%|██▊       | 150/534 [03:39<09:17,  1.45s/it, loss=0.317, v_num=661]Epoch 55:  28%|██▊       | 150/534 [03:39<09:17,  1.45s/it, loss=0.312, v_num=661]Epoch 55:  30%|██▉       | 160/534 [03:52<09:00,  1.44s/it, loss=0.312, v_num=661]Epoch 55:  30%|██▉       | 160/534 [03:52<09:00,  1.44s/it, loss=0.295, v_num=661]Epoch 55:  32%|███▏      | 170/534 [04:13<08:59,  1.48s/it, loss=0.295, v_num=661]Epoch 55:  32%|███▏      | 170/534 [04:13<08:59,  1.48s/it, loss=0.304, v_num=661]Epoch 55:  34%|███▎      | 180/534 [04:25<08:38,  1.47s/it, loss=0.304, v_num=661]Epoch 55:  34%|███▎      | 180/534 [04:25<08:38,  1.47s/it, loss=0.312, v_num=661]validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9822764992713928
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9833061695098877
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9816282987594604
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9794838428497314
/it, loss=0.307, v_num=661]Epoch 55:  37%|███▋      | 200/534 [04:39<07:43,  1.39s/it, loss=0.304, v_num=661]validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9822078347206116
validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9821546673774719
validation_epoch_end
graph acc: 0.5119760479041916
valid accuracy: 0.9832658767700195

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9833029508590698
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9813586473464966

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.71it/s][AEpoch 55:  41%|████      | 220/534 [04:40<06:39,  1.27s/it, loss=0.304, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:37,  8.29it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:32,  9.34it/s][AEpoch 55:  45%|████▍     | 240/534 [04:42<05:44,  1.17s/it, loss=0.304, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.04it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:29,  9.71it/s][AEpoch 55:  49%|████▊     | 260/534 [04:44<04:58,  1.09s/it, loss=0.304, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.11it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.75it/s][AEpoch 55:  52%|█████▏    | 280/534 [04:47<04:19,  1.02s/it, loss=0.304, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.27it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.32it/s][AEpoch 55:  56%|█████▌    | 300/534 [04:48<03:44,  1.04it/s, loss=0.304, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.63it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.95it/s][AEpoch 55:  60%|█████▉    | 320/534 [04:50<03:13,  1.11it/s, loss=0.304, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.91it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:18, 11.12it/s][AEpoch 55:  64%|██████▎   | 340/534 [04:52<02:46,  1.17it/s, loss=0.304, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:16, 11.61it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.59it/s][AEpoch 55:  67%|██████▋   | 360/534 [04:53<02:21,  1.23it/s, loss=0.304, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.26it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.63it/s][AEpoch 55:  71%|███████   | 380/534 [04:55<01:59,  1.29it/s, loss=0.304, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.11it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.54it/s][AEpoch 55:  75%|███████▍  | 400/534 [04:56<01:39,  1.35it/s, loss=0.304, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 13.27it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:09, 12.43it/s][AEpoch 55:  79%|███████▊  | 420/534 [04:58<01:20,  1.41it/s, loss=0.304, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.71it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.86it/s][AEpoch 55:  82%|████████▏ | 440/534 [05:00<01:03,  1.47it/s, loss=0.304, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.91it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 11.92it/s][AEpoch 55:  86%|████████▌ | 460/534 [05:01<00:48,  1.53it/s, loss=0.304, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.31it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.77it/s][AEpoch 55:  90%|████████▉ | 480/534 [05:03<00:34,  1.59it/s, loss=0.304, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.24it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 13.46it/s][AEpoch 55:  94%|█████████▎| 500/534 [05:04<00:20,  1.64it/s, loss=0.304, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.18it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.71it/s][AEpoch 55:  97%|█████████▋| 520/534 [05:06<00:08,  1.70it/s, loss=0.304, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.78it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.00it/s][Avalidation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9815924167633057
Epoch 55: 100%|██████████| 534/534 [05:14<00:00,  1.70it/s, loss=0.304, v_num=661]
                                                             [AEpoch 55:   0%|          | 0/534 [00:00<00:00, 12018.06it/s, loss=0.304, v_num=661]Epoch 56:   0%|          | 0/534 [00:00<00:00, 2839.75it/s, loss=0.304, v_num=661] Epoch 56:   0%|          | 0/534 [00:10<1:32:03, 10.34s/it, loss=0.304, v_num=661]Epoch 56:   2%|▏         | 10/534 [00:26<21:01,  2.41s/it, loss=0.304, v_num=661] Epoch 56:   2%|▏         | 10/534 [00:26<21:01,  2.41s/it, loss=0.292, v_num=661]Epoch 56:   4%|▎         | 20/534 [00:40<16:23,  1.91s/it, loss=0.292, v_num=661]Epoch 56:   4%|▎         | 20/534 [00:40<16:23,  1.91s/it, loss=0.288, v_num=661]Epoch 56:   6%|▌         | 30/534 [00:54<14:52,  1.77s/it, loss=0.288, v_num=661]Epoch 56:   6%|▌         | 30/534 [00:54<14:52,  1.77s/it, loss=0.287, v_num=661]Epoch 56:   7%|▋         | 40/534 [01:11<14:19,  1.74s/it, loss=0.287, v_num=661]Epoch 56:   7%|▋         | 40/534 [01:11<14:19,  1.74s/it, loss=0.285, v_num=661]Epoch 56:   9%|▉         | 50/534 [01:27<13:46,  1.71s/it, loss=0.285, v_num=661]Epoch 56:   9%|▉         | 50/534 [01:27<13:46,  1.71s/it, loss=0.289, v_num=661]Epoch 56:  11%|█         | 60/534 [01:41<13:05,  1.66s/it, loss=0.289, v_num=661]Epoch 56:  11%|█         | 60/534 [01:41<13:05,  1.66s/it, loss=0.294, v_num=661]Epoch 56:  13%|█▎        | 70/534 [01:56<12:44,  1.65s/it, loss=0.294, v_num=661]Epoch 56:  13%|█▎        | 70/534 [01:56<12:44,  1.65s/it, loss=0.294, v_num=661]Epoch 56:  15%|█▍        | 80/534 [02:11<12:16,  1.62s/it, loss=0.294, v_num=661]Epoch 56:  15%|█▍        | 80/534 [02:11<12:16,  1.62s/it, loss=0.3, v_num=661]  Epoch 56:  17%|█▋        | 90/534 [02:23<11:42,  1.58s/it, loss=0.3, v_num=661]Epoch 56:  17%|█▋        | 90/534 [02:24<11:42,  1.58s/it, loss=0.299, v_num=661]Epoch 56:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.299, v_num=661]Epoch 56:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.293, v_num=661]Epoch 56:  21%|██        | 110/534 [02:54<11:06,  1.57s/it, loss=0.293, v_num=661]Epoch 56:  21%|██        | 110/534 [02:54<11:06,  1.57s/it, loss=0.293, v_num=661]Epoch 56:  22%|██▏       | 120/534 [03:07<10:40,  1.55s/it, loss=0.293, v_num=661]Epoch 56:  22%|██▏       | 120/534 [03:07<10:40,  1.55s/it, loss=0.297, v_num=661]Epoch 56:  24%|██▍       | 130/534 [03:21<10:22,  1.54s/it, loss=0.297, v_num=661]Epoch 56:  24%|██▍       | 130/534 [03:21<10:22,  1.54s/it, loss=0.297, v_num=661]Epoch 56:  26%|██▌       | 140/534 [03:35<10:02,  1.53s/it, loss=0.297, v_num=661]Epoch 56:  26%|██▌       | 140/534 [03:35<10:02,  1.53s/it, loss=0.298, v_num=661]Epoch 56:  28%|██▊       | 150/534 [03:47<09:39,  1.51s/it, loss=0.298, v_num=661]Epoch 56:  28%|██▊       | 150/534 [03:47<09:39,  1.51s/it, loss=0.295, v_num=661]Epoch 56:  30%|██▉       | 160/534 [04:04<09:26,  1.52s/it, loss=0.295, v_num=661]Epoch 56:  30%|██▉       | 160/534 [04:04<09:26,  1.52s/it, loss=0.279, v_num=661]Epoch 56:  32%|███▏      | 170/534 [04:17<09:07,  1.50s/it, loss=0.279, v_num=661]Epoch 56:  32%|███▏      | 170/534 [04:17<09:07,  1.50s/it, loss=0.286, v_num=661]Epoch 56:  34%|███▎      | 180/534 [04:35<08:59,  1.52s/it, loss=0.286, v_num=661]Epoch 56:  34%|███▎      | 180/534 [04:35<08:59,  1.52s/it, loss=0.303, v_num=661]Epoch 56:  36%|███▌      | 190/534 [04:41<08:26,  1.47s/it, loss=0.303, v_num=661]Epoch 56:  36%|███▌      | 190/534 [04:41<08:26,  1.47s/it, loss=0.307, v_num=661]validation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9832286834716797
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.979445219039917
validation_epoch_end
graph acc: 0.5149700598802395
valid accuracy: 0.9813480973243713
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9813563227653503
3415527344
validation_epoch_end
graph acc: 0.5149700598802395
valid accuracy: 0.982348620891571
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9810861349105835
validation_epoch_end
graph acc: 0.5269461077844312
valid accuracy: 0.9836790561676025

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9815874099731445

Validating:   3%|▎         | 10/334 [00:01<00:58,  5.50it/s][AEpoch 56:  41%|████      | 220/534 [04:45<06:45,  1.29s/it, loss=0.313, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:41,  7.49it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:32,  9.37it/s][AEpoch 56:  45%|████▍     | 240/534 [04:47<05:50,  1.19s/it, loss=0.313, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.87it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.49it/s][AEpoch 56:  49%|████▊     | 260/534 [04:49<05:03,  1.11s/it, loss=0.313, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 10.98it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.72it/s][AEpoch 56:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.313, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25,  9.83it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:21, 11.15it/s][AEpoch 56:  56%|█████▌    | 300/534 [04:53<03:47,  1.03it/s, loss=0.313, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:24,  9.60it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.68it/s][AEpoch 56:  60%|█████▉    | 320/534 [04:55<03:16,  1.09it/s, loss=0.313, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.42it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:23,  8.85it/s][AEpoch 56:  64%|██████▎   | 340/534 [04:57<02:49,  1.15it/s, loss=0.313, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.78it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.56it/s][AEpoch 56:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.313, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.29it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.14it/s][AEpoch 56:  71%|███████   | 380/534 [05:00<02:01,  1.27it/s, loss=0.313, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.70it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.95it/s][AEpoch 56:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.313, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.47it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:11, 11.07it/s][AEpoch 56:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.313, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.82it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.00it/s][AEpoch 56:  82%|████████▏ | 440/534 [05:05<01:05,  1.45it/s, loss=0.313, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.01it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.83it/s][AEpoch 56:  86%|████████▌ | 460/534 [05:07<00:49,  1.50it/s, loss=0.313, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 11.40it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 12.86it/s][AEpoch 56:  90%|████████▉ | 480/534 [05:08<00:34,  1.56it/s, loss=0.313, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.15it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 13.09it/s][AEpoch 56:  94%|█████████▎| 500/534 [05:10<00:21,  1.62it/s, loss=0.313, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.01it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.61it/s][AEpoch 56:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.313, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.05it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.51it/s][A
Validating: 100%|██████████| 334/334 [00:29<00:00, 14.84it/s][Avalidation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9816722869873047
Epoch 56: 100%|██████████| 534/534 [05:19<00:00,  1.67it/s, loss=0.313, v_num=661]
                                                             [AEpoch 56:   0%|          | 0/534 [00:00<00:00, 6732.43it/s, loss=0.313, v_num=661]Epoch 57:   0%|          | 0/534 [00:00<00:00, 1908.24it/s, loss=0.313, v_num=661]Epoch 57:   0%|          | 0/534 [00:10<1:29:09, 10.02s/it, loss=0.313, v_num=661]Epoch 57:   2%|▏         | 10/534 [00:27<21:48,  2.50s/it, loss=0.313, v_num=661] Epoch 57:   2%|▏         | 10/534 [00:27<21:48,  2.50s/it, loss=0.297, v_num=661]Epoch 57:   4%|▎         | 20/534 [00:37<15:15,  1.78s/it, loss=0.297, v_num=661]Epoch 57:   4%|▎         | 20/534 [00:37<15:15,  1.78s/it, loss=0.282, v_num=661]Epoch 57:   4%|▎         | 20/534 [00:50<20:24,  2.38s/it, loss=0.282, v_num=661]Epoch 57:   6%|▌         | 30/534 [00:57<15:37,  1.86s/it, loss=0.282, v_num=661]Epoch 57:   6%|▌         | 30/534 [00:57<15:37,  1.86s/it, loss=0.278, v_num=661]Epoch 57:   7%|▋         | 40/534 [01:09<13:57,  1.69s/it, loss=0.278, v_num=661]Epoch 57:   7%|▋         | 40/534 [01:09<13:57,  1.69s/it, loss=0.279, v_num=661]Epoch 57:   9%|▉         | 50/534 [01:21<12:56,  1.61s/it, loss=0.279, v_num=661]Epoch 57:   9%|▉         | 50/534 [01:21<12:56,  1.61s/it, loss=0.289, v_num=661]Epoch 57:  11%|█         | 60/534 [01:36<12:27,  1.58s/it, loss=0.289, v_num=661]Epoch 57:  11%|█         | 60/534 [01:36<12:27,  1.58s/it, loss=0.293, v_num=661]Epoch 57:  13%|█▎        | 70/534 [01:50<12:04,  1.56s/it, loss=0.293, v_num=661]Epoch 57:  13%|█▎        | 70/534 [01:50<12:04,  1.56s/it, loss=0.286, v_num=661]Epoch 57:  15%|█▍        | 80/534 [02:02<11:27,  1.51s/it, loss=0.286, v_num=661]Epoch 57:  15%|█▍        | 80/534 [02:02<11:27,  1.51s/it, loss=0.286, v_num=661]Epoch 57:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.286, v_num=661]Epoch 57:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.29, v_num=661] Epoch 57:  19%|█▊        | 100/534 [02:31<10:49,  1.50s/it, loss=0.29, v_num=661]Epoch 57:  19%|█▊        | 100/534 [02:31<10:49,  1.50s/it, loss=0.29, v_num=661]Epoch 57:  21%|██        | 110/534 [02:50<10:53,  1.54s/it, loss=0.29, v_num=661]Epoch 57:  21%|██        | 110/534 [02:50<10:53,  1.54s/it, loss=0.289, v_num=661]Epoch 57:  22%|██▏       | 120/534 [03:07<10:43,  1.55s/it, loss=0.289, v_num=661]Epoch 57:  22%|██▏       | 120/534 [03:07<10:43,  1.55s/it, loss=0.288, v_num=661]Epoch 57:  24%|██▍       | 130/534 [03:22<10:25,  1.55s/it, loss=0.288, v_num=661]Epoch 57:  24%|██▍       | 130/534 [03:22<10:25,  1.55s/it, loss=0.303, v_num=661]Epoch 57:  26%|██▌       | 140/534 [03:36<10:04,  1.53s/it, loss=0.303, v_num=661]Epoch 57:  26%|██▌       | 140/534 [03:36<10:04,  1.53s/it, loss=0.308, v_num=661]Epoch 57:  28%|██▊       | 150/534 [03:51<09:48,  1.53s/it, loss=0.308, v_num=661]Epoch 57:  28%|██▊       | 150/534 [03:51<09:48,  1.53s/it, loss=0.29, v_num=661] Epoch 57:  30%|██▉       | 160/534 [04:09<09:39,  1.55s/it, loss=0.29, v_num=661]Epoch 57:  30%|██▉       | 160/534 [04:09<09:39,  1.55s/it, loss=0.276, v_num=661]Epoch 57:  32%|███▏      | 170/534 [04:22<09:17,  1.53s/it, loss=0.276, v_num=661]Epoch 57:  32%|███▏      | 170/534 [04:22<09:17,  1.53s/it, loss=0.276, v_num=661]Epoch 57:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.276, v_num=661]Epoch 57:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.282, v_num=661]validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9816763401031494
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9833449125289917
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9811479449272156
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9796770215034485
0.301, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9829063415527344
validation_epoch_end
graph acc: 0.5209580838323353
valid accuracy: 0.9822322130203247
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9821691513061523
validation_epoch_end
graph acc: 0.5209580838323353
valid accuracy: 0.9836790561676025
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9811680912971497
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9820180535316467

Validating:   3%|▎         | 10/334 [00:01<00:59,  5.46it/s][AEpoch 57:  41%|████      | 220/534 [04:43<06:42,  1.28s/it, loss=0.301, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:45,  6.96it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:32,  9.28it/s][AEpoch 57:  45%|████▍     | 240/534 [04:45<05:47,  1.18s/it, loss=0.301, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.72it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.36it/s][AEpoch 57:  49%|████▊     | 260/534 [04:47<05:01,  1.10s/it, loss=0.301, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:26, 10.30it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.55it/s][AEpoch 57:  52%|█████▏    | 280/534 [04:50<04:22,  1.03s/it, loss=0.301, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.19it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.32it/s][AEpoch 57:  56%|█████▌    | 300/534 [04:51<03:46,  1.03it/s, loss=0.301, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:23,  9.97it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.84it/s][AEpoch 57:  60%|█████▉    | 320/534 [04:53<03:15,  1.09it/s, loss=0.301, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.13it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:21,  9.51it/s][AEpoch 57:  64%|██████▎   | 340/534 [04:55<02:48,  1.15it/s, loss=0.301, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.10it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.61it/s][AEpoch 57:  67%|██████▋   | 360/534 [04:56<02:22,  1.22it/s, loss=0.301, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:13, 12.83it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.77it/s][AEpoch 57:  71%|███████   | 380/534 [04:58<02:00,  1.28it/s, loss=0.301, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.92it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.75it/s][AEpoch 57:  75%|███████▍  | 400/534 [04:59<01:40,  1.34it/s, loss=0.301, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.43it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.17it/s][AEpoch 57:  79%|███████▊  | 420/534 [05:01<01:21,  1.40it/s, loss=0.301, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.01it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.97it/s][AEpoch 57:  82%|████████▏ | 440/534 [05:02<01:04,  1.46it/s, loss=0.301, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:07, 12.84it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.83it/s][AEpoch 57:  86%|████████▌ | 460/534 [05:05<00:48,  1.51it/s, loss=0.301, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 12.30it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.16it/s][AEpoch 57:  90%|████████▉ | 480/534 [05:06<00:34,  1.57it/s, loss=0.301, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:05, 10.78it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.14it/s][AEpoch 57:  94%|█████████▎| 500/534 [05:08<00:20,  1.63it/s, loss=0.301, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.35it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:02, 11.74it/s][AEpoch 57:  97%|█████████▋| 520/534 [05:09<00:08,  1.68it/s, loss=0.301, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.45it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.78it/s][Avalidation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9814326763153076
Epoch 57: 100%|██████████| 534/534 [05:16<00:00,  1.69it/s, loss=0.301, v_num=661]
                                                             [AEpoch 57:   0%|          | 0/534 [00:00<00:00, 14364.05it/s, loss=0.301, v_num=661]Epoch 58:   0%|          | 0/534 [00:00<00:00, 3454.95it/s, loss=0.301, v_num=661] Epoch 58:   0%|          | 0/534 [00:12<1:50:36, 12.43s/it, loss=0.301, v_num=661]Epoch 58:   2%|▏         | 10/534 [00:24<19:26,  2.23s/it, loss=0.301, v_num=661] Epoch 58:   2%|▏         | 10/534 [00:24<19:26,  2.23s/it, loss=0.285, v_num=661]Epoch 58:   4%|▎         | 20/534 [00:37<15:26,  1.80s/it, loss=0.285, v_num=661]Epoch 58:   4%|▎         | 20/534 [00:37<15:26,  1.80s/it, loss=0.271, v_num=661]Epoch 58:   6%|▌         | 30/534 [00:51<14:04,  1.68s/it, loss=0.271, v_num=661]Epoch 58:   6%|▌         | 30/534 [00:51<14:04,  1.68s/it, loss=0.266, v_num=661]Epoch 58:   7%|▋         | 40/534 [01:05<13:14,  1.61s/it, loss=0.266, v_num=661]Epoch 58:   7%|▋         | 40/534 [01:05<13:14,  1.61s/it, loss=0.263, v_num=661]Epoch 58:   9%|▉         | 50/534 [01:20<12:41,  1.57s/it, loss=0.263, v_num=661]Epoch 58:   9%|▉         | 50/534 [01:20<12:41,  1.57s/it, loss=0.273, v_num=661]Epoch 58:  11%|█         | 60/534 [01:34<12:13,  1.55s/it, loss=0.273, v_num=661]Epoch 58:  11%|█         | 60/534 [01:34<12:13,  1.55s/it, loss=0.277, v_num=661]Epoch 58:  13%|█▎        | 70/534 [01:49<11:55,  1.54s/it, loss=0.277, v_num=661]Epoch 58:  13%|█▎        | 70/534 [01:49<11:55,  1.54s/it, loss=0.287, v_num=661]Epoch 58:  15%|█▍        | 80/534 [02:05<11:42,  1.55s/it, loss=0.287, v_num=661]Epoch 58:  15%|█▍        | 80/534 [02:05<11:42,  1.55s/it, loss=0.284, v_num=661]Epoch 58:  17%|█▋        | 90/534 [02:25<11:52,  1.60s/it, loss=0.284, v_num=661]Epoch 58:  17%|█▋        | 90/534 [02:25<11:52,  1.60s/it, loss=0.277, v_num=661]Epoch 58:  19%|█▊        | 100/534 [02:42<11:37,  1.61s/it, loss=0.277, v_num=661]Epoch 58:  19%|█▊        | 100/534 [02:42<11:37,  1.61s/it, loss=0.284, v_num=661]Epoch 58:  21%|██        | 110/534 [02:58<11:21,  1.61s/it, loss=0.284, v_num=661]Epoch 58:  21%|██        | 110/534 [02:58<11:21,  1.61s/it, loss=0.285, v_num=661]Epoch 58:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.285, v_num=661]Epoch 58:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.288, v_num=661]Epoch 58:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.288, v_num=661]Epoch 58:  24%|██▍       | 130/534 [03:27<10:39,  1.58s/it, loss=0.288, v_num=661]Epoch 58:  26%|██▌       | 140/534 [03:38<10:11,  1.55s/it, loss=0.288, v_num=661]Epoch 58:  26%|██▌       | 140/534 [03:38<10:11,  1.55s/it, loss=0.28, v_num=661] Epoch 58:  28%|██▊       | 150/534 [03:50<09:45,  1.53s/it, loss=0.28, v_num=661]Epoch 58:  28%|██▊       | 150/534 [03:50<09:45,  1.53s/it, loss=0.278, v_num=661]Epoch 58:  30%|██▉       | 160/534 [04:04<09:27,  1.52s/it, loss=0.278, v_num=661]Epoch 58:  30%|██▉       | 160/534 [04:04<09:27,  1.52s/it, loss=0.278, v_num=661]Epoch 58:  32%|███▏      | 170/534 [04:20<09:14,  1.52s/it, loss=0.278, v_num=661]Epoch 58:  32%|███▏      | 170/534 [04:20<09:14,  1.52s/it, loss=0.277, v_num=661]Epoch 58:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.277, v_num=661]Epoch 58:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.279, v_num=661]validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9818363785743713
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9796770215034485
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9833061695098877
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9811879992485046
v_num=661]Epoch 58:  37%|███▋      | 200/534 [04:41<07:48,  1.40s/it, loss=0.293, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5269461077844312
valid accuracy: 0.9832797050476074
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.98294597864151
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9827736020088196
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9817780256271362
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9818597435951233

Validating:   3%|▎         | 10/334 [00:01<01:00,  5.37it/s][AEpoch 58:  41%|████      | 220/534 [04:43<06:43,  1.28s/it, loss=0.293, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:39,  7.88it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  9.08it/s][AEpoch 58:  45%|████▍     | 240/534 [04:45<05:48,  1.18s/it, loss=0.293, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.06it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.34it/s][AEpoch 58:  49%|████▊     | 260/534 [04:47<05:01,  1.10s/it, loss=0.293, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.86it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.85it/s][AEpoch 58:  52%|█████▏    | 280/534 [04:50<04:22,  1.03s/it, loss=0.293, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25, 10.03it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24, 10.16it/s][AEpoch 58:  56%|█████▌    | 300/534 [04:51<03:46,  1.03it/s, loss=0.293, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.60it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.52it/s][AEpoch 58:  60%|█████▉    | 320/534 [04:52<03:15,  1.10it/s, loss=0.293, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.06it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  8.97it/s][AEpoch 58:  64%|██████▎   | 340/534 [04:55<02:48,  1.15it/s, loss=0.293, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 10.99it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:13, 13.14it/s][AEpoch 58:  67%|██████▋   | 360/534 [04:56<02:22,  1.22it/s, loss=0.293, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.28it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.41it/s][AEpoch 58:  71%|███████   | 380/534 [04:58<02:00,  1.28it/s, loss=0.293, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.39it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.53it/s][AEpoch 58:  75%|███████▍  | 400/534 [05:00<01:40,  1.34it/s, loss=0.293, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.44it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.19it/s][AEpoch 58:  79%|███████▊  | 420/534 [05:01<01:21,  1.40it/s, loss=0.293, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:10, 11.27it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.77it/s][AEpoch 58:  82%|████████▏ | 440/534 [05:03<01:04,  1.45it/s, loss=0.293, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:07, 12.98it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.50it/s][AEpoch 58:  86%|████████▌ | 460/534 [05:05<00:48,  1.51it/s, loss=0.293, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.61it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 12.92it/s][AEpoch 58:  90%|████████▉ | 480/534 [05:06<00:34,  1.57it/s, loss=0.293, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.61it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.87it/s][AEpoch 58:  94%|█████████▎| 500/534 [05:08<00:20,  1.63it/s, loss=0.293, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.74it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:02, 11.42it/s][AEpoch 58:  97%|█████████▋| 520/534 [05:09<00:08,  1.68it/s, loss=0.293, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.83it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.36it/s][Avalidation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9811531901359558
Epoch 58: 100%|██████████| 534/534 [05:17<00:00,  1.69it/s, loss=0.293, v_num=661]
                                                             [AEpoch 58:   0%|          | 0/534 [00:00<00:00, 6626.07it/s, loss=0.293, v_num=661]Epoch 59:   0%|          | 0/534 [00:00<00:00, 2236.96it/s, loss=0.293, v_num=661]Epoch 59:   0%|          | 0/534 [00:14<2:06:54, 14.26s/it, loss=0.293, v_num=661]Epoch 59:   2%|▏         | 10/534 [00:21<17:08,  1.96s/it, loss=0.293, v_num=661] Epoch 59:   2%|▏         | 10/534 [00:21<17:08,  1.96s/it, loss=0.277, v_num=661]Epoch 59:   4%|▎         | 20/534 [00:38<15:34,  1.82s/it, loss=0.277, v_num=661]Epoch 59:   4%|▎         | 20/534 [00:38<15:34,  1.82s/it, loss=0.27, v_num=661] Epoch 59:   6%|▌         | 30/534 [00:52<14:21,  1.71s/it, loss=0.27, v_num=661]Epoch 59:   6%|▌         | 30/534 [00:52<14:21,  1.71s/it, loss=0.272, v_num=661]Epoch 59:   7%|▋         | 40/534 [01:08<13:41,  1.66s/it, loss=0.272, v_num=661]Epoch 59:   7%|▋         | 40/534 [01:08<13:41,  1.66s/it, loss=0.269, v_num=661]Epoch 59:   9%|▉         | 50/534 [01:23<13:09,  1.63s/it, loss=0.269, v_num=661]Epoch 59:   9%|▉         | 50/534 [01:23<13:09,  1.63s/it, loss=0.267, v_num=661]Epoch 59:  11%|█         | 60/534 [01:36<12:28,  1.58s/it, loss=0.267, v_num=661]Epoch 59:  11%|█         | 60/534 [01:36<12:28,  1.58s/it, loss=0.262, v_num=661]Epoch 59:  13%|█▎        | 70/534 [01:49<11:54,  1.54s/it, loss=0.262, v_num=661]Epoch 59:  13%|█▎        | 70/534 [01:49<11:54,  1.54s/it, loss=0.26, v_num=661] Epoch 59:  15%|█▍        | 80/534 [02:02<11:26,  1.51s/it, loss=0.26, v_num=661]Epoch 59:  15%|█▍        | 80/534 [02:02<11:26,  1.51s/it, loss=0.265, v_num=661]Epoch 59:  17%|█▋        | 90/534 [02:19<11:19,  1.53s/it, loss=0.265, v_num=661]Epoch 59:  17%|█▋        | 90/534 [02:19<11:19,  1.53s/it, loss=0.26, v_num=661] Epoch 59:  19%|█▊        | 100/534 [02:35<11:10,  1.54s/it, loss=0.26, v_num=661]Epoch 59:  19%|█▊        | 100/534 [02:35<11:10,  1.54s/it, loss=0.267, v_num=661]Epoch 59:  21%|██        | 110/534 [02:47<10:41,  1.51s/it, loss=0.267, v_num=661]Epoch 59:  21%|██        | 110/534 [02:47<10:41,  1.51s/it, loss=0.284, v_num=661]Epoch 59:  22%|██▏       | 120/534 [02:59<10:14,  1.48s/it, loss=0.284, v_num=661]Epoch 59:  22%|██▏       | 120/534 [02:59<10:14,  1.48s/it, loss=0.279, v_num=661]Epoch 59:  24%|██▍       | 130/534 [03:11<09:51,  1.46s/it, loss=0.279, v_num=661]Epoch 59:  24%|██▍       | 130/534 [03:11<09:51,  1.46s/it, loss=0.272, v_num=661]Epoch 59:  26%|██▌       | 140/534 [03:24<09:32,  1.45s/it, loss=0.272, v_num=661]Epoch 59:  26%|██▌       | 140/534 [03:24<09:32,  1.45s/it, loss=0.27, v_num=661] Epoch 59:  28%|██▊       | 150/534 [03:39<09:18,  1.45s/it, loss=0.27, v_num=661]Epoch 59:  28%|██▊       | 150/534 [03:39<09:18,  1.45s/it, loss=0.274, v_num=661]Epoch 59:  30%|██▉       | 160/534 [03:52<09:01,  1.45s/it, loss=0.274, v_num=661]Epoch 59:  30%|██▉       | 160/534 [03:52<09:01,  1.45s/it, loss=0.28, v_num=661] Epoch 59:  32%|███▏      | 170/534 [04:06<08:44,  1.44s/it, loss=0.28, v_num=661]Epoch 59:  32%|███▏      | 170/534 [04:06<08:44,  1.44s/it, loss=0.273, v_num=661]Epoch 59:  34%|███▎      | 180/534 [04:21<08:31,  1.44s/it, loss=0.273, v_num=661]Epoch 59:  34%|███▎      | 180/534 [04:21<08:31,  1.44s/it, loss=0.272, v_num=661]validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9795224666595459
validation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9811079502105713
validation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9834223985671997
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9820364117622375
=0.276, v_num=661]validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9823874235153198
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9825469255447388
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.981937050819397

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5209580838323353
valid accuracy: 0.9839270114898682
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.983025312423706
validation_epoch_end
graph acc: 0.45209580838323354
valid accuracy: 0.9808249473571777

Validating:   3%|▎         | 10/334 [00:01<01:01,  5.30it/s][AEpoch 59:  41%|████      | 220/534 [04:37<06:33,  1.25s/it, loss=0.276, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.31it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:30, 10.06it/s][AEpoch 59:  45%|████▍     | 240/534 [04:38<05:40,  1.16s/it, loss=0.276, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:34,  8.60it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.04it/s][AEpoch 59:  49%|████▊     | 260/534 [04:40<04:54,  1.08s/it, loss=0.276, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.63it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.93it/s][AEpoch 59:  52%|█████▏    | 280/534 [04:43<04:16,  1.01s/it, loss=0.276, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.43it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.74it/s][AEpoch 59:  56%|█████▌    | 300/534 [04:44<03:41,  1.06it/s, loss=0.276, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.60it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.01it/s][AEpoch 59:  60%|█████▉    | 320/534 [04:46<03:11,  1.12it/s, loss=0.276, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:16, 12.89it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  8.98it/s][AEpoch 59:  64%|██████▎   | 340/534 [04:49<02:44,  1.18it/s, loss=0.276, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.67it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.66it/s][AEpoch 59:  67%|██████▋   | 360/534 [04:50<02:19,  1.24it/s, loss=0.276, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:13, 12.53it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.12it/s][AEpoch 59:  71%|███████   | 380/534 [04:51<01:58,  1.30it/s, loss=0.276, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.01it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.95it/s][AEpoch 59:  75%|███████▍  | 400/534 [04:53<01:38,  1.37it/s, loss=0.276, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:11, 12.07it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 11.58it/s][AEpoch 59:  79%|███████▊  | 420/534 [04:55<01:19,  1.43it/s, loss=0.276, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.42it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.22it/s][AEpoch 59:  82%|████████▏ | 440/534 [04:56<01:03,  1.49it/s, loss=0.276, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.41it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.92it/s][AEpoch 59:  86%|████████▌ | 460/534 [04:58<00:47,  1.54it/s, loss=0.276, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 12.32it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 12.98it/s][AEpoch 59:  90%|████████▉ | 480/534 [04:59<00:33,  1.60it/s, loss=0.276, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.67it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 13.57it/s][AEpoch 59:  94%|█████████▎| 500/534 [05:01<00:20,  1.66it/s, loss=0.276, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.16it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 13.28it/s][AEpoch 59:  97%|█████████▋| 520/534 [05:02<00:08,  1.72it/s, loss=0.276, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.74it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.49it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9815125465393066
Epoch 59: 100%|██████████| 534/534 [05:10<00:00,  1.72it/s, loss=0.276, v_num=661]
                                                             [AEpoch 59:   0%|          | 0/534 [00:00<00:00, 8272.79it/s, loss=0.276, v_num=661]Epoch 60:   0%|          | 0/534 [00:00<00:00, 2626.36it/s, loss=0.276, v_num=661]Epoch 60:   0%|          | 0/534 [00:12<1:52:08, 12.60s/it, loss=0.276, v_num=661]Epoch 60:   2%|▏         | 10/534 [00:27<21:26,  2.46s/it, loss=0.276, v_num=661] Epoch 60:   2%|▏         | 10/534 [00:27<21:26,  2.46s/it, loss=0.267, v_num=661]Epoch 60:   4%|▎         | 20/534 [00:41<16:54,  1.97s/it, loss=0.267, v_num=661]Epoch 60:   4%|▎         | 20/534 [00:41<16:54,  1.97s/it, loss=0.27, v_num=661] Epoch 60:   6%|▌         | 30/534 [00:57<15:26,  1.84s/it, loss=0.27, v_num=661]Epoch 60:   6%|▌         | 30/534 [00:57<15:26,  1.84s/it, loss=0.264, v_num=661]Epoch 60:   7%|▋         | 40/534 [01:10<14:11,  1.72s/it, loss=0.264, v_num=661]Epoch 60:   7%|▋         | 40/534 [01:10<14:11,  1.72s/it, loss=0.259, v_num=661]Epoch 60:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.259, v_num=661]Epoch 60:   9%|▉         | 50/534 [01:22<13:06,  1.63s/it, loss=0.258, v_num=661]Epoch 60:  11%|█         | 60/534 [01:36<12:30,  1.58s/it, loss=0.258, v_num=661]Epoch 60:  11%|█         | 60/534 [01:36<12:30,  1.58s/it, loss=0.257, v_num=661]Epoch 60:  13%|█▎        | 70/534 [01:53<12:18,  1.59s/it, loss=0.257, v_num=661]Epoch 60:  13%|█▎        | 70/534 [01:53<12:18,  1.59s/it, loss=0.257, v_num=661]Epoch 60:  15%|█▍        | 80/534 [02:06<11:46,  1.56s/it, loss=0.257, v_num=661]Epoch 60:  15%|█▍        | 80/534 [02:06<11:46,  1.56s/it, loss=0.261, v_num=661]Epoch 60:  17%|█▋        | 90/534 [02:18<11:14,  1.52s/it, loss=0.261, v_num=661]Epoch 60:  17%|█▋        | 90/534 [02:18<11:14,  1.52s/it, loss=0.257, v_num=661]Epoch 60:  19%|█▊        | 100/534 [02:35<11:07,  1.54s/it, loss=0.257, v_num=661]Epoch 60:  19%|█▊        | 100/534 [02:35<11:07,  1.54s/it, loss=0.262, v_num=661]Epoch 60:  21%|██        | 110/534 [02:49<10:47,  1.53s/it, loss=0.262, v_num=661]Epoch 60:  21%|██        | 110/534 [02:49<10:47,  1.53s/it, loss=0.258, v_num=661]Epoch 60:  22%|██▏       | 120/534 [03:05<10:33,  1.53s/it, loss=0.258, v_num=661]Epoch 60:  22%|██▏       | 120/534 [03:05<10:33,  1.53s/it, loss=0.257, v_num=661]Epoch 60:  24%|██▍       | 130/534 [03:17<10:08,  1.51s/it, loss=0.257, v_num=661]Epoch 60:  24%|██▍       | 130/534 [03:17<10:08,  1.51s/it, loss=0.265, v_num=661]Epoch 60:  26%|██▌       | 140/534 [03:31<09:52,  1.50s/it, loss=0.265, v_num=661]Epoch 60:  26%|██▌       | 140/534 [03:31<09:52,  1.50s/it, loss=0.26, v_num=661] Epoch 60:  28%|██▊       | 150/534 [03:47<09:39,  1.51s/it, loss=0.26, v_num=661]Epoch 60:  28%|██▊       | 150/534 [03:47<09:39,  1.51s/it, loss=0.269, v_num=661]Epoch 60:  30%|██▉       | 160/534 [04:00<09:18,  1.49s/it, loss=0.269, v_num=661]Epoch 60:  30%|██▉       | 160/534 [04:00<09:18,  1.49s/it, loss=0.283, v_num=661]Epoch 60:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.283, v_num=661]Epoch 60:  32%|███▏      | 170/534 [04:18<09:10,  1.51s/it, loss=0.276, v_num=661]Epoch 60:  34%|███▎      | 180/534 [04:32<08:52,  1.50s/it, loss=0.276, v_num=661]Epoch 60:  34%|███▎      | 180/534 [04:32<08:52,  1.50s/it, loss=0.268, v_num=661]Epoch 60:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.268, v_num=661]Epoch 60:  36%|███▌      | 190/534 [04:37<08:19,  1.45s/it, loss=0.274, v_num=661]validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9810279011726379
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.983654797077179
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9818363785743713
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.979638397693634
racy: 0.9820144176483154

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5089820359281437
valid accuracy: 0.9834311604499817
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.9831046462059021
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9828491806983948
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9811299443244934

Validating:   3%|▎         | 10/334 [00:01<01:00,  5.40it/s][AEpoch 60:  41%|████      | 220/534 [04:45<06:45,  1.29s/it, loss=0.261, v_num=661]
Validating:   6%|▌         | 20/334 [00:03<00:46,  6.68it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.86it/s][AEpoch 60:  45%|████▍     | 240/534 [04:47<05:50,  1.19s/it, loss=0.261, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.76it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.21it/s][AEpoch 60:  49%|████▊     | 260/534 [04:48<05:03,  1.11s/it, loss=0.261, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.59it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.89it/s][AEpoch 60:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.261, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.74it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.74it/s][AEpoch 60:  56%|█████▌    | 300/534 [04:52<03:47,  1.03it/s, loss=0.261, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 11.07it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.05it/s][AEpoch 60:  60%|█████▉    | 320/534 [04:54<03:16,  1.09it/s, loss=0.261, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.47it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.13it/s][AEpoch 60:  64%|██████▎   | 340/534 [04:57<02:48,  1.15it/s, loss=0.261, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.07it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.45it/s][AEpoch 60:  67%|██████▋   | 360/534 [04:58<02:23,  1.21it/s, loss=0.261, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:13, 12.89it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.67it/s][AEpoch 60:  71%|███████   | 380/534 [04:59<02:01,  1.27it/s, loss=0.261, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.80it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:11, 12.51it/s][AEpoch 60:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.261, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.69it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 11.75it/s][AEpoch 60:  79%|███████▊  | 420/534 [05:03<01:22,  1.39it/s, loss=0.261, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 11.55it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.88it/s][AEpoch 60:  82%|████████▏ | 440/534 [05:04<01:04,  1.45it/s, loss=0.261, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.45it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 10.63it/s][AEpoch 60:  86%|████████▌ | 460/534 [05:06<00:49,  1.50it/s, loss=0.261, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 11.45it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 11.26it/s][AEpoch 60:  90%|████████▉ | 480/534 [05:08<00:34,  1.56it/s, loss=0.261, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:05, 10.37it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.27it/s][AEpoch 60:  94%|█████████▎| 500/534 [05:09<00:21,  1.62it/s, loss=0.261, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.80it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.33it/s][AEpoch 60:  97%|█████████▋| 520/534 [05:11<00:08,  1.67it/s, loss=0.261, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.15it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.83it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9814326763153076
Epoch 60: 100%|██████████| 534/534 [05:19<00:00,  1.68it/s, loss=0.261, v_num=661]
                                                             [AEpoch 60:   0%|          | 0/534 [00:00<00:00, 13842.59it/s, loss=0.261, v_num=661]Epoch 61:   0%|          | 0/534 [00:00<00:00, 3387.97it/s, loss=0.261, v_num=661] Epoch 61:   0%|          | 0/534 [00:12<1:53:27, 12.75s/it, loss=0.261, v_num=661]Epoch 61:   2%|▏         | 10/534 [00:24<19:16,  2.21s/it, loss=0.261, v_num=661] Epoch 61:   2%|▏         | 10/534 [00:24<19:16,  2.21s/it, loss=0.258, v_num=661]Epoch 61:   4%|▎         | 20/534 [00:40<16:36,  1.94s/it, loss=0.258, v_num=661]Epoch 61:   4%|▎         | 20/534 [00:40<16:36,  1.94s/it, loss=0.264, v_num=661]Epoch 61:   6%|▌         | 30/534 [00:57<15:34,  1.85s/it, loss=0.264, v_num=661]Epoch 61:   6%|▌         | 30/534 [00:57<15:34,  1.85s/it, loss=0.252, v_num=661]Epoch 61:   7%|▋         | 40/534 [01:11<14:25,  1.75s/it, loss=0.252, v_num=661]Epoch 61:   7%|▋         | 40/534 [01:11<14:25,  1.75s/it, loss=0.251, v_num=661]Epoch 61:   9%|▉         | 50/534 [01:23<13:11,  1.64s/it, loss=0.251, v_num=661]Epoch 61:   9%|▉         | 50/534 [01:23<13:11,  1.64s/it, loss=0.257, v_num=661]Epoch 61:  11%|█         | 60/534 [01:38<12:48,  1.62s/it, loss=0.257, v_num=661]Epoch 61:  11%|█         | 60/534 [01:38<12:48,  1.62s/it, loss=0.255, v_num=661]Epoch 61:  13%|█▎        | 70/534 [01:52<12:12,  1.58s/it, loss=0.255, v_num=661]Epoch 61:  13%|█▎        | 70/534 [01:52<12:12,  1.58s/it, loss=0.247, v_num=661]Epoch 61:  15%|█▍        | 80/534 [02:15<12:41,  1.68s/it, loss=0.247, v_num=661]Epoch 61:  15%|█▍        | 80/534 [02:15<12:41,  1.68s/it, loss=0.255, v_num=661]Epoch 61:  17%|█▋        | 90/534 [02:26<11:56,  1.61s/it, loss=0.255, v_num=661]Epoch 61:  17%|█▋        | 90/534 [02:26<11:56,  1.61s/it, loss=0.261, v_num=661]Epoch 61:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.261, v_num=661]Epoch 61:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.259, v_num=661]Epoch 61:  21%|██        | 110/534 [02:51<10:55,  1.55s/it, loss=0.259, v_num=661]Epoch 61:  21%|██        | 110/534 [02:51<10:55,  1.55s/it, loss=0.261, v_num=661]Epoch 61:  22%|██▏       | 120/534 [03:05<10:35,  1.54s/it, loss=0.261, v_num=661]Epoch 61:  22%|██▏       | 120/534 [03:05<10:35,  1.54s/it, loss=0.252, v_num=661]Epoch 61:  24%|██▍       | 130/534 [03:16<10:06,  1.50s/it, loss=0.252, v_num=661]Epoch 61:  24%|██▍       | 130/534 [03:16<10:06,  1.50s/it, loss=0.251, v_num=661]Epoch 61:  26%|██▌       | 140/534 [03:33<09:57,  1.52s/it, loss=0.251, v_num=661]Epoch 61:  26%|██▌       | 140/534 [03:33<09:57,  1.52s/it, loss=0.247, v_num=661]Epoch 61:  28%|██▊       | 150/534 [03:45<09:32,  1.49s/it, loss=0.247, v_num=661]Epoch 61:  28%|██▊       | 150/534 [03:45<09:32,  1.49s/it, loss=0.251, v_num=661]Epoch 61:  30%|██▉       | 160/534 [04:01<09:20,  1.50s/it, loss=0.251, v_num=661]Epoch 61:  30%|██▉       | 160/534 [04:01<09:20,  1.50s/it, loss=0.263, v_num=661]Epoch 61:  32%|███▏      | 170/534 [04:15<09:03,  1.49s/it, loss=0.263, v_num=661]Epoch 61:  32%|███▏      | 170/534 [04:15<09:03,  1.49s/it, loss=0.253, v_num=661]Epoch 61:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.253, v_num=661]Epoch 61:  34%|███▎      | 180/534 [04:29<08:47,  1.49s/it, loss=0.253, v_num=661]validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9836934804916382
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9809478521347046
validation_epoch_end
graph acc: 0.4550898203592814
valid accuracy: 0.9815163016319275
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9795224666595459
921882629
validation_epoch_end
graph acc: 0.5419161676646707
valid accuracy: 0.9835512638092041
validation_epoch_end
graph acc: 0.5299401197604791
valid accuracy: 0.9840509295463562
Epoch 61:  37%|███▋      | 200/534 [04:39<07:44,  1.39s/it, loss=0.264, v_num=661]validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9820829629898071
Epoch 61:  37%|███▋      | 200/534 [04:39<07:44,  1.39s/it, loss=0.268, v_num=661]validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9835408926010132

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][A
Validating:   3%|▎         | 10/334 [00:01<01:00,  5.39it/s][AEpoch 61:  41%|████      | 220/534 [04:41<06:39,  1.27s/it, loss=0.268, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.40it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  9.03it/s][AEpoch 61:  45%|████▍     | 240/534 [04:43<05:45,  1.18s/it, loss=0.268, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.83it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:27, 10.30it/s][AEpoch 61:  49%|████▊     | 260/534 [04:45<04:59,  1.09s/it, loss=0.268, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.91it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.45it/s][AEpoch 61:  52%|█████▏    | 280/534 [04:48<04:20,  1.02s/it, loss=0.268, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.07it/s][A
Validating:  27%|██▋       | 90/334 [00:10<00:26,  9.05it/s][AEpoch 61:  56%|█████▌    | 300/534 [04:49<03:45,  1.04it/s, loss=0.268, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.26it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.32it/s][AEpoch 61:  60%|█████▉    | 320/534 [04:51<03:14,  1.10it/s, loss=0.268, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.87it/s][A
Validating:  39%|███▉      | 130/334 [00:14<00:23,  8.81it/s][AEpoch 61:  64%|██████▎   | 340/534 [04:53<02:47,  1.16it/s, loss=0.268, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.77it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:14, 12.74it/s][AEpoch 61:  67%|██████▋   | 360/534 [04:54<02:22,  1.23it/s, loss=0.268, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:15, 10.93it/s][A
Validating:  51%|█████     | 170/334 [00:17<00:14, 11.63it/s][AEpoch 61:  71%|███████   | 380/534 [04:56<01:59,  1.28it/s, loss=0.268, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:14, 10.52it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.56it/s][AEpoch 61:  75%|███████▍  | 400/534 [04:58<01:39,  1.34it/s, loss=0.268, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.32it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.81it/s][AEpoch 61:  79%|███████▊  | 420/534 [05:00<01:21,  1.40it/s, loss=0.268, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 12.07it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.68it/s][AEpoch 61:  82%|████████▏ | 440/534 [05:01<01:04,  1.46it/s, loss=0.268, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.07it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.04it/s][AEpoch 61:  86%|████████▌ | 460/534 [05:03<00:48,  1.52it/s, loss=0.268, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 12.04it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:05, 12.50it/s][AEpoch 61:  90%|████████▉ | 480/534 [05:04<00:34,  1.58it/s, loss=0.268, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 11.47it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.64it/s][AEpoch 61:  94%|█████████▎| 500/534 [05:06<00:20,  1.64it/s, loss=0.268, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.61it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:02, 11.67it/s][AEpoch 61:  97%|█████████▋| 520/534 [05:08<00:08,  1.69it/s, loss=0.268, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.93it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.06it/s][A
Validating: 100%|██████████| 334/334 [00:29<00:00, 14.28it/s][Avalidation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9815924167633057
Epoch 61: 100%|██████████| 534/534 [05:16<00:00,  1.69it/s, loss=0.268, v_num=661]
                                                             [AEpoch 61:   0%|          | 0/534 [00:00<00:00, 10433.59it/s, loss=0.268, v_num=661]Epoch 62:   0%|          | 0/534 [00:00<00:00, 2394.01it/s, loss=0.268, v_num=661] Epoch 62:   0%|          | 0/534 [00:15<2:17:38, 15.47s/it, loss=0.268, v_num=661]Epoch 62:   2%|▏         | 10/534 [00:29<23:34,  2.70s/it, loss=0.268, v_num=661] Epoch 62:   2%|▏         | 10/534 [00:29<23:34,  2.70s/it, loss=0.263, v_num=661]Epoch 62:   4%|▎         | 20/534 [00:42<17:27,  2.04s/it, loss=0.263, v_num=661]Epoch 62:   4%|▎         | 20/534 [00:42<17:28,  2.04s/it, loss=0.25, v_num=661] Epoch 62:   6%|▌         | 30/534 [00:58<15:47,  1.88s/it, loss=0.25, v_num=661]Epoch 62:   6%|▌         | 30/534 [00:58<15:47,  1.88s/it, loss=0.248, v_num=661]Epoch 62:   7%|▋         | 40/534 [01:16<15:26,  1.88s/it, loss=0.248, v_num=661]Epoch 62:   7%|▋         | 40/534 [01:16<15:26,  1.88s/it, loss=0.251, v_num=661]Epoch 62:   9%|▉         | 50/534 [01:31<14:25,  1.79s/it, loss=0.251, v_num=661]Epoch 62:   9%|▉         | 50/534 [01:31<14:25,  1.79s/it, loss=0.244, v_num=661]Epoch 62:  11%|█         | 60/534 [01:44<13:35,  1.72s/it, loss=0.244, v_num=661]Epoch 62:  11%|█         | 60/534 [01:44<13:35,  1.72s/it, loss=0.251, v_num=661]Epoch 62:  13%|█▎        | 70/534 [01:58<12:55,  1.67s/it, loss=0.251, v_num=661]Epoch 62:  13%|█▎        | 70/534 [01:58<12:55,  1.67s/it, loss=0.257, v_num=661]Epoch 62:  15%|█▍        | 80/534 [02:13<12:26,  1.65s/it, loss=0.257, v_num=661]Epoch 62:  15%|█▍        | 80/534 [02:13<12:26,  1.65s/it, loss=0.249, v_num=661]Epoch 62:  17%|█▋        | 90/534 [02:23<11:41,  1.58s/it, loss=0.249, v_num=661]Epoch 62:  17%|█▋        | 90/534 [02:23<11:41,  1.58s/it, loss=0.245, v_num=661]Epoch 62:  19%|█▊        | 100/534 [02:36<11:10,  1.55s/it, loss=0.245, v_num=661]Epoch 62:  19%|█▊        | 100/534 [02:36<11:10,  1.55s/it, loss=0.25, v_num=661] Epoch 62:  21%|██        | 110/534 [02:50<10:51,  1.54s/it, loss=0.25, v_num=661]Epoch 62:  21%|██        | 110/534 [02:50<10:51,  1.54s/it, loss=0.259, v_num=661]Epoch 62:  22%|██▏       | 120/534 [03:02<10:24,  1.51s/it, loss=0.259, v_num=661]Epoch 62:  22%|██▏       | 120/534 [03:02<10:24,  1.51s/it, loss=0.258, v_num=661]Epoch 62:  24%|██▍       | 130/534 [03:16<10:04,  1.50s/it, loss=0.258, v_num=661]Epoch 62:  24%|██▍       | 130/534 [03:16<10:04,  1.50s/it, loss=0.254, v_num=661]Epoch 62:  26%|██▌       | 140/534 [03:29<09:45,  1.49s/it, loss=0.254, v_num=661]Epoch 62:  26%|██▌       | 140/534 [03:29<09:45,  1.49s/it, loss=0.248, v_num=661]Epoch 62:  28%|██▊       | 150/534 [03:45<09:34,  1.50s/it, loss=0.248, v_num=661]Epoch 62:  28%|██▊       | 150/534 [03:45<09:34,  1.50s/it, loss=0.25, v_num=661] Epoch 62:  30%|██▉       | 160/534 [03:59<09:15,  1.49s/it, loss=0.25, v_num=661]Epoch 62:  30%|██▉       | 160/534 [03:59<09:15,  1.49s/it, loss=0.26, v_num=661]Epoch 62:  32%|███▏      | 170/534 [04:11<08:54,  1.47s/it, loss=0.26, v_num=661]Epoch 62:  32%|███▏      | 170/534 [04:11<08:54,  1.47s/it, loss=0.266, v_num=661]Epoch 62:  34%|███▎      | 180/534 [04:23<08:35,  1.46s/it, loss=0.266, v_num=661]Epoch 62:  34%|███▎      | 180/534 [04:23<08:35,  1.46s/it, loss=0.258, v_num=661]Epoch 62:  36%|███▌      | 190/534 [04:31<08:08,  1.42s/it, loss=0.258, v_num=661]Epoch 62:  36%|███▌      | 190/534 [04:31<08:08,  1.42s/it, loss=0.245, v_num=661]validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9814763069152832
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.982725203037262
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9811879992485046
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9799088835716248
acy: 0.9821591973304749

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5419161676646707
valid accuracy: 0.9846293926239014
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9824336171150208
validation_epoch_end
graph acc: 0.5269461077844312
valid accuracy: 0.9828529357910156
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9825172424316406

Validating:   3%|▎         | 10/334 [00:01<01:00,  5.37it/s][AEpoch 62:  41%|████      | 220/534 [04:40<06:39,  1.27s/it, loss=0.254, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:41,  7.55it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:30, 10.00it/s][AEpoch 62:  45%|████▍     | 240/534 [04:42<05:44,  1.17s/it, loss=0.254, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.85it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28,  9.95it/s][AEpoch 62:  49%|████▊     | 260/534 [04:44<04:58,  1.09s/it, loss=0.254, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:22, 12.14it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.61it/s][AEpoch 62:  52%|█████▏    | 280/534 [04:47<04:19,  1.02s/it, loss=0.254, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:28,  8.94it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:25,  9.75it/s][AEpoch 62:  56%|█████▌    | 300/534 [04:48<03:44,  1.04it/s, loss=0.254, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.28it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.94it/s][AEpoch 62:  60%|█████▉    | 320/534 [04:50<03:13,  1.10it/s, loss=0.254, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.17it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:23,  8.73it/s][AEpoch 62:  64%|██████▎   | 340/534 [04:53<02:46,  1.16it/s, loss=0.254, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.62it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.53it/s][AEpoch 62:  67%|██████▋   | 360/534 [04:54<02:21,  1.23it/s, loss=0.254, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:15, 11.04it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.00it/s][AEpoch 62:  71%|███████   | 380/534 [04:56<01:59,  1.29it/s, loss=0.254, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:15, 10.17it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.62it/s][AEpoch 62:  75%|███████▍  | 400/534 [04:57<01:39,  1.35it/s, loss=0.254, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.55it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.83it/s][AEpoch 62:  79%|███████▊  | 420/534 [04:59<01:21,  1.41it/s, loss=0.254, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:10, 11.01it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.39it/s][AEpoch 62:  82%|████████▏ | 440/534 [05:00<01:04,  1.47it/s, loss=0.254, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.50it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.44it/s][AEpoch 62:  86%|████████▌ | 460/534 [05:02<00:48,  1.52it/s, loss=0.254, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 12.23it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:05, 12.39it/s][AEpoch 62:  90%|████████▉ | 480/534 [05:04<00:34,  1.58it/s, loss=0.254, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 10.92it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.09it/s][AEpoch 62:  94%|█████████▎| 500/534 [05:05<00:20,  1.64it/s, loss=0.254, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.76it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.53it/s][AEpoch 62:  97%|█████████▋| 520/534 [05:07<00:08,  1.70it/s, loss=0.254, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.28it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.01it/s][Avalidation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9812729954719543
Epoch 62: 100%|██████████| 534/534 [05:15<00:00,  1.70it/s, loss=0.254, v_num=661]
                                                             [AEpoch 62:   0%|          | 0/534 [00:00<00:00, 10754.63it/s, loss=0.254, v_num=661]Epoch 63:   0%|          | 0/534 [00:00<00:00, 2528.21it/s, loss=0.254, v_num=661] Epoch 63:   0%|          | 0/534 [00:19<2:54:17, 19.58s/it, loss=0.254, v_num=661]Epoch 63:   2%|▏         | 10/534 [00:27<22:04,  2.53s/it, loss=0.254, v_num=661] Epoch 63:   2%|▏         | 10/534 [00:27<22:04,  2.53s/it, loss=0.253, v_num=661]Epoch 63:   4%|▎         | 20/534 [00:46<19:04,  2.23s/it, loss=0.253, v_num=661]Epoch 63:   4%|▎         | 20/534 [00:46<19:05,  2.23s/it, loss=0.243, v_num=661]Epoch 63:   6%|▌         | 30/534 [00:59<16:00,  1.91s/it, loss=0.243, v_num=661]Epoch 63:   6%|▌         | 30/534 [00:59<16:00,  1.91s/it, loss=0.247, v_num=661]Epoch 63:   7%|▋         | 40/534 [01:10<14:07,  1.71s/it, loss=0.247, v_num=661]Epoch 63:   7%|▋         | 40/534 [01:10<14:07,  1.71s/it, loss=0.247, v_num=661]Epoch 63:   9%|▉         | 50/534 [01:20<12:44,  1.58s/it, loss=0.247, v_num=661]Epoch 63:   9%|▉         | 50/534 [01:20<12:44,  1.58s/it, loss=0.252, v_num=661]Epoch 63:  11%|█         | 60/534 [01:37<12:35,  1.59s/it, loss=0.252, v_num=661]Epoch 63:  11%|█         | 60/534 [01:37<12:35,  1.59s/it, loss=0.246, v_num=661]Epoch 63:  13%|█▎        | 70/534 [01:49<11:58,  1.55s/it, loss=0.246, v_num=661]Epoch 63:  13%|█▎        | 70/534 [01:49<11:58,  1.55s/it, loss=0.239, v_num=661]Epoch 63:  15%|█▍        | 80/534 [02:05<11:42,  1.55s/it, loss=0.239, v_num=661]Epoch 63:  15%|█▍        | 80/534 [02:05<11:42,  1.55s/it, loss=0.246, v_num=661]Epoch 63:  17%|█▋        | 90/534 [02:17<11:11,  1.51s/it, loss=0.246, v_num=661]Epoch 63:  17%|█▋        | 90/534 [02:17<11:11,  1.51s/it, loss=0.251, v_num=661]Epoch 63:  19%|█▊        | 100/534 [02:42<11:36,  1.61s/it, loss=0.251, v_num=661]Epoch 63:  19%|█▊        | 100/534 [02:42<11:36,  1.61s/it, loss=0.244, v_num=661]Epoch 63:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.244, v_num=661]Epoch 63:  21%|██        | 110/534 [02:55<11:10,  1.58s/it, loss=0.247, v_num=661]Epoch 63:  22%|██▏       | 120/534 [03:08<10:46,  1.56s/it, loss=0.247, v_num=661]Epoch 63:  22%|██▏       | 120/534 [03:08<10:46,  1.56s/it, loss=0.257, v_num=661]Epoch 63:  24%|██▍       | 130/534 [03:21<10:20,  1.53s/it, loss=0.257, v_num=661]Epoch 63:  24%|██▍       | 130/534 [03:21<10:20,  1.53s/it, loss=0.251, v_num=661]Epoch 63:  26%|██▌       | 140/534 [03:34<10:00,  1.52s/it, loss=0.251, v_num=661]Epoch 63:  26%|██▌       | 140/534 [03:34<10:00,  1.52s/it, loss=0.252, v_num=661]Epoch 63:  28%|██▊       | 150/534 [03:49<09:42,  1.52s/it, loss=0.252, v_num=661]Epoch 63:  28%|██▊       | 150/534 [03:49<09:42,  1.52s/it, loss=0.257, v_num=661]Epoch 63:  30%|██▉       | 160/534 [03:59<09:17,  1.49s/it, loss=0.257, v_num=661]Epoch 63:  30%|██▉       | 160/534 [03:59<09:17,  1.49s/it, loss=0.247, v_num=661]Epoch 63:  32%|███▏      | 170/534 [04:13<08:58,  1.48s/it, loss=0.247, v_num=661]Epoch 63:  32%|███▏      | 170/534 [04:13<08:58,  1.48s/it, loss=0.242, v_num=661]Epoch 63:  34%|███▎      | 180/534 [04:28<08:45,  1.48s/it, loss=0.242, v_num=661]Epoch 63:  34%|███▎      | 180/534 [04:28<08:45,  1.48s/it, loss=0.245, v_num=661]validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9803724884986877
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9837322235107422
validation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.982636570930481
validation_epoch_end
graph acc: 0.5239520958083832
valid accuracy: 0.9815882444381714
s=0.247, v_num=661]validation_epoch_end
graph acc: 0.5508982035928144
valid accuracy: 0.9836288094520569
validation_epoch_end
graph acc: 0.437125748502994
valid accuracy: 0.9811680912971497

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9819757342338562
validation_epoch_end
graph acc: 0.5419161676646707
valid accuracy: 0.9845054745674133
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9829856753349304
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9823958277702332

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.75it/s][AEpoch 63:  41%|████      | 220/534 [04:40<06:38,  1.27s/it, loss=0.247, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:44,  7.02it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:29, 10.16it/s][AEpoch 63:  45%|████▍     | 240/534 [04:42<05:44,  1.17s/it, loss=0.247, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:35,  8.17it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28,  9.83it/s][AEpoch 63:  49%|████▊     | 260/534 [04:44<04:58,  1.09s/it, loss=0.247, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.33it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.61it/s][AEpoch 63:  52%|█████▏    | 280/534 [04:47<04:19,  1.02s/it, loss=0.247, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:27,  9.21it/s][A
Validating:  27%|██▋       | 90/334 [00:10<00:26,  9.24it/s][AEpoch 63:  56%|█████▌    | 300/534 [04:49<03:44,  1.04it/s, loss=0.247, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.38it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.26it/s][AEpoch 63:  60%|█████▉    | 320/534 [04:50<03:13,  1.10it/s, loss=0.247, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.33it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:20,  9.97it/s][AEpoch 63:  64%|██████▎   | 340/534 [04:52<02:46,  1.17it/s, loss=0.247, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.55it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:15, 11.81it/s][AEpoch 63:  67%|██████▋   | 360/534 [04:54<02:21,  1.23it/s, loss=0.247, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:16, 10.63it/s][A
Validating:  51%|█████     | 170/334 [00:17<00:15, 10.51it/s][AEpoch 63:  71%|███████   | 380/534 [04:56<01:59,  1.29it/s, loss=0.247, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:15, 10.06it/s][A
Validating:  57%|█████▋    | 190/334 [00:19<00:12, 11.27it/s][AEpoch 63:  75%|███████▍  | 400/534 [04:57<01:39,  1.35it/s, loss=0.247, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.35it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.32it/s][AEpoch 63:  79%|███████▊  | 420/534 [04:59<01:21,  1.41it/s, loss=0.247, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 12.04it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.97it/s][AEpoch 63:  82%|████████▏ | 440/534 [05:00<01:04,  1.47it/s, loss=0.247, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 14.12it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.24it/s][AEpoch 63:  86%|████████▌ | 460/534 [05:02<00:48,  1.52it/s, loss=0.247, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 12.11it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:05, 12.11it/s][AEpoch 63:  90%|████████▉ | 480/534 [05:04<00:34,  1.58it/s, loss=0.247, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:05, 10.40it/s][A
Validating:  87%|████████▋ | 290/334 [00:27<00:03, 12.52it/s][AEpoch 63:  94%|█████████▎| 500/534 [05:05<00:20,  1.64it/s, loss=0.247, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 14.71it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.85it/s][AEpoch 63:  97%|█████████▋| 520/534 [05:07<00:08,  1.69it/s, loss=0.247, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.74it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.70it/s][A
Validating: 100%|██████████| 334/334 [00:29<00:00, 15.12it/s][Avalidation_epoch_end
graph acc: 0.4940119760479042
valid accuracy: 0.9816722869873047
Epoch 63: 100%|██████████| 534/534 [05:16<00:00,  1.69it/s, loss=0.247, v_num=661]
                                                             [AEpoch 63:   0%|          | 0/534 [00:00<00:00, 14364.05it/s, loss=0.247, v_num=661]Epoch 64:   0%|          | 0/534 [00:00<00:00, 3412.78it/s, loss=0.247, v_num=661] Epoch 64:   0%|          | 0/534 [00:12<1:49:16, 12.28s/it, loss=0.247, v_num=661]Epoch 64:   2%|▏         | 10/534 [00:23<18:35,  2.13s/it, loss=0.247, v_num=661] Epoch 64:   2%|▏         | 10/534 [00:23<18:35,  2.13s/it, loss=0.24, v_num=661] Epoch 64:   4%|▎         | 20/534 [00:42<17:23,  2.03s/it, loss=0.24, v_num=661]Epoch 64:   4%|▎         | 20/534 [00:42<17:23,  2.03s/it, loss=0.238, v_num=661]Epoch 64:   6%|▌         | 30/534 [00:57<15:33,  1.85s/it, loss=0.238, v_num=661]Epoch 64:   6%|▌         | 30/534 [00:57<15:33,  1.85s/it, loss=0.233, v_num=661]Epoch 64:   7%|▋         | 40/534 [01:10<14:14,  1.73s/it, loss=0.233, v_num=661]Epoch 64:   7%|▋         | 40/534 [01:10<14:14,  1.73s/it, loss=0.228, v_num=661]Epoch 64:   9%|▉         | 50/534 [01:29<14:04,  1.75s/it, loss=0.228, v_num=661]Epoch 64:   9%|▉         | 50/534 [01:29<14:04,  1.75s/it, loss=0.231, v_num=661]Epoch 64:  11%|█         | 60/534 [01:43<13:22,  1.69s/it, loss=0.231, v_num=661]Epoch 64:  11%|█         | 60/534 [01:43<13:22,  1.69s/it, loss=0.241, v_num=661]Epoch 64:  13%|█▎        | 70/534 [01:57<12:50,  1.66s/it, loss=0.241, v_num=661]Epoch 64:  13%|█▎        | 70/534 [01:57<12:50,  1.66s/it, loss=0.253, v_num=661]Epoch 64:  15%|█▍        | 80/534 [02:13<12:25,  1.64s/it, loss=0.253, v_num=661]Epoch 64:  15%|█▍        | 80/534 [02:13<12:25,  1.64s/it, loss=0.239, v_num=661]Epoch 64:  17%|█▋        | 90/534 [02:26<11:54,  1.61s/it, loss=0.239, v_num=661]Epoch 64:  17%|█▋        | 90/534 [02:26<11:54,  1.61s/it, loss=0.23, v_num=661] Epoch 64:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.23, v_num=661]Epoch 64:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.244, v_num=661]Epoch 64:  21%|██        | 110/534 [02:53<11:04,  1.57s/it, loss=0.244, v_num=661]Epoch 64:  21%|██        | 110/534 [02:53<11:04,  1.57s/it, loss=0.238, v_num=661]Epoch 64:  22%|██▏       | 120/534 [03:05<10:35,  1.53s/it, loss=0.238, v_num=661]Epoch 64:  22%|██▏       | 120/534 [03:05<10:35,  1.53s/it, loss=0.237, v_num=661]Epoch 64:  24%|██▍       | 130/534 [03:22<10:24,  1.55s/it, loss=0.237, v_num=661]Epoch 64:  24%|██▍       | 130/534 [03:22<10:24,  1.55s/it, loss=0.236, v_num=661]Epoch 64:  26%|██▌       | 140/534 [03:36<10:05,  1.54s/it, loss=0.236, v_num=661]Epoch 64:  26%|██▌       | 140/534 [03:36<10:05,  1.54s/it, loss=0.232, v_num=661]Epoch 64:  28%|██▊       | 150/534 [03:50<09:47,  1.53s/it, loss=0.232, v_num=661]Epoch 64:  28%|██▊       | 150/534 [03:50<09:47,  1.53s/it, loss=0.234, v_num=661]Epoch 64:  30%|██▉       | 160/534 [04:01<09:21,  1.50s/it, loss=0.234, v_num=661]Epoch 64:  30%|██▉       | 160/534 [04:01<09:21,  1.50s/it, loss=0.234, v_num=661]Epoch 64:  32%|███▏      | 170/534 [04:16<09:05,  1.50s/it, loss=0.234, v_num=661]Epoch 64:  32%|███▏      | 170/534 [04:16<09:05,  1.50s/it, loss=0.247, v_num=661]Epoch 64:  34%|███▎      | 180/534 [04:27<08:42,  1.48s/it, loss=0.247, v_num=661]Epoch 64:  34%|███▎      | 180/534 [04:27<08:42,  1.48s/it, loss=0.252, v_num=661]Epoch 64:  36%|███▌      | 190/534 [04:34<08:13,  1.44s/it, loss=0.252, v_num=661]Epoch 64:  36%|███▌      | 190/534 [04:34<08:13,  1.44s/it, loss=0.256, v_num=661]validation_epoch_end
graph acc: 0.5119760479041916
valid accuracy: 0.9814281463623047
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9802179336547852
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.981956422328949
validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9830350279808044
201796531677
validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.983472466468811
validation_epoch_end
graph acc: 0.44610778443113774
valid accuracy: 0.9827476739883423
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9825091361999512
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9818983674049377
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9809012413024902

Validating:   3%|▎         | 10/334 [00:01<00:57,  5.68it/s][AEpoch 64:  41%|████      | 220/534 [04:41<06:40,  1.28s/it, loss=0.251, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:43,  7.24it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:34,  8.86it/s][AEpoch 64:  45%|████▍     | 240/534 [04:43<05:46,  1.18s/it, loss=0.251, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:34,  8.42it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.07it/s][AEpoch 64:  49%|████▊     | 260/534 [04:45<05:00,  1.09s/it, loss=0.251, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.06it/s][A
Validating:  21%|██        | 70/334 [00:08<00:32,  8.09it/s][AEpoch 64:  52%|█████▏    | 280/534 [04:48<04:20,  1.03s/it, loss=0.251, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.71it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.88it/s][AEpoch 64:  56%|█████▌    | 300/534 [04:49<03:45,  1.04it/s, loss=0.251, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:19, 12.09it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 11.02it/s][AEpoch 64:  60%|█████▉    | 320/534 [04:51<03:14,  1.10it/s, loss=0.251, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.28it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.25it/s][AEpoch 64:  64%|██████▎   | 340/534 [04:53<02:47,  1.16it/s, loss=0.251, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.29it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:15, 12.17it/s][AEpoch 64:  67%|██████▋   | 360/534 [04:54<02:22,  1.22it/s, loss=0.251, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.45it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 12.12it/s][AEpoch 64:  71%|███████   | 380/534 [04:56<01:59,  1.28it/s, loss=0.251, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.11it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.72it/s][AEpoch 64:  75%|███████▍  | 400/534 [04:58<01:39,  1.34it/s, loss=0.251, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.94it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.33it/s][AEpoch 64:  79%|███████▊  | 420/534 [04:59<01:21,  1.40it/s, loss=0.251, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.53it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.79it/s][AEpoch 64:  82%|████████▏ | 440/534 [05:01<01:04,  1.46it/s, loss=0.251, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 13.63it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:06, 12.12it/s][AEpoch 64:  86%|████████▌ | 460/534 [05:03<00:48,  1.52it/s, loss=0.251, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.52it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.65it/s][AEpoch 64:  90%|████████▉ | 480/534 [05:04<00:34,  1.58it/s, loss=0.251, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.20it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 13.32it/s][AEpoch 64:  94%|█████████▎| 500/534 [05:06<00:20,  1.64it/s, loss=0.251, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.02it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.69it/s][AEpoch 64:  97%|█████████▋| 520/534 [05:07<00:08,  1.69it/s, loss=0.251, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.66it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 14.02it/s][Avalidation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9808736443519592
Epoch 64: 100%|██████████| 534/534 [05:15<00:00,  1.69it/s, loss=0.251, v_num=661]
                                                             [AEpoch 64:   0%|          | 0/534 [00:00<00:00, 11915.64it/s, loss=0.251, v_num=661]Epoch 65:   0%|          | 0/534 [00:00<00:00, 3024.01it/s, loss=0.251, v_num=661] Epoch 65:   0%|          | 0/534 [00:15<2:18:09, 15.52s/it, loss=0.251, v_num=661]Epoch 65:   2%|▏         | 10/534 [00:28<22:42,  2.60s/it, loss=0.251, v_num=661] Epoch 65:   2%|▏         | 10/534 [00:28<22:42,  2.60s/it, loss=0.231, v_num=661]Epoch 65:   4%|▎         | 20/534 [00:50<20:43,  2.42s/it, loss=0.231, v_num=661]Epoch 65:   4%|▎         | 20/534 [00:50<20:43,  2.42s/it, loss=0.233, v_num=661]Epoch 65:   6%|▌         | 30/534 [01:00<16:25,  1.96s/it, loss=0.233, v_num=661]Epoch 65:   6%|▌         | 30/534 [01:00<16:25,  1.96s/it, loss=0.23, v_num=661] Epoch 65:   7%|▋         | 40/534 [01:14<15:03,  1.83s/it, loss=0.23, v_num=661]Epoch 65:   7%|▋         | 40/534 [01:14<15:03,  1.83s/it, loss=0.217, v_num=661]Epoch 65:   9%|▉         | 50/534 [01:26<13:39,  1.69s/it, loss=0.217, v_num=661]Epoch 65:   9%|▉         | 50/534 [01:26<13:39,  1.69s/it, loss=0.225, v_num=661]Epoch 65:  11%|█         | 60/534 [01:39<12:55,  1.64s/it, loss=0.225, v_num=661]Epoch 65:  11%|█         | 60/534 [01:39<12:55,  1.64s/it, loss=0.241, v_num=661]Epoch 65:  13%|█▎        | 70/534 [01:53<12:23,  1.60s/it, loss=0.241, v_num=661]Epoch 65:  13%|█▎        | 70/534 [01:53<12:23,  1.60s/it, loss=0.241, v_num=661]Epoch 65:  15%|█▍        | 80/534 [02:03<11:32,  1.53s/it, loss=0.241, v_num=661]Epoch 65:  15%|█▍        | 80/534 [02:03<11:32,  1.53s/it, loss=0.237, v_num=661]Epoch 65:  15%|█▍        | 80/534 [02:15<12:39,  1.67s/it, loss=0.237, v_num=661]Epoch 65:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.237, v_num=661]Epoch 65:  17%|█▋        | 90/534 [02:16<11:06,  1.50s/it, loss=0.239, v_num=661]Epoch 65:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.239, v_num=661]Epoch 65:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.246, v_num=661]Epoch 65:  21%|██        | 110/534 [02:48<10:44,  1.52s/it, loss=0.246, v_num=661]Epoch 65:  21%|██        | 110/534 [02:48<10:44,  1.52s/it, loss=0.242, v_num=661]Epoch 65:  22%|██▏       | 120/534 [03:01<10:21,  1.50s/it, loss=0.242, v_num=661]Epoch 65:  22%|██▏       | 120/534 [03:01<10:21,  1.50s/it, loss=0.234, v_num=661]Epoch 65:  24%|██▍       | 130/534 [03:21<10:22,  1.54s/it, loss=0.234, v_num=661]Epoch 65:  24%|██▍       | 130/534 [03:21<10:22,  1.54s/it, loss=0.234, v_num=661]Epoch 65:  26%|██▌       | 140/534 [03:36<10:04,  1.54s/it, loss=0.234, v_num=661]Epoch 65:  26%|██▌       | 140/534 [03:36<10:04,  1.54s/it, loss=0.237, v_num=661]Epoch 65:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.237, v_num=661]Epoch 65:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.238, v_num=661]Epoch 65:  30%|██▉       | 160/534 [04:08<09:36,  1.54s/it, loss=0.238, v_num=661]Epoch 65:  30%|██▉       | 160/534 [04:08<09:36,  1.54s/it, loss=0.237, v_num=661]Epoch 65:  32%|███▏      | 170/534 [04:16<09:06,  1.50s/it, loss=0.237, v_num=661]Epoch 65:  32%|███▏      | 170/534 [04:16<09:06,  1.50s/it, loss=0.239, v_num=661]Epoch 65:  34%|███▎      | 180/534 [04:24<08:36,  1.46s/it, loss=0.239, v_num=661]Epoch 65:  34%|███▎      | 180/534 [04:24<08:36,  1.46s/it, loss=0.24, v_num=661] validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9817483425140381
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9817563891410828
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9807588458061218
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9830737709999084
 1.38s/it, loss=0.236, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9809012413024902
validation_epoch_end
graph acc: 0.5359281437125748
valid accuracy: 0.9842162132263184
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9822465181350708
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9824336171150208
validation_epoch_end
graph acc: 0.5329341317365269
valid accuracy: 0.9826977849006653
validation_epoch_end
graph acc: 0.46107784431137727
valid accuracy: 0.982866644859314

Validating:   3%|▎         | 10/334 [00:01<01:03,  5.12it/s][AEpoch 65:  41%|████      | 220/534 [04:39<06:37,  1.27s/it, loss=0.236, v_num=661]
Validating:   6%|▌         | 20/334 [00:03<00:44,  7.01it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:35,  8.56it/s][AEpoch 65:  45%|████▍     | 240/534 [04:41<05:43,  1.17s/it, loss=0.236, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:31,  9.21it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:26, 10.52it/s][AEpoch 65:  49%|████▊     | 260/534 [04:43<04:57,  1.09s/it, loss=0.236, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:24, 11.22it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.35it/s][AEpoch 65:  52%|█████▏    | 280/534 [04:46<04:18,  1.02s/it, loss=0.236, v_num=661]
Validating:  24%|██▍       | 80/334 [00:09<00:28,  8.89it/s][A
Validating:  27%|██▋       | 90/334 [00:10<00:25,  9.76it/s][AEpoch 65:  56%|█████▌    | 300/534 [04:47<03:43,  1.05it/s, loss=0.236, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:23, 10.13it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.44it/s][AEpoch 65:  60%|█████▉    | 320/534 [04:49<03:13,  1.11it/s, loss=0.236, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.81it/s][A
Validating:  39%|███▉      | 130/334 [00:14<00:22,  9.17it/s][AEpoch 65:  64%|██████▎   | 340/534 [04:51<02:46,  1.17it/s, loss=0.236, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.29it/s][A
Validating:  45%|████▍     | 150/334 [00:15<00:15, 11.89it/s][AEpoch 65:  67%|██████▋   | 360/534 [04:53<02:21,  1.23it/s, loss=0.236, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:14, 12.17it/s][A
Validating:  51%|█████     | 170/334 [00:17<00:14, 11.49it/s][AEpoch 65:  71%|███████   | 380/534 [04:54<01:59,  1.29it/s, loss=0.236, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:18<00:14, 10.37it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.38it/s][AEpoch 65:  75%|███████▍  | 400/534 [04:56<01:39,  1.35it/s, loss=0.236, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.29it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 12.13it/s][AEpoch 65:  79%|███████▊  | 420/534 [04:58<01:20,  1.41it/s, loss=0.236, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:21<00:09, 12.33it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.67it/s][AEpoch 65:  82%|████████▏ | 440/534 [04:59<01:03,  1.47it/s, loss=0.236, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 13.87it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.31it/s][AEpoch 65:  86%|████████▌ | 460/534 [05:01<00:48,  1.53it/s, loss=0.236, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:06, 12.19it/s][A
Validating:  81%|████████  | 270/334 [00:25<00:04, 12.88it/s][AEpoch 65:  90%|████████▉ | 480/534 [05:02<00:34,  1.59it/s, loss=0.236, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:26<00:04, 11.29it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.99it/s][AEpoch 65:  94%|█████████▎| 500/534 [05:04<00:20,  1.64it/s, loss=0.236, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:27<00:02, 15.36it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.92it/s][AEpoch 65:  97%|█████████▋| 520/534 [05:06<00:08,  1.70it/s, loss=0.236, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.36it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.56it/s][Avalidation_epoch_end
graph acc: 0.47005988023952094
valid accuracy: 0.9814326763153076
Epoch 65: 100%|██████████| 534/534 [05:14<00:00,  1.70it/s, loss=0.236, v_num=661]
                                                             [AEpoch 65:   0%|          | 0/534 [00:00<00:00, 5468.45it/s, loss=0.236, v_num=661]Epoch 66:   0%|          | 0/534 [00:00<00:00, 1600.27it/s, loss=0.236, v_num=661]Epoch 66:   0%|          | 0/534 [00:10<1:31:53, 10.32s/it, loss=0.236, v_num=661]Epoch 66:   2%|▏         | 10/534 [00:28<22:26,  2.57s/it, loss=0.236, v_num=661] Epoch 66:   2%|▏         | 10/534 [00:28<22:26,  2.57s/it, loss=0.231, v_num=661]Epoch 66:   4%|▎         | 20/534 [00:40<16:40,  1.95s/it, loss=0.231, v_num=661]Epoch 66:   4%|▎         | 20/534 [00:40<16:40,  1.95s/it, loss=0.221, v_num=661]Epoch 66:   6%|▌         | 30/534 [00:55<14:58,  1.78s/it, loss=0.221, v_num=661]Epoch 66:   6%|▌         | 30/534 [00:55<14:58,  1.78s/it, loss=0.219, v_num=661]Epoch 66:   7%|▋         | 40/534 [01:08<13:45,  1.67s/it, loss=0.219, v_num=661]Epoch 66:   7%|▋         | 40/534 [01:08<13:45,  1.67s/it, loss=0.222, v_num=661]Epoch 66:   9%|▉         | 50/534 [01:27<13:50,  1.72s/it, loss=0.222, v_num=661]Epoch 66:   9%|▉         | 50/534 [01:27<13:50,  1.72s/it, loss=0.227, v_num=661]Epoch 66:  11%|█         | 60/534 [01:42<13:14,  1.68s/it, loss=0.227, v_num=661]Epoch 66:  11%|█         | 60/534 [01:42<13:14,  1.68s/it, loss=0.229, v_num=661]Epoch 66:  13%|█▎        | 70/534 [01:54<12:30,  1.62s/it, loss=0.229, v_num=661]Epoch 66:  13%|█▎        | 70/534 [01:54<12:30,  1.62s/it, loss=0.227, v_num=661]Epoch 66:  15%|█▍        | 80/534 [02:09<12:03,  1.59s/it, loss=0.227, v_num=661]Epoch 66:  15%|█▍        | 80/534 [02:09<12:03,  1.59s/it, loss=0.229, v_num=661]Epoch 66:  17%|█▋        | 90/534 [02:37<12:50,  1.74s/it, loss=0.229, v_num=661]Epoch 66:  17%|█▋        | 90/534 [02:37<12:50,  1.74s/it, loss=0.23, v_num=661] Epoch 66:  19%|█▊        | 100/534 [02:52<12:20,  1.71s/it, loss=0.23, v_num=661]Epoch 66:  19%|█▊        | 100/534 [02:52<12:20,  1.71s/it, loss=0.239, v_num=661]Epoch 66:  21%|██        | 110/534 [03:04<11:46,  1.67s/it, loss=0.239, v_num=661]Epoch 66:  21%|██        | 110/534 [03:04<11:46,  1.67s/it, loss=0.236, v_num=661]Epoch 66:  22%|██▏       | 120/534 [03:18<11:18,  1.64s/it, loss=0.236, v_num=661]Epoch 66:  22%|██▏       | 120/534 [03:18<11:18,  1.64s/it, loss=0.226, v_num=661]Epoch 66:  24%|██▍       | 130/534 [03:31<10:52,  1.61s/it, loss=0.226, v_num=661]Epoch 66:  24%|██▍       | 130/534 [03:31<10:52,  1.61s/it, loss=0.23, v_num=661] Epoch 66:  26%|██▌       | 140/534 [03:46<10:31,  1.60s/it, loss=0.23, v_num=661]Epoch 66:  26%|██▌       | 140/534 [03:46<10:31,  1.60s/it, loss=0.231, v_num=661]Epoch 66:  28%|██▊       | 150/534 [04:01<10:13,  1.60s/it, loss=0.231, v_num=661]Epoch 66:  28%|██▊       | 150/534 [04:01<10:13,  1.60s/it, loss=0.227, v_num=661]Epoch 66:  30%|██▉       | 160/534 [04:15<09:52,  1.59s/it, loss=0.227, v_num=661]Epoch 66:  30%|██▉       | 160/534 [04:15<09:52,  1.59s/it, loss=0.227, v_num=661]Epoch 66:  32%|███▏      | 170/534 [04:31<09:37,  1.59s/it, loss=0.227, v_num=661]Epoch 66:  32%|███▏      | 170/534 [04:31<09:37,  1.59s/it, loss=0.226, v_num=661]Epoch 66:  34%|███▎      | 180/534 [04:36<08:59,  1.53s/it, loss=0.226, v_num=661]Epoch 66:  34%|███▎      | 180/534 [04:36<08:59,  1.53s/it, loss=0.222, v_num=661]validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9820764660835266
validation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9805656671524048
validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9828801155090332
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9805876016616821
 1.40s/it, loss=0.243, v_num=661]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.5299401197604791
valid accuracy: 0.9832409024238586
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9832236170768738
validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9840509295463562
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9825469255447388
validation_epoch_end
graph acc: 0.4431137724550898
valid accuracy: 0.9818923473358154
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9830200672149658

Validating:   3%|▎         | 10/334 [00:01<00:58,  5.57it/s][AEpoch 66:  41%|████      | 220/534 [04:42<06:41,  1.28s/it, loss=0.243, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:43,  7.23it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:31,  9.79it/s][AEpoch 66:  45%|████▍     | 240/534 [04:44<05:47,  1.18s/it, loss=0.243, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:34,  8.56it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28,  9.99it/s][AEpoch 66:  49%|████▊     | 260/534 [04:46<05:00,  1.10s/it, loss=0.243, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.70it/s][A
Validating:  21%|██        | 70/334 [00:08<00:35,  7.54it/s][AEpoch 66:  52%|█████▏    | 280/534 [04:49<04:21,  1.03s/it, loss=0.243, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:27,  9.33it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:25,  9.63it/s][AEpoch 66:  56%|█████▌    | 300/534 [04:51<03:46,  1.03it/s, loss=0.243, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.73it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:20, 10.79it/s][AEpoch 66:  60%|█████▉    | 320/534 [04:52<03:15,  1.10it/s, loss=0.243, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:18, 11.80it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:19, 10.42it/s][AEpoch 66:  64%|██████▎   | 340/534 [04:54<02:47,  1.16it/s, loss=0.243, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.02it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.54it/s][AEpoch 66:  67%|██████▋   | 360/534 [04:55<02:22,  1.22it/s, loss=0.243, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:15, 11.44it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.60it/s][AEpoch 66:  71%|███████   | 380/534 [04:57<02:00,  1.28it/s, loss=0.243, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.83it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.48it/s][AEpoch 66:  75%|███████▍  | 400/534 [04:59<01:40,  1.34it/s, loss=0.243, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 13.05it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.86it/s][AEpoch 66:  79%|███████▊  | 420/534 [05:01<01:21,  1.40it/s, loss=0.243, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.19it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:08, 12.92it/s][AEpoch 66:  82%|████████▏ | 440/534 [05:02<01:04,  1.46it/s, loss=0.243, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.37it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.50it/s][AEpoch 66:  86%|████████▌ | 460/534 [05:04<00:48,  1.51it/s, loss=0.243, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.41it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.14it/s][AEpoch 66:  90%|████████▉ | 480/534 [05:05<00:34,  1.57it/s, loss=0.243, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.25it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 13.15it/s][AEpoch 66:  94%|█████████▎| 500/534 [05:07<00:20,  1.63it/s, loss=0.243, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.78it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 12.21it/s][AEpoch 66:  97%|█████████▋| 520/534 [05:08<00:08,  1.69it/s, loss=0.243, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:00, 14.02it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.27it/s][Avalidation_epoch_end
graph acc: 0.4880239520958084
valid accuracy: 0.9815125465393066
Epoch 66: 100%|██████████| 534/534 [05:16<00:00,  1.69it/s, loss=0.243, v_num=661]
                                                             [AEpoch 66:   0%|          | 0/534 [00:00<00:00, 8097.11it/s, loss=0.243, v_num=661]Epoch 67:   0%|          | 0/534 [00:00<00:00, 2222.74it/s, loss=0.243, v_num=661]Epoch 67:   0%|          | 0/534 [00:12<1:52:22, 12.63s/it, loss=0.243, v_num=661]Epoch 67:   2%|▏         | 10/534 [00:23<18:16,  2.09s/it, loss=0.243, v_num=661] Epoch 67:   2%|▏         | 10/534 [00:23<18:16,  2.09s/it, loss=0.223, v_num=661]Epoch 67:   4%|▎         | 20/534 [00:38<15:30,  1.81s/it, loss=0.223, v_num=661]Epoch 67:   4%|▎         | 20/534 [00:38<15:30,  1.81s/it, loss=0.216, v_num=661]Epoch 67:   6%|▌         | 30/534 [00:53<14:33,  1.73s/it, loss=0.216, v_num=661]Epoch 67:   6%|▌         | 30/534 [00:53<14:33,  1.73s/it, loss=0.224, v_num=661]Epoch 67:   7%|▋         | 40/534 [01:19<16:01,  1.95s/it, loss=0.224, v_num=661]Epoch 67:   7%|▋         | 40/534 [01:19<16:01,  1.95s/it, loss=0.22, v_num=661] Epoch 67:   9%|▉         | 50/534 [01:31<14:25,  1.79s/it, loss=0.22, v_num=661]Epoch 67:   9%|▉         | 50/534 [01:31<14:25,  1.79s/it, loss=0.222, v_num=661]Epoch 67:  11%|█         | 60/534 [01:43<13:27,  1.70s/it, loss=0.222, v_num=661]Epoch 67:  11%|█         | 60/534 [01:43<13:27,  1.70s/it, loss=0.234, v_num=661]Epoch 67:  13%|█▎        | 70/534 [01:54<12:27,  1.61s/it, loss=0.234, v_num=661]Epoch 67:  13%|█▎        | 70/534 [01:54<12:27,  1.61s/it, loss=0.228, v_num=661]Epoch 67:  15%|█▍        | 80/534 [02:05<11:45,  1.55s/it, loss=0.228, v_num=661]Epoch 67:  15%|█▍        | 80/534 [02:05<11:45,  1.55s/it, loss=0.219, v_num=661]Epoch 67:  17%|█▋        | 90/534 [02:28<12:04,  1.63s/it, loss=0.219, v_num=661]Epoch 67:  17%|█▋        | 90/534 [02:28<12:04,  1.63s/it, loss=0.232, v_num=661]Epoch 67:  19%|█▊        | 100/534 [02:41<11:34,  1.60s/it, loss=0.232, v_num=661]Epoch 67:  19%|█▊        | 100/534 [02:41<11:34,  1.60s/it, loss=0.237, v_num=661]Epoch 67:  21%|██        | 110/534 [02:58<11:20,  1.61s/it, loss=0.237, v_num=661]Epoch 67:  21%|██        | 110/534 [02:58<11:20,  1.61s/it, loss=0.228, v_num=661]Epoch 67:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.228, v_num=661]Epoch 67:  22%|██▏       | 120/534 [03:12<10:57,  1.59s/it, loss=0.22, v_num=661] Epoch 67:  24%|██▍       | 130/534 [03:23<10:27,  1.55s/it, loss=0.22, v_num=661]Epoch 67:  24%|██▍       | 130/534 [03:23<10:27,  1.55s/it, loss=0.223, v_num=661]Epoch 67:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.223, v_num=661]Epoch 67:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.229, v_num=661]Epoch 67:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.229, v_num=661]Epoch 67:  28%|██▊       | 150/534 [03:52<09:50,  1.54s/it, loss=0.231, v_num=661]Epoch 67:  30%|██▉       | 160/534 [04:06<09:33,  1.53s/it, loss=0.231, v_num=661]Epoch 67:  30%|██▉       | 160/534 [04:06<09:33,  1.53s/it, loss=0.217, v_num=661]Epoch 67:  32%|███▏      | 170/534 [04:19<09:12,  1.52s/it, loss=0.217, v_num=661]Epoch 67:  32%|███▏      | 170/534 [04:19<09:12,  1.52s/it, loss=0.215, v_num=661]Epoch 67:  34%|███▎      | 180/534 [04:31<08:50,  1.50s/it, loss=0.215, v_num=661]Epoch 67:  34%|███▎      | 180/534 [04:31<08:50,  1.50s/it, loss=0.241, v_num=661]Epoch 67:  36%|███▌      | 190/534 [04:35<08:15,  1.44s/it, loss=0.241, v_num=661]Epoch 67:  36%|███▌      | 190/534 [04:35<08:15,  1.44s/it, loss=0.245, v_num=661]validation_epoch_end
graph acc: 0.49101796407185627
valid accuracy: 0.9799475073814392
validation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9820364117622375
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9831125140190125
validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9810678958892822
873706817627
validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9828141331672668
validation_epoch_end
graph acc: 0.5299401197604791
valid accuracy: 0.9842575192451477
validation_epoch_end
graph acc: 0.49700598802395207
valid accuracy: 0.9825172424316406
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9813586473464966
validation_epoch_end
graph acc: 0.5029940119760479
valid accuracy: 0.9829624891281128

Validating:   3%|▎         | 10/334 [00:01<00:55,  5.81it/s][AEpoch 67:  41%|████      | 220/534 [04:40<06:37,  1.27s/it, loss=0.237, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:43,  7.28it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:31,  9.53it/s][AEpoch 67:  45%|████▍     | 240/534 [04:41<05:43,  1.17s/it, loss=0.237, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.00it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:26, 10.80it/s][AEpoch 67:  49%|████▊     | 260/534 [04:43<04:57,  1.09s/it, loss=0.237, v_num=661]
Validating:  18%|█▊        | 60/334 [00:05<00:21, 12.51it/s][A
Validating:  21%|██        | 70/334 [00:08<00:32,  8.08it/s][AEpoch 67:  52%|█████▏    | 280/534 [04:46<04:18,  1.02s/it, loss=0.237, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.70it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:22, 10.97it/s][AEpoch 67:  56%|█████▌    | 300/534 [04:47<03:43,  1.05it/s, loss=0.237, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.53it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:19, 11.27it/s][AEpoch 67:  60%|█████▉    | 320/534 [04:49<03:12,  1.11it/s, loss=0.237, v_num=661]
Validating:  36%|███▌      | 120/334 [00:11<00:17, 12.43it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.22it/s][AEpoch 67:  64%|██████▎   | 340/534 [04:51<02:45,  1.17it/s, loss=0.237, v_num=661]
Validating:  42%|████▏     | 140/334 [00:13<00:17, 10.85it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.94it/s][AEpoch 67:  67%|██████▋   | 360/534 [04:52<02:21,  1.23it/s, loss=0.237, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 11.73it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:14, 11.44it/s][AEpoch 67:  71%|███████   | 380/534 [04:54<01:59,  1.29it/s, loss=0.237, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.48it/s][A
Validating:  57%|█████▋    | 190/334 [00:17<00:11, 12.79it/s][AEpoch 67:  75%|███████▍  | 400/534 [04:56<01:38,  1.35it/s, loss=0.237, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:09, 13.91it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.19it/s][AEpoch 67:  79%|███████▊  | 420/534 [04:57<01:20,  1.41it/s, loss=0.237, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:08, 12.89it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 13.60it/s][AEpoch 67:  82%|████████▏ | 440/534 [04:59<01:03,  1.47it/s, loss=0.237, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.02it/s][A
Validating:  75%|███████▍  | 250/334 [00:22<00:07, 10.98it/s][AEpoch 67:  86%|████████▌ | 460/534 [05:01<00:48,  1.53it/s, loss=0.237, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:06, 11.76it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:05, 12.54it/s][AEpoch 67:  90%|████████▉ | 480/534 [05:02<00:33,  1.59it/s, loss=0.237, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.31it/s][A
Validating:  87%|████████▋ | 290/334 [00:25<00:03, 13.14it/s][AEpoch 67:  94%|█████████▎| 500/534 [05:04<00:20,  1.65it/s, loss=0.237, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 15.25it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:01, 13.23it/s][AEpoch 67:  97%|█████████▋| 520/534 [05:05<00:08,  1.71it/s, loss=0.237, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:27<00:00, 14.68it/s][A
Validating:  99%|█████████▉| 330/334 [00:28<00:00, 13.82it/s][Avalidation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9817521572113037
Epoch 67: 100%|██████████| 534/534 [05:13<00:00,  1.71it/s, loss=0.237, v_num=661]
                                                             [AEpoch 67:   0%|          | 0/534 [00:00<00:00, 12052.60it/s, loss=0.237, v_num=661]Epoch 68:   0%|          | 0/534 [00:00<00:00, 2737.80it/s, loss=0.237, v_num=661] Epoch 68:   0%|          | 0/534 [00:18<2:45:30, 18.60s/it, loss=0.237, v_num=661]Epoch 68:   2%|▏         | 10/534 [00:26<20:46,  2.38s/it, loss=0.237, v_num=661] Epoch 68:   2%|▏         | 10/534 [00:26<20:46,  2.38s/it, loss=0.224, v_num=661]Epoch 68:   4%|▎         | 20/534 [00:39<16:17,  1.90s/it, loss=0.224, v_num=661]Epoch 68:   4%|▎         | 20/534 [00:39<16:17,  1.90s/it, loss=0.211, v_num=661]Epoch 68:   6%|▌         | 30/534 [01:12<19:34,  2.33s/it, loss=0.211, v_num=661]Epoch 68:   6%|▌         | 30/534 [01:12<19:34,  2.33s/it, loss=0.211, v_num=661]Epoch 68:   7%|▋         | 40/534 [01:27<17:35,  2.14s/it, loss=0.211, v_num=661]Epoch 68:   7%|▋         | 40/534 [01:27<17:35,  2.14s/it, loss=0.214, v_num=661]Epoch 68:   9%|▉         | 50/534 [01:42<16:16,  2.02s/it, loss=0.214, v_num=661]Epoch 68:   9%|▉         | 50/534 [01:42<16:16,  2.02s/it, loss=0.225, v_num=661]Epoch 68:  11%|█         | 60/534 [01:54<14:49,  1.88s/it, loss=0.225, v_num=661]Epoch 68:  11%|█         | 60/534 [01:54<14:49,  1.88s/it, loss=0.23, v_num=661] Epoch 68:  13%|█▎        | 70/534 [02:08<13:59,  1.81s/it, loss=0.23, v_num=661]Epoch 68:  13%|█▎        | 70/534 [02:08<13:59,  1.81s/it, loss=0.225, v_num=661]Epoch 68:  15%|█▍        | 80/534 [02:22<13:17,  1.76s/it, loss=0.225, v_num=661]Epoch 68:  15%|█▍        | 80/534 [02:22<13:17,  1.76s/it, loss=0.218, v_num=661]Epoch 68:  17%|█▋        | 90/534 [02:37<12:48,  1.73s/it, loss=0.218, v_num=661]Epoch 68:  17%|█▋        | 90/534 [02:37<12:48,  1.73s/it, loss=0.215, v_num=661]Epoch 68:  19%|█▊        | 100/534 [02:50<12:11,  1.69s/it, loss=0.215, v_num=661]Epoch 68:  19%|█▊        | 100/534 [02:50<12:11,  1.69s/it, loss=0.226, v_num=661]Epoch 68:  21%|██        | 110/534 [03:07<11:56,  1.69s/it, loss=0.226, v_num=661]Epoch 68:  21%|██        | 110/534 [03:07<11:56,  1.69s/it, loss=0.228, v_num=661]Epoch 68:  22%|██▏       | 120/534 [03:22<11:31,  1.67s/it, loss=0.228, v_num=661]Epoch 68:  22%|██▏       | 120/534 [03:22<11:31,  1.67s/it, loss=0.226, v_num=661]Epoch 68:  24%|██▍       | 130/534 [03:34<11:01,  1.64s/it, loss=0.226, v_num=661]Epoch 68:  24%|██▍       | 130/534 [03:34<11:01,  1.64s/it, loss=0.221, v_num=661]Epoch 68:  26%|██▌       | 140/534 [03:49<10:40,  1.62s/it, loss=0.221, v_num=661]Epoch 68:  26%|██▌       | 140/534 [03:49<10:40,  1.62s/it, loss=0.224, v_num=661]Epoch 68:  28%|██▊       | 150/534 [04:01<10:14,  1.60s/it, loss=0.224, v_num=661]Epoch 68:  28%|██▊       | 150/534 [04:01<10:14,  1.60s/it, loss=0.222, v_num=661]Epoch 68:  30%|██▉       | 160/534 [04:15<09:53,  1.59s/it, loss=0.222, v_num=661]Epoch 68:  30%|██▉       | 160/534 [04:15<09:53,  1.59s/it, loss=0.215, v_num=661]Epoch 68:  32%|███▏      | 170/534 [04:26<09:27,  1.56s/it, loss=0.215, v_num=661]Epoch 68:  32%|███▏      | 170/534 [04:26<09:27,  1.56s/it, loss=0.221, v_num=661]Epoch 68:  34%|███▎      | 180/534 [04:32<08:53,  1.51s/it, loss=0.221, v_num=661]Epoch 68:  34%|███▎      | 180/534 [04:32<08:53,  1.51s/it, loss=0.219, v_num=661]validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9831900000572205
validation_epoch_end
graph acc: 0.5209580838323353
valid accuracy: 0.9814281463623047
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9819964170455933
validation_epoch_end
graph acc: 0.47604790419161674
valid accuracy: 0.9796770215034485
36011886597
validation_epoch_end
graph acc: 0.4491017964071856
valid accuracy: 0.9827873706817627
Epoch 68:  37%|███▋      | 200/534 [04:43<07:50,  1.41s/it, loss=0.22, v_num=661] validation_epoch_end
graph acc: 0.48502994011976047
valid accuracy: 0.9827736020088196
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.981625497341156

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9828653335571289
validation_epoch_end
graph acc: 0.5209580838323353
valid accuracy: 0.983396053314209

Validating:   3%|▎         | 10/334 [00:01<00:58,  5.57it/s][AEpoch 68:  41%|████      | 220/534 [04:44<06:44,  1.29s/it, loss=0.22, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:44,  7.06it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:31,  9.76it/s][AEpoch 68:  45%|████▍     | 240/534 [04:46<05:49,  1.19s/it, loss=0.22, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:32,  9.02it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.13it/s][AEpoch 68:  49%|████▊     | 260/534 [04:48<05:02,  1.11s/it, loss=0.22, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.62it/s][A
Validating:  21%|██        | 70/334 [00:08<00:34,  7.71it/s][AEpoch 68:  52%|█████▏    | 280/534 [04:51<04:23,  1.04s/it, loss=0.22, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:26,  9.70it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:24, 10.15it/s][AEpoch 68:  56%|█████▌    | 300/534 [04:52<03:47,  1.03it/s, loss=0.22, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:21, 10.97it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:21, 10.66it/s][AEpoch 68:  60%|█████▉    | 320/534 [04:54<03:16,  1.09it/s, loss=0.22, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 11.95it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:20, 10.01it/s][AEpoch 68:  64%|██████▎   | 340/534 [04:56<02:48,  1.15it/s, loss=0.22, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:17, 11.18it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:14, 12.59it/s][AEpoch 68:  67%|██████▋   | 360/534 [04:57<02:23,  1.21it/s, loss=0.22, v_num=661]
Validating:  48%|████▊     | 160/334 [00:15<00:14, 12.38it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.72it/s][AEpoch 68:  71%|███████   | 380/534 [04:59<02:01,  1.27it/s, loss=0.22, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:13, 11.04it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.38it/s][AEpoch 68:  75%|███████▍  | 400/534 [05:01<01:40,  1.33it/s, loss=0.22, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:18<00:10, 12.39it/s][A
Validating:  63%|██████▎   | 210/334 [00:19<00:10, 12.22it/s][AEpoch 68:  79%|███████▊  | 420/534 [05:02<01:21,  1.39it/s, loss=0.22, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.25it/s][A
Validating:  69%|██████▉   | 230/334 [00:20<00:07, 14.15it/s][AEpoch 68:  82%|████████▏ | 440/534 [05:04<01:04,  1.45it/s, loss=0.22, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:21<00:06, 14.05it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.24it/s][AEpoch 68:  86%|████████▌ | 460/534 [05:06<00:49,  1.51it/s, loss=0.22, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:23<00:05, 12.41it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.02it/s][AEpoch 68:  90%|████████▉ | 480/534 [05:07<00:34,  1.56it/s, loss=0.22, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:05, 10.47it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.75it/s][AEpoch 68:  94%|█████████▎| 500/534 [05:09<00:20,  1.62it/s, loss=0.22, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.84it/s][A
Validating:  93%|█████████▎| 310/334 [00:27<00:02, 11.87it/s][AEpoch 68:  97%|█████████▋| 520/534 [05:10<00:08,  1.68it/s, loss=0.22, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.94it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 13.07it/s][Avalidation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9815125465393066
Epoch 68: 100%|██████████| 534/534 [05:18<00:00,  1.68it/s, loss=0.22, v_num=661]
                                                             [AEpoch 68:   0%|          | 0/534 [00:00<00:00, 6786.90it/s, loss=0.22, v_num=661]Epoch 69:   0%|          | 0/534 [00:00<00:00, 2369.66it/s, loss=0.22, v_num=661]Epoch 69:   0%|          | 0/534 [00:19<2:56:01, 19.78s/it, loss=0.22, v_num=661]Epoch 69:   2%|▏         | 10/534 [00:26<20:39,  2.36s/it, loss=0.22, v_num=661] Epoch 69:   2%|▏         | 10/534 [00:26<20:39,  2.36s/it, loss=0.22, v_num=661]Epoch 69:   4%|▎         | 20/534 [00:38<15:40,  1.83s/it, loss=0.22, v_num=661]Epoch 69:   4%|▎         | 20/534 [00:38<15:40,  1.83s/it, loss=0.215, v_num=661]Epoch 69:   6%|▌         | 30/534 [00:53<14:28,  1.72s/it, loss=0.215, v_num=661]Epoch 69:   6%|▌         | 30/534 [00:53<14:28,  1.72s/it, loss=0.218, v_num=661]Epoch 69:   7%|▋         | 40/534 [01:08<13:42,  1.67s/it, loss=0.218, v_num=661]Epoch 69:   7%|▋         | 40/534 [01:08<13:42,  1.67s/it, loss=0.214, v_num=661]Epoch 69:   9%|▉         | 50/534 [01:19<12:37,  1.57s/it, loss=0.214, v_num=661]Epoch 69:   9%|▉         | 50/534 [01:19<12:37,  1.57s/it, loss=0.204, v_num=661]Epoch 69:  11%|█         | 60/534 [01:35<12:23,  1.57s/it, loss=0.204, v_num=661]Epoch 69:  11%|█         | 60/534 [01:35<12:23,  1.57s/it, loss=0.208, v_num=661]Epoch 69:  13%|█▎        | 70/534 [01:47<11:44,  1.52s/it, loss=0.208, v_num=661]Epoch 69:  13%|█▎        | 70/534 [01:47<11:44,  1.52s/it, loss=0.21, v_num=661] Epoch 69:  15%|█▍        | 80/534 [01:58<11:06,  1.47s/it, loss=0.21, v_num=661]Epoch 69:  15%|█▍        | 80/534 [01:58<11:06,  1.47s/it, loss=0.206, v_num=661]Epoch 69:  17%|█▋        | 90/534 [02:17<11:12,  1.51s/it, loss=0.206, v_num=661]Epoch 69:  17%|█▋        | 90/534 [02:17<11:12,  1.51s/it, loss=0.211, v_num=661]Epoch 69:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.211, v_num=661]Epoch 69:  19%|█▊        | 100/534 [02:38<11:19,  1.57s/it, loss=0.217, v_num=661]Epoch 69:  21%|██        | 110/534 [02:50<10:52,  1.54s/it, loss=0.217, v_num=661]Epoch 69:  21%|██        | 110/534 [02:50<10:52,  1.54s/it, loss=0.222, v_num=661]Epoch 69:  22%|██▏       | 120/534 [03:01<10:21,  1.50s/it, loss=0.222, v_num=661]Epoch 69:  22%|██▏       | 120/534 [03:01<10:21,  1.50s/it, loss=0.222, v_num=661]Epoch 69:  24%|██▍       | 130/534 [03:14<10:01,  1.49s/it, loss=0.222, v_num=661]Epoch 69:  24%|██▍       | 130/534 [03:14<10:01,  1.49s/it, loss=0.218, v_num=661]Epoch 69:  26%|██▌       | 140/534 [03:32<09:54,  1.51s/it, loss=0.218, v_num=661]Epoch 69:  26%|██▌       | 140/534 [03:32<09:54,  1.51s/it, loss=0.22, v_num=661] Epoch 69:  28%|██▊       | 150/534 [03:45<09:32,  1.49s/it, loss=0.22, v_num=661]Epoch 69:  28%|██▊       | 150/534 [03:45<09:32,  1.49s/it, loss=0.21, v_num=661]Epoch 69:  30%|██▉       | 160/534 [03:58<09:13,  1.48s/it, loss=0.21, v_num=661]Epoch 69:  30%|██▉       | 160/534 [03:58<09:13,  1.48s/it, loss=0.209, v_num=661]Epoch 69:  32%|███▏      | 170/534 [04:10<08:53,  1.46s/it, loss=0.209, v_num=661]Epoch 69:  32%|███▏      | 170/534 [04:10<08:53,  1.46s/it, loss=0.213, v_num=661]Epoch 69:  34%|███▎      | 180/534 [04:23<08:34,  1.45s/it, loss=0.213, v_num=661]Epoch 69:  34%|███▎      | 180/534 [04:23<08:34,  1.45s/it, loss=0.218, v_num=661]validation_epoch_end
graph acc: 0.5179640718562875
valid accuracy: 0.9816282987594604
validation_epoch_end
graph acc: 0.47904191616766467
valid accuracy: 0.9829188585281372
validation_epoch_end
graph acc: 0.46706586826347307
valid accuracy: 0.9803338646888733
validation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9824365377426147
=0.222, v_num=661]validation_epoch_end
graph acc: 0.5059880239520959
valid accuracy: 0.9826602935791016

Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/334 [00:00<?, ?it/s][Avalidation_epoch_end
graph acc: 0.4820359281437126
valid accuracy: 0.9824785590171814
validation_epoch_end
graph acc: 0.45808383233532934
valid accuracy: 0.9825023412704468
validation_epoch_end
graph acc: 0.5329341317365269
valid accuracy: 0.983885645866394
validation_epoch_end
graph acc: 0.47305389221556887
valid accuracy: 0.9835805892944336
validation_epoch_end
graph acc: 0.5299401197604791
valid accuracy: 0.9827753305435181

Validating:   3%|▎         | 10/334 [00:01<00:56,  5.71it/s][AEpoch 69:  41%|████      | 220/534 [04:35<06:30,  1.24s/it, loss=0.222, v_num=661]
Validating:   6%|▌         | 20/334 [00:02<00:42,  7.36it/s][A
Validating:   9%|▉         | 30/334 [00:03<00:33,  8.95it/s][AEpoch 69:  45%|████▍     | 240/534 [04:36<05:37,  1.15s/it, loss=0.222, v_num=661]
Validating:  12%|█▏        | 40/334 [00:04<00:33,  8.72it/s][A
Validating:  15%|█▍        | 50/334 [00:05<00:28, 10.09it/s][AEpoch 69:  49%|████▊     | 260/534 [04:38<04:52,  1.07s/it, loss=0.222, v_num=661]
Validating:  18%|█▊        | 60/334 [00:06<00:23, 11.84it/s][A
Validating:  21%|██        | 70/334 [00:08<00:33,  7.90it/s][AEpoch 69:  52%|█████▏    | 280/534 [04:41<04:14,  1.00s/it, loss=0.222, v_num=661]
Validating:  24%|██▍       | 80/334 [00:08<00:25, 10.07it/s][A
Validating:  27%|██▋       | 90/334 [00:09<00:23, 10.50it/s][AEpoch 69:  56%|█████▌    | 300/534 [04:42<03:39,  1.06it/s, loss=0.222, v_num=661]
Validating:  30%|██▉       | 100/334 [00:10<00:22, 10.26it/s][A
Validating:  33%|███▎      | 110/334 [00:11<00:22, 10.18it/s][AEpoch 69:  60%|█████▉    | 320/534 [04:44<03:09,  1.13it/s, loss=0.222, v_num=661]
Validating:  36%|███▌      | 120/334 [00:12<00:17, 12.04it/s][A
Validating:  39%|███▉      | 130/334 [00:13<00:22,  9.17it/s][AEpoch 69:  64%|██████▎   | 340/534 [04:46<02:43,  1.19it/s, loss=0.222, v_num=661]
Validating:  42%|████▏     | 140/334 [00:14<00:18, 10.58it/s][A
Validating:  45%|████▍     | 150/334 [00:14<00:15, 12.22it/s][AEpoch 69:  67%|██████▋   | 360/534 [04:48<02:18,  1.25it/s, loss=0.222, v_num=661]
Validating:  48%|████▊     | 160/334 [00:16<00:16, 10.87it/s][A
Validating:  51%|█████     | 170/334 [00:16<00:13, 11.91it/s][AEpoch 69:  71%|███████   | 380/534 [04:49<01:57,  1.31it/s, loss=0.222, v_num=661]
Validating:  54%|█████▍    | 180/334 [00:17<00:14, 10.59it/s][A
Validating:  57%|█████▋    | 190/334 [00:18<00:12, 11.87it/s][AEpoch 69:  75%|███████▍  | 400/534 [04:51<01:37,  1.37it/s, loss=0.222, v_num=661]
Validating:  60%|█████▉    | 200/334 [00:19<00:10, 12.65it/s][A
Validating:  63%|██████▎   | 210/334 [00:20<00:10, 11.60it/s][AEpoch 69:  79%|███████▊  | 420/534 [04:53<01:19,  1.43it/s, loss=0.222, v_num=661]
Validating:  66%|██████▌   | 220/334 [00:20<00:09, 12.03it/s][A
Validating:  69%|██████▉   | 230/334 [00:21<00:07, 13.11it/s][AEpoch 69:  82%|████████▏ | 440/534 [04:54<01:02,  1.50it/s, loss=0.222, v_num=661]
Validating:  72%|███████▏  | 240/334 [00:22<00:06, 13.84it/s][A
Validating:  75%|███████▍  | 250/334 [00:23<00:07, 11.49it/s][AEpoch 69:  86%|████████▌ | 460/534 [04:56<00:47,  1.55it/s, loss=0.222, v_num=661]
Validating:  78%|███████▊  | 260/334 [00:24<00:05, 12.34it/s][A
Validating:  81%|████████  | 270/334 [00:24<00:04, 13.43it/s][AEpoch 69:  90%|████████▉ | 480/534 [04:57<00:33,  1.61it/s, loss=0.222, v_num=661]
Validating:  84%|████████▍ | 280/334 [00:25<00:04, 11.26it/s][A
Validating:  87%|████████▋ | 290/334 [00:26<00:03, 12.73it/s][AEpoch 69:  94%|█████████▎| 500/534 [04:59<00:20,  1.67it/s, loss=0.222, v_num=661]
Validating:  90%|████████▉ | 300/334 [00:26<00:02, 14.49it/s][A
Validating:  93%|█████████▎| 310/334 [00:28<00:01, 12.07it/s][AEpoch 69:  97%|█████████▋| 520/534 [05:01<00:08,  1.73it/s, loss=0.222, v_num=661]
Validating:  96%|█████████▌| 320/334 [00:28<00:01, 13.61it/s][A
Validating:  99%|█████████▉| 330/334 [00:29<00:00, 12.75it/s][Avalidation_epoch_end
graph acc: 0.46407185628742514
valid accuracy: 0.9812729954719543
Epoch 69: 100%|██████████| 534/534 [05:08<00:00,  1.73it/s, loss=0.222, v_num=661]
                                                             [AEpoch 69:   0%|          | 0/534 [00:00<00:00, 14820.86it/s, loss=0.222, v_num=661]Epoch 70:   0%|          | 0/534 [00:00<00:00, 3374.34it/s, loss=0.222, v_num=661] Epoch 70:   0%|          | 0/534 [00:19<2:57:57, 20.00s/it, loss=0.222, v_num=661]Epoch 70:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.222, v_num=661] Epoch 70:   2%|▏         | 10/534 [00:24<19:24,  2.22s/it, loss=0.22, v_num=661] Epoch 70:   4%|▎         | 20/534 [00:41<16:44,  1.95s/it, loss=0.22, v_num=661]Epoch 70:   4%|▎         | 20/534 [00:41<16:44,  1.95s/it, loss=0.21, v_num=661]Epoch 70:   6%|▌         | 30/534 [00:52<14:12,  1.69s/it, loss=0.21, v_num=661]Epoch 70:   6%|▌         | 30/534 [00:52<14:12,  1.69s/it, loss=0.21, v_num=661]Epoch 70:   7%|▋         | 40/534 [01:05<13:15,  1.61s/it, loss=0.21, v_num=661]Epoch 70:   7%|▋         | 40/534 [01:05<13:15,  1.61s/it, loss=0.206, v_num=661]Epoch 70:   9%|▉         | 50/534 [01:22<13:02,  1.62s/it, loss=0.206, v_num=661]Epoch 70:   9%|▉         | 50/534 [01:22<13:02,  1.62s/it, loss=0.204, v_num=661]Epoch 70:  11%|█         | 60/534 [01:36<12:32,  1.59s/it, loss=0.204, v_num=661]Epoch 70:  11%|█         | 60/534 [01:36<12:32,  1.59s/it, loss=0.204, v_num=661]Epoch 70:  13%|█▎        | 70/534 [01:52<12:14,  1.58s/it, loss=0.204, v_num=661]Epoch 70:  13%|█▎        | 70/534 [01:52<12:14,  1.58s/it, loss=0.204, v_num=661]Epoch 70:  15%|█▍        | 80/534 [02:07<11:53,  1.57s/it, loss=0.204, v_num=661]Epoch 70:  15%|█▍        | 80/534 [02:07<11:53,  1.57s/it, loss=0.198, v_num=661]Epoch 70:  17%|█▋        | 90/534 [02:22<11:34,  1.56s/it, loss=0.198, v_num=661]Epoch 70:  17%|█▋        | 90/534 [02:22<11:34,  1.56s/it, loss=0.206, v_num=661]Epoch 70:  19%|█▊        | 100/534 [02:39<11:23,  1.58s/it, loss=0.206, v_num=661]Epoch 70:  19%|█▊        | 100/534 [02:39<11:23,  1.58s/it, loss=0.212, v_num=661]Epoch 70:  21%|██        | 110/534 [02:51<10:56,  1.55s/it, loss=0.212, v_num=661]Epoch 70:  21%|██        | 110/534 [02:51<10:56,  1.55s/it, loss=0.218, v_num=661]Epoch 70:  22%|██▏       | 120/534 [03:06<10:36,  1.54s/it, loss=0.218, v_num=661]Epoch 70:  22%|██▏       | 120/534 [03:06<10:36,  1.54s/it, loss=0.216, v_num=661]Epoch 70:  24%|██▍       | 130/534 [03:19<10:15,  1.52s/it, loss=0.216, v_num=661]Epoch 70:  24%|██▍       | 130/534 [03:19<10:15,  1.52s/it, loss=0.213, v_num=661]Epoch 70:  26%|██▌       | 140/534 [03:37<10:06,  1.54s/it, loss=0.213, v_num=661]Epoch 70:  26%|██▌       | 140/534 [03:37<10:07,  1.54s/it, loss=0.215, v_num=661]Epoch 70:  28%|██▊       | 150/534 [03:50<09:46,  1.53s/it, loss=0.215, v_num=661]Epoch 70:  28%|██▊       | 150/534 [03:50<09:46,  1.53s/it, loss=0.203, v_num=661]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f87c034f3d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f8801453170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f6504544510>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f654cea7170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdb28487b50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fdb72710170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9932432770729065
len(train_dataloader) 1286
validatiovalidation_epoch_end
graph acc: 0.5
valid accuracy: 0.9849624037742615
                                                              len(train_dataloader) 1286
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 1286
Training:   0%|          | 0/420 [00:00<00:00, 16777.22it/s]Epoch 60:   0%|          | 0/420 [00:00<00:00, 3795.75it/s] validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9830508232116699
len(train_dataloader) 1286
y_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f858ae22f10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f85d5897170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb43da367d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb4884a8170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=1024, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=12, hidden_dim=256, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=15, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7faf71bcf850>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fafbc638170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 256, padding_idx=0)
  (edge_encoder): Embedding(769, 12, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 12, padding_idx=0)
  (in_degree_encoder): Embedding(512, 256, padding_idx=0)
  (out_degree_encoder): Embedding(512, 256, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(543, 256, padding_idx=0)
  (centrality_encoder): Embedding(50, 12, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=12, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=12, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=256, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=252, bias=True)
        (linear_k): Linear(in_features=256, out_features=252, bias=True)
        (linear_v): Linear(in_features=256, out_features=252, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=252, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(11, 256)
  (graph_token_virtual_distance): Embedding(1, 12)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=12, bias=True)
)
total params: 12758351
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9918032288551331
len(train_dataloader) 1286
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9960784912109375
len(train_dataloader) 1286
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.9466667175292969
len(train_dataloader) 1286
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9878048300743103
len(train_dataloader) 1286
validation_epoch_end
graph acc: 1.0
valid accuracy: 1.0
len(train_dataloader) 1286
validation_epoch_end
graph acc: 0.5
valid accuracy: 0.9835390448570251
len(train_dataloader) 1286
Epoch 60:   2%|▏         | 10/420 [00:52<32:19,  4.73s/it] Epoch 60:   2%|▏         | 10/420 [00:52<32:19,  4.73s/it, loss=0.202, v_num=662]Epoch 60:   5%|▍         | 20/420 [01:24<26:40,  4.00s/it, loss=0.202, v_num=662]Epoch 60:   5%|▍         | 20/420 [01:24<26:40,  4.00s/it, loss=0.196, v_num=662]Epoch 60:   7%|▋         | 30/420 [02:02<25:36,  3.94s/it, loss=0.196, v_num=662]Epoch 60:   7%|▋         | 30/420 [02:02<25:36,  3.94s/it, loss=0.194, v_num=662]Epoch 60:  10%|▉         | 40/420 [02:38<24:29,  3.87s/it, loss=0.194, v_num=662]Epoch 60:  10%|▉         | 40/420 [02:38<24:29,  3.87s/it, loss=0.194, v_num=662]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ff60b7eaf10>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
Converting SMILES strings into graphs...
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ff656256170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f2d74ae0b50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f2dbf4d5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdb2a3be5d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fdb74e4e170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f9fa2816a90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f9fe3919170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f0e1408b490>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0e5518a170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=6, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f22d41b68d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f231e21b170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 85662779
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f470674f7d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f47511bb170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa1275a2890>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=1.0, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa172014170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 12873
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/8940 [00:00<00:00, 32768.00it/s]Epoch 63:   0%|          | 0/8940 [00:00<00:01, 6921.29it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 12873
Epoch 63:   0%|          | 0/8940 [00:04<10:57:20,  4.41s/it]
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdc9a3be790>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fdce4e30170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fa2ec406a90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fa336e78170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 12873
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/3227798 [00:00<02:19, 23172.95it/s]Epoch 63:   0%|          | 0/3227798 [00:00<10:53, 4940.29it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 12873
Epoch 63:   0%|          | 0/3227798 [00:04<4129:19:15,  4.61s/it]
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fc5c62bd990>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fc61054d170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f12e04d3690>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f132af43170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 12873
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/3227798 [00:00<02:16, 23696.63it/s]Epoch 63:   0%|          | 0/3227798 [00:00<09:37, 5592.41it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 12873
Epoch 63:   0%|          | 0/3227798 [00:04<4180:14:25,  4.66s/it]
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fdd944c3110>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fddde6ab170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5005
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7fb99ae190d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7fb9e58a5170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(460545), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5005
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 12873
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/3227798 [00:00<02:45, 19508.39it/s]validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 12873
Epoch 63:   0%|          | 0/3227798 [00:00<12:22, 4346.43it/s] Epoch 63:   0%|          | 0/3227798 [00:04<4145:42:21,  4.62s/it]
Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f4e63431110>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f4ead6bc170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f952ef21cd0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f9570025170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5000
len(val_dataloader) 5000
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 1143
Training: -1it [00:00, ?it/s]validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 1143
Training:   0%|          | 0/285572 [00:00<00:11, 23831.27it/s]Epoch 63:   0%|          | 0/285572 [00:00<01:04, 4415.06it/s] Epoch 63:   0%|          | 10/285572 [00:29<209:28:04,  2.64s/it]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2500 [00:00<?, ?it/s][A
Validating:   0%|          | 10/2500 [00:04<19:06,  2.17it/s][AEpoch 63:   0%|          | 20/285572 [00:33<127:07:02,  1.60s/it]
Validating:   1%|          | 20/2500 [00:06<12:11,  3.39it/s][AEpoch 63:   0%|          | 30/285572 [00:35<90:41:41,  1.14s/it] 
Validating:   1%|          | 30/2500 [00:08<10:22,  3.97it/s][AEpoch 63:   0%|          | 40/285572 [00:37<72:27:22,  1.09it/s]
Validating:   2%|▏         | 40/2500 [00:11<10:27,  3.92it/s][AEpoch 63:   0%|          | 50/285572 [00:40<62:16:55,  1.27it/s]
Validating:   2%|▏         | 50/2500 [00:12<09:26,  4.32it/s][AEpoch 63:   0%|          | 60/285572 [00:41<54:32:05,  1.45it/s]
Validating:   2%|▏         | 60/2500 [00:14<08:01,  5.06it/s][AEpoch 63:   0%|          | 70/285572 [00:43<48:19:26,  1.64it/s]
Validating:   3%|▎         | 70/2500 [00:15<06:57,  5.81it/s][AEpoch 63:   0%|          | 80/285572 [00:44<43:31:33,  1.82it/s]
Validating:   3%|▎         | 80/2500 [00:17<06:47,  5.94it/s][AEpoch 63:   0%|          | 90/285572 [00:46<40:08:25,  1.98it/s]
Validating:   4%|▎         | 90/2500 [00:21<10:32,  3.81it/s][AEpoch 63:   0%|          | 100/285572 [00:50<39:51:04,  1.99it/s]
Validating:   4%|▍         | 100/2500 [00:23<09:50,  4.06it/s][AEpoch 63:   0%|          | 110/285572 [00:52<37:45:23,  2.10it/s]
Validating:   4%|▍         | 110/2500 [00:25<08:37,  4.61it/s][AEpoch 63:   0%|          | 120/285572 [00:54<35:37:05,  2.23it/s]
Validating:   5%|▍         | 120/2500 [00:27<08:37,  4.60it/s][AEpoch 63:   0%|          | 130/285572 [00:56<34:13:23,  2.32it/s]
Validating:   5%|▌         | 130/2500 [00:29<07:53,  5.00it/s][AEpoch 63:   0%|          | 140/285572 [00:58<32:41:34,  2.43it/s]Epoch 63:   0%|          | 140/285572 [01:02<35:13:46,  2.25it/s]

                                                              [ANamespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f03cb7a2490>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f0416223170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5000
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ef92b238210>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ef96d992170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 1143
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/285572 [00:00<00:12, 22671.91it/s]Epoch 63:   0%|          | 0/285572 [00:00<00:55, 5108.77it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 1143
Epoch 63:   0%|          | 10/285572 [00:29<209:28:37,  2.64s/it]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2500 [00:00<?, ?it/s][A
Validating:   0%|          | 10/2500 [00:04<18:39,  2.22it/s][AEpoch 63:   0%|          | 20/285572 [00:33<126:42:49,  1.60s/it]
Validating:   1%|          | 20/2500 [00:06<12:15,  3.37it/s][AEpoch 63:   0%|          | 30/285572 [00:35<90:40:32,  1.14s/it] 
Validating:   1%|          | 30/2500 [00:08<10:33,  3.90it/s][AEpoch 63:   0%|          | 40/285572 [00:37<72:35:50,  1.09it/s]
Validating:   2%|▏         | 40/2500 [00:10<09:57,  4.12it/s][AEpoch 63:   0%|          | 50/285572 [00:39<61:48:34,  1.28it/s]
Validating:   2%|▏         | 50/2500 [00:12<09:08,  4.47it/s][AEpoch 63:   0%|          | 60/285572 [00:41<54:08:46,  1.46it/s]
Validating:   2%|▏         | 60/2500 [00:14<08:12,  4.96it/s][AEpoch 63:   0%|          | 70/285572 [00:43<48:17:39,  1.64it/s]
Validating:   3%|▎         | 70/2500 [00:15<07:14,  5.59it/s][AEpoch 63:   0%|          | 80/285572 [00:44<43:37:09,  1.82it/s]
Validating:   3%|▎         | 80/2500 [00:17<07:04,  5.70it/s][AEpoch 63:   0%|          | 90/285572 [00:46<40:17:32,  1.97it/s]
Validating:   4%|▎         | 90/2500 [00:21<09:52,  4.07it/s][AEpoch 63:   0%|          | 100/285572 [00:50<39:26:57,  2.01it/s]
Validating:   4%|▍         | 100/2500 [00:23<09:38,  4.15it/s][AEpoch 63:   0%|          | 110/285572 [00:52<37:32:00,  2.11it/s]
Validating:   4%|▍         | 110/2500 [00:24<08:07,  4.90it/s][AEpoch 63:   0%|          | 120/285572 [00:53<35:13:11,  2.25it/s]
Validating:   5%|▍         | 120/2500 [00:26<08:24,  4.72it/s][AEpoch 63:   0%|          | 130/285572 [00:56<33:55:21,  2.34it/s]
Validating:   5%|▌         | 130/2500 [00:28<07:44,  5.10it/s][AEpoch 63:   0%|          | 140/285572 [00:57<32:24:49,  2.45it/s]
Validating:   6%|▌         | 140/2500 [00:30<07:31,  5.22it/s][AEpoch 63:   0%|          | 150/285572 [00:59<31:12:49,  2.54it/s]Epoch 63:   0%|          | 150/285572 [01:02<32:39:29,  2.43it/s]

                                                              [ANamespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f9e609fc4d0>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f9eaaa70170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5000
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f851264ef90>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f855c840170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 1143
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/285572 [00:00<00:11, 25266.89it/s]Epoch 63:   0%|          | 0/285572 [00:00<00:48, 5915.80it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 1143
Epoch 63:   0%|          | 10/285572 [00:31<225:49:24,  2.85s/it]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2500 [00:00<?, ?it/s][A
Validating:   0%|          | 10/2500 [00:04<20:37,  2.01it/s][AEpoch 63:   0%|          | 20/285572 [00:36<137:03:59,  1.73s/it]
Validating:   1%|          | 20/2500 [00:07<13:34,  3.04it/s][AEpoch 63:   0%|          | 30/285572 [00:38<98:14:07,  1.24s/it] 
Validating:   1%|          | 30/2500 [00:09<11:30,  3.58it/s][AEpoch 63:   0%|          | 40/285572 [00:40<78:33:10,  1.01it/s]
Validating:   2%|▏         | 40/2500 [00:11<10:00,  4.10it/s][AEpoch 63:   0%|          | 50/285572 [00:42<66:05:46,  1.20it/s]
Validating:   2%|▏         | 50/2500 [00:12<08:32,  4.78it/s][AEpoch 63:   0%|          | 60/285572 [00:43<57:10:33,  1.39it/s]Epoch 63:   0%|          | 60/285572 [00:48<62:47:57,  1.26it/s]

                                                             [ANamespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7ef881738f50>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7ef8cc1ae170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
Validation sanity check: 0it [00:00, ?it/s]len(val_dataloader) 5000
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Namespace(accelerator='ddp', accumulate_grad_batches=5, amp_backend='native', amp_level='O2', attention_dropout_rate=0.1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=35, beam=1, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='', dataset_name='uspto', default_root_dir='/home/jovyan/g2gt_github/src', deterministic=False, devices=None, distributed_backend=None, dropout_rate=0.1, edge_type='one_hop', end_lr=1e-06, fast_dev_run=False, ffn_dim=2048, flag=False, flag_m=3, flag_mag=0.001, flag_step_size=0.001, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=4.0, head_size=24, hidden_dim=768, intput_dropout_rate=0.05, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_steps=700001, max_time=None, min_epochs=100, min_steps=None, move_metrics_to_cpu=False, multi_hop_max_dist=5, multiple_trainloader_mode='max_size_cycle', n_layers=8, num_nodes=2, num_processes=1, num_sanity_val_steps=2, num_workers=5, overfit_batches=0.0, peak_lr=0.00025, plugins=<pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7f53744ba750>, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=10, rel_pos_max=1024, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=0, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test=False, tot_updates=700000, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, val_check_interval=5, validate=False, warmup_updates=3000, weight_decay=0.0, weights_save_path=None, weights_summary='top')
 > uspto loaded!
{'num_class': 1, 'loss_fn': <function cross_entropy at 0x7f53be6c1170>, 'metric': 'train_loss', 'metric_mode': 'min', 'evaluator': 'none', 'dataset': UsptoDataset(50000), 'max_node': 420}
 > dataset info ends
GraphFormer(
  (atom_encoder): Embedding(4737, 768, padding_idx=0)
  (edge_encoder): Embedding(769, 24, padding_idx=0)
  (rel_pos_encoder): Embedding(512, 24, padding_idx=0)
  (in_degree_encoder): Embedding(512, 768, padding_idx=0)
  (out_degree_encoder): Embedding(512, 768, padding_idx=0)
  (input_dropout): Dropout(p=0.05, inplace=False)
  (input_dropout2d): Dropout2d(p=0.05, inplace=False)
  (gelu): GELU()
  (atom_edge_encoder): Embedding(552, 768, padding_idx=0)
  (centrality_encoder): Embedding(50, 24, padding_idx=0)
  (lpe_linear): Linear(in_features=2, out_features=24, bias=True)
  (lpe_linear3): Linear(in_features=30, out_features=24, bias=True)
  (position): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (outp_logits): Linear(in_features=768, out_features=531, bias=True)
  (decoderLayers): ModuleList(
    (0): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): DecoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mask_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (mem_att_sublayer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_mem_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_mem_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (5): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (6): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (7): EncoderLayer(
      (self_attention_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=768, out_features=768, bias=True)
        (linear_k): Linear(in_features=768, out_features=768, bias=True)
        (linear_v): Linear(in_features=768, out_features=768, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=768, out_features=768, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=768, out_features=2048, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=2048, out_features=768, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (graph_token): Embedding(20, 768)
  (graph_token_virtual_distance): Embedding(1, 24)
  (rbf): RBFLayer()
  (rel_pos_3d_proj): Linear(in_features=256, out_features=24, bias=True)
)
total params: 112446523
args.resume_from_checkpoint /home/jovyan/g2gt_github/src/lightning_logs/checkpoints/last.ckpt
len(val_dataloader) 5000
validation_epoch_end
graph acc: 0.0
valid accuracy: 0.39263802766799927
                                                              len(train_dataloader) 1143
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/285572 [00:00<00:11, 24105.20it/s]Epoch 63:   0%|          | 0/285572 [00:00<00:47, 5966.29it/s] validation_epoch_end
graph acc: 0.0
valid accuracy: 0.3804347813129425
len(train_dataloader) 1143
Epoch 63:   0%|          | 10/285572 [00:32<233:00:56,  2.94s/it]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2500 [00:00<?, ?it/s][A
Validating:   0%|          | 10/2500 [00:05<21:08,  1.96it/s][AEpoch 63:   0%|          | 20/285572 [00:37<141:17:37,  1.78s/it]
Validating:   1%|          | 20/2500 [00:07<13:19,  3.10it/s][AEpoch 63:   0%|          | 30/285572 [00:39<100:36:14,  1.27s/it]
Validating:   1%|          | 30/2500 [00:08<10:21,  3.98it/s][AEpoch 63:   0%|          | 40/285572 [00:40<79:18:04,  1.00it/s] 